URL: https://doc.akka.io/libraries/akka/snapshot/general/configuration-reference.html
Default configuration • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




akka-actor


akka-actor-typed


akka-cluster-typed


akka-cluster


akka-discovery


akka-coordination


akka-multi-node-testkit


akka-persistence-typed


akka-persistence


akka-persistence-query


akka-persistence-testkit


akka-remote artery


akka-testkit


akka-cluster-metrics


akka-cluster-tools


akka-cluster-sharding-typed


akka-cluster-sharding


akka-distributed-data


akka-stream


akka-stream-testkit






Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




akka-actor


akka-actor-typed


akka-cluster-typed


akka-cluster


akka-discovery


akka-coordination


akka-multi-node-testkit


akka-persistence-typed


akka-persistence


akka-persistence-query


akka-persistence-testkit


akka-remote artery


akka-testkit


akka-cluster-metrics


akka-cluster-tools


akka-cluster-sharding-typed


akka-cluster-sharding


akka-distributed-data


akka-stream


akka-stream-testkit






Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Default configuration


Each Akka module has a 
reference.conf
 file with the default values.


Make your edits/overrides in your 
application.conf
. Don’t override default values if you are not sure of the implications. 
Akka Config Checker
 is a useful tool for finding potential configuration issues.


The purpose of 
reference.conf
 files is for libraries, like Akka, to define default values that are used if an application doesn’t define a more specific value. It’s also a good place to document the existence and meaning of the configuration properties. One library must not try to override properties in its own 
reference.conf
 for properties originally defined by another library’s 
reference.conf
, because the effective value would be nondeterministic when loading the configuration.`




akka-actor


copy
source
####################################
# Akka Actor Reference Config File #
####################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# Akka version, checked against the runtime version of Akka. Loaded from generated conf file.
include "version"

akka {
  # Home directory of Akka, modules in the deploy directory will be loaded
  home = ""

  # The license key to use Akka. Free keys at https://akka.io/key
  license-key = ""

  # If false, will not warn if there is no Akka license. Note, this only
  # suppresses the log messages, it does not stop the actor system from
  # terminating.
  warn-on-no-license-key = true

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.Logging$DefaultLogger"]

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream. It can perform
  # fine grained filtering based on the log source. The default
  # implementation filters on the `loglevel`.
  # FQCN of the LoggingFilter. The Class of the FQCN must implement
  # akka.event.LoggingFilter and have a public constructor with
  # (akka.actor.ActorSystem.Settings, akka.event.EventStream) parameters.
  logging-filter = "akka.event.DefaultLoggingFilter"

  # Specifies the default loggers dispatcher
  loggers-dispatcher = "akka.actor.default-dispatcher"

  # Loggers are created and registered synchronously during ActorSystem
  # start-up, and since they are actors, this timeout is used to bound the
  # waiting time
  logger-startup-timeout = 5s

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "INFO"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "WARNING"

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # Log at info level when messages are sent to dead letters, or published to
  # eventStream as `DeadLetter`, `Dropped` or `UnhandledMessage`.
  # Possible values:
  # on: all dead letters are logged
  # off: no logging of dead letters
  # n: positive integer, number of dead letters that will be logged
  log-dead-letters = 10

  # Possibility to turn off logging of dead letters while the actor system
  # is shutting down. Logging is only done when enabled by 'log-dead-letters'
  # setting.
  log-dead-letters-during-shutdown = off

  # When log-dead-letters is enabled, this will re-enable the logging after configured duration.
  # infinite: suspend the logging forever;
  # or a duration (eg: 5 minutes), after which the logging will be re-enabled.
  log-dead-letters-suspend-duration = 5 minutes

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Library extensions are regular extensions that are loaded at startup and are
  # available for third party library authors to enable auto-loading of extensions when
  # present on the classpath. This is done by appending entries:
  # 'library-extensions += "Extension"' in the library `reference.conf`.
  #
  # Should not be set by end user applications in 'application.conf', use the extensions property for that
  #
  library-extensions = ${?akka.library-extensions} ["akka.serialization.SerializationExtension$"]

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Should be on the format: 'extensions = ["foo", "bar"]' etc.
  # See the Akka Documentation for more info about Extensions
  extensions = []

  # Toggles whether threads created by this ActorSystem should be daemons or not
  daemonic = off

  # JVM shutdown, System.exit(-1), in case of a fatal error,
  # such as OutOfMemoryError
  jvm-exit-on-fatal-error = on

  # Akka installs JVM shutdown hooks by default, e.g. in CoordinatedShutdown and Artery. This property will
  # not disable user-provided hooks registered using `CoordinatedShutdown#addCancellableJvmShutdownHook`.
  # This property is related to `akka.coordinated-shutdown.run-by-jvm-shutdown-hook` below.
  # This property makes it possible to disable all such hooks if the application itself
  # or a higher level framework such as Play prefers to install the JVM shutdown hook and
  # terminate the ActorSystem itself, with or without using CoordinatedShutdown.
  jvm-shutdown-hooks = on

  # Version must be the same across all modules and if they are different the startup
  # will fail. It's possible but not recommended, to disable this check, and only log a warning,
  # by setting this property to `off`.
  fail-mixed-versions = on

  actor {

    # Either one of "local", "remote" or "cluster" or the
    # FQCN of the ActorRefProvider to be used; the below is the built-in default,
    # note that "remote" and "cluster" requires the akka-remote and akka-cluster
    # artifacts to be on the classpath.
    provider = "local"

    # The guardian "/user" will use this class to obtain its supervisorStrategy.
    # It needs to be a subclass of akka.actor.SupervisorStrategyConfigurator.
    # In addition to the default there is akka.actor.StoppingSupervisorStrategy.
    guardian-supervisor-strategy = "akka.actor.DefaultSupervisorStrategy"

    # Timeout for Extension creation and a few other potentially blocking
    # initialization tasks.
    creation-timeout = 20s

    # Serializes and deserializes (non-primitive) messages to ensure immutability,
    # this is only intended for testing.
    serialize-messages = off

    # Serializes and deserializes creators (in Props) to ensure that they can be
    # sent over the network, this is only intended for testing. Purely local deployments
    # as marked with deploy.scope == LocalScope are exempt from verification.
    serialize-creators = off

    # If serialize-messages or serialize-creators are enabled classes that starts with
    # a prefix listed here are not verified.
    no-serialization-verification-needed-class-prefix = ["akka."]

    # Timeout for send operations to top-level actors which are in the process
    # of being started. This is only relevant if using a bounded mailbox or the
    # CallingThreadDispatcher for a top-level actor.
    unstarted-push-timeout = 10s

    # Mapping between Â´deployment.router' short names to fully qualified class names
    router.type-mapping {
      from-code = "akka.routing.NoRouter"
      round-robin-pool = "akka.routing.RoundRobinPool"
      round-robin-group = "akka.routing.RoundRobinGroup"
      random-pool = "akka.routing.RandomPool"
      random-group = "akka.routing.RandomGroup"
      balancing-pool = "akka.routing.BalancingPool"
      smallest-mailbox-pool = "akka.routing.SmallestMailboxPool"
      broadcast-pool = "akka.routing.BroadcastPool"
      broadcast-group = "akka.routing.BroadcastGroup"
      scatter-gather-pool = "akka.routing.ScatterGatherFirstCompletedPool"
      scatter-gather-group = "akka.routing.ScatterGatherFirstCompletedGroup"
      tail-chopping-pool = "akka.routing.TailChoppingPool"
      tail-chopping-group = "akka.routing.TailChoppingGroup"
      consistent-hashing-pool = "akka.routing.ConsistentHashingPool"
      consistent-hashing-group = "akka.routing.ConsistentHashingGroup"
    }

    deployment {

      # deployment id pattern - on the format: /parent/child etc.
      default {

        # The id of the dispatcher to use for this actor.
        # If undefined or empty the dispatcher specified in code
        # (Props.withDispatcher) is used, or default-dispatcher if not
        # specified at all.
        dispatcher = ""

        # The id of the mailbox to use for this actor.
        # If undefined or empty the default mailbox of the configured dispatcher
        # is used or if there is no mailbox configuration the mailbox specified
        # in code (Props.withMailbox) is used.
        # If there is a mailbox defined in the configured dispatcher then that
        # overrides this setting.
        mailbox = ""

        # routing (load-balance) scheme to use
        # - available: "from-code", "round-robin", "random", "smallest-mailbox",
        #              "scatter-gather", "broadcast"
        # - or:        Fully qualified class name of the router class.
        #              The class must extend akka.routing.CustomRouterConfig and
        #              have a public constructor with com.typesafe.config.Config
        #              and optional akka.actor.DynamicAccess parameter.
        # - default is "from-code";
        # Whether or not an actor is transformed to a Router is decided in code
        # only (Props.withRouter). The type of router can be overridden in the
        # configuration; specifying "from-code" means that the values specified
        # in the code shall be used.
        # In case of routing, the actors to be routed to can be specified
        # in several ways:
        # - nr-of-instances: will create that many children
        # - routees.paths: will route messages to these paths using ActorSelection,
        #   i.e. will not create children
        # - resizer: dynamically resizable number of routees as specified in
        #   resizer below
        router = "from-code"

        # number of children to create in case of a router;
        # this setting is ignored if routees.paths is given
        nr-of-instances = 1

        # within is the timeout used for routers containing future calls
        within = 5 seconds

        # number of virtual nodes per node for consistent-hashing router
        virtual-nodes-factor = 10

        tail-chopping-router {
          # interval is duration between sending message to next routee
          interval = 10 milliseconds
        }

        routees {
          # Alternatively to giving nr-of-instances you can specify the full
          # paths of those actors which should be routed to. This setting takes
          # precedence over nr-of-instances
          paths = []
        }

        # To use a dedicated dispatcher for the routees of the pool you can
        # define the dispatcher configuration inline with the property name
        # 'pool-dispatcher' in the deployment section of the router.
        # For example:
        # pool-dispatcher {
        #   fork-join-executor.parallelism-min = 5
        #   fork-join-executor.parallelism-max = 5
        # }

        # Routers with dynamically resizable number of routees; this feature is
        # enabled by including (parts of) this section in the deployment
        resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # Threshold used to evaluate if a routee is considered to be busy
          # (under pressure). Implementation depends on this value (default is 1).
          # 0:   number of routees currently processing a message.
          # 1:   number of routees currently processing a message has
          #      some messages in mailbox.
          # > 1: number of routees with at least the configured pressure-threshold
          #      messages in their mailbox. Note that estimating mailbox size of
          #      default UnboundedMailbox is O(N) operation.
          pressure-threshold = 1

          # Percentage to increase capacity whenever all routees are busy.
          # For example, 0.2 would increase 20% (rounded up), i.e. if current
          # capacity is 6 it will request an increase of 2 more routees.
          rampup-rate = 0.2

          # Minimum fraction of busy routees before backing off.
          # For example, if this is 0.3, then we'll remove some routees only when
          # less than 30% of routees are busy, i.e. if current capacity is 10 and
          # 3 are busy then the capacity is unchanged, but if 2 or less are busy
          # the capacity is decreased.
          # Use 0.0 or negative to avoid removal of routees.
          backoff-threshold = 0.3

          # Fraction of routees to be removed when the resizer reaches the
          # backoffThreshold.
          # For example, 0.1 would decrease 10% (rounded up), i.e. if current
          # capacity is 9 it will request an decrease of 1 routee.
          backoff-rate = 0.1

          # Number of messages between resize operation.
          # Use 1 to resize before each message.
          messages-per-resize = 10
        }

        # Routers with dynamically resizable number of routees based on
        # performance metrics.
        # This feature is enabled by including (parts of) this section in
        # the deployment, cannot be enabled together with default resizer.
        optimal-size-exploring-resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # probability of doing a ramping down when all routees are busy
          # during exploration.
          chance-of-ramping-down-when-full = 0.2

          # Interval between each resize attempt
          action-interval = 5s

          # If the routees have not been fully utilized (i.e. all routees busy)
          # for such length, the resizer will downsize the pool.
          downsize-after-underutilized-for = 72h

          # Duration exploration, the ratio between the largest step size and
          # current pool size. E.g. if the current pool size is 50, and the
          # explore-step-size is 0.1, the maximum pool size change during
          # exploration will be +- 5
          explore-step-size = 0.1

          # Probability of doing an exploration v.s. optimization.
          chance-of-exploration = 0.4

          # When downsizing after a long streak of under-utilization, the resizer
          # will downsize the pool to the highest utilization multiplied by a
          # a downsize ratio. This downsize ratio determines the new pools size
          # in comparison to the highest utilization.
          # E.g. if the highest utilization is 10, and the down size ratio
          # is 0.8, the pool will be downsized to 8
          downsize-ratio = 0.8

          # When optimizing, the resizer only considers the sizes adjacent to the
          # current size. This number indicates how many adjacent sizes to consider.
          optimization-range = 16

          # The weight of the latest metric over old metrics when collecting
          # performance metrics.
          # E.g. if the last processing speed is 10 millis per message at pool
          # size 5, and if the new processing speed collected is 6 millis per
          # message at pool size 5. Given a weight of 0.3, the metrics
          # representing pool size 5 will be 6 * 0.3 + 10 * 0.7, i.e. 8.8 millis
          # Obviously, this number should be between 0 and 1.
          weight-of-latest-metric = 0.5
        }
      }

      "/IO-DNS/inet-address" {
        mailbox = "unbounded"
        router = "consistent-hashing-pool"
        nr-of-instances = 4
      }

      "/IO-DNS/inet-address/*" {
        dispatcher = "akka.actor.default-blocking-io-dispatcher"
      }

      "/IO-DNS/async-dns" {
        mailbox = "unbounded"
        router = "round-robin-pool"
        nr-of-instances = 1
      }
    }

    default-dispatcher {
      # Must be one of the following
      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
      # MessageDispatcherConfigurator with a public constructor with
      # both com.typesafe.config.Config parameter and
      # akka.dispatch.DispatcherPrerequisites parameters.
      # PinnedDispatcher must be used together with executor=thread-pool-executor.
      type = "Dispatcher"

      # Which kind of ExecutorService to use for this dispatcher
      # Valid options:
      #  - "default-executor" requires a "default-executor" section
      #  - "fork-join-executor" requires a "fork-join-executor" section
      #  - "thread-pool-executor" requires a "thread-pool-executor" section
      #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
      #  - A FQCN of a class extending ExecutorServiceConfigurator
      executor = "default-executor"

      # This will be used if you have set "executor = "default-executor"".
      # If an ActorSystem is created with a given ExecutionContext, this
      # ExecutionContext will be used as the default executor for all
      # dispatchers in the ActorSystem configured with
      # executor = "default-executor". Note that "default-executor"
      # is the default value for executor, and therefore used if not
      # specified otherwise. If no ExecutionContext is given,
      # the executor configured in "fallback" will be used.
      default-executor {
        fallback = "fork-join-executor"
      }

      # This will be used if you have set "executor = "affinity-pool-executor""
      # Underlying thread pool implementation is akka.dispatch.affinity.AffinityPool.
      # This executor is classified as "ApiMayChange".
      affinity-pool-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 0.8

        # Max number of threads to cap factor-based parallelism number to.
        parallelism-max = 64

        # Each worker in the pool uses a separate bounded MPSC queue. This value
        # indicates the upper bound of the queue. Whenever an attempt to enqueue
        # a task is made and the queue does not have capacity to accommodate
        # the task, the rejection handler created by the rejection handler specified
        # in "rejection-handler" is invoked.
        task-queue-size = 512

        # FQCN of the Rejection handler used in the pool.
        # Must have an empty public constructor and must
        # implement akka.actor.affinity.RejectionHandlerFactory.
        rejection-handler = "akka.dispatch.affinity.ThrowOnOverflowRejectionHandler"

        # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
        # The tradeoff is that to have low latency more CPU time must be used to be
        # able to react quickly on incoming messages or send as fast as possible after
        # backoff backpressure.
        # Level 1 strongly prefer low CPU consumption over low latency.
        # Level 10 strongly prefer low latency over low CPU consumption.
        idle-cpu-level = 5

        # FQCN of the akka.dispatch.affinity.QueueSelectorFactory.
        # The Class of the FQCN must have a public constructor with a
        # (com.typesafe.config.Config) parameter.
        # A QueueSelectorFactory create instances of akka.dispatch.affinity.QueueSelector,
        # that is responsible for determining which task queue a Runnable should be enqueued in.
        queue-selector = "akka.dispatch.affinity.FairDistributionHashCache"

        # When using the "akka.dispatch.affinity.FairDistributionHashCache" queue selector
        # internally the AffinityPool uses two methods to determine which task
        # queue to allocate a Runnable to:
        # - map based - maintains a round robin counter and a map of Runnable
        # hashcodes to queues that they have been associated with. This ensures
        # maximum fairness in terms of work distribution, meaning that each worker
        # will get approximately equal amount of mailboxes to execute. This is suitable
        # in cases where we have a small number of actors that will be scheduled on
        # the pool and we want to ensure the maximum possible utilization of the
        # available threads.
        # - hash based - the task - queue in which the runnable should go is determined
        # by using an uniformly distributed int to int hash function which uses the
        # hash code of the Runnable as an input. This is preferred in situations where we
        # have enough number of distinct actors to ensure statistically uniform
        # distribution of work across threads or we are ready to sacrifice the
        # former for the added benefit of avoiding map look-ups.
        fair-work-distribution {
          # The value serves as a threshold which determines the point at which the
          # pool switches from the first to the second work distribution schemes.
          # For example, if the value is set to 128, the pool can observe up to
          # 128 unique actors and schedule their mailboxes using the map based
          # approach. Once this number is reached the pool switches to hash based
          # task distribution mode. If the value is set to 0, the map based
          # work distribution approach is disabled and only the hash based is
          # used irrespective of the number of unique actors. Valid range is
          # 0 to 2048 (inclusive)
          threshold = 128
        }
      }

      # This will be used if you have set "executor = "fork-join-executor""
      # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 8

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 1.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 64

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
        # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }

      # This will be used if you have set "executor = "thread-pool-executor""
      # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
      thread-pool-executor {
        # Keep alive time for threads
        keep-alive-time = 60s

        # Define a fixed thread pool size with this property. The corePoolSize
        # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
        # value, if it is defined. Then the other pool-size properties will not
        # be used.
        #
        # Valid values are: `off` or a positive integer.
        fixed-pool-size = off

        # Min number of threads to cap factor-based corePoolSize number to
        core-pool-size-min = 8

        # The core-pool-size-factor is used to determine corePoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor).
        # Resulting size is then bounded by the core-pool-size-min and
        # core-pool-size-max values.
        core-pool-size-factor = 3.0

        # Max number of threads to cap factor-based corePoolSize number to
        core-pool-size-max = 64

        # Minimum number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-min = 8

        # The max-pool-size-factor is used to determine maximumPoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor)
        # The maximumPoolSize will not be less than corePoolSize.
        # It is only used if using a bounded task queue.
        max-pool-size-factor  = 3.0

        # Max number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-max = 64

        # Specifies the bounded capacity of the task queue (< 1 == unbounded)
        task-queue-size = -1

        # Specifies which type of task queue will be used, can be "array" or
        # "linked" (default)
        task-queue-type = "linked"

        # Allow core threads to time out
        allow-core-timeout = on
      }

      # How long time the dispatcher will wait for new actors until it shuts down
      shutdown-timeout = 1s

      # Throughput defines the number of messages that are processed in a batch
      # before the thread is returned to the pool. Set to 1 for as fair as possible.
      throughput = 5

      # Throughput deadline for Dispatcher, set to 0 or negative for no deadline
      throughput-deadline-time = 0ms

      # For BalancingDispatcher: If the balancing dispatcher should attempt to
      # schedule idle actors using the same dispatcher when a message comes in,
      # and the dispatchers ExecutorService is not fully busy already.
      attempt-teamwork = on

      # If this dispatcher requires a specific type of mailbox, specify the
      # fully-qualified class name here; the actually created mailbox will
      # be a subtype of this type. The empty string signifies no requirement.
      mailbox-requirement = ""
    }

    # Default separate internal dispatcher to run Akka internal tasks and actors on
    # protecting them against starvation because of accidental blocking in user actors (which run on the
    # default dispatcher)
    internal-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      throughput = 5
      fork-join-executor {
        parallelism-min = 4
        parallelism-factor = 1.0
        parallelism-max = 64
      }
    }

    default-blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      throughput = 1

      thread-pool-executor {
        fixed-pool-size = 16
      }
    }

    default-mailbox {
      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      mailbox-type = "akka.dispatch.UnboundedMailbox"

      # If the mailbox is bounded then it uses this setting to determine its
      # capacity. The provided value must be positive.
      # NOTICE:
      # Up to version 2.1 the mailbox type was determined based on this setting;
      # this is no longer the case, the type must explicitly be a bounded mailbox.
      mailbox-capacity = 1000

      # If the mailbox is bounded then this is the timeout for enqueueing
      # in case the mailbox is full. Negative values signify infinite
      # timeout, which should be avoided as it bears the risk of dead-lock.
      mailbox-push-timeout-time = 10s

      # For Actor with Stash: The default capacity of the stash.
      # If negative (or zero) then an unbounded stash is used (default)
      # If positive then a bounded stash is used and the capacity is set using
      # the property
      stash-capacity = -1
    }

    mailbox {
      # Mapping between message queue semantics and mailbox configurations.
      # Used by akka.dispatch.RequiresMessageQueue[T] to enforce different
      # mailbox types on actors.
      # If your Actor implements RequiresMessageQueue[T], then when you create
      # an instance of that actor its mailbox type will be decided by looking
      # up a mailbox configuration via T in this mapping
      requirements {
        "akka.dispatch.UnboundedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.BoundedMessageQueueSemantics" =
          akka.actor.mailbox.bounded-queue-based
        "akka.dispatch.DequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.UnboundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.BoundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.bounded-deque-based
        "akka.dispatch.MultipleConsumerSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.ControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.UnboundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.BoundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.bounded-control-aware-queue-based
        "akka.event.LoggerMessageQueueSemantics" =
          akka.actor.mailbox.logger-queue
      }

      unbounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedMailbox"
      }

      bounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedMailbox"
      }

      unbounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedDequeBasedMailbox"
      }

      bounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedDequeBasedMailbox"
      }

      unbounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
      }

      bounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedControlAwareMailbox"
      }

      # The LoggerMailbox will drain all messages in the mailbox
      # when the system is shutdown and deliver them to the StandardOutLogger.
      # Do not change this unless you know what you are doing.
      logger-queue {
        mailbox-type = "akka.event.LoggerMailboxType"
      }
    }

    debug {
      # enable function of Actor.loggable(), which is to log any received message
      # at DEBUG level, see the âTesting Actor Systemsâ section of the Akka
      # Documentation at https://akka.io/docs
      receive = off

      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)
      autoreceive = off

      # enable DEBUG logging of actor lifecycle changes
      lifecycle = off

      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers
      fsm = off

      # enable DEBUG logging of subscription changes on the eventStream
      event-stream = off

      # enable DEBUG logging of unhandled messages
      unhandled = off

      # enable WARN logging of misconfigured routers
      router-misconfiguration = off
    }

    # SECURITY BEST-PRACTICE is to disable java serialization for its multiple
    # known attack surfaces.
    #
    # This setting is a short-cut to
    # - using DisabledJavaSerializer instead of JavaSerializer
    #
    # Completely disable the use of `akka.serialization.JavaSerialization` by the
    # Akka Serialization extension, instead DisabledJavaSerializer will
    # be inserted which will fail explicitly if attempts to use java serialization are made.
    #
    # The log messages emitted by such serializer SHOULD be treated as potential
    # attacks which the serializer prevented, as they MAY indicate an external operator
    # attempting to send malicious messages intending to use java serialization as attack vector.
    # The attempts are logged with the SECURITY marker.
    #
    # Please note that this option does not stop you from manually invoking java serialization
    #
    allow-java-serialization = off

    # Log warnings when the Java serialization is used to serialize messages.
    # Java serialization is not very performant and should not be used in production
    # environments unless you don't care about performance and security. In that case
    # you can turn this off.
    warn-about-java-serializer-usage = on

    # To be used with the above warn-about-java-serializer-usage
    # When warn-about-java-serializer-usage = on, and this warn-on-no-serialization-verification = off,
    # warnings are suppressed for classes extending NoSerializationVerificationNeeded
    # to reduce noise.
    warn-on-no-serialization-verification = on

    # Entries for pluggable serializers and their bindings.
    serializers {
      java = "akka.serialization.JavaSerializer"
      bytes = "akka.serialization.ByteArraySerializer"
      primitive-long = "akka.serialization.LongSerializer"
      primitive-int = "akka.serialization.IntSerializer"
      primitive-string = "akka.serialization.StringSerializer"
      primitive-bytestring = "akka.serialization.ByteStringSerializer"
      primitive-boolean = "akka.serialization.BooleanSerializer"
    }

    # Class to Serializer binding. You only need to specify the name of an
    # interface or abstract base class of the messages. In case of ambiguity it
    # is using the most specific configured class, or giving a warning and
    # choosing the âfirstâ one.
    #
    # To disable one of the default serializers, assign its class to "none", like
    # "java.io.Serializable" = none
    serialization-bindings {
      "[B" = bytes
      "java.io.Serializable" = java

      "java.lang.String" = primitive-string
      "akka.util.ByteString$ByteString1C" = primitive-bytestring
      "akka.util.ByteString$ByteString1" = primitive-bytestring
      "akka.util.ByteString$ByteStrings" = primitive-bytestring
      "java.lang.Long" = primitive-long
      "scala.Long" = primitive-long
      "java.lang.Integer" = primitive-int
      "scala.Int" = primitive-int
      "java.lang.Boolean" = primitive-boolean
      "scala.Boolean" = primitive-boolean
    }

    # Configuration namespace of serialization identifiers.
    # Each serializer implementation must have an entry in the following format:
    # `akka.actor.serialization-identifiers."FQCN" = ID`
    # where `FQCN` is fully qualified class name of the serializer implementation
    # and `ID` is globally unique serializer identifier number.
    # Identifier values from 0 to 40 are reserved for Akka internal usage.
    serialization-identifiers {
      "akka.serialization.JavaSerializer" = 1
      "akka.serialization.ByteArraySerializer" = 4

      primitive-long = 18
      primitive-int = 19
      primitive-string = 20
      primitive-bytestring = 21
      primitive-boolean = 35
    }

  }

  serialization.protobuf {
    # deprecated, use `allowed-classes` instead
    whitelist-class = [
      "com.google.protobuf.GeneratedMessage",
      "com.google.protobuf.GeneratedMessageV3",
      "scalapb.GeneratedMessageCompanion",
      "akka.protobufv3.internal.GeneratedMessageV3"
    ]

    # Additional classes that are allowed even if they are not defined in `serialization-bindings`.
    # It can be exact class name or name of super class or interfaces (one level).
    # This is useful when a class is not used for serialization any more and therefore removed
    # from `serialization-bindings`, but should still be possible to deserialize.
    allowed-classes = ${akka.serialization.protobuf.whitelist-class}

  }

  # Used to set the behavior of the scheduler.
  # Changing the default values may change the system behavior drastically so make
  # sure you know what you're doing! See the Scheduler section of the Akka
  # Documentation for more details.
  scheduler {
    # The LightArrayRevolverScheduler is used as the default scheduler in the
    # system. It does not execute the scheduled tasks on exact time, but on every
    # tick, it will run everything that is (over)due. You can increase or decrease
    # the accuracy of the execution timing by specifying smaller or larger tick
    # duration. If you are scheduling a lot of tasks you should consider increasing
    # the ticks per wheel.
    # Note that it might take up to 1 tick to stop the Timer, so setting the
    # tick-duration to a high value will make shutting down the actor system
    # take longer.
    tick-duration = 10ms

    # The timer uses a circular wheel of buckets to store the timer tasks.
    # This should be set such that the majority of scheduled timeouts (for high
    # scheduling frequency) will be shorter than one rotation of the wheel
    # (ticks-per-wheel * ticks-duration)
    # THIS MUST BE A POWER OF TWO!
    ticks-per-wheel = 512

    # This setting selects the timer implementation which shall be loaded at
    # system start-up.
    # The class given here must implement the akka.actor.Scheduler interface
    # and offer a public constructor which takes three arguments:
    #  1) com.typesafe.config.Config
    #  2) akka.event.LoggingAdapter
    #  3) java.util.concurrent.ThreadFactory
    implementation = akka.actor.LightArrayRevolverScheduler

    # When shutting down the scheduler, there will typically be a thread which
    # needs to be stopped, and this timeout determines how long to wait for
    # that to happen. In case of timeout the shutdown of the actor system will
    # proceed without running possibly still enqueued tasks.
    shutdown-timeout = 5s
  }

  # How frequent the clock for recency-based strategies is updated. Can be set to 0 for usage of
  # `System.nanoTime` for each call but that might have some overhead for high message throughput.
  scheduled-clock-interval = 1 s

  io {

    # By default the select loops run on dedicated threads, hence using a
    # PinnedDispatcher
    pinned-dispatcher {
      type = "PinnedDispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.allow-core-timeout = off
    }

    tcp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this TCP module; there is
      # no intrinsic general limit, this setting is meant to enable DoS
      # protection by limiting the number of concurrently connected clients.
      # Also note that this is a "soft" limit; in certain cases the implementation
      # will accept a few connections more or a few less than the number configured
      # here. Must be an integer > 0 or "unlimited".
      max-channels = 256000

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of connection that are accepted in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      batch-accept-limit = 10

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # The duration a connection actor waits for a `Register` message from
      # its commander before aborting the connection.
      register-timeout = 5s

      # The maximum number of bytes delivered by a `Received` message. Before
      # more data is read from the network the connection actor will try to
      # do other work.
      # The purpose of this setting is to impose a smaller limit than the
      # configured receive buffer size. When using value 'unlimited' it will
      # try to read all from the receive buffer.
      max-received-message-size = unlimited

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # on which file IO tasks are scheduled
      file-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # The maximum number of bytes (or "unlimited") to transfer in one batch
      # when using `WriteFile` command which uses `FileChannel.transferTo` to
      # pipe files to a TCP socket. On some OS like Linux `FileChannel.transferTo`
      # may block for a long time when network IO is faster than file IO.
      # Decreasing the value may improve fairness while increasing may improve
      # throughput.
      file-io-transferTo-limit = 512 KiB

      # The number of times to retry the `finishConnect` call after being notified about
      # OP_CONNECT. Retries are needed if the OP_CONNECT notification doesn't imply that
      # `finishConnect` will succeed, which is the case on Android.
      finish-connect-retries = 5

      # On Windows connection aborts are not reliably detected unless an OP_READ is
      # registered on the selector _after_ the connection has been reset. This
      # workaround enables an OP_CONNECT which forces the abort to be visible on Windows.
      # Enabling this setting on other platforms than Windows will cause various failures
      # and undefined behavior.
      # Possible values of this key are on, off and auto where auto will enable the
      # workaround if Windows is detected automatically.
      windows-connection-abort-workaround-enabled = off
    }

    udp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"
    }

    udp-connected {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"
    }

    dns {
      # Fully qualified config path which holds the dispatcher configuration
      # for the manager and resolver router actors.
      # For actual router configuration see akka.actor.deployment./IO-DNS/*
      dispatcher = "akka.actor.internal-dispatcher"

      # Name of the subconfig at path akka.io.dns, see inet-address below
      #
      # Change to `async-dns` to use the new "native" DNS resolver,
      # which is also capable of resolving SRV records.
      resolver = "inet-address"

      # To-be-deprecated DNS resolver implementation which uses the Java InetAddress to resolve DNS records.
      # To be replaced by `akka.io.dns.async` which implements the DNS protocol natively and without blocking (which InetAddress does)
      inet-address {
        # This configuration entry is deprecated since Akka 2.6.0 and functionality for defining a custom provider
        # will be removed in a future Akka version
        provider-object = "akka.io.InetAddressDnsProvider"

        # To set the time to cache name resolutions
        # Possible values:
        # default: sun.net.InetAddressCachePolicy.get() and getNegative()
        # forever: cache forever
        # never: no caching
        # n [time unit]: positive timeout with unit, for example 30s
        positive-ttl = default
        negative-ttl = default

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s
      }

      async-dns {
        # This configuration entry is deprecated since Akka 2.6.0 and functionality for defining a custom provider
        # will be removed in a future Akka version
        provider-object = "akka.io.dns.internal.AsyncDnsProvider"

        # Set upper bound for caching successfully resolved dns entries
        # if the DNS record has a smaller TTL value than the setting that
        # will be used. Default is to use the record TTL with no cap.
        # Possible values:
        # forever: always use the minimum TTL from the found records
        # never: never cache
        # n [time unit] = cap the caching to this value
        positive-ttl = forever

        # Set how long the fact that a DNS record could not be found is
        # cached. If a new resolution is done while the fact is cached it will
        # be failed and not result in an actual DNS resolution. Default is
        # to never cache.
        # Possible values:
        # never: never cache
        # forever: cache a missing DNS record forever (you probably will not want to do this)
        # n [time unit] = cache for this long
        negative-ttl = never

        # Configures nameservers to query during DNS resolution.
        # Defaults to the nameservers that would be used by the JVM by default.
        # Set to a list of IPs to override the servers, e.g. [ "8.8.8.8", "8.8.4.4" ] for Google's servers
        # If multiple are defined then they are tried in order until one responds
        nameservers = default

        # The time that a request is allowed to live before being discarded
        # given no reply. The lower bound of this should always be the amount
        # of time to reasonably expect a DNS server to reply within.
        # If multiple name servers are provided then each gets this long to response before trying
        # the next one
        resolve-timeout = 5s

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s

        # Configures the list of search domains.
        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
        # other platforms, will not make any attempt to lookup the search domains). Set to a single domain, or
        # a list of domains, eg, [ "example.com", "example.net" ].
        search-domains = default

        # Any hosts that have a number of dots less than this will not be looked up directly, instead, a search on
        # the search domains will be tried first. This corresponds to the ndots option in /etc/resolv.conf, see
        # https://linux.die.net/man/5/resolver for more info.
        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
        # other platforms, will default to 1).
        ndots = default

        # Which SecureRandom algorithm to use for generating DNS request IDs.  The default "" or "SecureRandom"
        # is likely sufficient, but you may supply an alternative algorithm, in which case resolution will
        # proceed as in `SecureRandom.getInstance()`
        id-strategy = ""
      }
    }
  }


  # CoordinatedShutdown is an extension that will perform registered
  # tasks in the order that is defined by the phases. It is started
  # by calling CoordinatedShutdown(system).run(). This can be triggered
  # by different things, for example:
  # - JVM shutdown hook will by default run CoordinatedShutdown
  # - Cluster node will automatically run CoordinatedShutdown when it
  #   sees itself as Exiting
  # - A management console or other application specific command can
  #   run CoordinatedShutdown
  coordinated-shutdown {
    # The timeout that will be used for a phase if not specified with
    # 'timeout' in the phase
    default-phase-timeout = 5 s

    # Terminate the ActorSystem in the last phase actor-system-terminate.
    terminate-actor-system = on

    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = off

    # Exit status to use on System.exit(int) when 'exit-jvm' is 'on'.
    exit-code = 0

    # Run the coordinated shutdown when the JVM process exits, e.g.
    # via kill SIGTERM signal (SIGINT ctrl-c doesn't work).
    # This property is related to `akka.jvm-shutdown-hooks` above.
    run-by-jvm-shutdown-hook = on

    # Run the coordinated shutdown when ActorSystem.terminate is called.
    # Enabling this and disabling terminate-actor-system is not a supported
    # combination (will throw ConfigurationException at startup).
    run-by-actor-system-terminate = on

    # When Coordinated Shutdown is triggered an instance of `Reason` is
    # required. That value can be used to override the default settings.
    # Only 'exit-jvm', 'exit-code' and 'terminate-actor-system' may be
    # overridden depending on the reason.
    reason-overrides {
      # Overrides are applied using the `reason.getClass.getName`.
      # Overrides the `exit-code` when the `Reason` is a cluster
      # Downing or a Cluster Join Unsuccessful event
      "akka.actor.CoordinatedShutdown$ClusterDowningReason$" {
        exit-code = -1
      }
      "akka.actor.CoordinatedShutdown$ClusterJoinUnsuccessfulReason$" {
        exit-code = -1
      }
    }

    #//#coordinated-shutdown-phases
    # CoordinatedShutdown is enabled by default and will run the tasks that
    # are added to these phases by individual Akka modules and user logic.
    #
    # The phases are ordered as a DAG by defining the dependencies between the phases
    # to make sure shutdown tasks are run in the right order.
    #
    # In general user tasks belong in the first few phases, but there may be use
    # cases where you would want to hook in new phases or register tasks later in
    # the DAG.
    #
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # - enabled=off: Skip all tasks registered in this phase. DO NOT use
    #                this to disable phases unless you are absolutely sure what the
    #                consequences are. Many of the built in tasks depend on other tasks
    #                having been executed in earlier phases and may break if those are disabled.
    # depends-on=[]: Run the phase after the given phases
    phases {

      # The first pre-defined phase that applications can add tasks to.
      # Note that more phases can be added in the application's
      # configuration by overriding this phase with an additional
      # depends-on.
      before-service-unbind {
      }

      # Stop accepting new incoming connections.
      # This is where you can register tasks that makes a server stop accepting new connections. Already
      # established connections should be allowed to continue and complete if possible.
      service-unbind {
        depends-on = [before-service-unbind]
      }

      # Wait for requests that are in progress to be completed.
      # This is where you register tasks that will wait for already established connections to complete, potentially
      # also first telling them that it is time to close down.
      service-requests-done {
        depends-on = [service-unbind]
      }

      # Final shutdown of service endpoints.
      # This is where you would add tasks that forcefully kill connections that are still around.
      service-stop {
        depends-on = [service-requests-done]
      }

      # Phase for custom application tasks that are to be run
      # after service shutdown and before cluster shutdown.
      before-cluster-shutdown {
        depends-on = [service-stop]
      }

      # Graceful shutdown of the Cluster Sharding regions.
      # This phase is not meant for users to add tasks to.
      cluster-sharding-shutdown-region {
        timeout = 10 s
        depends-on = [before-cluster-shutdown]
      }

      # Emit the leave command for the node that is shutting down.
      # This phase is not meant for users to add tasks to.
      cluster-leave {
        depends-on = [cluster-sharding-shutdown-region]
      }

      # Shutdown cluster singletons
      # This is done as late as possible to allow the shard region shutdown triggered in
      # the "cluster-sharding-shutdown-region" phase to complete before the shard coordinator is shut down.
      # This phase is not meant for users to add tasks to.
      cluster-exiting {
        timeout = 10 s
        depends-on = [cluster-leave]
      }

      # Wait until exiting has been completed
      # This phase is not meant for users to add tasks to.
      cluster-exiting-done {
        depends-on = [cluster-exiting]
      }

      # Shutdown the cluster extension
      # This phase is not meant for users to add tasks to.
      cluster-shutdown {
        depends-on = [cluster-exiting-done]
      }

      # Phase for custom application tasks that are to be run
      # after cluster shutdown and before ActorSystem termination.
      before-actor-system-terminate {
        depends-on = [cluster-shutdown]
      }

      # Last phase. See terminate-actor-system and exit-jvm above.
      # Don't add phases that depends on this phase because the
      # dispatcher and scheduler of the ActorSystem have been shutdown.
      # This phase is not meant for users to add tasks to.
      actor-system-terminate {
        timeout = 10 s
        depends-on = [before-actor-system-terminate]
      }
    }
    #//#coordinated-shutdown-phases
  }

  #//#circuit-breaker-default
  # Configuration for circuit breakers created with the APIs accepting an id to
  # identify or look up the circuit breaker.
  # Note: Circuit breakers created without ids are not affected by this configuration.
  # A child configuration section with the same name as the circuit breaker identifier
  # will be used, with fallback to the `akka.circuit-breaker.default` section.
  circuit-breaker {

    # Default configuration that is used if a configuration section
    # with the circuit breaker identifier is not defined.
    default {
      # Number of failures before opening the circuit.
      max-failures = 10

      # Duration of time after which to consider a call a failure.
      call-timeout = 10s

      # Duration of time in open state after which to attempt to close
      # the circuit, by first entering the half-open state.
      reset-timeout = 15s

      # The upper bound of reset-timeout
      max-reset-timeout = 36500d

      # Exponential backoff
      # For details see https://en.wikipedia.org/wiki/Exponential_backoff
      exponential-backoff = 1.0

      # Additional random delay based on this factor is added to backoff
      # For example 0.2 adds up to 20% delay
      # In order to skip this additional delay set as 0
      random-factor = 0.0

      # A allowlist of fqcn of Exceptions that the CircuitBreaker
      # should not consider failures. By default all exceptions are
      # considered failures.
      exception-allowlist = []
    }
  }
  #//#circuit-breaker-default

}




akka-actor-typed


copy
source
akka.actor.typed {

  # List FQCN of `akka.actor.typed.ExtensionId`s which shall be loaded at actor system startup.
  # Should be on the format: 'extensions = ["com.example.MyExtId1", "com.example.MyExtId2"]' etc.
  # See the Akka Documentation for more info about Extensions
  extensions = []

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Library extensions are regular extensions that are loaded at startup and are
  # available for third party library authors to enable auto-loading of extensions when
  # present on the classpath. This is done by appending entries:
  # 'library-extensions += "Extension"' in the library `reference.conf`.
  #
  # Should not be set by end user applications in 'application.conf', use the extensions property for that
  #
  library-extensions = ${?akka.actor.typed.library-extensions} []

  # Receptionist is started eagerly to allow clustered receptionist to gather remote registrations early on.
  library-extensions += "akka.actor.typed.receptionist.Receptionist$"

  # While an actor is restarted (waiting for backoff to expire and children to stop)
  # incoming messages and signals are stashed, and delivered later to the newly restarted
  # behavior. This property defines the capacity in number of messages of the stash
  # buffer. If the capacity is exceed then additional incoming messages are dropped.
  restart-stash-capacity = 1000

  # Typed mailbox defaults to the single consumer mailbox as balancing dispatcher is not supported
  default-mailbox {
    mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
  }

  pub-sub {
    # When a message is published to a topic with no subscribers send it to the dead letters.
    send-to-dead-letters-when-no-subscribers = on
  }
}

# Load typed extensions by a classic extension.
akka.library-extensions += "akka.actor.typed.internal.adapter.ActorSystemAdapter$LoadTypedExtensions"

akka.actor {
  serializers {
    typed-misc = "akka.actor.typed.internal.MiscMessageSerializer"
    service-key = "akka.actor.typed.internal.receptionist.ServiceKeySerializer"
  }

  serialization-identifiers {
    "akka.actor.typed.internal.MiscMessageSerializer" = 24
    "akka.actor.typed.internal.receptionist.ServiceKeySerializer" = 26
  }

  serialization-bindings {
    "akka.actor.typed.ActorRef" = typed-misc
    "akka.actor.typed.internal.adapter.ActorRefAdapter" = typed-misc
    "akka.actor.typed.internal.receptionist.DefaultServiceKey" = service-key
  }
}

# When using Akka Typed (having akka-actor-typed in classpath) the
# akka.event.slf4j.Slf4jLogger is enabled instead of the DefaultLogger
# even though it has not been explicitly defined in `akka.loggers`
# configuration.
#
# Slf4jLogger will be used for all Akka classic logging via eventStream,
# including logging from Akka internals. The Slf4jLogger is then using
# an ordinary org.slf4j.Logger to emit the log events.
#
# The Slf4jLoggingFilter is also enabled automatically.
#
# This behavior can be disabled by setting this property to `off`.
akka.use-slf4j = on

akka.reliable-delivery {
  producer-controller {

    # To avoid head of line blocking from serialization and transfer
    # of large messages this can be enabled.
    # Large messages are chunked into pieces of the given size in bytes. The
    # chunked messages are sent separately and assembled on the consumer side.
    # Serialization and deserialization is performed by the ProducerController and
    # ConsumerController respectively instead of in the remote transport layer.
    chunk-large-messages = off

    durable-queue {
      # The ProducerController uses this timeout for the requests to
      # the durable queue. If there is no reply within the timeout it
      # will be retried.
      request-timeout = 3s

      # The ProducerController retries requests to the durable queue this
      # number of times before failing.
      retry-attempts = 10

      # The ProducerController retries sending the first message with this interval
      # until it has been confirmed.
      resend-first-interval = 1s
    }
  }

  consumer-controller {
    # Number of messages in flight between ProducerController and
    # ConsumerController. The ConsumerController requests for more messages
    # when half of the window has been used.
    flow-control-window = 50

    # The ConsumerController resends flow control messages to the
    # ProducerController with the resend-interval-min, and increasing
    # it gradually to resend-interval-max when idle.
    resend-interval-min = 2s
    resend-interval-max = 30s

    # If this is enabled lost messages will not be resent, but flow control is used.
    # This can be more efficient since messages don't have to be
    # kept in memory in the `ProducerController` until they have been
    # confirmed, but the drawback is that lost messages will not be delivered.
    only-flow-control = false
  }

  work-pulling {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 1000

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 60s

      # Chunked messages not implemented for work-pulling yet. Override to not
      # propagate property from akka.reliable-delivery.producer-controller.
      chunk-large-messages = off
    }
  }
}




akka-cluster-typed


copy
source
############################################
# Akka Cluster Typed Reference Config File #
############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

akka.cluster.typed.receptionist {
  # Updates with Distributed Data are done with this consistency level.
  # Possible values: local, majority, all, 2, 3, 4 (n)
  write-consistency = local

  # Period task to remove actor references that are hosted by removed nodes,
  # in case of abrupt termination.
  pruning-interval = 3 s

  # The periodic task to remove actor references that are hosted by removed nodes
  # will only remove entries older than this duration. The reason for this
  # is to avoid removing entries of nodes that haven't been visible as joining.
  prune-removed-older-than = 60 s

  # Shard the services over this many Distributed Data keys, with large amounts of different
  # service keys storing all of them in the same Distributed Data entry would lead to large updates
  # etc. instead the keys are sharded across this number of keys. This must be the same on all nodes
  # in a cluster, changing it requires a full cluster restart (stopping all nodes before starting them again)
  distributed-key-count = 5

  # Settings for the Distributed Data replicator used by Receptionist.
  # Same layout as akka.cluster.distributed-data.
  distributed-data = ${akka.cluster.distributed-data}
  # make sure that by default it's for all roles (Play loads config in different way)
  distributed-data.role = ""
}

akka.cluster.ddata.typed {
  # The timeout to use for ask operations in ReplicatorMessageAdapter.
  # This should be longer than the timeout given in Replicator.WriteConsistency and
  # Replicator.ReadConsistency. The replicator will always send a reply within those
  # timeouts so the unexpected ask timeout should not occur, but for cleanup in a
  # failure situation it must still exist.
  # If askUpdate, askGet or askDelete takes longer then this timeout a
  # java.util.concurrent.TimeoutException will be thrown by the requesting actor and
  # may be handled by supervision.
  replicator-message-adapter-unexpected-ask-timeout = 20 s
}

akka {
  actor {
    serialization-identifiers {
      "akka.cluster.typed.internal.AkkaClusterTypedSerializer" = 28
      "akka.cluster.typed.internal.delivery.ReliableDeliverySerializer" = 36
    }
    serializers {
      typed-cluster = "akka.cluster.typed.internal.AkkaClusterTypedSerializer"
      reliable-delivery = "akka.cluster.typed.internal.delivery.ReliableDeliverySerializer"
    }
    serialization-bindings {
      "akka.cluster.typed.internal.receptionist.ClusterReceptionist$Entry" = typed-cluster
      "akka.actor.typed.internal.pubsub.TopicImpl$MessagePublished" = typed-cluster
      "akka.actor.typed.delivery.internal.DeliverySerializable" = reliable-delivery
    }
  }
  cluster.configuration-compatibility-check.checkers {
    receptionist = "akka.cluster.typed.internal.receptionist.ClusterReceptionistConfigCompatChecker"
  }
}




akka-cluster


copy
source
######################################
# Akka Cluster Reference Config File #
######################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

akka {

  cluster {
    # Initial contact points of the cluster.
    # The nodes to join automatically at startup.
    # Comma separated full URIs defined by a string on the form of
    # "akka://system@hostname:port"
    # Leave as empty if the node is supposed to be joined manually.
    seed-nodes = []

    # How long to wait for one of the seed nodes to reply to initial join request.
    # When this is the first seed node and there is no positive reply from the other
    # seed nodes within this timeout it will join itself to bootstrap the cluster.
    # When this is not the first seed node the join attempts will be performed with
    # this interval.  
    seed-node-timeout = 5s

    # If a join request fails it will be retried after this period.
    # Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s
    
    # The joining of given seed nodes will by default be retried indefinitely until
    # a successful join. That process can be aborted if unsuccessful by defining this
    # timeout. When aborted it will run CoordinatedShutdown, which by default will
    # terminate the ActorSystem. CoordinatedShutdown can also be configured to exit
    # the JVM. It is useful to define this timeout if the seed-nodes are assembled
    # dynamically and a restart with new seed-nodes should be tried after unsuccessful
    # attempts.   
    shutdown-after-unsuccessful-join-seed-nodes = off

    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # Disable with "off" or specify a duration to enable.
    #
    # When using the `akka.cluster.sbr.SplitBrainResolver` as downing provider it will use
    # the akka.cluster.split-brain-resolver.stable-after as the default down-removal-margin
    # if this down-removal-margin is undefined.
    down-removal-margin = off

    # Pluggable support for downing of nodes in the cluster.
    # If this setting is left empty the `NoDowning` provider is used and no automatic downing will be performed.
    #
    # If specified the value must be the fully qualified class name of a subclass of
    # `akka.cluster.DowningProvider` having a public one argument constructor accepting an `ActorSystem`
    downing-provider-class = ""

    # Artery only setting
    # When a node has been gracefully removed, let this time pass (to allow for example
    # cluster singleton handover to complete) and then quarantine the removed node.
    quarantine-removed-node-after = 5s

    # If this is set to "off", the leader will not move 'Joining' members to 'Up' during a network
    # split. This feature allows the leader to accept 'Joining' members to be 'WeaklyUp'
    # so they become part of the cluster even during a network split. The leader will
    # move `Joining` members to 'WeaklyUp' after this configured duration without convergence.
    # The leader will move 'WeaklyUp' members to 'Up' status once convergence has been reached.
    allow-weakly-up-members = 7s

    # The roles of this member. List of strings, e.g. roles = ["A", "B"].
    # The roles are part of the membership information and can be used by
    # routers or other services to distribute work to certain member types,
    # e.g. front-end and back-end nodes.
    # Roles are not allowed to start with "dc-" as that is reserved for the
    # special role assigned from the data-center a node belongs to (see the
    # multi-data-center section below)
    roles = []
    
    # Run the coordinated shutdown from phase 'cluster-shutdown' when the cluster
    # is shutdown for other reasons than when leaving, e.g. when downing. This
    # will terminate the ActorSystem when the cluster extension is shutdown.
    run-coordinated-shutdown-when-down = on

    role {
      # Minimum required number of members of a certain role before the leader
      # changes member status of 'Joining' members to 'Up'. Typically used together
      # with 'Cluster.registerOnMemberUp' to defer some action, such as starting
      # actors, until the cluster has reached a certain size.
      # E.g. to require 2 nodes with role 'frontend' and 3 nodes with role 'backend':
      #   frontend.min-nr-of-members = 2
      #   backend.min-nr-of-members = 3
      #<role-name>.min-nr-of-members = 1
    }

    # Application version of the deployment. Used by rolling update features
    # to distinguish between old and new nodes. The typical convention is to use
    # 3 digit version numbers `major.minor.patch`, but 1 or two digits are also
    # supported.
    #
    # If no `.` is used it is interpreted as a single digit version number or as
    # plain alphanumeric if it couldn't be parsed as a number.
    #
    # It may also have a qualifier at the end for 2 or 3 digit version numbers such
    # as "1.2-RC1".
    # For 1 digit with qualifier, 1-RC1, it is interpreted as plain alphanumeric.
    #
    # It has support for https://github.com/dwijnand/sbt-dynver format with `+` or
    # `-` separator. The number of commits from the tag is handled as a numeric part.
    # For example `1.0.0+3-73475dce26` is less than `1.0.0+10-ed316bd024` (3 < 10).
    app-version = "0.0.0"

    # Minimum required number of members before the leader changes member status
    # of 'Joining' members to 'Up'. Typically used together with
    # 'Cluster.registerOnMemberUp' to defer some action, such as starting actors,
    # until the cluster has reached a certain size.
    min-nr-of-members = 1

    # Enable/disable info level logging of cluster events.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info = on

    # Enable/disable verbose info-level logging of cluster events
    # for temporary troubleshooting. Defaults to 'off'.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info-verbose = off

    # Enable or disable JMX MBeans for management of the cluster
    jmx.enabled = on

    # Enable or disable multiple JMX MBeans in the same JVM
    # If this is disabled, the MBean Object name is "akka:type=Cluster"
    # If this is enabled, them MBean Object names become "akka:type=Cluster,port=$clusterPortNumber"
    jmx.multi-mbeans-in-same-jvm = off

    # how long should the node wait before starting the periodic tasks
    # maintenance tasks?
    periodic-tasks-initial-delay = 1s

    # how often should the node send out gossip information?
    gossip-interval = 1s
    
    # discard incoming gossip messages if not handled within this duration
    gossip-time-to-live = 2s

    # how often should the leader perform maintenance tasks?
    leader-actions-interval = 1s

    # how often should the node move nodes, marked as unreachable by the failure
    # detector, out of the membership ring?
    unreachable-nodes-reaper-interval = 1s

    # How often the current internal stats should be published.
    # A value of 0s can be used to always publish the stats, when it happens.
    # Disable with "off".
    publish-stats-interval = off

    # The id of the dispatcher to use for cluster actors.
    # If specified you need to define the settings of the actual dispatcher.
    use-dispatcher = "akka.actor.internal-dispatcher"

    # Gossip to random node with newer or older state information, if any with
    # this probability. Otherwise Gossip to any random live node.
    # Probability value is between 0.0 and 1.0. 0.0 means never, 1.0 means always.
    gossip-different-view-probability = 0.8
    
    # Reduced the above probability when the number of nodes in the cluster
    # greater than this value.
    reduce-gossip-different-view-probability = 400

    # When a node is removed the removal is marked with a tombstone
    # which is kept at least this long, after which it is pruned, if there is a partition
    # longer than this it could lead to removed nodes being re-added to the cluster
    prune-gossip-tombstones-after = 24h

    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf
    # [Hayashibara et al]) used by the cluster subsystem to detect unreachable
    # members.
    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within
    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,
    # i.e. around 5.5 seconds with default settings.
    failure-detector {

      # FQCN of the failure detector implementation.
      # It must implement akka.remote.FailureDetector and have
      # a public constructor with a com.typesafe.config.Config and
      # akka.actor.EventStream parameter.
      implementation-class = "akka.remote.PhiAccrualFailureDetector"

      # How often keep-alive heartbeat messages should be sent to each connection.
      heartbeat-interval = 1 s

      # Defines the failure detector threshold.
      # A low threshold is prone to generate many wrong suspicions but ensures
      # a quick detection in the event of a real crash. Conversely, a high
      # threshold generates fewer mistakes but needs more time to detect
      # actual crashes.
      threshold = 8.0

      # Number of the samples of inter-heartbeat arrival times to adaptively
      # calculate the failure timeout for connections.
      max-sample-size = 1000

      # Minimum standard deviation to use for the normal distribution in
      # AccrualFailureDetector. Too low standard deviation might result in
      # too much sensitivity for sudden, but normal, deviations in heartbeat
      # inter arrival times.
      min-std-deviation = 100 ms

      # Number of potentially lost/delayed heartbeats that will be
      # accepted before considering it to be an anomaly.
      # This margin is important to be able to survive sudden, occasional,
      # pauses in heartbeat arrivals, due to for example garbage collect or
      # network drop.
      acceptable-heartbeat-pause = 3 s

      # Number of member nodes that each member will send heartbeat messages to,
      # i.e. each node will be monitored by this number of other nodes.
      monitored-by-nr-of-members = 9
      
      # After the heartbeat request has been sent the first failure detection
      # will start after this period, even though no heartbeat message has
      # been received.
      expected-response-after = 1 s

    }

    # Deprecated: Use Akka Distributed Cluster instead.
    #
    # Configures multi-dc specific heartbeating and other mechanisms,
    # many of them have a direct counter-part in "one datacenter mode",
    # in which case these settings would not be used at all - they only apply,
    # if your cluster nodes are configured with at-least 2 different `akka.cluster.data-center` values.
    multi-data-center {

      # Defines which data center this node belongs to. It is typically used to make islands of the
      # cluster that are colocated. This can be used to make the cluster aware that it is running
      # across multiple availability zones or regions. It can also be used for other logical
      # grouping of nodes.
      self-data-center = "default"


      # Try to limit the number of connections between data centers. Used for gossip and heartbeating.
      # This will not limit connections created for the messaging of the application.
      # If the cluster does not span multiple data centers, this value has no effect.
      cross-data-center-connections = 5

      # The n oldest nodes in a data center will choose to gossip to another data center with
      # this probability. Must be a value between 0.0 and 1.0 where 0.0 means never, 1.0 means always.
      # When a data center is first started (nodes < 5) a higher probability is used so other data
      # centers find out about the new nodes more quickly
      cross-data-center-gossip-probability = 0.2

      failure-detector {
        # FQCN of the failure detector implementation.
        # It must implement akka.remote.FailureDetector and have
        # a public constructor with a com.typesafe.config.Config and
        # akka.actor.EventStream parameter.
        implementation-class = "akka.remote.DeadlineFailureDetector"
  
        # Number of potentially lost/delayed heartbeats that will be
        # accepted before considering it to be an anomaly.
        # This margin is important to be able to survive sudden, occasional,
        # pauses in heartbeat arrivals, due to for example garbage collect or
        # network drop.
        acceptable-heartbeat-pause = 10 s
        
        # How often keep-alive heartbeat messages should be sent to each connection.
        heartbeat-interval = 3 s
  
        # After the heartbeat request has been sent the first failure detection
        # will start after this period, even though no heartbeat message has
        # been received.
        expected-response-after = 1 s
      }
    }

    # If the tick-duration of the default scheduler is longer than the
    # tick-duration configured here a dedicated scheduler will be used for
    # periodic tasks of the cluster, otherwise the default scheduler is used.
    # See akka.scheduler settings for more details.
    scheduler {
      tick-duration = 33ms
      ticks-per-wheel = 512
    }

    debug {
      # Log heartbeat events (very verbose, useful mostly when debugging heartbeating issues).
      # These are logged with logger name `akka.cluster.ClusterHeartbeat`.
      verbose-heartbeat-logging = off

      # log verbose details about gossip
      verbose-gossip-logging = off
    }

    configuration-compatibility-check {

      # Enforce configuration compatibility checks when joining a cluster.
      # Set to off to allow joining nodes to join a cluster even when configuration incompatibilities are detected or
      # when the cluster does not support this feature. Compatibility checks are always performed and warning and
      # error messages are logged.
      #
      # This is particularly useful for rolling updates on clusters that do not support that feature. Since the old
      # cluster won't be able to send the compatibility confirmation to the joining node, the joining node won't be able
      # to 'know' if its allowed to join.
      enforce-on-join = on

      # Add named entry to this section with fully qualified class name of the JoinConfigCompatChecker
      # to enable.
      # Checkers defined in reference.conf can be disabled by application by using empty string value
      # for the named entry.
      checkers {
        akka-cluster = "akka.cluster.JoinConfigCompatCheckCluster"
      }

      # Some configuration properties might not be appropriate to transfer between nodes
      # and such properties can be excluded from the configuration compatibility check by adding
      # the paths of the properties to this list. Sensitive paths are grouped by key. Modules and third-party libraries
      # can define their own set of sensitive paths without clashing with each other (as long they use unique keys).
      #
      # All properties starting with the paths defined here are excluded, i.e. you can add the path of a whole
      # section here to skip everything inside that section.
      sensitive-config-paths {
        akka = [
          "user.home", "user.name", "user.dir",
          "socksNonProxyHosts", "http.nonProxyHosts", "ftp.nonProxyHosts",
          "akka.remote.secure-cookie",
          # Pre 2.8 path, keep around to avoid sending things misconfigured with old paths
          "akka.remote.classic.netty.ssl.security",
          # Pre 2.6 path, keep around to avoid sending things misconfigured with old paths
          "akka.remote.netty.ssl.security",
          "akka.remote.artery.ssl"
        ]
      }

    }
  }

  actor.deployment.default.cluster {
    # enable cluster aware router that deploys to nodes in the cluster
    enabled = off

    # Maximum number of routees that will be deployed on each cluster
    # member node.
    # Note that max-total-nr-of-instances defines total number of routees, but
    # number of routees per node will not be exceeded, i.e. if you
    # define max-total-nr-of-instances = 50 and max-nr-of-instances-per-node = 2
    # it will deploy 2 routees per new member in the cluster, up to
    # 25 members.
    max-nr-of-instances-per-node = 1
    
    # Maximum number of routees that will be deployed, in total
    # on all nodes. See also description of max-nr-of-instances-per-node.
    # For backwards compatibility reasons, nr-of-instances
    # has the same purpose as max-total-nr-of-instances for cluster
    # aware routers and nr-of-instances (if defined by user) takes
    # precedence over max-total-nr-of-instances. 
    max-total-nr-of-instances = 10000

    # Defines if routees are allowed to be located on the same node as
    # the head router actor, or only on remote nodes.
    # Useful for master-worker scenario where all routees are remote.
    allow-local-routees = on

    # Use members with all specified roles, or all members if undefined or empty.
    use-roles = []

    # Deprecated, since Akka 2.5.4, replaced by use-roles
    # Use members with specified role, or all members if undefined or empty.
    use-role = ""
  }

  # Protobuf serializer for cluster messages
  actor {
    serializers {
      akka-cluster = "akka.cluster.protobuf.ClusterMessageSerializer"
    }

    serialization-bindings {
      "akka.cluster.ClusterMessage" = akka-cluster
      "akka.cluster.routing.ClusterRouterPool" = akka-cluster
    }
    
    serialization-identifiers {
      "akka.cluster.protobuf.ClusterMessageSerializer" = 5
    }
    
  }

}

#//#split-brain-resolver

# To enable the split brain resolver you first need to enable the provider in your application.conf:
# akka.cluster.downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

akka.cluster.split-brain-resolver {
  # Select one of the available strategies (see descriptions below):
  # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
  active-strategy = keep-majority

  #//#stable-after
  # Time margin after which shards or singletons that belonged to a downed/removed
  # partition are created in surviving partition. The purpose of this margin is that
  # in case of a network partition the persistent actors in the non-surviving partitions
  # must be stopped before corresponding persistent actors are started somewhere else.
  # This is useful if you implement downing strategies that handle network partitions,
  # e.g. by keeping the larger side of the partition and shutting down the smaller side.
  # Decision is taken by the strategy when there has been no membership or
  # reachability changes for this duration, i.e. the cluster state is stable.
  stable-after = 20s
  #//#stable-after

  # When reachability observations by the failure detector are changed the SBR decisions
  # are deferred until there are no changes within the 'stable-after' duration.
  # If this continues for too long it might be an indication of an unstable system/network
  # and it could result in delayed or conflicting decisions on separate sides of a network
  # partition.
  # As a precaution for that scenario all nodes are downed if no decision is made within
  # `stable-after + down-all-when-unstable` from the first unreachability event.
  # The measurement is reset if all unreachable have been healed, downed or removed.
  # The value can be on, off, or a duration.
  # By default it is 'on' and then it is derived to be 3/4 of stable-after, but not less than
  # 4 seconds.
  down-all-when-unstable = on

}
#//#split-brain-resolver

# Down the unreachable nodes if the number of remaining nodes are greater than or equal to
# the given 'quorum-size'. Otherwise down the reachable nodes, i.e. it will shut down that
# side of the partition. In other words, the 'size' defines the minimum number of nodes
# that the cluster must have to be operational. If there are unreachable nodes when starting
# up the cluster, before reaching this limit, the cluster may shutdown itself immediately.
# This is not an issue if you start all nodes at approximately the same time.
#
# Note that you must not add more members to the cluster than 'quorum-size * 2 - 1', because
# then both sides may down each other and thereby form two separate clusters. For example,
# quorum-size configured to 3 in a 6 node cluster may result in a split where each side
# consists of 3 nodes each, i.e. each side thinks it has enough nodes to continue by
# itself. A warning is logged if this recommendation is violated.
#//#static-quorum
akka.cluster.split-brain-resolver.static-quorum {
  # minimum number of nodes that the cluster must have
  quorum-size = undefined

  # if the 'role' is defined the decision is based only on members with that 'role'
  role = ""
}
#//#static-quorum

# Down the unreachable nodes if the current node is in the majority part based the last known
# membership information. Otherwise down the reachable nodes, i.e. the own part. If the
# the parts are of equal size the part containing the node with the lowest address is kept.
# Note that if there are more than two partitions and none is in majority each part
# will shutdown itself, terminating the whole cluster.
#//#keep-majority
akka.cluster.split-brain-resolver.keep-majority {
  # if the 'role' is defined the decision is based only on members with that 'role'
  role = ""
}
#//#keep-majority

# Down the part that does not contain the oldest member (current singleton).
#
# There is one exception to this rule if 'down-if-alone' is defined to 'on'.
# Then, if the oldest node has partitioned from all other nodes the oldest
# will down itself and keep all other nodes running. The strategy will not
# down the single oldest node when it is the only remaining node in the cluster.
#
# Note that if the oldest node crashes the others will remove it from the cluster
# when 'down-if-alone' is 'on', otherwise they will down themselves if the
# oldest node crashes, i.e. shutdown the whole cluster together with the oldest node.
#//#keep-oldest
akka.cluster.split-brain-resolver.keep-oldest {
  # Enable downing of the oldest node when it is partitioned from all other nodes
  down-if-alone = on

  # if the 'role' is defined the decision is based only on members with that 'role',
  # i.e. using the oldest member (singleton) within the nodes with that role
  role = ""
}
#//#keep-oldest

# Keep the part that can acquire the lease, and down the other part.
# Best effort is to keep the side that has most nodes, i.e. the majority side.
# This is achieved by adding a delay before trying to acquire the lease on the
# minority side.
#//#lease-majority
akka.cluster.split-brain-resolver.lease-majority {
  lease-implementation = ""

  # The recommended format for the lease name is "<service-name>-akka-sbr".
  # When lease-name is not defined, the name will be set to "<actor-system-name>-akka-sbr"
  lease-name = ""

  # This delay is used on the minority side before trying to acquire the lease,
  # as an best effort to try to keep the majority side.
  acquire-lease-delay-for-minority = 2s

  # Release the lease after this duration.
  release-after = 40s

  # If the 'role' is defined the majority/minority is based only on members with that 'role'.
  role = ""
}
#//#lease-majority




akka-discovery


copy
source
######################################################
# Akka Discovery Config                              #
######################################################

akka.actor.deployment {
  "/SD-DNS/async-dns" {
    mailbox = "unbounded"
    router = "round-robin-pool"
    nr-of-instances = 1
  }
}

akka.discovery {

  # Users MUST configure this value to set the default discovery method.
  #
  # The value can be an implementation config path name, such as "akka-dns",
  # which would attempt to resolve as `akka.discovery.akka-dns` which is expected
  # to contain a `class` setting. As fallback, the root `akka-dns` setting scope
  # would be used. If none of those contained a `class` setting, then the value is
  # assumed to be a class name, and an attempt is made to instantiate it.
  method = "<method>"

  # Config based service discovery
  config {
    class = akka.discovery.config.ConfigServiceDiscovery

    # Location of the services in configuration
    services-path = "akka.discovery.config.services"

    # A map of services to resolve from configuration.
    # See docs for more examples.
    # A list of endpoints with host/port where port is optional e.g.
    # services {
    #  service1 {
    #    endpoints = [
    #      {
    #        host = "cat.com"
    #        port = 1233
    #      },
    #      {
    #        host = "dog.com"
    #      }
    #    ]
    #  },
    #  service2 {
    #    endpoints = [
    #    {
    #        host = "fish.com"
    #        port = 1233
    #      }
    #    ]
    #  }
    # }
    services = {

    }
  }

  # Aggregate multiple service discovery mechanisms
  aggregate {
    class = akka.discovery.aggregate.AggregateServiceDiscovery

    # List of service discovery methods to try in order. E.g config then fall back to DNS
    # ["config", "akka-dns"]
    discovery-methods = []

  }

  # DNS based service discovery
  akka-dns {
    class = akka.discovery.dns.DnsServiceDiscovery
  }
}





akka-coordination


copy
source
akka.coordination {

  # Defaults for any lease implementation that doesn't include these properties
  lease {

    # FQCN of the implementation of the Lease
    lease-class = ""

    #defaults
    # if the node that acquired the leases crashes, how long should the lease be held before another owner can get it
    heartbeat-timeout = 120s

    # interval for communicating with the third party to confirm the lease is still held
    heartbeat-interval = 12s

    # lease implementations are expected to time out acquire and release calls or document
    # that they do not implement an operation timeout
    lease-operation-timeout = 5s

    #defaults
  }
}




akka-multi-node-testkit


copy
source
#############################################
# Akka Remote Testing Reference Config File #
#############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

akka {
  testconductor {

    # Timeout for joining a barrier: this is the maximum time any participants
    # waits for everybody else to join a named barrier.
    barrier-timeout = 30s
    
    # Timeout for interrogation of TestConductorâs Controller actor
    query-timeout = 10s
    
    # Threshold for packet size in time unit above which the failure injector will
    # split the packet and deliver in smaller portions; do not give value smaller
    # than HashedWheelTimer resolution (would not make sense)
    packet-split-threshold = 100ms
    
    # amount of time for the ClientFSM to wait for the connection to the conductor
    # to be successful
    connect-timeout = 20s
    
    # Number of connect attempts to be made to the conductor controller
    client-reconnects = 30
    
    # minimum time interval which is to be inserted between reconnect attempts
    reconnect-backoff = 1s

    netty {
      # (I&O) Used to configure the number of I/O worker threads on server sockets
      server-socket-worker-pool {
        # Min number of threads to cap factor-based number to
        pool-size-min = 1

        # The pool size factor is used to determine thread pool size
        # using the following formula: ceil(available processors * factor).
        # Resulting size is then bounded by the pool-size-min and
        # pool-size-max values.
        pool-size-factor = 1.0

        # Max number of threads to cap factor-based number to
        pool-size-max = 2
      }

      # (I&O) Used to configure the number of I/O worker threads on client sockets
      client-socket-worker-pool {
        # Min number of threads to cap factor-based number to
        pool-size-min = 1

        # The pool size factor is used to determine thread pool size
        # using the following formula: ceil(available processors * factor).
        # Resulting size is then bounded by the pool-size-min and
        # pool-size-max values.
        pool-size-factor = 1.0

        # Max number of threads to cap factor-based number to
        pool-size-max = 2
      }
    }
  }
}




akka-persistence-typed


copy
source
akka.actor {

  serialization-identifiers {
    "akka.persistence.typed.serialization.ReplicatedEventSourcingSerializer" = 40
  }

  serializers.replicated-event-sourcing = "akka.persistence.typed.serialization.ReplicatedEventSourcingSerializer"

  serialization-bindings {
    "akka.persistence.typed.internal.VersionVector" = replicated-event-sourcing
    "akka.persistence.typed.crdt.Counter" = replicated-event-sourcing
    "akka.persistence.typed.crdt.Counter$Updated" = replicated-event-sourcing
    "akka.persistence.typed.crdt.ORSet" = replicated-event-sourcing
    "akka.persistence.typed.crdt.ORSet$DeltaOp" = replicated-event-sourcing
    "akka.persistence.typed.internal.ReplicatedEventMetadata" = replicated-event-sourcing
    "akka.persistence.typed.internal.ReplicatedSnapshotMetadata" = replicated-event-sourcing
    "akka.persistence.typed.internal.PublishedEventImpl" = replicated-event-sourcing
  }
}

akka.persistence.typed {

  # Persistent actors stash while recovering or persisting events,
  # this setting configures the default capacity of this stash.
  #
  # Stashing is always bounded to the size that is defined in this setting.
  # You can set it to large values, however "unbounded" buffering is not supported.
  # Negative or 0 values are not allowed.
  stash-capacity = 4096

  # Configure how to react when the event sourced stash overflows. This can happen in two scenarios:
  # when a event sourced actor is doing recovery, persisting or snapshotting and it gets more than
  # 'stash-capacity' commands, or if more than 'stash-capacity' commands are manually stashed with the
  # 'stash' effect.
  #
  # Possible options
  # - drop - the message is published as a akka.actor.typed.Dropped message on the event bus
  # - fail - an exception is thrown so that the actor is failed
  stash-overflow-strategy = "drop"

  # enables automatic DEBUG level logging of messages stashed automatically by an EventSourcedBehavior,
  # this may happen while it receives commands while it is recovering events or while it is persisting events
  log-stashing = off

  # By default, internal event sourced behavior logging are sent to
  # akka.persistence.typed.internal.EventSourcedBehaviorImpl
  # this can be changed by setting this to 'true' in which case the internal logging is sent to
  # the actor context logger.
  use-context-logger-for-internal-logging = false

  event-writer {
    # The maximum number of events to batch together when writing to the journal through the event writer
    max-batch-size = 10
    # The event-writer occasionally needs to ask the journal about highest sequence number to handle duplicate
    # writes, this timeout is for that interaction
    ask-timeout = 20s
    # When fillSequenceNumberGaps is enabled it will keep latest sequence
    # number in memory for this many persistence ids.
    latest-sequence-number-cache-capacity = 1000
  }
}

akka.reliable-delivery {
  producer-controller {
    event-sourced-durable-queue {
      # Max duration for the exponential backoff for persist failures.
      restart-max-backoff = 10s

      # Snapshot after this number of events. See RetentionCriteria.
      snapshot-every = 1000

      # Number of snapshots to keep. See RetentionCriteria.
      keep-n-snapshots = 2

      # Delete events after snapshotting. See RetentionCriteria.
      delete-events = on

      # Cleanup entries that haven't be used for this duration.
      cleanup-unused-after = 3600s

      # The journal plugin to use, by default it will use the plugin configured by
      # `akka.persistence.journal.plugin`.
      journal-plugin-id = ""

      # The journal plugin to use, by default it will use the plugin configured by
      # `akka.persistence.snapshot-store.plugin`.
      snapshot-plugin-id = ""
    }
  }
}




akka-persistence


copy
source
###########################################################
# Akka Persistence Extension Reference Configuration File #
###########################################################

# This is the reference config file that contains all the default settings.
# Make your edits in your application.conf in order to override these settings.

# Directory of persistence journal and snapshot store plugins is available at the 
# Akka Community Projects page https://akka.io/community/

# Default persistence extension settings.
akka.persistence {

    # When starting many persistent actors at the same time the journal
    # and its data store is protected from being overloaded by limiting number
    # of recoveries that can be in progress at the same time. When
    # exceeding the limit the actors will wait until other recoveries have
    # been completed.   
    max-concurrent-recoveries = 50

    # Fully qualified class name providing a default internal stash overflow strategy.
    # It needs to be a subclass of akka.persistence.StashOverflowStrategyConfigurator.
    # The default strategy throws StashOverflowException.
    internal-stash-overflow-strategy = "akka.persistence.ThrowExceptionConfigurator"
    journal {
        # Absolute path to the journal plugin configuration entry used by 
        # persistent actor by default.
        # Persistent actor can override `journalPluginId` method 
        # in order to rely on a different journal plugin.
        plugin = ""
        # List of journal plugins to start automatically. Use "" for the default journal plugin.
        auto-start-journals = []
    }
    snapshot-store {
        # Absolute path to the snapshot plugin configuration entry used by
        # persistent actor by default.
        # Persistent actor can override `snapshotPluginId` method
        # in order to rely on a different snapshot plugin.
        # It is not mandatory to specify a snapshot store plugin.
        # If you don't use snapshots you don't have to configure it.
        # Note that Cluster Sharding is using snapshots, so if you
        # use Cluster Sharding you need to define a snapshot store plugin. 
        plugin = ""
        # List of snapshot stores to start automatically. Use "" for the default snapshot store.
        auto-start-snapshot-stores = []
    }
    # used as default-snapshot store if no plugin configured 
    # (see `akka.persistence.snapshot-store`)
    no-snapshot-store {
      class = "akka.persistence.snapshot.NoSnapshotStore"
    }
    # Default reliable delivery settings.
    at-least-once-delivery {
        # Interval between re-delivery attempts.
        redeliver-interval = 5s
        # Maximum number of unconfirmed messages that will be sent in one 
        # re-delivery burst.
        redelivery-burst-limit = 10000
        # After this number of delivery attempts a 
        # `ReliableRedelivery.UnconfirmedWarning`, message will be sent to the actor.
        warn-after-number-of-unconfirmed-attempts = 5
        # Maximum number of unconfirmed messages that an actor with 
        # AtLeastOnceDelivery is allowed to hold in memory.
        max-unconfirmed-messages = 100000
    }
    # Default persistent extension thread pools.
    # Deprecated: These are not used by default from Akka 2.7.0.
    # Plugins should define their own custom dispatchers if needed, otherwise the
    # akka.actor.default-dispatcher is used by default.
    dispatchers {
        default-plugin-dispatcher {
            type = PinnedDispatcher
            executor = "thread-pool-executor"
        }
        default-replay-dispatcher {
            type = Dispatcher
            executor = "fork-join-executor"
            fork-join-executor {
                parallelism-min = 2
                parallelism-max = 8
            }
        }
        default-stream-dispatcher {
            type = Dispatcher
            executor = "fork-join-executor"
            fork-join-executor {
                parallelism-min = 2
                parallelism-max = 8
            }
        }
    }

    # Fallback settings for journal plugin configurations.
    # These settings are used if they are not defined in plugin config section.
    journal-plugin-fallback {

      # Fully qualified class name providing journal plugin api implementation.
      # It is mandatory to specify this property.
      # The class must have a constructor without parameters or constructor with
      # one `com.typesafe.config.Config` parameter.
      class = ""

      # Dispatcher for the plugin actor.
      plugin-dispatcher = "akka.actor.default-dispatcher"

      # Dispatcher for message replay.
      replay-dispatcher = "akka.actor.default-dispatcher"

      # Removed: used to be the Maximum size of a persistent message batch written to the journal.
      # Now this setting is without function, PersistentActor will write as many messages
      # as it has accumulated since the last write.
      max-message-batch-size = 200

      # If there is more time in between individual events gotten from the journal
      # recovery than this the recovery will fail.
      # Note that it also affects reading the snapshot before replaying events on
      # top of it, even though it is configured for the journal.
      recovery-event-timeout = 30s

      circuit-breaker {
        max-failures = 10
        call-timeout = 10s
        reset-timeout = 30s
        max-reset-timeout = ${akka.circuit-breaker.default.max-reset-timeout}
        exponential-backoff = ${akka.circuit-breaker.default.exponential-backoff}
        random-factor = ${akka.circuit-breaker.default.random-factor}
        exception-allowlist = []
      }

      # The replay filter can detect a corrupt event stream by inspecting
      # sequence numbers and writerUuid when replaying events.
      replay-filter {
        # What the filter should do when detecting invalid events.
        # Supported values:
        # `repair-by-discard-old` : discard events from old writers,
        #                           warning is logged
        # `fail` : fail the replay, error is logged
        # `warn` : log warning but emit events untouched
        # `off` : disable this feature completely
        mode = repair-by-discard-old

        # It uses a look ahead buffer for analyzing the events.
        # This defines the size (in number of events) of the buffer.
        window-size = 100

        # How many old writerUuid to remember
        max-old-writers = 10

        # Set this to `on` to enable detailed debug logging of each
        # replayed event.
        debug = off
      }
    }

    # Fallback settings for snapshot store plugin configurations
    # These settings are used if they are not defined in plugin config section.
    snapshot-store-plugin-fallback {

      # Fully qualified class name providing snapshot store plugin api
      # implementation. It is mandatory to specify this property if
      # snapshot store is enabled.
      # The class must have a constructor without parameters or constructor with
      # one `com.typesafe.config.Config` parameter.
      class = ""

      # Dispatcher for the plugin actor.
      plugin-dispatcher = "akka.actor.default-dispatcher"

      circuit-breaker {
        max-failures = 5
        call-timeout = 20s
        reset-timeout = 60s
        max-reset-timeout = ${akka.circuit-breaker.default.max-reset-timeout}
        exponential-backoff = ${akka.circuit-breaker.default.exponential-backoff}
        random-factor = ${akka.circuit-breaker.default.random-factor}
        exception-allowlist = []
      }

      # Set this to true if successful loading of snapshot is not necessary.
      # This can be useful when it is alright to ignore snapshot in case of
      # for example deserialization errors. When snapshot loading fails it will instead
      # recover by replaying all events.
      # Don't set to true if events are deleted because that would
      # result in wrong recovered state if snapshot load fails.
      snapshot-is-optional = false

      # Some snapshot store plugins only store the latest snapshot and can set this
      # to true. That enables optimizations in retention strategies based on that
      # old snapshots don't have to be deleted.
      only-one-snapshot = false

    }

    # DurableStateStore settings
    state {
      # Absolute path to the KeyValueStore plugin configuration entry used by
      # DurableStateBehavior actors by default.
      # DurableStateBehavior can override `durableStateStorePluginId` method (`withDurableStateStorePluginId`)
      # in order to rely on a different plugin.
      plugin = ""
    }

    # Fallback settings for DurableStateStore plugin configurations
    # These settings are used if they are not defined in plugin config section.
    state-plugin-fallback {
      recovery-timeout = 30s
    }
}

# Protobuf serialization for the persistent extension messages.
akka.actor {
    serializers {
        akka-persistence-message = "akka.persistence.serialization.MessageSerializer"
        akka-persistence-snapshot = "akka.persistence.serialization.SnapshotSerializer"
        akka-persistence-payload = "akka.persistence.serialization.PayloadSerializer"
    }
    serialization-bindings {
        "akka.persistence.serialization.Message" = akka-persistence-message
        "akka.persistence.serialization.Snapshot" = akka-persistence-snapshot
        "akka.persistence.FilteredPayload$" = akka-persistence-payload
        "akka.persistence.SerializedEvent" = akka-persistence-payload
        "akka.persistence.CompositeMetadata" = akka-persistence-payload
    }
    serialization-identifiers {
        "akka.persistence.serialization.MessageSerializer" = 7
        "akka.persistence.serialization.SnapshotSerializer" = 8
        "akka.persistence.serialization.PayloadSerializer" = 34
    }
}


###################################################
# Persistence plugins included with the extension #
###################################################

# In-memory journal plugin.
akka.persistence.journal.inmem {
    # Class name of the plugin.
    class = "akka.persistence.journal.inmem.InmemJournal"
    # Dispatcher for the plugin actor.
    plugin-dispatcher = "akka.actor.default-dispatcher"

    # Turn this on to test serialization of the events
    test-serialization = off
    # Useful for tests, increase to make writes take time like an actual persistent journal
    delay-writes = 0s
}

# Local file system snapshot store plugin.
akka.persistence.snapshot-store.local {
    # Class name of the plugin.
    class = "akka.persistence.snapshot.local.LocalSnapshotStore"
    # Dispatcher for the plugin actor.
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    # Dispatcher for streaming snapshot IO.
    stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
    # Storage location of snapshot files.
    dir = "snapshots"
    # Number load attempts when recovering from the latest snapshot fails
    # yet older snapshot files are available. Each recovery attempt will try
    # to recover using an older than previously failed-on snapshot file 
    # (if any are present). If all attempts fail the recovery will fail and
    # the persistent actor will be stopped.
    max-load-attempts = 3
}

# LevelDB journal plugin.
# Note: this plugin requires explicit LevelDB dependency, see below. 
akka.persistence.journal.leveldb {
    # Class name of the plugin.
    class = "akka.persistence.journal.leveldb.LeveldbJournal"
    # Dispatcher for the plugin actor.
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    # Dispatcher for message replay.
    replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"
    # Storage location of LevelDB files.
    dir = "journal"
    # Use fsync on write.
    fsync = on
    # Verify checksum on read.
    checksum = off
    # Native LevelDB (via JNI) or LevelDB Java port.
    native = on
    # Number of deleted messages per persistence id that will trigger journal compaction
    compaction-intervals {
    }
}

# Shared LevelDB journal plugin (for testing only).
# Note: this plugin requires explicit LevelDB dependency, see below. 
akka.persistence.journal.leveldb-shared {
    # Class name of the plugin.
    class = "akka.persistence.journal.leveldb.SharedLeveldbJournal"
    # Dispatcher for the plugin actor.
    plugin-dispatcher = "akka.actor.default-dispatcher"
    # Timeout for async journal operations.
    timeout = 10s
    store {
        # Dispatcher for shared store actor.
        store-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
        # Dispatcher for message replay.
        replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"
        # Storage location of LevelDB files.
        dir = "journal"
        # Use fsync on write.
        fsync = on
        # Verify checksum on read.
        checksum = off
        # Native LevelDB (via JNI) or LevelDB Java port.
        native = on
        # Number of deleted messages per persistence id that will trigger journal compaction
        compaction-intervals {
        }
    }
}

akka.persistence.journal.proxy {
  # Class name of the plugin.
  class = "akka.persistence.journal.PersistencePluginProxy"
  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"
  # Set this to on in the configuration of the ActorSystem
  # that will host the target journal
  start-target-journal = off
  # The journal plugin config path to use for the target journal
  target-journal-plugin = ""
  # The address of the proxy to connect to from other nodes. Optional setting.
  target-journal-address = ""
  # Initialization timeout of target lookup
  init-timeout = 10s
}

akka.persistence.snapshot-store.proxy {
  # Class name of the plugin.
  class = "akka.persistence.journal.PersistencePluginProxy"
  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"
  # Set this to on in the configuration of the ActorSystem
  # that will host the target snapshot-store
  start-target-snapshot-store = off
  # The journal plugin config path to use for the target snapshot-store
  target-snapshot-store-plugin = ""
  # The address of the proxy to connect to from other nodes. Optional setting.
  target-snapshot-store-address = ""
  # Initialization timeout of target lookup
  init-timeout = 10s
}

# LevelDB persistence requires the following dependency declarations:
#
# SBT:
#       "org.iq80.leveldb"            % "leveldb"          % "0.7"
#       "org.fusesource.leveldbjni"   % "leveldbjni-all"   % "1.8"
#
# Maven:
#        <dependency>
#            <groupId>org.iq80.leveldb</groupId>
#            <artifactId>leveldb</artifactId>
#            <version>0.7</version>
#        </dependency>
#        <dependency>
#            <groupId>org.fusesource.leveldbjni</groupId>
#            <artifactId>leveldbjni-all</artifactId>
#            <version>1.8</version>
#        </dependency>




akka-persistence-query


copy
source
#######################################################
# Akka Persistence Query Reference Configuration File #
#######################################################

# This is the reference config file that contains all the default settings.
# Make your edits in your application.conf in order to override these settings.

#//#query-leveldb
# Configuration for the LeveldbReadJournal
akka.persistence.query.journal.leveldb {
  # Implementation class of the LevelDB ReadJournalProvider
  class = "akka.persistence.query.journal.leveldb.LeveldbReadJournalProvider"
  
  # Absolute path to the write journal plugin configuration entry that this 
  # query journal will connect to. That must be a LeveldbJournal or SharedLeveldbJournal.
  # If undefined (or "") it will connect to the default journal as specified by the
  # akka.persistence.journal.plugin property.
  write-plugin = ""
  
  # The LevelDB write journal is notifying the query side as soon as things
  # are persisted, but for efficiency reasons the query side retrieves the events 
  # in batches that sometimes can be delayed up to the configured `refresh-interval`.
  refresh-interval = 3s
  
  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = 100
}
#//#query-leveldb

akka.persistence.query.events-by-slice-firehose {
  class = "akka.persistence.query.typed.EventsBySliceFirehoseReadJournalProvider"

  # The identifier (config path) of the underlying EventsBySlice query plugin.
  # This must be defined by the application.
  delegate-query-plugin-id = ""

  # Buffer size of the BroadcastHub that will fan out the shared firehose stream
  # to attached consumer streams. If too small, some consumers may slow down other
  # consumers before the slow consumers have been aborted. If too large, it will
  # use more memory by holding more events in the buffer memory.
  # Must be a power of two and less than 4096.
  broadcast-buffer-size = 256

  # The shared firehose stream will be closed after this timeout when all consumer
  # streams have been closed. It will be started again when new consumers attach,
  # but there is some overhead of stopping and starting so it's good to keep it
  # around for a while. For example, keep around long enough to cover Projection
  # restarts.
  firehose-linger-timeout = 40s

  # When the catchup stream for a new consumer has caught up to the shared firehose
  # stream events will be retrieved from both during this time of overlap. The reason
  # is to ensure that no events are missed when switching over. After that,
  # the catchup stream will be closed. Time is based on the timestamps of the
  # EventEnvelope.
  catchup-overlap = 10s

  # Approximately number of entries of the deduplication cache.
  # During the overlap period events will be deduplicated by keeping track of emitted
  # persistenceId and seqNr.
  deduplication-capacity = 10000

  # Slow consumers are detected and aborted by a background task that is running
  # with this interval. Should be less than `slow-consumer-lag-threshold`.
  slow-consumer-reaper-interval = 2s

  # Slow consumer candidates are determined if the fastest consumer has a lag greater
  # than this duration, and the slow consumer is behind the fastest consumer by more
  # than half of the `broadcast-buffer-size`.
  # Slow consumers are then confirmed to be slow if they stay as such for at
  # least `abort-slow-consumer-after`.
  slow-consumer-lag-threshold = 5s

  # See `slow-consumer-lag-threshold`.
  # This duration is based on wall clock time.
  abort-slow-consumer-after = 2s

  # Provide a higher level of details in the debug logs, often per event. Be careful about enabling
  # in production systems.
  verbose-debug-logging = off
}

akka.actor {
  serializers {
    akka-persistence-query = "akka.persistence.query.internal.QuerySerializer"
  }
  serialization-bindings {
    "akka.persistence.query.typed.EventEnvelope" = akka-persistence-query
    "akka.persistence.query.Offset" = akka-persistence-query
  }
  serialization-identifiers {
    "akka.persistence.query.internal.QuerySerializer" = 39
  }
}




akka-persistence-testkit


copy
source
##################################################
# Akka Persistence Testkit Reference Config File #
##################################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

akka.persistence.testkit {

  # configuration for persistence testkit for events
  events {
    # enable serialization of the persisted events
    serialize = true
    # timeout for assertions
    assert-timeout = 3s
    # poll interval for assertions with timeout
    assert-poll-interval = 100millis
  }

  # configuration for persistence testkit for snapshots
  snapshots {
    # enable serialization of the persisted snapshots
    serialize = true
    # timeout for assertions
    assert-timeout = 3s
    # poll interval for assertions with timeout
    assert-poll-interval = 100millis
  }

}

akka.persistence.testkit.query {
  class = "akka.persistence.testkit.query.PersistenceTestKitReadJournalProvider"
}

akka.persistence.testkit.state {
  class = "akka.persistence.testkit.state.PersistenceTestKitDurableStateStoreProvider"
}




akka-remote artery


copy
source
#####################################
# Akka Remote Reference Config File #
#####################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# comments about akka.actor settings left out where they are already in akka-
# actor.jar, because otherwise they would be repeated in config rendering.

akka {

  actor {

    serializers {
      akka-containers = "akka.remote.serialization.MessageContainerSerializer"
      akka-misc = "akka.remote.serialization.MiscMessageSerializer"
      artery = "akka.remote.serialization.ArteryMessageSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      daemon-create = "akka.remote.serialization.DaemonMsgCreateSerializer"
      akka-system-msg = "akka.remote.serialization.SystemMessageSerializer"
    }

    serialization-bindings {
      "akka.actor.ActorSelectionMessage" = akka-containers

      "akka.remote.DaemonMsgCreate" = daemon-create

      "akka.remote.artery.ArteryMessage" = artery

      "akka.protobufv3.internal.GeneratedMessageV3" = proto

      # Since com.google.protobuf.Message does not extend Serializable but
      # GeneratedMessage does, need to use the more specific one here in order
      # to avoid ambiguity.
      # This com.google.protobuf serialization binding is only used if the class can be loaded,
      # i.e. com.google.protobuf dependency has been added in the application project.
      "com.google.protobuf.GeneratedMessage" = proto
      "com.google.protobuf.GeneratedMessageV3" = proto
      # optional binding if ScalaPb dependency is included
      "scalapb.GeneratedMessage" = proto

      "akka.actor.Identify" = akka-misc
      "akka.actor.ActorIdentity" = akka-misc
      "scala.Some" = akka-misc
      "scala.None$" = akka-misc
      "java.util.Optional" = akka-misc
      "akka.actor.Status$Success" = akka-misc
      "akka.actor.Status$Failure" = akka-misc
      "akka.actor.ActorRef" = akka-misc
      "akka.actor.PoisonPill$" = akka-misc
      "akka.actor.Kill$" = akka-misc
      "akka.remote.RemoteWatcher$Heartbeat$" = akka-misc
      "akka.remote.RemoteWatcher$HeartbeatRsp" = akka-misc
      "akka.Done" = akka-misc
      "akka.NotUsed" = akka-misc
      "akka.actor.Address" = akka-misc
      "akka.remote.UniqueAddress" = akka-misc

      "akka.actor.ActorInitializationException" = akka-misc
      "akka.actor.IllegalActorStateException" = akka-misc
      "akka.actor.ActorKilledException" = akka-misc
      "akka.actor.InvalidActorNameException" = akka-misc
      "akka.actor.InvalidMessageException" = akka-misc
      "java.util.concurrent.TimeoutException" = akka-misc
      "akka.remote.serialization.ThrowableNotSerializableException" = akka-misc

      "akka.actor.LocalScope$" = akka-misc
      "akka.remote.RemoteScope" = akka-misc

      "com.typesafe.config.impl.SimpleConfig" = akka-misc
      "com.typesafe.config.Config" = akka-misc

      "akka.routing.FromConfig" = akka-misc
      "akka.routing.DefaultResizer" = akka-misc
      "akka.routing.BalancingPool" = akka-misc
      "akka.routing.BroadcastGroup" = akka-misc
      "akka.routing.BroadcastPool" = akka-misc
      "akka.routing.RandomGroup" = akka-misc
      "akka.routing.RandomPool" = akka-misc
      "akka.routing.RoundRobinGroup" = akka-misc
      "akka.routing.RoundRobinPool" = akka-misc
      "akka.routing.ScatterGatherFirstCompletedGroup" = akka-misc
      "akka.routing.ScatterGatherFirstCompletedPool" = akka-misc
      "akka.routing.SmallestMailboxPool" = akka-misc
      "akka.routing.TailChoppingGroup" = akka-misc
      "akka.routing.TailChoppingPool" = akka-misc
      "akka.remote.routing.RemoteRouterConfig" = akka-misc

      "akka.pattern.StatusReply" = akka-misc

      "akka.dispatch.sysmsg.SystemMessage" = akka-system-msg

      # Java Serializer is by default used for exceptions and will by default
      # not be allowed to be serialized, but in certain cases they are replaced
      # by `akka.remote.serialization.ThrowableNotSerializableException` if
      # no specific serializer has been defined:
      # - when wrapped in `akka.actor.Status.Failure` for ask replies
      # - when wrapped in system messages for exceptions from remote deployed child actors
      #
      # It's recommended that you implement custom serializer for exceptions that are
      # sent remotely, You can add binding to akka-misc (MiscMessageSerializer) for the
      # exceptions that have a constructor with single message String or constructor with
      # message String as first parameter and cause Throwable as second parameter. Note that it's not
      # safe to add this binding for general exceptions such as IllegalArgumentException
      # because it may have a subclass without required constructor.
      "java.lang.Throwable" = java
    }

    serialization-identifiers {
      "akka.remote.serialization.ProtobufSerializer" = 2
      "akka.remote.serialization.DaemonMsgCreateSerializer" = 3
      "akka.remote.serialization.MessageContainerSerializer" = 6
      "akka.remote.serialization.MiscMessageSerializer" = 16
      "akka.remote.serialization.ArteryMessageSerializer" = 17

      "akka.remote.serialization.SystemMessageSerializer" = 22

      # deprecated in 2.6.0, moved to akka-actor
      "akka.remote.serialization.LongSerializer" = 18
      # deprecated in 2.6.0, moved to akka-actor
      "akka.remote.serialization.IntSerializer" = 19
      # deprecated in 2.6.0, moved to akka-actor
      "akka.remote.serialization.StringSerializer" = 20
      # deprecated in 2.6.0, moved to akka-actor
      "akka.remote.serialization.ByteStringSerializer" = 21
    }

    deployment {

      default {

        # if this is set to a valid remote address, the named actor will be
        # deployed at that node e.g. "akka://sys@host:port"
        remote = ""

        target {

          # A list of hostnames and ports for instantiating the children of a
          # router
          #   The format should be on "akka://sys@host:port", where:
          #    - sys is the remote actor system name
          #    - hostname can be either hostname or IP address the remote actor
          #      should connect to
          #    - port should be the port for the remote server on the other node
          # The number of actor instances to be spawned is still taken from the
          # nr-of-instances setting as for local routers; the instances will be
          # distributed round-robin among the given nodes.
          nodes = []

        }
      }
    }
  }

  remote {

    # Using remoting directly is typically not desirable, so a warning will
    # be shown to make this clear. Set this setting to 'off' to suppress that
    # warning.
    warn-about-direct-use = on

    # If Cluster is not used, remote watch and deployment are disabled.
    # To optionally use them while not using Cluster, set to 'on'.
    use-unsafe-remote-features-outside-cluster = off

    # A warning will be logged on remote watch attempts if Cluster
    # is not in use and 'use-unsafe-remote-features-outside-cluster'
    # is 'off'. Set this to 'off' to suppress these.
    warn-unsafe-watch-outside-cluster = on

    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf
    # [Hayashibara et al]) used for remote death watch.
    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within
    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,
    # i.e. around 12.5 seconds with default settings.
    watch-failure-detector {

      # FQCN of the failure detector implementation.
      # It must implement akka.remote.FailureDetector and have
      # a public constructor with a com.typesafe.config.Config and
      # akka.actor.EventStream parameter.
      implementation-class = "akka.remote.PhiAccrualFailureDetector"

      # How often keep-alive heartbeat messages should be sent to each connection.
      heartbeat-interval = 1 s

      # Defines the failure detector threshold.
      # A low threshold is prone to generate many wrong suspicions but ensures
      # a quick detection in the event of a real crash. Conversely, a high
      # threshold generates fewer mistakes but needs more time to detect
      # actual crashes.
      threshold = 10.0

      # Number of the samples of inter-heartbeat arrival times to adaptively
      # calculate the failure timeout for connections.
      max-sample-size = 200

      # Minimum standard deviation to use for the normal distribution in
      # AccrualFailureDetector. Too low standard deviation might result in
      # too much sensitivity for sudden, but normal, deviations in heartbeat
      # inter arrival times.
      min-std-deviation = 100 ms

      # Number of potentially lost/delayed heartbeats that will be
      # accepted before considering it to be an anomaly.
      # This margin is important to be able to survive sudden, occasional,
      # pauses in heartbeat arrivals, due to for example garbage collect or
      # network drop.
      acceptable-heartbeat-pause = 10 s


      # How often to check for nodes marked as unreachable by the failure
      # detector
      unreachable-nodes-reaper-interval = 1s

      # After the heartbeat request has been sent the first failure detection
      # will start after this period, even though no heartbeat mesage has
      # been received.
      expected-response-after = 1 s

    }

    # remote deployment configuration section
    deployment {
      # deprecated, use `enable-allow-list`
      enable-whitelist = off

      # If true, will only allow specific classes listed in `allowed-actor-classes` to be instanciated on this
      # system via remote deployment
      enable-allow-list = ${akka.remote.deployment.enable-whitelist}


      # deprecated, use `allowed-actor-classes`
      whitelist = []

      allowed-actor-classes = ${akka.remote.deployment.whitelist}
    }

    ### Default dispatcher for the remoting subsystem
    default-remote-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 0.5
        parallelism-max = 16
      }
      throughput = 10
    }

    artery {

      # This flag disabled Artery in Akka 2.6.x and 2.7.x. If it is set to off with Akka 2.8.0 or later
      # an exception will be thrown at startup with the purpose to notify the user that Classic Remoting
      # has been removed.
      enabled = on

      # Select the underlying transport implementation.
      #
      # Possible values: aeron-udp, tcp, tls-tcp
      # See https://doc.akka.io/libraries/akka-core/current/remoting-artery.html#selecting-a-transport for the tradeoffs
      # for each transport
      transport = tcp

      # Canonical address is the address other clients should connect to.
      # Artery transport will expect messages to this address.
      canonical {

        # The default remote server port clients should connect to.
        # Default is 25520, use 0 if you want a random available port
        # This port needs to be unique for each actor system on the same machine.
        port = 25520

        # Hostname clients should connect to. Can be set to an ip, hostname
        # or one of the following special values:
        #   "<getHostAddress>"   InetAddress.getLocalHost.getHostAddress
        #   "<getHostName>"      InetAddress.getLocalHost.getHostName
        #
        hostname = "<getHostAddress>"
      }

      # Use these settings to bind a network interface to a different address
      # than artery expects messages at. This may be used when running Akka
      # nodes in a separated networks (under NATs or in containers). If canonical
      # and bind addresses are different, then network configuration that relays
      # communications from canonical to bind addresses is expected.
      bind {

        # Port to bind a network interface to. Can be set to a port number
        # of one of the following special values:
        #   0    random available port
        #   ""   akka.remote.artery.canonical.port
        #
        port = ""

        # Hostname to bind a network interface to. Can be set to an ip, hostname
        # or one of the following special values:
        #   "0.0.0.0"            all interfaces
        #   ""                   akka.remote.artery.canonical.hostname
        #   "<getHostAddress>"   InetAddress.getLocalHost.getHostAddress
        #   "<getHostName>"      InetAddress.getLocalHost.getHostName
        #
        hostname = ""

        # Time to wait for Aeron/TCP to bind
        bind-timeout = 3s
      }


      # Actor paths to use the large message stream for when a message
      # is sent to them over remoting. The large message stream dedicated
      # is separate from "normal" and system messages so that sending a
      # large message does not interfere with them.
      # Entries should be the full path to the actor. Wildcards in the form of "*"
      # can be supplied at any place and matches any name at that segment -
      # "/user/supervisor/actor/*" will match any direct child to actor,
      # while "/supervisor/*/child" will match any grandchild to "supervisor" that
      # has the name "child"
      # Entries have to be specified on both the sending and receiving side.
      # Messages sent to ActorSelections will not be passed through the large message
      # stream, to pass such messages through the large message stream the selections
      # but must be resolved to ActorRefs first.
      large-message-destinations = []

      # Enable untrusted mode, which discards inbound system messages, PossiblyHarmful and
      # ActorSelection messages. E.g. remote watch and remote deployment will not work.
      # ActorSelection messages can be enabled for specific paths with the trusted-selection-paths
      untrusted-mode = off

      # When 'untrusted-mode=on' inbound actor selections are by default discarded.
      # Actors with paths defined in this list are granted permission to receive actor
      # selections messages.
      # E.g. trusted-selection-paths = ["/user/receptionist", "/user/namingService"]
      trusted-selection-paths = []

      # If this is "on", all inbound remote messages will be logged at DEBUG level,
      # if off then they are not logged
      log-received-messages = off

      # If this is "on", all outbound remote messages will be logged at DEBUG level,
      # if off then they are not logged
      log-sent-messages = off

      # Logging of message types with payload size in bytes larger than
      # this value. Maximum detected size per message type is logged once,
      # with an increase threshold of 10%.
      # By default this feature is turned off. Activate it by setting the property to
      # a value in bytes, such as 1000b. Note that for all messages larger than this
      # limit there will be extra performance and scalability cost.
      log-frame-size-exceeding = off

      advanced {

        # Maximum serialized message size, including header data.
        maximum-frame-size = 256 KiB

        # Direct byte buffers are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-frame-size'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        buffer-pool-size = 128

        # Maximum serialized message size for the large messages, including header data.
        # If the value of akka.remote.artery.transport is set to aeron-udp, it is currently
        # restricted to 1/8th the size of a term buffer that can be configured by setting the
        # 'aeron.term.buffer.length' system property.
        # See 'large-message-destinations'.
        maximum-large-frame-size = 2 MiB

        # Direct byte buffers for the large messages are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-large-frame-size'.
        # See 'large-message-destinations'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        large-buffer-pool-size = 32

        # For enabling testing features, such as blackhole in akka-remote-testkit.
        test-mode = off

        # Settings for the materializer that is used for the remote streams.
        materializer = ${akka.stream.materializer}

        # Remoting will use the given dispatcher for the ordinary and large message
        # streams.
        use-dispatcher = "akka.remote.default-remote-dispatcher"

        # Remoting will use the given dispatcher for the control stream.
        # It can be good to not use the same dispatcher for the control stream as
        # the dispatcher for the ordinary message stream so that heartbeat messages
        # are not disturbed.
        use-control-stream-dispatcher = "akka.actor.internal-dispatcher"


        # Total number of inbound lanes, shared among all inbound associations. A value
        # greater than 1 means that deserialization can be performed in parallel for
        # different destination actors. The selection of lane is based on consistent
        # hashing of the recipient ActorRef to preserve message ordering per receiver.
        # Lowest latency can be achieved with inbound-lanes=1 because of one less
        # asynchronous boundary.
        inbound-lanes = 4

        # Number of outbound lanes for each outbound association. A value greater than 1
        # means that serialization and other work can be performed in parallel for different
        # destination actors. The selection of lane is based on consistent hashing of the
        # recipient ActorRef to preserve message ordering per receiver. Note that messages
        # for different destination systems (hosts) are handled by different streams also
        # when outbound-lanes=1. Lowest latency can be achieved with outbound-lanes=1
        # because of one less asynchronous boundary.
        outbound-lanes = 1

        # Size of the send queue for outgoing messages. Messages will be dropped if
        # the queue becomes full. This may happen if you send a burst of many messages
        # without end-to-end flow control. Note that there is one such queue per
        # outbound association. The trade-off of using a larger queue size is that
        # it consumes more memory, since the queue is based on preallocated array with
        # fixed size.
        outbound-message-queue-size = 3072

        # Size of the send queue for outgoing control messages, such as system messages.
        # If this limit is reached the remote system is declared to be dead and its UID
        # marked as quarantined. Note that there is one such queue per outbound association.
        # It is a linked queue so it will not use more memory than needed but by increasing
        # too much you may risk OutOfMemoryError in the worst case.
        outbound-control-queue-size = 20000

        # Size of the send queue for outgoing large messages. Messages will be dropped if
        # the queue becomes full. This may happen if you send a burst of many messages
        # without end-to-end flow control. Note that there is one such queue per
        # outbound association.
        # It is a linked queue so it will not use more memory than needed but by increasing
        # too much you may risk OutOfMemoryError, especially since the message payload
        # of these messages may be large.
        outbound-large-message-queue-size = 256

        # This setting defines the maximum number of unacknowledged system messages
        # allowed for a remote system. If this limit is reached the remote system is
        # declared to be dead and its UID marked as quarantined.
        system-message-buffer-size = 20000

        # unacknowledged system messages are re-delivered with this interval
        system-message-resend-interval = 1 second



        # The timeout for outbound associations to perform the initial handshake.
        # This timeout must be greater than the 'image-liveness-timeout' when
        # transport is aeron-udp.
        handshake-timeout = 20 seconds

        # incomplete initial handshake attempt is retried with this interval
        handshake-retry-interval = 1 second

        # Handshake requests are performed periodically with this interval,
        # also after the handshake has been completed to be able to establish
        # a new session with a restarted destination system.
        inject-handshake-interval = 1 second


        # System messages that are not acknowledged after re-sending for this period are
        # dropped and will trigger quarantine. The value should be longer than the length
        # of a network partition that you need to survive.
        give-up-system-message-after = 6 hours

        # Outbound streams are stopped when they haven't been used for this duration.
        # They are started again when new messages are sent.
        stop-idle-outbound-after = 5 minutes

        # Outbound streams are quarantined when they haven't been used for this duration
        # to cleanup resources used by the association, such as compression tables.
        # This will cleanup association to crashed systems that didn't announce their
        # termination.
        # The value should be longer than the length of a network partition that you
        # need to survive.
        # The value must also be greater than stop-idle-outbound-after.
        # Once every 1/10 of this duration an extra handshake message will be sent.
        # Therfore it's also recommended to use a value that is greater than 10 times
        # the stop-idle-outbound-after, since otherwise the idle streams will not be
        # stopped.
        quarantine-idle-outbound-after = 6 hours

        # Stop outbound stream of a quarantined association after this idle timeout, i.e.
        # when not used any more.
        stop-quarantined-after-idle = 3 seconds

        # After catastrophic communication failures that could result in the loss of system
        # messages or after the remote DeathWatch triggers the remote system gets
        # quarantined to prevent inconsistent behavior.
        # This setting controls how long the quarantined association will be kept around
        # before being removed to avoid long-term memory leaks. It must be quarantined
        # and also unused for this duration before it's removed. When removed the historical
        # information about which UIDs that were quarantined for that hostname:port is
        # gone which could result in communication with a previously quarantined node
        # if it wakes up again. Therfore this shouldn't be set too low.
        remove-quarantined-association-after = 1 h

        # during ActorSystem termination the remoting will wait this long for
        # an acknowledgment by the destination system that flushing of outstanding
        # remote messages has been completed
        shutdown-flush-timeout = 1 second

        # Before sending notificaiton of terminated actor (DeathWatchNotification) other messages
        # will be flushed to make sure that the Terminated message arrives after other messages.
        # It will wait this long for the flush acknowledgement before continuing.
        # The flushing can be disabled by setting this to `off`.
        death-watch-notification-flush-timeout = 3 seconds

        # See 'inbound-max-restarts'
        inbound-restart-timeout = 5 seconds

        # Max number of restarts within 'inbound-restart-timeout' for the inbound streams.
        # If more restarts occurs the ActorSystem will be terminated.
        inbound-max-restarts = 5

        # Retry outbound connection after this backoff.
        # Only used when transport is tcp or tls-tcp.
        outbound-restart-backoff = 1 second

        # See 'outbound-max-restarts'
        outbound-restart-timeout = 5 seconds

        # Max number of restarts within 'outbound-restart-timeout' for the outbound streams.
        # If more restarts occurs the ActorSystem will be terminated.
        outbound-max-restarts = 5

        # compression of common strings in remoting messages, like actor destinations, serializers etc
        compression {

          actor-refs {
            # Max number of compressed actor-refs
            # Note that compression tables are "rolling" (i.e. a new table replaces the old
            # compression table once in a while), and this setting is only about the total number
            # of compressions within a single such table.
            # Must be a positive natural number. Can be disabled with "off".
            max = 256

            # interval between new table compression advertisements.
            # this means the time during which we collect heavy-hitter data and then turn it into a compression table.
            advertisement-interval = 1 minute
          }
          manifests {
            # Max number of compressed manifests
            # Note that compression tables are "rolling" (i.e. a new table replaces the old
            # compression table once in a while), and this setting is only about the total number
            # of compressions within a single such table.
            # Must be a positive natural number. Can be disabled with "off".
            max = 256

            # interval between new table compression advertisements.
            # this means the time during which we collect heavy-hitter data and then turn it into a compression table.
            advertisement-interval = 1 minute
          }
        }

        # List of fully qualified class names of remote instruments which should
        # be initialized and used for monitoring of remote messages.
        # The class must extend akka.remote.artery.RemoteInstrument and
        # have a public constructor with empty parameters or one ExtendedActorSystem
        # parameter.
        # A new instance of RemoteInstrument will be created for each encoder and decoder.
        # It's only called from the stage, so if it dosn't delegate to any shared instance
        # it doesn't have to be thread-safe.
        # Refer to `akka.remote.artery.RemoteInstrument` for more information.
        instruments = ${?akka.remote.artery.advanced.instruments} []

        # Only used when transport is aeron-udp
        aeron {
          # Only used when transport is aeron-udp.
          log-aeron-counters = false

          # Controls whether to start the Aeron media driver in the same JVM or use external
          # process. Set to 'off' when using external media driver, and then also set the
          # 'aeron-dir'.
          # Only used when transport is aeron-udp.
          embedded-media-driver = on

          # Directory used by the Aeron media driver. It's mandatory to define the 'aeron-dir'
          # if using external media driver, i.e. when 'embedded-media-driver = off'.
          # Embedded media driver will use a this directory, or a temporary directory if this
          # property is not defined (empty).
          # Only used when transport is aeron-udp.
          aeron-dir = ""

          # Whether to delete aeron embedded driver directory upon driver stop.
          # Only used when transport is aeron-udp.
          delete-aeron-dir = yes

          # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
          # The tradeoff is that to have low latency more CPU time must be used to be
          # able to react quickly on incoming messages or send as fast as possible after
          # backoff backpressure.
          # Level 1 strongly prefer low CPU consumption over low latency.
          # Level 10 strongly prefer low latency over low CPU consumption.
          # Only used when transport is aeron-udp.
          idle-cpu-level = 5

          # messages that are not accepted by Aeron are dropped after retrying for this period
          # Only used when transport is aeron-udp.
          give-up-message-after = 60 seconds

          # Timeout after which aeron driver has not had keepalive messages
          # from a client before it considers the client dead.
          # Only used when transport is aeron-udp.
          client-liveness-timeout = 20 seconds

          # Timout after after which an uncommitted publication will be unblocked
          # Only used when transport is aeron-udp.
          publication-unblock-timeout = 40 seconds

          # Timeout for each the INACTIVE and LINGER stages an aeron image
          # will be retained for when it is no longer referenced.
          # This timeout must be less than the 'handshake-timeout'.
          # Only used when transport is aeron-udp.
          image-liveness-timeout = 10 seconds

          # Timeout after which the aeron driver is considered dead
          # if it does not update its C'n'C timestamp.
          # Only used when transport is aeron-udp.
          driver-timeout = 20 seconds
        }

        # Only used when transport is tcp or tls-tcp.
        tcp {
          # Timeout of establishing outbound connections.
          connection-timeout = 5 seconds

          # The local address that is used for the client side of the TCP connection.
          outbound-client-hostname = ""
        }

      }

      # SSL configuration that is used when transport=tls-tcp.
      ssl {
        # Factory of SSLEngine.
        # Must implement akka.remote.artery.tcp.SSLEngineProvider and have a public
        # constructor with an ActorSystem parameter.
        # The default ConfigSSLEngineProvider is configured by properties in section
        # akka.remote.artery.ssl.config-ssl-engine
        ssl-engine-provider = akka.remote.artery.tcp.ConfigSSLEngineProvider

        # Config of akka.remote.artery.tcp.ConfigSSLEngineProvider
        config-ssl-engine {

          # This is the Java Key Store used by the server connection
          key-store = "keystore"

          # This password is used for decrypting the key store
          # Use substitution from environment variables for passwords. Don't define
          # real passwords in config files. key-store-password=${SSL_KEY_STORE_PASSWORD}
          key-store-password = "changeme"

          # This password is used for decrypting the key
          # Use substitution from environment variables for passwords. Don't define
          # real passwords in config files. key-password=${SSL_KEY_PASSWORD}
          key-password = "changeme"

          # This is the Java Key Store used by the client connection
          trust-store = "truststore"

          # This password is used for decrypting the trust store
          # Use substitution from environment variables for passwords. Don't define
          # real passwords in config files. trust-store-password=${SSL_TRUST_STORE_PASSWORD}
          trust-store-password = "changeme"

          # Protocol to use for SSL encryption.
          protocol = "TLSv1.2"

          # Example: ["TLS_DHE_RSA_WITH_AES_128_GCM_SHA256", 
          #   "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
          #   "TLS_DHE_RSA_WITH_AES_256_GCM_SHA384",
          #   "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"]
          # When doing rolling upgrades, make sure to include both the algorithm used 
          # by old nodes and the preferred algorithm.
          # If you use a JDK 8 prior to 8u161 you need to install
          # the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256.
          # More info here:
          # https://www.oracle.com/java/technologies/javase-jce-all-downloads.html
          enabled-algorithms = ["TLS_DHE_RSA_WITH_AES_256_GCM_SHA384",
            "TLS_RSA_WITH_AES_128_CBC_SHA"]

          # There are two options, and the default SecureRandom is recommended:
          # "" or "SecureRandom" => (default)
          # "SHA1PRNG" => Can be slow because of blocking issues on Linux
          #
          # Setting a value here may require you to supply the appropriate cipher
          # suite (see enabled-algorithms section above)
          random-number-generator = ""

          # Require mutual authentication between TLS peers
          #
          # Without mutual authentication only the peer that actively establishes a connection (TLS client side)
          # checks if the passive side (TLS server side) sends over a trusted certificate. With the flag turned on,
          # the passive side will also request and verify a certificate from the connecting peer.
          #
          # To prevent man-in-the-middle attacks this setting is enabled by default.
          require-mutual-authentication = on

          # Set this to `on` to verify hostnames with sun.security.util.HostnameChecker
          # If possible it is recommended to have this enabled. Hostname verification is designed for
          # situations where things locate each other by hostname, in scenarios where host names are dynamic
          # and not known up front it can make sense to have this disabled.
          hostname-verification = off
        }

        # Config of akka.remote.artery.tcp.ssl.RotatingKeysSSLEngineProvider
        # This engine provider reads PEM files from a mount point shared with the secret
        # manager. The constructed SSLContext is cached some time (configurable) so when
        # the credentials rotate the new credentials are eventually picked up.
        # By default mTLS is enabled.
        # This provider also includes a verification phase that runs after the TLS handshake
        # phase. In this verification, both peers run an authorization and verify they are
        # part of the same akka cluster. The verification happens via comparing the subject
        # names in the peer's certificate with the name on the own certificate so if you
        # use this SSLEngineProvider you should make sure all nodes on the cluster include
        # at least one common subject name (CN or SAN).
        # The Key setup this implementation supports has some limitations:
        #   1. the private key must be provided on a PKCS#1 or a non-encrypted PKCS#8 PEM-formatted file
        #   2. the private key must be be of an algorythm supported by `akka-pki` tools (e.g. "RSA", not "EC")
        #   3. the node certificate must be issued by a root CA (not an intermediate CA)
        #   4. both the node and the CA certificates must be provided in PEM-formatted files
        rotating-keys-engine {

          # This is a convention that people may follow if they wish to save themselves some configuration
          secret-mount-point = /var/run/secrets/akka-tls/rotating-keys-engine

          # The absolute path the PEM file with the private key.
          key-file = ${akka.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/tls.key
          # The absolute path to the PEM file of the certificate for the private key above.
          cert-file = ${akka.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/tls.crt
          # The absolute path to the PEM file of the certificate of the CA that emited
          # the node certificate above.
          ca-cert-file = ${akka.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/ca.crt

          # There are two options, and the default SecureRandom is recommended:
          # "" or "SecureRandom" => (default)
          # "SHA1PRNG" => Can be slow because of blocking issues on Linux
          #
          # Setting a value here may require you to supply the appropriate cipher
          # suite (see enabled-algorithms section)
          random-number-generator = ""

          # Example: ["TLS_DHE_RSA_WITH_AES_128_GCM_SHA256",
          #   "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
          #   "TLS_DHE_RSA_WITH_AES_256_GCM_SHA384",
          #   "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"]
          # If you use a JDK 8 prior to 8u161 you need to install
          # the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256.
          # More info here:
          # https://www.oracle.com/java/technologies/javase-jce-all-downloads.html
          enabled-algorithms = ["TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"]

          # Protocol to use for SSL encryption.
          protocol = "TLSv1.2"

          # How long should an SSLContext instance be cached. When rotating keys and certificates,
          # there must a time overlap between the old certificate/key and the new ones. The
          # value of this setting should be lower than duration of that overlap.
          ssl-context-cache-ttl = 5m
        }
      }
    }
  }

}




akka-testkit


copy
source
######################################
# Akka Testkit Reference Config File #
######################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

akka {
  test {
    # factor by which to scale timeouts during tests, e.g. to account for shared
    # build system load
    timefactor =  1.0

    # duration of EventFilter.intercept waits after the block is finished until
    # all required messages are received
    filter-leeway = 3s

    # duration to wait in expectMsg and friends outside of within() block
    # by default, will be dilated by the timefactor.
    single-expect-default = 3s

    # duration to wait in expectNoMessage by default,
    # will be dilated by the timefactor.
    expect-no-message-default = 100ms

    # The timeout that is added as an implicit by DefaultTimeout trait
    default-timeout = 5s

    calling-thread-dispatcher {
      type = akka.testkit.CallingThreadDispatcherConfigurator
    }
  }

  actor {

    serializers {
      java-test = "akka.testkit.TestJavaSerializer"
    }

    serialization-identifiers {
      "akka.testkit.TestJavaSerializer" = 23
    }

    serialization-bindings {
      "akka.testkit.JavaSerializable" = java-test
    }
  }
}




akka-cluster-metrics


copy
source
##############################################
# Akka Cluster Metrics Reference Config File #
##############################################

# This is the reference config file that contains all the default settings.
# Make your edits in your application.conf in order to override these settings.

# Sigar provisioning:
#
#  User can provision sigar classes and native library in one of the following ways:
# 
#  1) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as a project dependency for the user project.
#  Metrics extension will extract and load sigar library on demand with help of Kamon sigar provisioner.
# 
#  2) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as java agent: `java -javaagent:/path/to/sigar-loader.jar`
#  Kamon sigar loader agent will extract and load sigar library during JVM start.
# 
#  3) Place `sigar.jar` on the `classpath` and sigar native library for the o/s on the `java.library.path`
#  User is required to manage both project dependency and library deployment manually.

# Cluster metrics extension.
# Provides periodic statistics collection and publication throughout the cluster.
akka.cluster.metrics {
  # Full path of dispatcher configuration key.
  dispatcher = "akka.actor.default-dispatcher"
  # How long should any actor wait before starting the periodic tasks.
  periodic-tasks-initial-delay = 1s
  # Sigar native library extract location.
  # Use per-application-instance scoped location, such as program working directory.
  native-library-extract-folder = ${user.dir}"/native"
  # Metrics supervisor actor.
  supervisor {
    # Actor name. Example name space: /system/cluster-metrics
    name = "cluster-metrics"
    # Supervision strategy.
    strategy {
      #
      # FQCN of class providing `akka.actor.SupervisorStrategy`.
      # Must have a constructor with signature `<init>(com.typesafe.config.Config)`.
      # Default metrics strategy provider is a configurable extension of `OneForOneStrategy`.
      provider = "akka.cluster.metrics.ClusterMetricsStrategy"
      #
      # Configuration of the default strategy provider.
      # Replace with custom settings when overriding the provider.
      configuration = {
        # Log restart attempts.
        loggingEnabled = true
        # Child actor restart-on-failure window.
        withinTimeRange = 3s
        # Maximum number of restart attempts before child actor is stopped.
        maxNrOfRetries = 3
      }
    }
  }
  # Metrics collector actor.
  collector {
    # Enable or disable metrics collector for load-balancing nodes.
    # Metrics collection can also be controlled at runtime by sending control messages
    # to /system/cluster-metrics actor: `akka.cluster.metrics.{CollectionStartMessage,CollectionStopMessage}`
    enabled = on
    # FQCN of the metrics collector implementation.
    # It must implement `akka.cluster.metrics.MetricsCollector` and
    # have public constructor with akka.actor.ActorSystem parameter.
    # Will try to load in the following order of priority:
    # 1) configured custom collector 2) internal `SigarMetricsCollector` 3) internal `JmxMetricsCollector`
    provider = ""
    # Try all 3 available collector providers, or else fail on the configured custom collector provider.
    fallback = true
    # How often metrics are sampled on a node.
    # Shorter interval will collect the metrics more often.
    # Also controls frequency of the metrics publication to the node system event bus.
    sample-interval = 3s
    # How often a node publishes metrics information to the other nodes in the cluster.
    # Shorter interval will publish the metrics gossip more often.
    gossip-interval = 3s
    # How quickly the exponential weighting of past data is decayed compared to
    # new data. Set lower to increase the bias toward newer values.
    # The relevance of each data sample is halved for every passing half-life
    # duration, i.e. after 4 times the half-life, a data sampleâs relevance is
    # reduced to 6% of its original relevance. The initial relevance of a data
    # sample is given by 1 â 0.5 ^ (collect-interval / half-life).
    # See https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average
    moving-average-half-life = 12s
  }
}

# Cluster metrics extension serializers and routers.
akka.actor {
  # Protobuf serializer for remote cluster metrics messages.
  serializers {
    akka-cluster-metrics = "akka.cluster.metrics.protobuf.MessageSerializer"
  }
  # Interface binding for remote cluster metrics messages.
  serialization-bindings {
    "akka.cluster.metrics.ClusterMetricsMessage" = akka-cluster-metrics
    "akka.cluster.metrics.AdaptiveLoadBalancingPool" = akka-cluster-metrics
    "akka.cluster.metrics.MixMetricsSelector" = akka-cluster-metrics
    "akka.cluster.metrics.CpuMetricsSelector$" = akka-cluster-metrics
    "akka.cluster.metrics.HeapMetricsSelector$" = akka-cluster-metrics
    "akka.cluster.metrics.SystemLoadAverageMetricsSelector$" = akka-cluster-metrics
  }
  # Globally unique metrics extension serializer identifier.
  serialization-identifiers {
    "akka.cluster.metrics.protobuf.MessageSerializer" = 10
  }
  #  Provide routing of messages based on cluster metrics.
  router.type-mapping {
    cluster-metrics-adaptive-pool  = "akka.cluster.metrics.AdaptiveLoadBalancingPool"
    cluster-metrics-adaptive-group = "akka.cluster.metrics.AdaptiveLoadBalancingGroup"
  }
}




akka-cluster-tools


copy
source
############################################
# Akka Cluster Tools Reference Config File #
############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# //#pub-sub-ext-config
# Settings for the DistributedPubSub extension
akka.cluster.pub-sub {
  # Actor name of the mediator actor, /system/distributedPubSubMediator
  name = distributedPubSubMediator

  # Start the mediator on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # The routing logic to use for 'Send'
  # Possible values: random, round-robin, broadcast
  routing-logic = random

  # How often the DistributedPubSubMediator should send out gossip information
  gossip-interval = 1s

  # Removed entries are pruned after this duration
  removed-time-to-live = 120s

  # Maximum number of elements to transfer in one message when synchronizing the registries.
  # Next chunk will be transferred in next round of gossip.
  max-delta-elements = 3000

  # When a message is published to a topic with no subscribers send it to the dead letters.
  send-to-dead-letters-when-no-subscribers = on
  
  # The id of the dispatcher to use for DistributedPubSubMediator actors. 
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = "akka.actor.internal-dispatcher"
}
# //#pub-sub-ext-config

# Protobuf serializer for cluster DistributedPubSubMeditor messages
akka.actor {
  serializers {
    akka-pubsub = "akka.cluster.pubsub.protobuf.DistributedPubSubMessageSerializer"
  }
  serialization-bindings {
    "akka.cluster.pubsub.DistributedPubSubMessage" = akka-pubsub
    "akka.cluster.pubsub.DistributedPubSubMediator$Internal$SendToOneSubscriber" = akka-pubsub
  }
  serialization-identifiers {
    "akka.cluster.pubsub.protobuf.DistributedPubSubMessageSerializer" = 9
  }
}

# //#singleton-config
akka.cluster.singleton {
  # The actor name of the child singleton actor.
  singleton-name = "singleton"
  
  # Singleton among the nodes tagged with specified role.
  # If the role is not specified it's a singleton among all nodes in the cluster.
  role = ""
  
  # When a node is becoming oldest it sends hand-over request to previous oldest, 
  # that might be leaving the cluster. This is retried with this interval until 
  # the previous oldest confirms that the hand over has started or the previous 
  # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
  hand-over-retry-interval = 1s
  
  # The number of retries are derived from hand-over-retry-interval and
  # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
  # but it will never be less than this property.
  # After the hand over retries and it's still not able to exchange the hand over messages
  # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,
  # to start from a clean state. After that it will still not start the singleton instance
  # until the previous oldest node has been removed from the cluster.
  # On the other side, on the previous oldest node, the same number of retries - 3 are used
  # and after that the singleton instance is stopped.
  # For large clusters it might be necessary to increase this to avoid too early timeouts while
  # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios
  # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios
  # the recovery might be faster.
  min-number-of-hand-over-retries = 15

  # Config path of the lease to be taken before creating the singleton actor
  # if the lease is lost then the actor is restarted and it will need to re-acquire the lease
  # the default is no lease
  use-lease = ""

  # The interval between retries for acquiring the lease
  lease-retry-interval = 5s

  # Custom lease name. Note that if you have several singletons each one must have a unique
  # lease name, which can be defined with the leaseSettings of ClusterSingletonSettings.
  # If undefined it will be derived from ActorSystem name and singleton actor path,
  # but that may result in too long lease names.
  # Note that for typed cluster it is not possible to change this through configuration
  # any value here is ignored, custom names must be set through programmatic API.
  lease-name = ""
}
# //#singleton-config

# //#singleton-proxy-config
akka.cluster.singleton-proxy {
  # The actor name of the singleton actor that is started by the ClusterSingletonManager
  singleton-name = ${akka.cluster.singleton.singleton-name}
  
  # The role of the cluster nodes where the singleton can be deployed.
  # Corresponding to the role used by the `ClusterSingletonManager`. If the role is not
  # specified it's a singleton among all nodes in the cluster, and the `ClusterSingletonManager`
  # must then also be configured in same way.
  role = ""
  
  # Interval at which the proxy will try to resolve the singleton instance.
  singleton-identification-interval = 1s
  
  # If the location of the singleton is unknown the proxy will buffer this
  # number of messages and deliver them when the singleton is identified. 
  # When the buffer is full old messages will be dropped when new messages are
  # sent via the proxy.
  # Use 0 to disable buffering, i.e. messages will be dropped immediately if
  # the location of the singleton is unknown.
  # Maximum allowed buffer size is 10000.
  buffer-size = 1000 
}
# //#singleton-proxy-config

# Serializer for cluster ClusterSingleton messages
akka.actor {
  serializers {
    akka-singleton = "akka.cluster.singleton.protobuf.ClusterSingletonMessageSerializer"
  }
  serialization-bindings {
    "akka.cluster.singleton.ClusterSingletonMessage" = akka-singleton
  }
  serialization-identifiers {
    "akka.cluster.singleton.protobuf.ClusterSingletonMessageSerializer" = 14
  }
}




akka-cluster-sharding-typed


copy
source
# //#sharding-ext-config
# //#number-of-shards
akka.cluster.sharding {
  # Number of shards used by the default HashCodeMessageExtractor
  # when no other message extractor is defined. This value must be
  # the same for all nodes in the cluster and that is verified by
  # configuration check when joining. Changing the value requires
  # stopping all nodes in the cluster.
  number-of-shards = 1000
}
# //#number-of-shards
# //#sharding-ext-config


# //#sharded-daemon-process
akka.cluster.sharded-daemon-process {
  # Settings for the sharded daemon process internal usage of sharding are using the akka.cluste.sharding defaults.
  # Some of the settings can be overridden specifically for the sharded daemon process here. For example can the
  # `role` setting limit what nodes the daemon processes and the keep alive pingers will run on.
  # Some settings can not be changed (remember-entities and related settings, passivation, number-of-shards),
  # overriding those settings will be ignored.
  sharding = ${akka.cluster.sharding}

  # Each entity is pinged at this interval from a few nodes in the
  # cluster to trigger a start if it has stopped, for example during
  # rebalancing.
  # See also keep-alive-from-number-of-nodes and keep-alive-throttle-interval
  # Note: How the set of actors is kept alive may change in the future meaning this setting may go away.
  keep-alive-interval = 10s

  # Keep alive messages from this number of nodes.
  keep-alive-from-number-of-nodes = 3

  # Keep alive messages are sent with this delay between each message.
  keep-alive-throttle-interval = 100 ms
}
# //#sharded-daemon-process

akka.cluster.configuration-compatibility-check.checkers {
  akka-cluster-sharding-hash-extractor = "akka.cluster.sharding.typed.internal.JoinConfigCompatCheckerClusterSharding"
}

akka.actor {
  serializers {
    typed-sharding = "akka.cluster.sharding.typed.internal.ShardingSerializer"
  }
  serialization-identifiers {
    "akka.cluster.sharding.typed.internal.ShardingSerializer" = 25
  }
  serialization-bindings {
    "akka.cluster.sharding.typed.internal.ClusterShardingTypedSerializable" = typed-sharding
  }
}

akka.reliable-delivery {
  sharding {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 1000

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 60s

      # If no messages are sent to an entity within this duration the
      # ProducerController for that entity will be removed.
      cleanup-unused-after = 120s

      # In case ShardingConsumerController is stopped and there are pending
      # unconfirmed messages the ShardingConsumerController has to "wake up"
      # the consumer again by resending the first unconfirmed message.
      resend-first-unconfirmed-idle-timeout = 10s

      # Chunked messages not implemented for sharding yet. Override to not
      # propagate property from akka.reliable-delivery.producer-controller.
      chunk-large-messages = off
    }

    consumer-controller = ${akka.reliable-delivery.consumer-controller}
    consumer-controller {
      # Limit of how many messages that can be buffered before the
      # ShardingConsumerController is initialized by the Start message.
      buffer-size = 1000
    }
  }
}




akka-cluster-sharding


copy
source
###############################################
# Akka Cluster Sharding Reference Config File #
###############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.


# //#sharding-ext-config
# Settings for the ClusterShardingExtension
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = off

  # When 'remember-entities' is enabled and the state store mode is ddata this controls
  # how the remembered entities and shards are stored. Possible values are "eventsourced" and "ddata"
  # Default is ddata for backwards compatibility.
  remember-entities-store = "ddata"

  # Deprecated: use the `passivation.default-idle-strategy.idle-entity.timeout` setting instead.
  # Set this to a time duration to have sharding passivate entities when they have not
  # received any message in this length of time. Set to 'off' to disable.
  # It is always disabled if `remember-entities` is enabled.
  passivate-idle-entity-after = null

  # Automatic entity passivation settings.
  passivation {
    # If an entity doesn't stop itself from a passivation request it will be forcefully stopped
    # after this timeout.
    stop-timeout = 10s

    # Automatic passivation strategy to use.
    # Set to "none" or "off" to disable automatic passivation.
    # Set to "default-strategy" to switch to the recommended default strategy with an active entity limit.
    # See the strategy-defaults section for possible passivation strategy settings and default values.
    # Passivation strategies are always disabled if `remember-entities` is enabled.
    #
    # API MAY CHANGE: Configuration for passivation strategies, except default-idle-strategy,
    # may change after additional testing and feedback.
    strategy = "default-idle-strategy"

    # Default passivation strategy without active entity limit; time out idle entities after 2 minutes.
    default-idle-strategy {
      idle-entity.timeout = 120s
    }

    # Recommended default strategy for automatic passivation with an active entity limit.
    # Configured with an adaptive recency-based admission window, a frequency-based admission filter, and
    # a segmented least recently used (SLRU) replacement policy for the main active entity tracking.
    default-strategy {
      # Default limit of 100k active entities in a shard region (in a cluster node).
      active-entity-limit = 100000

      # Admission window with LRU policy and adaptive sizing, and a frequency sketch admission filter to the main area.
      admission {
        window {
          policy = least-recently-used
          optimizer = hill-climbing
        }
        filter = frequency-sketch
      }

      # Main area with segmented LRU replacement policy with an 80% "protected" level by default.
      replacement {
        policy = least-recently-used
        least-recently-used {
          segmented {
            levels = 2
            proportions = [0.2, 0.8]
          }
        }
      }
    }

    strategy-defaults {
      # Passivate entities when they have not received a message for a specified length of time.
      idle-entity {
        # Passivate idle entities after the timeout. Set to "none" or "off" to disable.
        timeout = none

        # Check idle entities every interval. Set to "default" to use half the timeout by default.
        interval = default
      }

      # Limit of active entities in a shard region.
      # Passivate entities when the number of active entities in a shard region reaches this limit.
      # The per-region limit is divided evenly among the active shards in a region.
      # Set to "none" or "off" to disable limit-based automatic passivation, to only use idle entity timeouts.
      active-entity-limit = none

      # Entity replacement settings, for when the active entity limit is reached.
      replacement {
        # Entity replacement policy to use when the active entity limit is reached. Possible values are:
        #   - "least-recently-used"
        #   - "most-recently-used"
        #   - "least-frequently-used"
        # Set to "none" or "off" to disable the replacement policy and ignore the active entity limit.
        policy = none

        # Least recently used entity replacement policy.
        least-recently-used {
          # Optionally use a "segmented" least recently used strategy.
          # Disabled when segmented.levels are set to "none" or "off".
          segmented {
            # Number of segmented levels.
            levels = none

            # Fractional proportions for the segmented levels.
            # If empty then segments are divided evenly by the number of levels.
            proportions = []
          }
        }

        # Most recently used entity replacement policy.
        most-recently-used {}

        # Least frequently used entity replacement policy.
        least-frequently-used {
          # New frequency counts will be "dynamically aged" when enabled.
          dynamic-aging = off
        }
      }

      # An optional admission area, with a window for newly and recently activated entities, and an admission filter
      # to determine whether a candidate should be admitted to the main area of the passivation strategy.
      admission {
        # An optional window area, where newly created entities will be admitted initially, and when evicted
        # from the window area have an opportunity to move to the main area based on the admission filter.
        window {
          # The initial sizing for the window area (if enabled), as a fraction of the total active entity limit.
          proportion = 0.01

          # The minimum adaptive sizing for the window area, as a fraction of the total active entity limit.
          # Only applies when an adaptive window optimizer is enabled.
          minimum-proportion = 0.01

          # The maximum adaptive sizing for the window area, as a fraction of the total active entity limit.
          # Only applies when an adaptive window optimizer is enabled.
          maximum-proportion = 1.0

          # Adaptive optimizer to use for dynamically resizing the window area. Possible values are:
          #   - "hill-climbing"
          # Set to "none" or "off" to disable adaptive sizing of the window area.
          optimizer = off

          # A window proportion optimizer using a simple hill-climbing algorithm.
          hill-climbing {
            # Multiplier of the active entity limit for how often (in accesses) to adjust the window proportion.
            adjust-multiplier = 10.0

            # The size of the initial step to take (also used when the climbing restarts).
            initial-step = 0.0625

            # A threshold for the change in active rate (hit rate) to restart climbing.
            restart-threshold = 0.05

            # The decay ratio applied on each climbing step.
            step-decay = 0.98
          }

          # Replacement policy to use for the window area.
          # Entities that are evicted from the window area may move to the main area, based on the admission filter.
          # Possible values are the same as for the main replacement policy.
          # Set to "none" or "off" to disable the window area.
          policy = none

          least-recently-used {
            segmented {
              levels = none
              proportions = []
            }
          }

          most-recently-used {}

          least-frequently-used {
            dynamic-aging = off
          }
        }

        # The admission filter for the main area of the passivation strategy. Possible values are:
        #   - "frequency-sketch"
        # Set to "none" or "off" to disable the admission filter and always admit to the main area.
        filter = none

        # An admission filter based on a frequency sketch (a variation of a count-min sketch).
        frequency-sketch {
          # The depth of the frequency sketch (the number of hash functions).
          depth = 4

          # The size of the frequency counters in bits: 2, 4, 8, 16, 32, or 64 bits.
          counter-bits = 4

          # Multiplier of the active entity limit for the width of the frequency sketch.
          width-multiplier = 4

          # Multiplier of the active entity limit for how often the reset operation of the frequency sketch is applied.
          reset-multiplier = 10.0
        }
      }
    }
  }

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  # Additionally, if an entity doesn't handle the stopMessage
  # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully
  #
  # Note that, by default, if running in Kubernetes, the default time from SIGTERM
  # (which triggers a rebalance) and SIGKILL (forcible stop of the process) is 30 seconds.
  # This delay is set by `terminationGracePeriodSeconds` in Kubernetes.
  #
  # In the shutdown case, coordinated shutdown (see `akka.coordinated-shutdown`) may
  # also stop the JVM before this timeout is hit.
  handoff-timeout = 60 s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10 s

  # If the shard is remembering entities and can't store state changes, it
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10 s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate, the entity will be restarted after this duration or when
  # the next message for it is received, whichever occurs first.
  entity-restart-backoff = 10 s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined,
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  journal-plugin-id = ""

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined,
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  snapshot-plugin-id = ""

  # Defines how the coordinator stores its state. Same is also used by the
  # shards for rememberEntities.
  # Valid values are "ddata" or "persistence".
  # "persistence" mode is deprecated
  state-store-mode = "ddata"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times. A snapshot trigger might be delayed if a batch of updates is processed.
  # Only used when state-store-mode=persistence
  snapshot-after = 1000

  # The shard deletes persistent events (messages and snapshots) after doing snapshot
  # keeping this number of old persistent batches.
  # Batch is of size `snapshot-after`.
  # When set to 0, after snapshot is successfully done, all events with equal or lower sequence number will be deleted.
  # Default value of 2 leaves last maximum 2*`snapshot-after` events and 3 snapshots (2 old ones + latest snapshot).
  # If larger than 0, one additional batch of journal messages is kept when state-store-mode=persistence to include messages from delayed snapshots.
  keep-nr-of-batches = 2

  # Settings for LeastShardAllocationStrategy.
  #
  # A new rebalance algorithm was included in Akka 2.6.10. It can reach optimal balance in
  # less rebalance rounds (typically 1 or 2 rounds). The amount of shards to rebalance in each
  # round can still be limited to make it progress slower. For backwards compatibility,
  # the new algorithm is not enabled by default. Enable the new algorithm by setting
  # `rebalance-absolute-limit` > 0, for example:
  # akka.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit=20
  # The new algorithm is recommended and will become the default in future versions of Akka.
  least-shard-allocation-strategy {
    # Maximum number of shards that will be rebalanced in one rebalance round.
    # The lower of this and `rebalance-relative-limit` will be used.
    rebalance-absolute-limit = 0

    # Maximum number of shards that will be rebalanced in one rebalance round.
    # Fraction of total number of (known) shards.
    # The lower of this and `rebalance-absolute-limit` will be used.
    rebalance-relative-limit = 0.1

    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
    # when rebalance-absolute-limit > 0.
    #
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    # The difference between number of shards in the region with most shards and
    # the region with least shards must be greater than (>) the `rebalanceThreshold`
    # for the rebalance to occur.
    # It is also the maximum number of shards that will start rebalancing per rebalance-interval
    # 1 gives the best distribution and therefore typically the best choice.
    # Increasing the threshold can result in quicker rebalance but has the
    # drawback of increased difference between number of shards (and therefore load)
    # on different nodes before rebalance will occur.
    rebalance-threshold = 1

    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
    # when rebalance-absolute-limit > 0.
    #
    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  external-shard-allocation-strategy {
    # How long to wait for the client to persist an allocation to ddata or get all shard locations
    client-timeout = 5s
  }

  # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)
  # and for a shard to get its state when remembered entities is enabled
  # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond
  # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening
  waiting-for-state-timeout = 2 s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # Also used as timeout for writes of remember entities when that is enabled
  updating-state-timeout = 5 s

  # Timeout to wait for querying all shards for a given `ShardRegion`.
  shard-region-query-timeout = 3 s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting and is applied per remembered shard starting up (not for
  # entire shard region). The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy.
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    # The frequency is per sharding region (entity type).
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  event-sourced-remember-entities-store {
    # When using remember entities and the event sourced remember entities store the batches
    # written to the store are limited by this number to avoid getting a too large event for
    # the journal to handle. If using long persistence ids you may have to increase this.
    max-updates-per-write = 100
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role" if
  # "akka.cluster.sharding.coordinator-singleton-role-override" is enabled. Disabling it will allow to
  # use separate nodes for the shard coordinator and the shards themselves.
  # A lease can be configured in these settings for the coordinator singleton
  coordinator-singleton = ${akka.cluster.singleton}


  # By default, the role for the coordinator singleton is the same as the role for the shards
  # "akka.cluster.sharding.role". Set this to off to use the role from
  # "akka.cluster.sharding.coordinator-singleton.role" for the coordinator singleton.
  coordinator-singleton-role-override = on

  coordinator-state {
    # State updates are required to be written to a majority of nodes plus this
    # number of additional nodes. Can also be set to "all" to require
    # writes to all nodes. The reason for write/read to more than majority
    # is to have more tolerance for membership changes between write and read.
    # The tradeoff of increasing this is that updates will be slower.
    # It is more important to increase the `read-majority-plus`.
    write-majority-plus = 3
    # State retrieval when ShardCoordinator is started is required to be read
    # from a majority of nodes plus this number of additional nodes. Can also
    # be set to "all" to require reads from all nodes. The reason for write/read
    # to more than majority is to have more tolerance for membership changes between
    # write and read.
    # The tradeoff of increasing this is that coordinator startup will be slower.
    read-majority-plus = 5
  }
  
  # Settings for the Distributed Data replicator. 
  # Same layout as akka.cluster.distributed-data.
  # The "role" of the distributed-data configuration is not used. The distributed-data
  # role will be the same as "akka.cluster.sharding.role".
  # Note that there is one Replicator per role and it's not possible
  # to have different distributed-data settings for different sharding entity types.
  # Only used when state-store-mode=ddata
  distributed-data = ${akka.cluster.distributed-data}
  distributed-data {
    # minCap parameter to MajorityWrite and MajorityRead consistency level.
    majority-min-cap = 5
    durable.keys = ["shard-*"]
    
    # When using many entities with "remember entities" the Gossip message
    # can become too large if including too many in same message. Limit to
    # the same number as the number of ORSet per shard.
    max-delta-elements = 5

    # ShardCoordinator is singleton running on oldest
    prefer-oldest = on
  }

  # The id of the dispatcher to use for ClusterSharding actors.
  # If specified, you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = "akka.actor.internal-dispatcher"

  # Config path of the lease that each shard must acquire before starting entity actors
  # default is no lease
  # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
  use-lease = ""

  # The interval between retries for acquiring the lease
  lease-retry-interval = 5s

  # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling
  # in production systems.
  verbose-debug-logging = off

  # Throw an exception if the internal state machine in the Shard actor does an invalid state transition.
  # Mostly for the Akka test suite. If off, the invalid transition is logged as a warning instead of throwing and
  # crashing the shard.
  fail-on-invalid-entity-state-transition = off

  # Healthcheck that can be used with Akka management health checks: https://doc.akka.io/libraries/akka-management/current/healthchecks.html
  healthcheck {
    # sharding names to check have registered with the coordinator for the health check to pass
    # once initial registration has taken place the health check always returns true to prevent the coordinator
    # moving making the health check of all nodes fail
    # by default no sharding instances are monitored
    names = []

    # Timeout for the local shard region to respond. This should be lower than your monitoring system's
    # timeout for health checks
    timeout = 5s

    # The health check is only performed during this duration after
    # the member is up. After that the sharding check will not be performed (always returns success).
    # The purpose is to wait for Cluster Sharding registration to complete on initial startup.
    # After that, in case of Sharding Coordinator movement or reachability we still want to be ready
    # because requests can typically be served without involving the coordinator.
    # Another reason is that when a new entity type is added in a rolling update we don't want to fail
    # the ready check forever, which would stall the rolling update. Sharding Coordinator is expected
    # run on the oldest member, but in this scenario that is in the old deployment hasn't started the
    # coordinator for that entity type.
    disabled-after = 10s
  }
}
# //#sharding-ext-config

# Enable health check by default for when Akka management is on the classpath
akka.management.health-checks.readiness-checks {
  sharding = "akka.cluster.sharding.ClusterShardingHealthCheck"
}

akka.cluster {
  configuration-compatibility-check {
    checkers {
      akka-cluster-sharding = "akka.cluster.sharding.JoinConfigCompatCheckSharding"
    }
  }
}

# Protobuf serializer for Cluster Sharding messages
akka.actor {
  serializers {
    akka-sharding = "akka.cluster.sharding.protobuf.ClusterShardingMessageSerializer"
  }
  serialization-bindings {
    "akka.cluster.sharding.ClusterShardingSerializable" = akka-sharding
  }
  serialization-identifiers {
    "akka.cluster.sharding.protobuf.ClusterShardingMessageSerializer" = 13
  }
}




akka-distributed-data


copy
source
##############################################
# Akka Distributed DataReference Config File #
##############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.


#//#distributed-data
# Settings for the DistributedData extension
akka.cluster.distributed-data {
  # Actor name of the Replicator actor, /system/ddataReplicator
  name = ddataReplicator

  # Replicas are running on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # How often the Replicator should send out gossip information
  gossip-interval = 2 s
  
  # How often the subscribers will be notified of changes, if any
  notify-subscribers-interval = 500 ms

  # Logging of data with payload size in bytes larger than
  # this value. Maximum detected size per key is logged once,
  # with an increase threshold of 10%.
  # It can be disabled by setting the property to off.
  log-data-size-exceeding = 10 KiB

  # Maximum number of entries to transfer in one round of gossip exchange when
  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.
  # The actual number of data entries in each Gossip message is dynamically
  # adjusted to not exceed the maximum remote message size (maximum-frame-size).
  max-delta-elements = 500
  
  # The id of the dispatcher to use for Replicator actors.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = "akka.actor.internal-dispatcher"

  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes. If this is set to 'off' the pruning feature will
  # be completely disabled.
  pruning-interval = 120 s
  
  # How long time it takes to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is 
  # unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes even though typical dissemination time
  # is shorter (grows logarithmic with number of nodes). There is no advantage of 
  # setting this too low. Setting it to large value will delay the pruning process.
  max-pruning-dissemination = 300 s
  
  # The markers of that pruning has been performed for a removed node are kept for this
  # time and thereafter removed. If and old data entry that was never pruned is somehow
  # injected and merged with existing data after this time the value will not be correct.
  # This would be possible (although unlikely) in the case of a long network partition.
  # It should be in the magnitude of hours. For durable data it is configured by 
  # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
 pruning-marker-time-to-live = 6 h
  
  # Serialized Write and Read messages are cached when they are sent to 
  # several nodes. If no further activity they are removed from the cache
  # after this duration.
  serializer-cache-time-to-live = 10s

  # Update and Get operations are sent to oldest nodes first.
  # This is useful together with Cluster Singleton, which is running on oldest nodes.
  prefer-oldest = off
  
  # Settings for delta-CRDT
  delta-crdt {
    # enable or disable delta-CRDT replication
    enabled = on
    
    # Some complex deltas grow in size for each update and above this
    # threshold such deltas are discarded and sent as full state instead.
    # This is number of elements or similar size hint, not size in bytes.
    max-delta-size = 50
  }

  # Map of keys and inactivity duration for entries that will automatically be removed
  # without tombstones when they have been inactive for the given duration.
  # Prefix matching is supported by using * at the end of a key.
  # Matching tombstones will also be removed after the expiry duration.
  expire-keys-after-inactivity {
    # Example syntax:
    # "key-1" = 10 minutes
    # "cache-*" = 2 minutes
  }
  
  durable {
    # List of keys that are durable. Prefix matching is supported by using * at the
    # end of a key.  
    keys = []
    
    # The markers of that pruning has been performed for a removed node are kept for this
    # time and thereafter removed. If and old data entry that was never pruned is
    # injected and merged with existing data after this time the value will not be correct.
    # This would be possible if replica with durable data didn't participate in the pruning
    # (e.g. it was shutdown) and later started after this time. A durable replica should not 
    # be stopped for longer time than this duration and if it is joining again after this
    # duration its data should first be manually removed (from the lmdb directory).
    # It should be in the magnitude of days. Note that there is a corresponding setting
    # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
    pruning-marker-time-to-live = 10 d
    
    # Fully qualified class name of the durable store actor. It must be a subclass
    # of akka.actor.Actor and handle the protocol defined in 
    # akka.cluster.ddata.DurableStore. The class must have a constructor with 
    # com.typesafe.config.Config parameter.
    store-actor-class = akka.cluster.ddata.LmdbDurableStore
    
    use-dispatcher = akka.cluster.distributed-data.durable.pinned-store
    
    pinned-store {
      executor = thread-pool-executor
      type = PinnedDispatcher
    }
    
    # Config for the LmdbDurableStore
    lmdb {
      # Directory of LMDB file. There are two options:
      # 1. A relative or absolute path to a directory that ends with 'ddata'
      #    the full name of the directory will contain name of the ActorSystem
      #    and its remote port.
      # 2. Otherwise the path is used as is, as a relative or absolute path to
      #    a directory.
      #
      # When running in production you may want to configure this to a specific
      # path (alt 2), since the default directory contains the remote port of the
      # actor system to make the name unique. If using a dynamically assigned 
      # port (0) it will be different each time and the previously stored data 
      # will not be loaded.
      dir = "ddata"
      
      # Size in bytes of the memory mapped file.
      map-size = 100 MiB
      
      # Accumulate changes before storing improves performance with the
      # risk of losing the last writes if the JVM crashes.
      # The interval is by default set to 'off' to write each update immediately.
      # Enabling write behind by specifying a duration, e.g. 200ms, is especially 
      # efficient when performing many writes to the same key, because it is only 
      # the last value for each key that will be serialized and stored.  
      # write-behind-interval = 200 ms
      write-behind-interval = off
    }
  }
  
}
#//#distributed-data

# Protobuf serializer for cluster DistributedData messages
akka.actor {
  serializers {
    akka-data-replication = "akka.cluster.ddata.protobuf.ReplicatorMessageSerializer"
    akka-replicated-data = "akka.cluster.ddata.protobuf.ReplicatedDataSerializer"
  }
  serialization-bindings {
    "akka.cluster.ddata.Replicator$ReplicatorMessage" = akka-data-replication
    "akka.cluster.ddata.ReplicatedDataSerialization" = akka-replicated-data
  }
  serialization-identifiers {
    "akka.cluster.ddata.protobuf.ReplicatedDataSerializer" = 11
    "akka.cluster.ddata.protobuf.ReplicatorMessageSerializer" = 12
  }
}




akka-stream


copy
source
#####################################
# Akka Stream Reference Config File #
#####################################

# eager creation of the system wide materializer
akka.library-extensions += "akka.stream.SystemMaterializer$"
akka {
  stream {

    # Default materializer settings
    materializer {

      # Initial size of buffers used in stream elements
      initial-input-buffer-size = 4
      # Maximum size of buffers used in stream elements
      max-input-buffer-size = 16

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by ActorMaterializer when creating Actors.
      dispatcher = "akka.actor.default-dispatcher"

      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      # defaults to the single consumer mailbox for better performance.
      mailbox {
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
      }

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by stream operators that
      # perform blocking operations
      blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # Cleanup leaked publishers and subscribers when they are not used within a given
      # deadline
      subscription-timeout {
        # when the subscription timeout is reached one of the following strategies on
        # the "stale" publisher:
        # cancel - cancel it (via `onError` or subscribing to the publisher and
        #          `cancel()`ing the subscription right away
        # warn   - log a warning statement about the stale element (then drop the
        #          reference to it)
        # noop   - do nothing (not recommended)
        mode = cancel

        # time after which a subscriber / publisher is considered stale and eligible
        # for cancelation (see `akka.stream.subscription-timeout.mode`)
        timeout = 5s
      }

      # Enable additional troubleshooting logging at DEBUG log level
      debug-logging = off

      # Log any stream stage error at the specified log level: "error", "warning", "info", "debug" or "off".
      # If there is an `akka.stream.Attributes.LogLevels` attribute defined for a specific stream this value is ignored
      # and the `onFailure` value of the attribute is applied instead.
      stage-errors-default-log-level = error

      # Maximum number of elements emitted in batch if downstream signals large demand
      output-burst-limit = 1000

      # Enable automatic fusing of all graphs that are run. For short-lived streams
      # this may cause an initial runtime overhead, but most of the time fusing is
      # desirable since it reduces the number of Actors that are created.
      # Deprecated, since Akka 2.5.0, setting does not have any effect.
      auto-fusing = on

      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
      # buffer upon stream materialization if the requested buffer size is less than this
      # configuration parameter. The default is very high because failing early is better
      # than failing under load.
      #
      # Buffers sized larger than this will dynamically grow/shrink and consume more memory
      # per element than the fixed size buffers.
      max-fixed-buffer-size = 1000000000

      # Maximum number of sync messages that actor can process for stream to substream communication.
      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.
      # Allows to accelerate message processing that happening within same actor but keep system responsive.
      sync-processing-limit = 1000

      debug {
        # Enables the fuzzing mode which increases the chance of race conditions
        # by aggressively reordering events and making certain operations more
        # concurrent than usual.
        # This setting is for testing purposes, NEVER enable this in a production
        # environment!
        # To get the best results, try combining this setting with a throughput
        # of 1 on the corresponding dispatchers.
        fuzzing-mode = off
      }

      io.tcp {
        # The outgoing bytes are accumulated in a buffer while waiting for acknowledgment
        # of pending write. This improves throughput for small messages (frames) without
        # sacrificing latency. While waiting for the ack the stage will eagerly pull
        # from upstream until the buffer exceeds this size. That means that the buffer may hold
        # slightly more bytes than this limit (at most one element more). It can be set to 0
        # to disable the usage of the buffer.
        write-buffer-size = 16 KiB

        # In addition to the buffering described for property write-buffer-size, try to collect
        # more consecutive writes from the upstream stream producers.
        #
        # The rationale is to increase write efficiency by avoiding separate small 
        # writes to the network which is expensive to do. Merging those writes together
        # (up to `write-buffer-size`) improves throughput for small writes.
        #
        # The idea is that a running stream may produce multiple small writes consecutively
        # in one go without waiting for any external input. To probe the stream for
        # data, this features delays sending a write immediately by probing the stream
        # for more writes. This works by rescheduling the TCP connection stage via the
        # actor mailbox of the underlying actor. Thus, before the stage is reactivated
        # the upstream gets another opportunity to emit writes.
        #
        # When the stage is reactivated and if new writes are detected another round-trip
        # is scheduled. The loop repeats until either the number of round trips given in this
        # setting is reached, the buffer reaches `write-buffer-size`, or no new writes
        # were detected during the last round-trip.
        #
        # This mechanism ensures that a write is guaranteed to be sent when the remaining stream
        # becomes idle waiting for external signals.
        #
        # In most cases, the extra latency this mechanism introduces should be negligible,
        # but depending on the stream setup it may introduce a noticeable delay,
        # if the upstream continuously produces small amounts of writes in a
        # blocking (CPU-bound) way.
        #
        # In that case, the feature can either be disabled, or the producing CPU-bound
        # work can be taken off-stream to avoid excessive delays (e.g. using `mapAsync` instead of `map`).
        #
        # A value of 0 disables this feature.
        coalesce-writes = 10
      }

      # Time to wait for async materializer creation before throwing an exception
      creation-timeout = 20 seconds

      //#stream-ref
      # configure defaults for SourceRef and SinkRef
      stream-ref {
        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref
        #
        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
        # because the delay of requesting over network boundaries is much higher.
        buffer-capacity = 32

        # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
        #
        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
        #
        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
        demand-redelivery-interval = 1 second

        # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
        # This timeout does not have to be very low in normal situations, since the remote side may also need to
        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
        # in-active streams which are never subscribed to.
        subscription-timeout = 30 seconds

        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed
        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.
        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the
        # other side of the stream ref would never send the "final" terminal message.
        #
        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef
        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.
        final-termination-signal-deadline = 2 seconds
      }
      //#stream-ref
    }

    # Deprecated, left here to not break Akka HTTP which refers to it
    blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

    # Deprecated, will not be used unless user code refer to it, use 'akka.stream.materializer.blocking-io-dispatcher'
    # instead, or if from code, prefer the 'ActorAttributes.IODispatcher' attribute
    default-blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"
  }

  # configure overrides to ssl-configuration here (to be used by akka-streams, and akka-http â i.e. when serving https connections)
  ssl-config {
    protocol = "TLSv1.2"
  }

  actor {

    serializers {
      akka-stream-ref = "akka.stream.serialization.StreamRefSerializer"
    }

    serialization-bindings {
      "akka.stream.SinkRef"                           = akka-stream-ref
      "akka.stream.SourceRef"                         = akka-stream-ref
      "akka.stream.impl.streamref.StreamRefsProtocol" = akka-stream-ref
    }

    serialization-identifiers {
      "akka.stream.serialization.StreamRefSerializer" = 30
    }
  }
}

# ssl configuration
# folded in from former ssl-config-akka module
ssl-config {
  logger = "com.typesafe.sslconfig.akka.util.AkkaLoggerBridge"
}




akka-stream-testkit


copy
source
akka.stream.testkit {
  all-stages-stopped-timeout = 5 s
}














 
Configuration






Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/jmm.html
Akka and the Java Memory Model • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model




The Java Memory Model


Actors and the Java Memory Model


Futures and the Java Memory Model


Actors and shared mutable state




Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model




The Java Memory Model


Actors and the Java Memory Model


Futures and the Java Memory Model


Actors and shared mutable state




Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Akka and the Java Memory Model


A major benefit of using the Lightbend Platform, including Scala and Akka, is that it simplifies the process of writing concurrent software. This article discusses how the Lightbend Platform, and Akka in particular, approaches shared memory in concurrent applications.


The Java Memory Model


Prior to Java 5, the Java Memory Model (JMM) was ill defined. It was possible to get all kinds of strange results when shared memory was accessed by multiple threads, such as:




a thread not seeing values written by other threads: a visibility problem


a thread observing ‘impossible’ behavior of other threads, caused by instructions not being executed in the order expected: an instruction reordering problem.




With the implementation of JSR 133 in Java 5, a lot of these issues have been resolved. The JMM is a set of rules based on the “happens-before” relation, which constrain when one memory access must happen before another, and conversely, when they are allowed to happen out of order. Two examples of these rules are:




The monitor lock rule:
 a release of a lock happens before every subsequent acquire of the same lock.


The volatile variable rule:
 a write of a volatile variable happens before every subsequent read of the same volatile variable




Although the JMM can seem complicated, the specification tries to find a balance between ease of use and the ability to write performant and scalable concurrent data structures.


Actors and the Java Memory Model


With the Actors implementation in Akka, there are two ways multiple threads can execute actions on shared memory:




if a message is sent to an actor (e.g. by another actor). In most cases messages are immutable, but if that message is not a properly constructed immutable object, without a “happens before” rule, it would be possible for the receiver to see partially initialized data structures and possibly even values out of thin air (longs/doubles).


if an actor makes changes to its internal state while processing a message, and accesses that state while processing another message moments later. It is important to realize that with the actor model you don’t get any guarantee that the same thread will be executing the same actor for different messages.




To prevent visibility and reordering problems on actors, Akka guarantees the following two “happens before” rules:




The actor send rule:
 the send of the message to an actor happens before the receive of that message by the same actor.


The actor subsequent processing rule:
 processing of one message happens before processing of the next message by the same actor.


Note


In layman’s terms this means that changes to internal fields of the actor are visible when the next message is processed by that actor. So fields in your actor need not be volatile or equivalent.


Both rules only apply for the same actor instance and are not valid if different actors are used.


Futures and the Java Memory Model


The completion of a Future “happens before” the invocation of any callbacks registered to it are executed.


We recommend not to close over non-final fields (final in Java and val in Scala), and if you 
do
 choose to close over non-final fields, they must be marked 
volatile
 in order for the current value of the field to be visible to the callback.


If you close over a reference, you must also ensure that the instance that is referred to is thread safe. We highly recommend staying away from objects that use locking, since it can introduce performance problems and in the worst case, deadlocks. Such are the perils of synchronized.




Actors and shared mutable state


Since Akka runs on the JVM there are still some rules to be followed.


Most importantly, you must not close over internal Actor state and exposing it to other threads:




Scala




copy
source
class MyActor(context: ActorContext[MyActor.Command]) extends AbstractBehavior[MyActor.Command](context) {
  import MyActor._

  var state = ""
  val mySet = mutable.Set[String]()

  def onMessage(cmd: MyActor.Command) = cmd match {
    case Message(text, otherActor) =>
      // Very bad: shared mutable object allows
      // the other actor to mutate your own state,
      // or worse, you might get weird race conditions
      otherActor ! mySet

      implicit val ec = context.executionContext

      // Example of incorrect approach
      // Very bad: shared mutable state will cause your
      // application to break in weird ways
      Future { state = "This will race" }

      // Example of incorrect approach
      // Very bad: shared mutable state will cause your
      // application to break in weird ways
      expensiveCalculation().foreach { result =>
        state = s"new state: $result"
      }

      // Example of correct approach
      // Turn the future result into a message that is sent to
      // self when future completes
      val futureResult = expensiveCalculation()
      context.pipeToSelf(futureResult) {
        case Success(result) => UpdateState(result)
        case Failure(ex)     => throw ex
      }

      // Another example of incorrect approach
      // mutating actor state from ask future callback
      import akka.actor.typed.scaladsl.AskPattern._
      implicit val timeout: Timeout = 5.seconds // needed for `ask` below
      implicit val scheduler = context.system.scheduler
      val future: Future[String] = otherActor.ask(Query(_))
      future.foreach { result =>
        state = result
      }

      // use context.ask instead, turns the completion
      // into a message sent to self
      context.ask[Query, String](otherActor, Query(_)) {
        case Success(result) => UpdateState(result)
        case Failure(ex)     => throw ex
      }
      this

    case UpdateState(newState) =>
      // safe as long as `newState` is immutable, if it is mutable we'd need to
      // make a defensive copy
      state = newState
      this
  }
}


Java




copy
source
class MyActor extends AbstractBehavior<MyActor.Command> {

  interface Command {}

  class Message implements Command {
    public final ActorRef<Object> otherActor;

    public Message(ActorRef<Object> replyTo) {
      this.otherActor = replyTo;
    }
  }

  class UpdateState implements Command {
    public final String newState;

    public UpdateState(String newState) {
      this.newState = newState;
    }
  }

  private String state = "";
  private Set<String> mySet = new HashSet<>();

  public MyActor(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Message.class, this::onMessage)
        .onMessage(UpdateState.class, this::onUpdateState)
        .build();
  }

  private Behavior<Command> onMessage(Message message) {
    // Very bad: shared mutable object allows
    // the other actor to mutate your own state,
    // or worse, you might get weird race conditions
    message.otherActor.tell(mySet);

    // Example of incorrect approach
    // Very bad: shared mutable state will cause your
    // application to break in weird ways
    CompletableFuture.runAsync(
        () -> {
          state = "This will race";
        });

    // Example of incorrect approach
    // Very bad: shared mutable state will cause your
    // application to break in weird ways
    expensiveCalculation()
        .whenComplete(
            (result, failure) -> {
              if (result != null) state = "new state: " + result;
            });

    // Example of correct approach
    // Turn the future result into a message that is sent to
    // self when future completes
    CompletableFuture<String> futureResult = expensiveCalculation();
    getContext()
        .pipeToSelf(
            futureResult,
            (result, failure) -> {
              if (result != null) return new UpdateState(result);
              else throw new RuntimeException(failure);
            });

    // Another example of incorrect approach
    // mutating actor state from ask future callback
    CompletionStage<String> response =
        AskPattern.ask(
            message.otherActor,
            Query::new,
            Duration.ofSeconds(3),
            getContext().getSystem().scheduler());
    response.whenComplete(
        (result, failure) -> {
          if (result != null) state = "new state: " + result;
        });

    // use context.ask instead, turns the completion
    // into a message sent to self
    getContext()
        .ask(
            String.class,
            message.otherActor,
            Duration.ofSeconds(3),
            Query::new,
            (result, failure) -> {
              if (result != null) return new UpdateState(result);
              else throw new RuntimeException(failure);
            });
    return this;
  }

  private Behavior<Command> onUpdateState(UpdateState command) {
    // safe as long as `newState` is immutable, if it is mutable we'd need to
    // make a defensive copy
    this.state = command.newState;
    return this;
  }
}






Messages 
should
 be immutable, this is to avoid the shared mutable state trap.
















 
Location Transparency






Message Delivery Reliability 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/other-modules.html
Other Akka modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic




















Other Akka modules


This page describes modules that compliment libraries from the Akka core. See 
this overview
 instead for a guide on the core modules.


Akka HTTP


A full server- and client-side HTTP stack on top of akka-actor and akka-stream.


Akka gRPC


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams.


Alpakka


Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka.


Alpakka Kafka Connector


The Alpakka Kafka Connector connects Apache Kafka with Akka Streams.


Akka Projections


Akka Projections let you process a stream of events or records from a source to a projected model or external system.


Cassandra Plugin for Akka Persistence


An Akka Persistence journal and snapshot store backed by Apache Cassandra.


JDBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on 
Slick
.


R2DBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on 
R2DBC
.


Akka Management




Akka Management
 provides a central HTTP endpoint for Akka management extensions.


Akka Cluster Bootstrap
 helps bootstrapping an Akka cluster using Akka Discovery.


Akka Management Kubernetes Rolling Updates
 for smooth rolling updates.


Akka Management Cluster HTTP
 provides HTTP endpoints for introspecting and managing Akka clusters.


Akka Discovery for Kubernetes, Consul, Marathon, and AWS


Kubernetes Lease




Akka Diagnostics




Akka Thread Starvation Detector


Akka Configuration Checker




Akka Insights


Intelligent monitoring and observability purpose-built for Akka: 
Lightbend Telemetry


Community Projects


Akka has a vibrant and passionate user community, the members of which have created many independent projects using Akka as well as extensions to it. See 
Community Projects
.














 
Extending Akka






Package, Deploy and Run 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/deploying.html
Deploying • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying




Deploying to Kubernetes


Deploying to Docker containers




Rolling Updates


Building Native Images




Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying




Deploying to Kubernetes


Deploying to Docker containers




Rolling Updates


Building Native Images




Project Information


Akka Classic




















Deploying


Deploying to Kubernetes


You can deploy to Kubernetes according to the guide and example project for 
Deploying Akka Cluster to Kubernetes
.


Cluster bootstrap


To take advantage of running inside Kubernetes while forming a cluster, 
Akka Cluster Bootstrap
 helps forming or joining a cluster using Akka Discovery to discover peer nodes. with the Kubernetes API or Kubernetes via DNS. 


Rolling updates


Enable the 
Kubernetes Rolling Updates
 and 
app-version from Deployment
 features from Akka Management for smooth rolling updates.


Resource management


CPU requests and limits


To avoid CFS scheduler limits, it is best not to use 
resources.limits.cpu
 limits, but use 
resources.requests.cpu
 configuration instead.
Note


Some resource sizing for Akka and other library dependencies, such as for thread pools or direct memory pools, is based on the detected number of available processors. This will be the CPU limit, if configured, or otherwise all available CPU on the underlying Kubernetes node. While it’s recommended to not set a CPU limit, this can lead to over-sized resource allocation. The available processors detected by the JVM can be configured directly using the 
-XX:ActiveProcessorCount
 option.




Example
: Akka applications are being deployed to Kubernetes on 16 CPU nodes. Workloads are variable, so to schedule several pods on each node, a CPU request of 2 is being used. No CPU limit is set, so that pods can burst to more CPU usage as needed and when available. 
-XX:ActiveProcessorCount=4
 is added to the JVM options so that thread pools are sized appropriately for 4 CPU — rather than the full 16 CPU as detected automatically, and more than the 2 CPU request, for when the application is active and able to use more resources.


Memory requests and limits


For memory, it’s recommended to set both 
resources.requests.memory
 and 
resources.limits.memory
 to the same value. The 
-XX:InitialRAMPercentage
 and 
-XX:MaxRAMPercentage
 JVM options can be used to set the heap size relative to the memory limit.
Note


To account for non-heap memory areas (such as class metadata, threads, code cache, symbols, garbage collection, and direct memory), it’s recommended to set the heap percentage to 
70%
 of the available memory. This may need to be a smaller percentage for lower memory limits, or can be a higher percentage for higher memory limits.


Deploying to Docker containers


You can use both Akka remoting and Akka Cluster inside Docker containers. Note that you will need to take special care with the network configuration when using Docker, described here: 
Akka behind NAT or in a Docker container


For the JVM to run well in a Docker container, there are some general (not Akka specific) parameters that might need tuning:


Resource constraints


Docker allows 
constraining each containers’ resource usage
.


Memory


Any memory limits for the Docker container are detected automatically by current JVMs by default. The 
-XX:InitialRAMPercentage
 and 
-XX:MaxRAMPercentage
 JVM options can be used to set the heap size relative to the memory limit.
Note


To account for non-heap memory areas (such as class metadata, threads, code cache, symbols, garbage collection, and direct memory), it’s recommended to set the heap percentage to 
70%
 of the available memory. This may need to be a smaller percentage for lower memory limits, or can be a higher percentage for higher memory limits.


CPU


For multithreaded applications such as the JVM, the CFS scheduler limits are an ill fit, because they will restrict the allowed CPU usage even when more CPU cycles are available from the host system. This means your application may be starved of CPU time, but your system appears idle.


For this reason, it is best to avoid 
--cpus
 and 
--cpu-quota
 entirely, and instead specify relative container weights using 
--cpu-shares
 instead.
Note


Some resource sizing for Akka and other library dependencies, such as for thread pools or direct memory pools, is based on the detected number of available processors. This will be the CPU quota, if configured, or otherwise all CPU available to Docker. While it’s recommended to not set a CPU quota, this can lead to over-sized resource allocation. The available processors detected by the JVM can be configured directly using the 
-XX:ActiveProcessorCount
 option.














 
Operating a Cluster






Rolling Updates 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/alpakka-kafka/current/
Alpakka Kafka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Alpakka Kafka Documentation





Version 7.0.1





Java
Scala










Overview


Producer


Consumer


Service discovery


Akka Cluster Sharding


Error handling


At-Least-Once Delivery


Transactions


Serialization


Debugging


Testing


Production considerations


Snapshots


















Alpakka Kafka Documentation





Version 7.0.1





Java
Scala












Overview


Producer


Consumer


Service discovery


Akka Cluster Sharding


Error handling


At-Least-Once Delivery


Transactions


Serialization


Debugging


Testing


Production considerations


Snapshots




















Alpakka Kafka Documentation


The 
Alpakka project
 is an open source initiative to implement stream-aware and reactive integration pipelines for Java and Scala. It is built on top of 
Akka Streams
, and has been designed from the ground up to understand streaming natively and provide a DSL for reactive and stream-oriented programming, with built-in support for backpressure. Akka Streams is a 
Reactive Streams
 and JDK 9+ 
java.util.concurrent.Flow
-compliant implementation and therefore 
fully interoperable
 with other implementations.


This 
Alpakka Kafka connector
 lets you connect 
Apache Kafka
 to Akka Streams. It was formerly known as 
Akka Streams Kafka
 and even 
Reactive Kafka
.






Overview




Project Info


Matching Kafka Versions


Dependencies


Scala and Java APIs


Examples


Contributing


Release Notes




Producer




Choosing a producer


Settings


Producer as a Sink


Producing messages


Producer as a Flow


Connecting a Producer to a Consumer


Sharing the KafkaProducer instance


Accessing KafkaProducer metrics


Send Producer




Consumer




Choosing a consumer


Settings


Offset Storage external to Kafka


Offset Storage in Kafka - committing


Offset Storage in Kafka & external


Consume “at-most-once”


Consume “at-least-once”


Connecting Producer and Consumer


Source per partition


Sharing the KafkaConsumer instance


Accessing KafkaConsumer metrics


Accessing KafkaConsumer metadata


Controlled shutdown


Subscription


React on Partition Assignment


Consumer Metadata




Service discovery




Dependency


Configure consumer settings


Configure producer settings


Provide a service name via environment variables


Specify a different service discovery mechanism


Use Config (HOCON) to describe the bootstrap servers




Akka Cluster Sharding




Project Info


Dependency


Setup


Sharding Message Extractors


Rebalance Listener




Error handling




Failing consumer


Failing producer


Restarting the stream with a backoff stage


Unexpected consumer offset reset




At-Least-Once Delivery




Multiple Effects per Commit


Non-Sequential Processing


Conditional Message Processing




Transactions




Transactional Source


Transactional Sink and Flow


Consume-Transform-Produce Workflow


Caveats


Further Reading




Serialization




Protocol buffers


Jackson JSON


Spray JSON


Avro with Schema Registry




Debugging




Logging with SLF4J


Receive logging




Testing




Running Kafka with your tests


Alternative testing libraries


Mocking the Consumer or Producer


Testing with a Docker Kafka cluster




Production considerations




Alpakka Kafka API


Monitoring and Tracing


Security setup




Snapshots




Configure repository


Documentation
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Alpakka Kafka is available under the 
Business Source License 1.1
.



© 2011-2024 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-refs.html
StreamRefs - Reactive Streams over the network • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network




Dependency


Introduction


Stream References


Bulk Stream References


Serialization of SourceRef and SinkRef


Configuration




Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network




Dependency


Introduction


Stream References


Bulk Stream References


Serialization of SourceRef and SinkRef


Configuration




Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















StreamRefs - Reactive Streams over the network


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction
Warning


This module is currently marked as 
may change
 in the sense  of being the subject of final development. This means that the API or semantics can  change without warning or deprecation period, and it is not recommended to use  this module in production just yet.


Stream references, or “stream refs” for short, allow running Akka Streams across multiple nodes within an Akka Cluster. 


Unlike heavier “streaming data processing” frameworks, Akka Streams are neither “deployed” nor automatically distributed. Akka stream refs are, as the name implies, references to existing parts of a stream, and can be used to create a distributed processing framework or to introduce such capabilities in specific parts of your application.


Stream refs are trivial to use in existing clustered Akka applications and require no additional configuration or setup. They automatically maintain flow-control / back-pressure over the network and employ Akka’s failure detection mechanisms to fail-fast (“let it crash!”) in the case of failures of remote nodes. They can be seen as an implementation of the 
Work Pulling Pattern
, which one would otherwise implement manually.
Note


A useful way to think about stream refs is:  “like an 
ActorRef
, but for Akka Streams’s 
Source
 and 
Sink
”.


Stream refs refer to an already existing, possibly remote, 
Sink
 or 
Source
.  This is not to be mistaken with deploying streams remotely, which this feature is not intended for.
IMPORTANT


Use stream refs with Akka Cluster. The 
failure detector can cause quarantining
 if plain Akka remoting is used.


Stream References


The prime use case for stream refs is to replace raw actor or HTTP messaging in systems that expect long-running streams of data between two entities. Often they can be used to effectively achieve point-to-point streaming without the need to set up additional message brokers or similar secondary clusters.


Stream refs are well-suited for any system in which you need to send messages between nodes in a flow-controlled fashion. Typical examples include sending work requests to worker nodes as fast as possible, but not faster than the worker nodes can process them, or sending data elements that the downstream may be slow at processing. It is recommended to mix and introduce stream refs in actor-messaging-based systems, where the actor messaging is used to orchestrate and prepare such message flows, and later the stream refs are used to do the flow-controlled message transfer. 


Stream refs are not persistent. However, it is simple to build a resumable stream by introducing such a protocol in the actor messaging layer. Stream refs are absolutely expected to be sent over Akka remoting to other nodes within a cluster using Akka Cluster, and therefore complement, instead of compete, with plain Actor messaging. Actors would usually be used to establish the stream via some initial message saying, “I want to offer you many log elements (the stream ref),” or conversely, “if you need to send me much data, here is the stream ref you can use to do so”.


Since the two sides (“local” and “remote”) of each reference may be confusing to refer to as “remote” and “local” – since either side can be seen as “local” or “remote” depending how we look at it – we propose using the terminology “origin” and “target”, which is defined by where the stream ref was created. For 
SourceRef
s, the “origin” is the side which has the data that it is going to stream out. For 
SinkRef
s, the “origin” side is the actor system that is ready to receive the data and has allocated the ref. Those two may be seen as duals of each other. However, to explain patterns about sharing references, we found this wording to be rather useful.


Source Refs - offering streaming data to a remote system


A 
`SourceRef`
`SourceRef`
 can be offered to a remote actor system in order for it to consume some source of data that we have prepared locally. 


In order to share a 
Source
 with a remote endpoint you need to materialize it by running it into the 
Sink.sourceRef
. That 
Sink
 materializes the 
SourceRef
 that you can then send to other nodes.




Scala




copy
source
import akka.stream.SourceRef

case class RequestLogs(streamId: Int)
case class LogsOffer(streamId: Int, sourceRef: SourceRef[String])

class DataSource extends Actor {

  def receive = {
    case RequestLogs(streamId) =>
      // obtain the source you want to offer:
      val source: Source[String, NotUsed] = streamLogs(streamId)

      // materialize the SourceRef:
      val ref: SourceRef[String] = source.runWith(StreamRefs.sourceRef())

      // wrap the SourceRef in some domain message, such that the sender knows what source it is
      val reply = LogsOffer(streamId, ref)

      // reply to sender
      sender() ! reply
  }

  def streamLogs(streamId: Long): Source[String, NotUsed] = ???
}


Java




copy
source
static class RequestLogs {
  public final long streamId;

  public RequestLogs(long streamId) {
    this.streamId = streamId;
  }
}

static class LogsOffer {
  final SourceRef<String> sourceRef;

  public LogsOffer(SourceRef<String> sourceRef) {
    this.sourceRef = sourceRef;
  }
}

static class DataSource extends AbstractActor {
  @Override
  public Receive createReceive() {
    return receiveBuilder().match(RequestLogs.class, this::handleRequestLogs).build();
  }

  private void handleRequestLogs(RequestLogs requestLogs) {
    Source<String, NotUsed> logs = streamLogs(requestLogs.streamId);
    SourceRef<String> logsRef = logs.runWith(StreamRefs.sourceRef(), mat);

    getSender().tell(new LogsOffer(logsRef), getSelf());
  }

  private Source<String, NotUsed> streamLogs(long streamId) {
    return Source.repeat("[INFO] some interesting logs here (for id: " + streamId + ")");
  }
}




The origin actor which creates and owns the 
Source
 could also perform some validation or additional setup when preparing the 
Source
. Once it has handed out the 
SourceRef
, the remote side can run it like this:




Scala




copy
source
val sourceActor = system.actorOf(Props[DataSource](), "dataSource")

sourceActor ! RequestLogs(1337)
val offer = expectMsgType[LogsOffer]

// implicitly converted to a Source:
offer.sourceRef.runWith(Sink.foreach(println))
// alternatively explicitly obtain Source from SourceRef:
// offer.sourceRef.source.runWith(Sink.foreach(println))



Java




copy
source
ActorRef sourceActor = system.actorOf(Props.create(DataSource.class), "dataSource");

sourceActor.tell(new RequestLogs(1337), getTestActor());
LogsOffer offer = expectMsgClass(LogsOffer.class);

offer.sourceRef.getSource().runWith(Sink.foreach(log -> System.out.println(log)), mat);





The process of preparing and running a 
SourceRef
-powered distributed stream is shown by the animation below:


Warning


A 
SourceRef
 is 
by design
 “single-shot”; i.e., it may only be materialized once.  This is in order to not complicate the mental model of what materialization means.


Multicast can be mimicked by starting a 
BroadcastHub
 operator once, then attaching multiple new streams to it, each  emitting a new stream ref. This way, materialization of the 
BroadcastHub
s Source creates a unique single-shot  stream ref, however they can all be powered using a single 
Source
 – located before the 
BroadcastHub
 operator.


Sink Refs - offering to receive streaming data from a remote system


The dual of 
`SourceRef`
`SourceRef`
s.


They can be used to offer the other side the capability to send to the 
origin
 side data in a streaming, flow-controlled fashion. The origin here allocates a 
Sink
, which could be as simple as a 
Sink.foreach
 or as advanced as a complex 
Sink
 which streams the incoming data into various other systems (e.g., any of the Alpakka-provided 
Sink
s).
Note


To form a good mental model of 
SinkRef
s, you can think of them as being similar to “passive mode” in FTP.




Scala




copy
source
import akka.stream.SinkRef

case class PrepareUpload(id: String)
case class MeasurementsSinkReady(id: String, sinkRef: SinkRef[String])

class DataReceiver extends Actor {

  def receive = {
    case PrepareUpload(nodeId) =>
      // obtain the source you want to offer:
      val sink: Sink[String, NotUsed] = logsSinkFor(nodeId)

      // materialize the SinkRef (the remote is like a source of data for us):
      val ref: SinkRef[String] = StreamRefs.sinkRef[String]().to(sink).run()

      // wrap the SinkRef in some domain message, such that the sender knows what source it is
      val reply = MeasurementsSinkReady(nodeId, ref)

      // reply to sender
      sender() ! reply
  }

  def logsSinkFor(nodeId: String): Sink[String, NotUsed] = ???
}



Java




copy
source
static class PrepareUpload {
  final String id;

  public PrepareUpload(String id) {
    this.id = id;
  }
}

static class MeasurementsSinkReady {
  final String id;
  final SinkRef<String> sinkRef;

  public MeasurementsSinkReady(String id, SinkRef<String> ref) {
    this.id = id;
    this.sinkRef = ref;
  }
}

static class DataReceiver extends AbstractActor {
  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            PrepareUpload.class,
            prepare -> {
              Sink<String, NotUsed> sink = logsSinkFor(prepare.id);
              SinkRef<String> sinkRef = StreamRefs.<String>sinkRef().to(sink).run(mat);

              getSender().tell(new MeasurementsSinkReady(prepare.id, sinkRef), getSelf());
            })
        .build();
  }

  private Sink<String, NotUsed> logsSinkFor(String id) {
    return Sink.<String>ignore().mapMaterializedValue(done -> NotUsed.getInstance());
  }
}




Using the offered 
SinkRef
 to send data to the origin of the 
Sink
 is also simple, as we can treat the 
SinkRef
 as any other 
Sink
 and directly 
runWith
 or 
run
 with it.




Scala




copy
source
val receiver = system.actorOf(Props[DataReceiver](), "receiver")

receiver ! PrepareUpload("system-42-tmp")
val ready = expectMsgType[MeasurementsSinkReady]

// stream local metrics to Sink's origin:
localMetrics().runWith(ready.sinkRef)


Java




copy
source
ActorRef receiver = system.actorOf(Props.create(DataReceiver.class), "dataReceiver");

receiver.tell(new PrepareUpload("system-42-tmp"), getTestActor());
MeasurementsSinkReady ready = expectMsgClass(MeasurementsSinkReady.class);

Source.repeat("hello").runWith(ready.sinkRef.getSink(), mat);




The process of preparing and running a 
SinkRef
-powered distributed stream is shown by the animation below:


Warning


A 
SinkRef
 is 
by design
 “single-shot”; i.e., it may only be materialized once.  This is in order to not complicate the mental model of what materialization means.


If you have a use case for building a fan-in operation that accepts writes from multiple remote nodes,  you can build your 
Sink
 and prepend it with a 
MergeHub
 operator, each time materializing a new 
SinkRef
  targeting that 
MergeHub
. This has the added benefit of giving you full control of how to merge these streams  (i.e., by using “merge preferred” or any other variation of the fan-in operators).


Delivery guarantees


Stream refs utilise normal actor messaging for their transport, and therefore provide the same level of basic delivery guarantees. Stream refs do extend the semantics somewhat, through demand re-delivery and sequence fault detection. In other words:




messages are sent over actor remoting
    


which relies on TCP (classic remoting or Artery TCP) or Aeron UDP for basic redelivery mechanisms






messages are guaranteed to to be in-order


messages can be lost, however:
    


a 
dropped demand signal
 will be re-delivered automatically (similar to system messages)


a 
dropped element signal
 will cause the stream to 
fail








Bulk Stream References
Warning


Bulk stream references are not implemented yet.  See ticket 
Bulk Transfer Stream Refs #24276
 to track progress or signal demand for this feature. 


Bulk stream refs can be used to create simple side-channels to transfer humongous amounts of data such as huge log files, messages or even media, with as much ease as if it was a trivial local stream.


Serialization of SourceRef and SinkRef


StreamRefs require serialization, since the whole point is to send them between nodes of a cluster. A built in serializer is provided when 
SourceRef
 and 
SinkRef
 are sent directly as messages however the recommended use is to wrap them into your own actor message classes. 


When 
Akka Jackson
 is used, serialization of wrapped 
SourceRef
 and 
SinkRef
 will work out of the box.


If you are using some other form of serialization you will need to use the 
StreamRefResolver
StreamRefResolver
 extension from your serializer to get the 
SourceRef
 and 
SinkRef
. The extension provides the methods 
toSerializationFormat(sink or source)
 to transform from refs to string and 
resolve{Sink,Source}Ref(String)
 to resolve refs from strings.


Configuration


Stream reference subscription timeouts


All stream references have a subscription timeout, which is intended to prevent resource leaks in case a remote node requests the allocation of many streams but never actually runs them. In order to prevent this, each stream reference has a default timeout (of 30 seconds), after which the origin will abort the stream offer if the target has not materialized the stream ref. After the timeout has triggered, materialization of the target side will fail, pointing out that the origin is missing.


Since these timeouts are often very different based on the kind of stream offered, and there can be many different kinds of them in the same application, it is possible to not only configure this setting globally (
akka.stream.materializer.stream-ref.subscription-timeout
), but also via attributes:




Scala




copy
source
// configure the timeout for source
import scala.concurrent.duration._
import akka.stream.StreamRefAttributes

// configuring Sink.sourceRef (notice that we apply the attributes to the Sink!):
Source
  .repeat("hello")
  .runWith(StreamRefs.sourceRef().addAttributes(StreamRefAttributes.subscriptionTimeout(5.seconds)))

// configuring SinkRef.source:
StreamRefs
  .sinkRef()
  .addAttributes(StreamRefAttributes.subscriptionTimeout(5.seconds))
  .runWith(Sink.ignore) // not very interesting Sink, just an example


Java




copy
source
FiniteDuration timeout = FiniteDuration.create(5, TimeUnit.SECONDS);
Attributes timeoutAttributes = StreamRefAttributes.subscriptionTimeout(timeout);

// configuring Sink.sourceRef (notice that we apply the attributes to the Sink!):
Source.repeat("hello")
    .runWith(StreamRefs.<String>sourceRef().addAttributes(timeoutAttributes), mat);

// configuring SinkRef.source:
StreamRefs.<String>sinkRef()
    .addAttributes(timeoutAttributes)
    .runWith(Sink.<String>ignore(), mat); // not very interesting sink, just an example





General configuration


Other settings can be set globally in your 
application.conf
, by overriding any of the following values in the 
akka.stream.materializer.stream-ref.*
 keyspace:


copy
source
# configure defaults for SourceRef and SinkRef
stream-ref {
  # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref
  #
  # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
  # because the delay of requesting over network boundaries is much higher.
  buffer-capacity = 32

  # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
  # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
  # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
  #
  # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
  #
  # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
  # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
  demand-redelivery-interval = 1 second

  # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
  # This timeout does not have to be very low in normal situations, since the remote side may also need to
  # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
  # in-active streams which are never subscribed to.
  subscription-timeout = 30 seconds

  # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed
  # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.
  # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the
  # other side of the stream ref would never send the "final" terminal message.
  #
  # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef
  # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.
  final-termination-signal-deadline = 2 seconds
}














 
Working with streaming IO






Pipelining and Parallelism 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-actors.html
Classic Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors




Dependency


Classic Actors


Classic Supervision


Classic Fault Tolerance


Classic Dispatchers


Classic Mailboxes


Classic Routing


Classic FSM


Classic Persistence


Testing Classic Actors




Classic Clustering


Classic Networking


Classic Utilities




















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors




Dependency


Classic Actors


Classic Supervision


Classic Fault Tolerance


Classic Dispatchers


Classic Mailboxes


Classic Routing


Classic FSM


Classic Persistence


Testing Classic Actors




Classic Clustering


Classic Networking


Classic Utilities






















Classic Actors
Note


Akka Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Akka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see 
coexistence
. For new projects we recommend using 
the new Actor API
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Classic Akka Actors, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor" % AkkaVersion,
  "com.typesafe.akka" %% "akka-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-testkit_${versions.ScalaBinary}"
}






Classic Actors




Module info


Introduction


Creating Actors


Actor API


Identifying Actors via Actor Selection


Messages and immutability


Send messages


Receive messages


Reply to messages


Receive timeout


Timers, scheduled messages


Stopping actors


Become/Unbecome


Stash


Extending Actors using PartialFunction chaining


Initialization patterns




Classic Supervision




What Supervision Means


The Top-Level Supervisors


One-For-One Strategy vs. All-For-One Strategy




Classic Fault Tolerance




Dependency


Introduction


Fault Handling in Practice


Creating a Supervisor Strategy


Supervision of Top-Level Actors


Test Application


Delayed restarts for classic actors




Classic Dispatchers




Dependency


Looking up a Dispatcher


Setting the dispatcher for an Actor




Classic Mailboxes




Dependency


Introduction


Mailbox Selection


Mailbox configuration examples


Special Semantics of 
system.actorOf




Classic Routing




Dependency


Introduction


A Simple Router


A Router Actor


Router usage


Specially Handled Messages


Dynamically Resizable Pool


How Routing is Designed within Akka


Custom Router


Configuring Dispatchers




Classic FSM




Dependency


Overview


A Simple Example


Reference


Testing and Debugging Finite State Machines




Classic Persistence




Module info


Introduction


Example


Snapshots


Scaling out


At-Least-Once Delivery


Event Adapters


Custom serialization


Testing with LevelDB journal


Configuration


Multiple persistence plugin configurations


Give persistence plugin configurations at runtime


See also




Testing Classic Actors




Module info


Introduction


Asynchronous Testing: 
TestKit


CallingThreadDispatcher


Tracing Actor Invocations


Different Testing Frameworks


Configuration


Example


Synchronous Testing: 
TestActorRef




















 
Akka Classic






Classic Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/style-guide.html
Style guide • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide




Functional versus object-oriented style


Passing around too many parameters


Behavior factory method


Where to define messages


Public versus private messages


Lambdas versus method references


Partial versus total Function


How to compose Partial Functions


ask versus ?


ReceiveBuilder


Nesting setup


Additional naming conventions




Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide




Functional versus object-oriented style


Passing around too many parameters


Behavior factory method


Where to define messages


Public versus private messages


Lambdas versus method references


Partial versus total Function


How to compose Partial Functions


ask versus ?


ReceiveBuilder


Nesting setup


Additional naming conventions




Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Style guide


This is a style guide with recommendations of idioms and patterns for writing Akka actors. Note that this guide does not cover the classic actor API.


As with all style guides, treat this as a list of rules to be broken. There are certainly times when alternative styles should be preferred over the ones given here.


Functional versus object-oriented style


There are two flavors of the Actor APIs.




The functional programming style where you pass a function to a factory which then constructs a behavior,  for stateful actors this means passing immutable state around as parameters and switching to a new behavior  whenever you need to act on a changed state.


The object-oriented style where a concrete class for the actor behavior is defined and mutable  state is kept inside of it as fields.




An example of a counter actor implemented in the functional style:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  def apply(): Behavior[Command] =
    counter(0)

  private def counter(n: Int): Behavior[Command] =
    Behaviors.receive { (context, message) =>
      message match {
        case Increment =>
          val newValue = n + 1
          context.log.debug("Incremented counter to [{}]", newValue)
          counter(newValue)
        case GetValue(replyTo) =>
          replyTo ! Value(n)
          Behaviors.same
      }
    }
}


Java




copy
source
import akka.actor.typed.Behavior;
import akka.actor.typed.SupervisorStrategy;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;

public class Counter {
  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(context -> counter(context, 0));
  }

  private static Behavior<Command> counter(final ActorContext<Command> context, final int n) {

    return Behaviors.receive(Command.class)
        .onMessage(Increment.class, notUsed -> onIncrement(context, n))
        .onMessage(GetValue.class, command -> onGetValue(n, command))
        .build();
  }

  private static Behavior<Command> onIncrement(ActorContext<Command> context, int n) {
    int newValue = n + 1;
    context.getLog().debug("Incremented counter to [{}]", newValue);
    return counter(context, newValue);
  }

  private static Behavior<Command> onGetValue(int n, GetValue command) {
    command.replyTo.tell(new Value(n));
    return Behaviors.same();
  }
}




Corresponding actor implemented in the object-oriented style:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors
import akka.actor.typed.scaladsl.AbstractBehavior
import org.slf4j.Logger

object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  def apply(): Behavior[Command] = {
    Behaviors.setup(context => new Counter(context))
  }
}

class Counter(context: ActorContext[Counter.Command]) extends AbstractBehavior[Counter.Command](context) {
  import Counter._

  private var n = 0

  override def onMessage(msg: Command): Behavior[Counter.Command] = {
    msg match {
      case Increment =>
        n += 1
        context.log.debug("Incremented counter to [{}]", n)
        this
      case GetValue(replyTo) =>
        replyTo ! Value(n)
        this
    }
  }
}


Java




copy
source
import akka.actor.typed.Behavior;
import akka.actor.typed.SupervisorStrategy;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.Receive;

public class Counter extends AbstractBehavior<Counter.Command> {

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(Counter::new);
  }

  private int n;

  private Counter(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, notUsed -> onIncrement())
        .onMessage(GetValue.class, this::onGetValue)
        .build();
  }

  private Behavior<Command> onIncrement() {
    n++;
    getContext().getLog().debug("Incremented counter to [{}]", n);
    return this;
  }

  private Behavior<Command> onGetValue(GetValue command) {
    command.replyTo.tell(new Value(n));
    return this;
  }
}




Some similarities to note:




Messages are defined in the same way.


Both have 
an 
apply
 factory method in the companion object
a static 
create
 factory method
 to  create the initial behavior, i.e. from the outside they are used in the same way.


Pattern matching
Matching
 and handling of the messages are done in the same way.


The 
ActorContext
 API is the same.




A few differences to note:




There is no class in the functional style, but that is not strictly a requirement and sometimes it’s  convenient to use a class also with the functional style to reduce number of parameters in the methods.


Mutable state, such as the 
var n
int n
 is typically used in the object-oriented style.


In the functional style the state is updated by returning a new behavior that holds the new immutable state,  the 
n: Int
final int n
 parameter of the 
counter
 method.


The object-oriented style must use a new instance of the initial 
Behavior
 for each spawned actor instance,  since the state in 
AbstractBehavior
 instance must not be shared between actor instances.  This is “hidden” in the functional style since the immutable state is captured by the function.


In the object-oriented style one can return 
this
 to stay with the same behavior for next message.  In the functional style there is no 
this
 so 
Behaviors.same
 is used instead.


The 
ActorContext
 is accessed in different ways. In the object-oriented style it’s retrieved from  
Behaviors.setup
 and kept as an instance field, while in the functional style it’s passed in alongside  the message. That said, 
Behaviors.setup
 is often used in the functional style as well, and then  often together with 
Behaviors.receiveMessage
 that doesn’t pass in the context with the message.
 
The 
ActorContext
 is accessed with 
Behaviors.setup
 but then kept in different ways.  As an instance field versus a method parameter.




Which style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. An actor can switch between behaviors implemented in different styles. For example, it may have an initial behavior that is only stashing messages until some initial query has been completed and then switching over to its main active behavior that is maintaining some mutable state. Such initial behavior is nice in the functional style and the active behavior may be better with the object-oriented style.


We would recommend using the tool that is best for the job. The APIs are similar in many ways to make it easy to learn both. You may of course also decide to just stick to one style for consistency and familiarity reasons.


When developing in Scala the functional style will probably be the choice for many.


Some reasons why you may want to use the functional style:




You are familiar with a functional approach of structuring the code. Note that this API is still  not using any advanced functional programming or type theory constructs.


The state is immutable and can be passed to “next” behavior.


The 
Behavior
 is stateless.


The actor lifecycle has several different phases that can be represented by switching between different  behaviors, like a 
finite state machine
. This is also supported with the object-oriented style, but  it’s typically nicer with the functional style.


It’s less risk of accessing mutable state in the actor from other threads, like 
Future
 or Streams  callbacks.




Some reasons why you may want to use the object-oriented style:




You are more familiar with an object-oriented style of structuring the code with methods  in a class rather than functions.


Some state is not immutable.


It could be more familiar and easier to upgrade existing classic actors to this style.


Mutable state can sometimes have better performance, e.g. mutable collections and  avoiding allocating new instance for next behavior (be sure to benchmark if this is your  motivation).




When developing in Java the object-oriented style will probably be the choice for many.


Some reasons why you may want to use the object-oriented style:




You are more familiar with an object-oriented style of structuring the code with methods  in a class rather than functions.


Java lambdas can only close over final or effectively final fields, making it  impractical to use the functional style in behaviors that mutate their fields.


Some state is not immutable, e.g. immutable collections are not widely used in Java.  It is OK to use mutable state also with the functional style but you must make sure  that it’s not shared between different actor instances.


It could be more familiar and easier to upgrade existing classic actors to this style.


Mutable state can sometimes have better performance, e.g. mutable collections and  avoiding allocating new instance for next behavior (be sure to benchmark if this is your  motivation).




Some reasons why you may want to use the functional style:




You are familiar with a functional approach of structuring the code. Note that this API is still  not using any advanced functional programming or type theory constructs.


The state is immutable and can be passed to “next” behavior.


The 
Behavior
 is stateless.


The actor lifecycle has several different phases that can be represented by switching between different  behaviors, like a 
finite state machine
. This is also supported with the object-oriented style, but  it’s typically nicer with the functional style.


It’s less risk of accessing mutable state in the actor from other threads, like 
CompletionStage
 or Streams  callbacks.




Passing around too many parameters


One thing you will quickly run into when using the functional style is that you need to pass around many parameters.


Let’s add 
name
 parameter and timers to the previous 
Counter
 example. A first approach would be to just add those as separate parameters:




Scala




copy
source
// this is an anti-example, better solutions exists
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  def apply(name: String): Behavior[Command] =
    Behaviors.withTimers { timers =>
      counter(name, timers, 0)
    }

  private def counter(name: String, timers: TimerScheduler[Command], n: Int): Behavior[Command] =
    Behaviors.receive { (context, message) =>
      message match {
        case IncrementRepeatedly(interval) =>
          context.log.debug(
            "[{}] Starting repeated increments with interval [{}], current count is [{}]",
            name,
            interval.toString,
            n.toString)
          timers.startTimerWithFixedDelay(Increment, interval)
          Behaviors.same
        case Increment =>
          val newValue = n + 1
          context.log.debug("[{}] Incremented counter to [{}]", name, newValue)
          counter(name, timers, newValue)
        case GetValue(replyTo) =>
          replyTo ! Value(n)
          Behaviors.same
      }
    }
}


Java




copy
source
// this is an anti-example, better solutions exists
public class Counter {
  public interface Command {}

  public static class IncrementRepeatedly implements Command {
    public final Duration interval;

    public IncrementRepeatedly(Duration interval) {
      this.interval = interval;
    }
  }

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }

  public static Behavior<Command> create(String name) {
    return Behaviors.setup(
        context -> Behaviors.withTimers(timers -> counter(name, context, timers, 0)));
  }

  private static Behavior<Command> counter(
      final String name,
      final ActorContext<Command> context,
      final TimerScheduler<Command> timers,
      final int n) {

    return Behaviors.receive(Command.class)
        .onMessage(
            IncrementRepeatedly.class,
            command -> onIncrementRepeatedly(name, context, timers, n, command))
        .onMessage(Increment.class, notUsed -> onIncrement(name, context, timers, n))
        .onMessage(GetValue.class, command -> onGetValue(n, command))
        .build();
  }

  private static Behavior<Command> onIncrementRepeatedly(
      String name,
      ActorContext<Command> context,
      TimerScheduler<Command> timers,
      int n,
      IncrementRepeatedly command) {
    context
        .getLog()
        .debug(
            "[{}] Starting repeated increments with interval [{}], current count is [{}]",
            name,
            command.interval,
            n);
    timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);
    return Behaviors.same();
  }

  private static Behavior<Command> onIncrement(
      String name, ActorContext<Command> context, TimerScheduler<Command> timers, int n) {
    int newValue = n + 1;
    context.getLog().debug("[{}] Incremented counter to [{}]", name, newValue);
    return counter(name, context, timers, newValue);
  }

  private static Behavior<Command> onGetValue(int n, GetValue command) {
    command.replyTo.tell(new Value(n));
    return Behaviors.same();
  }
}




Ouch, that doesn’t look good. More things may be needed, such as stashing or application specific “constructor” parameters. As you can imagine, that will be too much boilerplate.


As a first step we can place all these parameters in a class so that we at least only have to pass around one thing. Still good to have the “changing” state, the 
n: Int
final int n
 here, as a separate parameter.




Scala




copy
source
// this is better than previous example, but even better solution exists
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  private case class Setup(name: String, context: ActorContext[Command], timers: TimerScheduler[Command])

  def apply(name: String): Behavior[Command] =
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        counter(Setup(name, context, timers), 0)
      }
    }

  private def counter(setup: Setup, n: Int): Behavior[Command] =
    Behaviors.receiveMessage {
      case IncrementRepeatedly(interval) =>
        setup.context.log.debug(
          "[{}] Starting repeated increments with interval [{}], current count is [{}]",
          setup.name,
          interval,
          n)
        setup.timers.startTimerWithFixedDelay(Increment, interval)
        Behaviors.same
      case Increment =>
        val newValue = n + 1
        setup.context.log.debug("[{}] Incremented counter to [{}]", setup.name, newValue)
        counter(setup, newValue)
      case GetValue(replyTo) =>
        replyTo ! Value(n)
        Behaviors.same
    }
}


Java




copy
source
// this is better than previous example, but even better solution exists
public class Counter {
  // messages omitted for brevity, same messages as above example

  private static class Setup {
    final String name;
    final ActorContext<Command> context;
    final TimerScheduler<Command> timers;

    private Setup(String name, ActorContext<Command> context, TimerScheduler<Command> timers) {
      this.name = name;
      this.context = context;
      this.timers = timers;
    }
  }

  public static Behavior<Command> create(String name) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(timers -> counter(new Setup(name, context, timers), 0)));
  }

  private static Behavior<Command> counter(final Setup setup, final int n) {

    return Behaviors.receive(Command.class)
        .onMessage(
            IncrementRepeatedly.class, command -> onIncrementRepeatedly(setup, n, command))
        .onMessage(Increment.class, notUsed -> onIncrement(setup, n))
        .onMessage(GetValue.class, command -> onGetValue(n, command))
        .build();
  }

  private static Behavior<Command> onIncrementRepeatedly(
      Setup setup, int n, IncrementRepeatedly command) {
    setup
        .context
        .getLog()
        .debug(
            "[{}] Starting repeated increments with interval [{}], current count is [{}]",
            setup.name,
            command.interval,
            n);
    setup.timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);
    return Behaviors.same();
  }

  private static Behavior<Command> onIncrement(Setup setup, int n) {
    int newValue = n + 1;
    setup.context.getLog().debug("[{}] Incremented counter to [{}]", setup.name, newValue);
    return counter(setup, newValue);
  }

  private static Behavior<Command> onGetValue(int n, GetValue command) {
    command.replyTo.tell(new Value(n));
    return Behaviors.same();
  }
}




That’s better. Only one thing to carry around and easy to add more things to it without rewriting everything. 
Note that we also placed the 
ActorContext
 in the 
Setup
 class, and therefore switched from 
Behaviors.receive
 to 
Behaviors.receiveMessage
 since we already have access to the 
context
.


It’s still rather annoying to have to pass the same thing around everywhere.


We can do better by introducing an enclosing class, even though it’s still using the functional style. The “constructor” parameters can be 
immutable
final
 instance fields and can be accessed from member methods.




Scala




copy
source
// this is better than previous examples
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  def apply(name: String): Behavior[Command] =
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        new Counter(name, context, timers).counter(0)
      }
    }
}

class Counter private (
    name: String,
    context: ActorContext[Counter.Command],
    timers: TimerScheduler[Counter.Command]) {
  import Counter._

  private def counter(n: Int): Behavior[Command] =
    Behaviors.receiveMessage {
      case IncrementRepeatedly(interval) =>
        context.log.debug(
          "[{}] Starting repeated increments with interval [{}], current count is [{}]",
          name,
          interval,
          n)
        timers.startTimerWithFixedDelay(Increment, interval)
        Behaviors.same
      case Increment =>
        val newValue = n + 1
        context.log.debug("[{}] Incremented counter to [{}]", name, newValue)
        counter(newValue)
      case GetValue(replyTo) =>
        replyTo ! Value(n)
        Behaviors.same
    }
}


Java




copy
source
// this is better than previous examples
public class Counter {
  // messages omitted for brevity, same messages as above example

  public static Behavior<Command> create(String name) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(timers -> new Counter(name, context, timers).counter(0)));
  }

  private final String name;
  private final ActorContext<Command> context;
  private final TimerScheduler<Command> timers;

  private Counter(String name, ActorContext<Command> context, TimerScheduler<Command> timers) {
    this.name = name;
    this.context = context;
    this.timers = timers;
  }

  private Behavior<Command> counter(final int n) {
    return Behaviors.receive(Command.class)
        .onMessage(IncrementRepeatedly.class, command -> onIncrementRepeatedly(n, command))
        .onMessage(Increment.class, notUsed -> onIncrement(n))
        .onMessage(GetValue.class, command -> onGetValue(n, command))
        .build();
  }

  private Behavior<Command> onIncrementRepeatedly(int n, IncrementRepeatedly command) {
    context
        .getLog()
        .debug(
            "[{}] Starting repeated increments with interval [{}], current count is [{}]",
            name,
            command.interval,
            n);
    timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);
    return Behaviors.same();
  }

  private Behavior<Command> onIncrement(int n) {
    int newValue = n + 1;
    context.getLog().debug("[{}] Incremented counter to [{}]", name, newValue);
    return counter(newValue);
  }

  private Behavior<Command> onGetValue(int n, GetValue command) {
    command.replyTo.tell(new Value(n));
    return Behaviors.same();
  }
}




That’s nice. One thing to be cautious with here is that it’s important that you create a new instance for each spawned actor, since those parameters must not be shared between different actor instances. That comes natural when creating the instance from 
Behaviors.setup
 as in the above example. Having a 
apply
 factory method in the companion object and making the constructor private is recommended.
 
static 
create
 factory method and making the constructor private is recommended.


This can also be useful when testing the behavior by creating a test subclass that overrides certain methods in the class. The test would create the instance without the 
apply
 factory method
static 
create
 factory method
. Then you need to relax the visibility constraints of the constructor and methods.


It’s not recommended to place mutable state and 
var
 members
non-final members
 in the enclosing class. It would be correct from an actor thread-safety perspective as long as the same instance of the enclosing class is not shared between different actor instances, but if that is what you need you should rather use the object-oriented style with the 
AbstractBehavior
 class.


Similar can be achieved without an enclosing class by placing the 
def counter
 inside the 
Behaviors.setup
 block. That works fine, but for more complex behaviors it can be better to structure the methods in a class. For completeness, here is how it would look like:




Scala




copy
source
// this works, but previous example is better for structuring more complex behaviors
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  def apply(name: String): Behavior[Command] =
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        def counter(n: Int): Behavior[Command] =
          Behaviors.receiveMessage {
            case IncrementRepeatedly(interval) =>
              context.log.debug(
                "[{}] Starting repeated increments with interval [{}], current count is [{}]",
                name,
                interval,
                n)
              timers.startTimerWithFixedDelay(Increment, interval)
              Behaviors.same
            case Increment =>
              val newValue = n + 1
              context.log.debug("[{}] Incremented counter to [{}]", name, newValue)
              counter(newValue)
            case GetValue(replyTo) =>
              replyTo ! Value(n)
              Behaviors.same
          }

        counter(0)
      }
    }
}





Behavior factory method


The initial behavior should be created via 
a factory method in the companion object
a static factory method
. Thereby the usage of the behavior doesn’t change when the implementation is changed, for example if changing between object-oriented and function style.


The factory method is a good place for retrieving resources like 
Behaviors.withTimers
, 
Behaviors.withStash
 and 
ActorContext
 with 
Behaviors.setup
.


When using the object-oriented style, 
AbstractBehavior
, a new instance should be created from a 
Behaviors.setup
 block in this factory method even though the 
ActorContext
 is not needed. This is important because a new instance should be created when restart supervision is used. Typically, the 
ActorContext
 is needed anyway.


The naming convention for the factory method is 
apply
 (when using Scala)
create
 (when using Java)
. Consistent naming makes it easier for readers of the code to find the “starting point” of the behavior.


In the functional style the factory could even have been defined as a 
val
static field
 if all state is immutable and captured by the function, but since most behaviors need some initialization parameters it is preferred to consistently use a method 
(
def
)
 for the factory.


Example:




Scala




copy
source
object CountDown {
  sealed trait Command
  case object Down extends Command

  // factory for the initial `Behavior`
  def apply(countDownFrom: Int, notifyWhenZero: ActorRef[Done]): Behavior[Command] =
    new CountDown(notifyWhenZero).counter(countDownFrom)
}

private class CountDown(notifyWhenZero: ActorRef[Done]) {
  import CountDown._

  private def counter(remaining: Int): Behavior[Command] = {
    Behaviors.receiveMessage {
      case Down =>
        if (remaining == 1) {
          notifyWhenZero.tell(Done)
          Behaviors.stopped
        } else
          counter(remaining - 1)
    }
  }

}


Java




copy
source
public class CountDown extends AbstractBehavior<CountDown.Command> {

  public interface Command {}

  public enum Down implements Command {
    INSTANCE
  }

  // factory for the initial `Behavior`
  public static Behavior<Command> create(int countDownFrom, ActorRef<Done> notifyWhenZero) {
    return Behaviors.setup(context -> new CountDown(context, countDownFrom, notifyWhenZero));
  }

  private final ActorRef<Done> notifyWhenZero;
  private int remaining;

  private CountDown(
      ActorContext<Command> context, int countDownFrom, ActorRef<Done> notifyWhenZero) {
    super(context);
    this.remaining = countDownFrom;
    this.notifyWhenZero = notifyWhenZero;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder().onMessage(Down.class, notUsed -> onDown()).build();
  }

  private Behavior<Command> onDown() {
    remaining--;
    if (remaining == 0) {
      notifyWhenZero.tell(Done.getInstance());
      return Behaviors.stopped();
    } else {
      return this;
    }
  }
}




When spawning an actor from this initial behavior it looks like:




Scala




copy
source
val countDown = context.spawn(CountDown(100, doneRef), "countDown")


Java




copy
source
ActorRef<CountDown.Command> countDown =
    context.spawn(CountDown.create(100, doneRef), "countDown");




Where to define messages


When sending or receiving actor messages they should be prefixed with the name of the actor/behavior that defines them to avoid ambiguities.




Scala




copy
source
countDown ! CountDown.Down


Java




copy
source
countDown.tell(CountDown.Down.INSTANCE);




Such a style is preferred over using 
importing 
Down
 and using 
countDown ! Down
 
importing 
Down
 and using 
countDown.tell(Down.INSTANCE);
. However, within the 
Behavior
 that handle these messages the short names can be used.


Therefore it is not recommended to define messages as top-level classes.


For the majority of cases it’s good style to define the messages 
in the companion object
as static inner classes
 together with the 
Behavior
.




Scala




copy
source
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)
}


Java




copy
source
public class Counter extends AbstractBehavior<Counter.Command> {

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }
}




If several actors share the same message protocol, it’s recommended to define those messages in a separate 
object
interface
 for that protocol.


Here’s an example of a shared message protocol setup:




Scala




copy
source
object CounterProtocol {
  sealed trait Command

  final case class Increment(delta: Int, replyTo: ActorRef[OperationResult]) extends Command
  final case class Decrement(delta: Int, replyTo: ActorRef[OperationResult]) extends Command

  sealed trait OperationResult
  case object Confirmed extends OperationResult
  final case class Rejected(reason: String) extends OperationResult
}


Java




copy
source
interface CounterProtocol {
  interface Command {}

  public static class Increment implements Command {
    public final int delta;
    private final ActorRef<OperationResult> replyTo;

    public Increment(int delta, ActorRef<OperationResult> replyTo) {
      this.delta = delta;
      this.replyTo = replyTo;
    }
  }

  public static class Decrement implements Command {
    public final int delta;
    private final ActorRef<OperationResult> replyTo;

    public Decrement(int delta, ActorRef<OperationResult> replyTo) {
      this.delta = delta;
      this.replyTo = replyTo;
    }
  }

  interface OperationResult {}

  enum Confirmed implements OperationResult {
    INSTANCE
  }

  public static class Rejected implements OperationResult {
    public final String reason;

    public Rejected(String reason) {
      this.reason = reason;
    }
  }
}




Note that the response message hierarchy in this case could be completely avoided by using the  API instead (see 
Generic Response Wrapper
).


Public versus private messages


Often an actor has some messages that are only for its internal implementation and not part of the public message protocol, such as timer messages or wrapper messages for 
ask
 or 
messageAdapter
.


Such messages should be declared 
private
 so they can’t be accessed and sent from the outside of the actor. Note that they must still 
extend
implement
 the public 
Command
 
trait
interface
.


Here is an example of using 
private
 for an internal message:




Scala




copy
source
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  // Tick is private so can't be sent from the outside
  private case object Tick extends Command

  def apply(name: String, tickInterval: FiniteDuration): Behavior[Command] =
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        timers.startTimerWithFixedDelay(Tick, tickInterval)
        new Counter(name, context).counter(0)
      }
    }
}

class Counter private (name: String, context: ActorContext[Counter.Command]) {
  import Counter._

  private def counter(n: Int): Behavior[Command] =
    Behaviors.receiveMessage {
      case Increment =>
        val newValue = n + 1
        context.log.debug("[{}] Incremented counter to [{}]", name, newValue)
        counter(newValue)
      case Tick =>
        val newValue = n + 1
        context.log.debug("[{}] Incremented counter by background tick to [{}]", name, newValue)
        counter(newValue)
      case GetValue(replyTo) =>
        replyTo ! Value(n)
        Behaviors.same
    }
}


Java




copy
source
public class Counter extends AbstractBehavior<Counter.Command> {

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }

  // Tick is private so can't be sent from the outside
  private enum Tick implements Command {
    INSTANCE
  }

  public static Behavior<Command> create(String name, Duration tickInterval) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(
                timers -> {
                  timers.startTimerWithFixedDelay(Tick.INSTANCE, tickInterval);
                  return new Counter(name, context);
                }));
  }

  private final String name;
  private int count;

  private Counter(String name, ActorContext<Command> context) {
    super(context);
    this.name = name;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, notUsed -> onIncrement())
        .onMessage(Tick.class, notUsed -> onTick())
        .onMessage(GetValue.class, this::onGetValue)
        .build();
  }


  private Behavior<Command> onIncrement() {
    count++;
    getContext().getLog().debug("[{}] Incremented counter to [{}]", name, count);
    return this;
  }

  private Behavior<Command> onTick() {
    count++;
    getContext()
        .getLog()
        .debug("[{}] Incremented counter by background tick to [{}]", name, count);
    return this;
  }

  private Behavior<Command> onGetValue(GetValue command) {
    command.replyTo.tell(new Value(count));
    return this;
  }

}




An alternative approach is using a type hierarchy and 
narrow
 to have a super-type for the public messages as a distinct type from the super-type of all actor messages. The former approach is recommended but it is good to know this alternative as it can be useful when using shared message protocol classes as described in 
Where to define messages
.


Here’s an example of using a type hierarchy to separate public and private messages:




Scala




copy
source
// above example is preferred, but this is possible and not wrong
object Counter {
  // The type of all public and private messages the Counter actor handles
  sealed trait Message

  /** Counter's public message protocol type. */
  sealed trait Command extends Message
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Value]) extends Command
  final case class Value(n: Int)

  // The type of the Counter actor's internal messages.
  sealed trait PrivateCommand extends Message
  // Tick is a private command so can't be sent to an ActorRef[Command]
  case object Tick extends PrivateCommand

  def apply(name: String, tickInterval: FiniteDuration): Behavior[Command] = {
    Behaviors
      .setup[Counter.Message] { context =>
        Behaviors.withTimers { timers =>
          timers.startTimerWithFixedDelay(Tick, tickInterval)
          new Counter(name, context).counter(0)
        }
      }
      .narrow // note narrow here
  }
}

class Counter private (name: String, context: ActorContext[Counter.Message]) {
  import Counter._

  private def counter(n: Int): Behavior[Message] =
    Behaviors.receiveMessage {
      case Increment =>
        val newValue = n + 1
        context.log.debug("[{}] Incremented counter to [{}]", name, newValue)
        counter(newValue)
      case Tick =>
        val newValue = n + 1
        context.log.debug("[{}] Incremented counter by background tick to [{}]", name, newValue)
        counter(newValue)
      case GetValue(replyTo) =>
        replyTo ! Value(n)
        Behaviors.same
    }
}


Java




copy
source
// above example is preferred, but this is possible and not wrong
public class Counter extends AbstractBehavior<Counter.Message> {

  // The type of all public and private messages the Counter actor handles
  public interface Message {}

  /** Counter's public message protocol type. */
  public interface Command extends Message {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final ActorRef<Value> replyTo;

    public GetValue(ActorRef<Value> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Value {
    public final int value;

    public Value(int value) {
      this.value = value;
    }
  }

  // The type of the Counter actor's internal messages.
  interface PrivateCommand extends Message {}

  // Tick is a private command so can't be sent to an ActorRef<Command>
  enum Tick implements PrivateCommand {
    INSTANCE
  }

  public static Behavior<Command> create(String name, Duration tickInterval) {
    return Behaviors.setup(
            (ActorContext<Message> context) ->
                Behaviors.withTimers(
                    timers -> {
                      timers.startTimerWithFixedDelay(Tick.INSTANCE, tickInterval);
                      return new Counter(name, context);
                    }))
        .narrow(); // note narrow here
  }

  private final String name;
  private int count;

  private Counter(String name, ActorContext<Message> context) {
    super(context);
    this.name = name;
  }

  @Override
  public Receive<Message> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, notUsed -> onIncrement())
        .onMessage(Tick.class, notUsed -> onTick())
        .onMessage(GetValue.class, this::onGetValue)
        .build();
  }

  private Behavior<Message> onIncrement() {
    count++;
    getContext().getLog().debug("[{}] Incremented counter to [{}]", name, count);
    return this;
  }

  private Behavior<Message> onTick() {
    count++;
    getContext()
        .getLog()
        .debug("[{}] Incremented counter by background tick to [{}]", name, count);
    return this;
  }

  private Behavior<Message> onGetValue(GetValue command) {
    command.replyTo.tell(new Value(count));
    return this;
  }
}




private
 visibility can be defined for the 
PrivateCommand
 messages but it’s not strictly needed since they can’t be sent to an 
ActorRef[Command]
ActorRef
, which is the public message type of the actor.


Singleton messages


For messages without parameters the 
enum
 singleton pattern is recommended:




Java




copy
source
public enum Increment implements Command {
  INSTANCE
}




In the 
ReceiveBuilder
 it can be matched in same way as other messages:




Java




copy
source
.onMessage(Increment.class, notUsed -> onIncrement())




Lambdas versus method references


It’s recommended to keep the message matching with the 
ReceiveBuilder
 as short and clean as possible and delegate to methods. This improves readability and ease of method navigation with an IDE.


The delegation can be with lambdas or 
method references
.


Example of delegation using a lambda:




Java




copy
source
@Override
public Receive<Command> createReceive() {
  return newReceiveBuilder()
      .onMessage(Increment.class, notUsed -> onIncrement())
      .build();
}

private Behavior<Command> onIncrement() {
  count++;
  getContext().getLog().debug("[{}] Incremented counter to [{}]", name, count);
  return this;
}




When possible it’s preferred to use method references instead of lambdas. The benefit is less verbosity and in some cases it can actually give better type inference.




Java




copy
source
@Override
public Receive<Command> createReceive() {
  return newReceiveBuilder()
      .onMessage(GetValue.class, this::onGetValue)
      .build();
}

private Behavior<Command> onGetValue(GetValue command) {
  command.replyTo.tell(new Value(count));
  return this;
}




this::onGetValue
 is a method reference in above example. It corresponds to 
command -> onGetValue(command)
.


If you are using IntelliJ IDEA it has support for converting lambdas to method references.


More important than the choice between lambdas or method references is to avoid lambdas with a large block of code. An anti-pattern would be to inline all message handling inside the lambdas like this:




Java




copy
source
// this is an anti-pattern, don't use lambdas with a large block of code
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(
            Increment.class,
            notUsed -> {
              count++;
              getContext().getLog().debug("[{}] Incremented counter to [{}]", name, count);
              return this;
            })
        .onMessage(
            Tick.class,
            notUsed -> {
              count++;
              getContext()
                  .getLog()
                  .debug("[{}] Incremented counter by background tick to [{}]", name, count);
              return this;
            })
        .onMessage(
            GetValue.class,
            command -> {
              command.replyTo.tell(new Value(count));
              return this;
            })
        .build();
  }




In a real application it would often be more than 3 lines for each message. It’s not only making it more difficult to get an overview of the message matching, but compiler errors related to lambdas can sometimes be difficult to understand.


Ideally, lambdas should be written in one line of code. Two lines can be ok, but three is probably too much. Also, don’t use braces and return statements in one-line lambda bodies.


Partial versus total Function


It’s recommended to use a 
sealed
 trait as the super type of the commands (incoming messages) of an actor as the compiler will emit a warning if a message type is forgotten in the pattern match.




Scala




copy
source
sealed trait Command
case object Down extends Command
final case class GetValue(replyTo: ActorRef[Value]) extends Command
final case class Value(n: Int)




That is the main reason for 
Behaviors.receive
, 
Behaviors.receiveMessage
 taking a 
Function
 rather than a 
PartialFunction
.


The compiler warning if 
GetValue
 is not handled would be:


[warn] ... Counter.scala:45:34: match may not be exhaustive.
[warn] It would fail on the following input: GetValue(_)
[warn]         Behaviors.receiveMessage {
[warn]                                  ^



Note that a 
MatchError
 will be thrown at runtime if a message is not handled, so it’s important to pay attention to those. If a 
Behavior
 should not handle certain messages you can still include them in the pattern match and return 
Behaviors.unhandled
.




Scala




copy
source
val zero: Behavior[Command] = {
  Behaviors.receiveMessage {
    case GetValue(replyTo) =>
      replyTo ! Value(0)
      Behaviors.same
    case Down =>
      Behaviors.unhandled
  }
}




It’s recommended to use the 
sealed
 trait and total functions with exhaustiveness check to detect mistakes of forgetting to handle some messages. Sometimes that can be inconvenient and then you can use a 
PartialFunction
 with 
Behaviors.receivePartial
 or 
Behaviors.receiveMessagePartial




Scala




copy
source
val zero: Behavior[Command] = {
  Behaviors.receiveMessagePartial {
    case GetValue(replyTo) =>
      replyTo ! Value(0)
      Behaviors.same
  }
}




How to compose Partial Functions


Following up from previous section, there are times when one might want to combine different 
PartialFunction
s into one 
Behavior
.


A good use case for composing two or more 
PartialFunction
s is when there is a bit of behavior that repeats across different states of the Actor. Below, you can find a simplified example for this use case.


The Command definition is still highly recommended be kept within a 
sealed
 Trait:




Scala




copy
source
sealed trait Command
case object Down extends Command
final case class GetValue(replyTo: ActorRef[Value]) extends Command
final case class Value(n: Int)




In this particular case, the Behavior that is repeating over is the one in charge to handle the 
GetValue
 Command, as it behaves the same regardless of the Actor’s internal state. Instead of defining the specific handlers as a 
Behavior
, we can define them as a 
PartialFunction
:




Scala




copy
source
def getHandler(value: Int): PartialFunction[Command, Behavior[Command]] = {
  case GetValue(replyTo) =>
    replyTo ! Value(value)
    Behaviors.same
}
def setHandlerNotZero(value: Int): PartialFunction[Command, Behavior[Command]] = {
  case Down =>
    if (value == 1)
      zero
    else
      nonZero(value - 1)
}
def setHandlerZero(log: Logger): PartialFunction[Command, Behavior[Command]] = {
  case Down =>
    log.error("Counter is already at zero!")
    Behaviors.same
}




Finally, we can go on defining the two different behaviors for this specific actor. For each 
Behavior
 we would go and concatenate all needed 
PartialFunction
 instances with 
orElse
 to finally apply the command to the resulting one:




Scala




copy
source
val zero: Behavior[Command] = Behaviors.setup { context =>
  Behaviors.receiveMessagePartial(getHandler(0).orElse(setHandlerZero(context.log)))
}

def nonZero(capacity: Int): Behavior[Command] =
  Behaviors.receiveMessagePartial(getHandler(capacity).orElse(setHandlerNotZero(capacity)))

// Default Initial Behavior for this actor
def apply(initialCapacity: Int): Behavior[Command] = nonZero(initialCapacity)




Even though in this particular example we could use 
receiveMessage
 as we cover all cases, we use 
receiveMessagePartial
 instead to cover potential future unhandled message cases.


ask versus ?


When using the 
AskPattern
 it’s recommended to use the 
ask
 method rather than the infix 
?
 operator, like so:




Scala




copy
source
import akka.actor.typed.scaladsl.AskPattern._
import akka.util.Timeout

implicit val timeout: Timeout = Timeout(3.seconds)
val counter: ActorRef[Command] = ???

val result: Future[OperationResult] = counter.ask(replyTo => Increment(delta = 2, replyTo))




You may also use the more terse placeholder syntax 
_
 instead of 
replyTo
:




Scala




copy
source
val result2: Future[OperationResult] = counter.ask(Increment(delta = 2, _))




However, using the infix operator 
?
 with the placeholder syntax 
_
, like is done in the following example, won’t typecheck because of the binding scope rules for wildcard parameters:




Scala




copy
source
// doesn't compile
val result3: Future[OperationResult] = counter ? Increment(delta = 2, _)




Adding the necessary parentheses (as shown below) makes it typecheck, but, subjectively, it’s rather ugly so the recommendation is to use 
ask
.




Scala




copy
source
val result3: Future[OperationResult] = counter ? (Increment(delta = 2, _))




Note that 
AskPattern
 is only intended for request-response interaction from outside an actor. If the requester is inside an actor, prefer 
ActorContext.ask
 as it provides better thread-safety by not requiring the use of a 
Future
CompletionStage
 inside the actor.


ReceiveBuilder


Using the 
ReceiveBuilder
 is the typical, and recommended, way of defining message handlers, but it can be good to know that it’s optional in case you would prefer a different approach. Alternatives could be like:




direct processing because there is only one message type


pattern matching (
Java 21 documentation
)


if or switch statements


annotation processor




In 
Behaviors
 there are 
receive
, 
receiveMessage
 and 
receiveSignal
 factory methods that takes functions instead of using the 
ReceiveBuilder
, which is the 
receive
 with the class parameter.


In 
AbstractBehavior
 you can return your own 
akka.actor.typed.javadsl.Receive
 from 
createReceive
 instead of using 
newReceiveBuilder
. Implement the 
receiveMessage
 and 
receiveSignal
 in the 
Receive
 subclass.


Nesting setup


When an actor behavior needs more than one of 
setup
, 
withTimers
 and 
withStash
 the methods can be nested to access the needed dependencies:




Scala




copy
source
def apply(): Behavior[Command] =
  Behaviors.setup[Command](context =>
    Behaviors.withStash(100)(stash =>
      Behaviors.withTimers { timers =>
        context.log.debug("Starting up")

        // behavior using context, stash and timers ...
      }))


Java




copy
source
public static Behavior<Command> apply() {
  return Behaviors.setup(
      context ->
          Behaviors.withStash(
              100,
              stash ->
                  Behaviors.withTimers(
                      timers -> {
                        context.getLog().debug("Starting up");

                        // behavior using context, stash and timers ...
                      })));
}




The order of the nesting does not change the behavior as long as there is no additional logic in any other function than the innermost one. It can be nice to default to put 
setup
 outermost as that is the least likely block that will be removed if the actor logic changes. 


Note that adding 
supervise
 to the mix is different as it will restart the behavior it wraps, but not the behavior around itself: 




Scala




copy
source
def apply(): Behavior[Command] =
  Behaviors.setup { context =>
    // only run on initial actor start, not on crash-restart
    context.log.info("Starting")

    Behaviors
      .supervise(Behaviors.withStash[Command](100) { stash =>
        // every time the actor crashes and restarts a new stash is created (previous stash is lost)
        context.log.debug("Starting up with stash")
        // Behaviors.receiveMessage { ... }
      })
      .onFailure[RuntimeException](SupervisorStrategy.restart)
  }


Java




copy
source
public static Behavior<Command> create() {
  return Behaviors.setup(
      context -> {
        // only run on initial actor start, not on crash-restart
        context.getLog().info("Starting");

        return Behaviors.<Command>supervise(
                Behaviors.withStash(
                    100,
                    stash -> {
                      // every time the actor crashes and restarts a new stash is created
                      // (previous stash is lost)
                      context.getLog().debug("Starting up with stash");
                      // Behaviors.receiveMessage { ... }
                    }))
            .onFailure(RuntimeException.class, SupervisorStrategy.restart());
      });
}




Additional naming conventions


Some naming conventions have already been mentioned in the context of other recommendations, but here is a list of additional conventions:






replyTo
 is the typical name for the 
ActorRef[Reply]
ActorRef<Reply>
 parameter in  messages to which a reply or acknowledgement should be sent.




Incoming messages to an actor are typically called commands, and therefore the super type of all  messages that an actor can handle is typically 
sealed trait Command
interface Command {}
.




Use past tense for the events persisted by an 
EventSourcedBehavior
 since those represent facts that have happened,  for example 
Incremented
.
















 
Coexistence






Learning Akka Typed from Classic 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/introduction.html
Introduction to Akka • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka




How to get started




Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka




How to get started




Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Introduction to Akka


Welcome to Akka, a set of libraries for designing scalable, resilient systems that span processor cores and networks. Akka allows you to focus on meeting business needs instead of writing low-level code to provide reliable behavior, fault tolerance, and high performance.


Many common practices and accepted programming models do not address important challenges inherent in designing systems for modern computer architectures. To be successful, distributed systems must cope in an environment where components crash without responding, messages get lost without a trace on the wire, and network latency fluctuates. These problems occur regularly in carefully managed intra-datacenter environments - even more so in virtualized architectures.


To help you deal with these realities, Akka provides:




Multi-threaded behavior without the use of low-level concurrency constructs like  atomics or locks — relieving you from even thinking about memory visibility issues.


Transparent remote communication between systems and their components — relieving you from writing and maintaining difficult networking code.


A clustered, high-availability architecture that is elastic, scales in or out, on demand — enabling you to deliver a truly reactive system.




Akka’s use of the actor model provides a level of abstraction that makes it easier to write correct concurrent, parallel and distributed systems. The actor model spans the full set of Akka libraries, providing you with a consistent way of understanding and using them. Thus, Akka offers a depth of integration that you cannot achieve by picking libraries to solve individual problems and trying to piece them together.


By learning Akka and how to use the actor model, you will gain access to a vast and deep set of tools that solve difficult distributed/parallel systems problems in a uniform programming model where everything fits together tightly and efficiently.


How to get started


If this is your first experience with Akka, we recommend that you start by running a simple Hello World project. See the 
first Hello World example
 for instructions on downloading and running the Hello World example. That example walks you through example code that introduces how to define actor systems, actors, and messages. Within 10 minutes, you should be able to run the Hello World example and learn how it is constructed.


This 
Getting Started
 guide provides the next level of information. It covers why the actor model fits the needs of modern distributed systems and includes a tutorial that will help further your knowledge of Akka. Topics include:




Why modern systems need a new programming model


How the actor model meets the needs of concurrent, distributed systems


Overview of Akka libraries and modules


A 
more complex example
 that builds on the Hello World example to illustrate common Akka patterns.
















 
Getting Started Guide






Why modern systems need a new programming model 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-grpc/current
Akka gRPC







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka gRPC





Version 2.5.3





Java
Scala


sbt
Gradle
Maven










Overview


Why gRPC?


Getting started


Protobuf Service Descriptors


Providing Services (Server)


Consuming Services (Client)


Build Tool Support


Binary compatibility


API Design


Deployment


Mutual authentication with TLS


Troubleshooting


















Akka gRPC





Version 2.5.3





Java
Scala


sbt
Gradle
Maven












Overview


Why gRPC?


Getting started


Protobuf Service Descriptors


Providing Services (Server)


Consuming Services (Client)


Build Tool Support


Binary compatibility


API Design


Deployment


Mutual authentication with TLS


Troubleshooting




















Akka gRPC






Overview




gRPC


Akka gRPC


Project Information


Project Status




Why gRPC?




gRPC vs REST


gRPC vs SOAP


gRPC vs Message Bus


gRPC vs Akka Remoting




Getting started




Akka gRPC Quickstart


Video Introduction




Protobuf Service Descriptors




Messages


Services


Code generation options




Providing Services (Server)




Walkthrough




Setting up




Dependencies




Writing a service definition


Generating interfaces and stubs


Implementing the service


Serving the service with Akka HTTP


Serving multiple services


Running the server


Stateful services




gRPC-Web


Server Reflection




Providing


Consuming




Akka HTTP interop




Example: authentication/authorization




Akka HTTP authentication route


Akka gRPC route


Securing the Akka gRPC route


Tying it all together




Example: logging, error handling, and passing request context




Implementation


Method to log, handle, and recover each RPC


Custom error mapping


Tying it all together


Results




Future work




Details




Accessing request metadata


Status codes


Rich error model




Kubernetes




LoadBalancer Service


NGINX Ingress


GCE Ingress


Google Cloud Endpoints






Consuming Services (Client)




Walkthrough




Setting up




Dependencies




Generating Service Stubs


Writing a Client Program




Configuration




By Code


By Configuration


Using Akka Discovery for Endpoint Discovery


Debug logging




Details




Client Lifecycle


Shared Channels


Channel laziness


Load balancing


Request Metadata


Rich error model






Build Tool Support




sbt




Configuring what to generate




Configurations


Generating server “power APIs”




Passing parameters to the generators




ScalaPB settings




Using a local 
protoc
 command




sbt-protoc
 settings




Loading proto files from artifacts


Starting your Akka gRPC server from sbt




Gradle




Configuring plugin




Installation


Available plugin options


Generating server “power APIs”




Protoc version


Proto source directory


Loading proto files from artifacts


Starting your Akka gRPC server from gradle




Maven




Configuring what to generate




Generating server “power APIs”




Proto source directory


Loading proto files from artifacts


Starting your Akka gRPC server from Maven






Binary compatibility




Limitations




New features


Deprecations


Internal and ApiMayChange API’s




Upstream libraries




API Design




Methods without request or response


Declare and enforce constraints for your request and response payloads with 
protoc-gen-validate




Java support


Scala support






Deployment




Serve gRPC over HTTPS


Building Native Images




Mutual authentication with TLS




Setting the server up


Setting the client up


Further limiting of access using client certificate identities




Troubleshooting




Client


Server
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka gRPC is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/routers.html
Routers • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers




Dependency


Introduction


Pool Router


Group Router


Routing strategies


Routers and performance




Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers




Dependency


Introduction


Pool Router


Group Router


Routing strategies


Routers and performance




Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Routers


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Routing
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


In some cases it is useful to distribute messages of the same type over a set of actors, so that messages can be processed in parallel - a single actor will only process one message at a time.


The router itself is a behavior that is spawned into a running actor that will then forward any message sent to it to one final recipient out of the set of routees.


There are two kinds of routers included in Akka Typed - the pool router and the group router.


Pool Router


The pool router is created with a routee 
Behavior
 and spawns a number of children with that behavior which it will then forward messages to.


If a child is stopped the pool router removes it from its set of routees. When the last child stops the router itself stops. To make a resilient router that deals with failures the routee 
Behavior
 must be supervised.


As actor children are always local the routees are never spread across a cluster with a pool router.


Let’s first introduce the routee:




Scala




copy
source
object Worker {
  sealed trait Command
  case class DoLog(text: String) extends Command

  def apply(): Behavior[Command] = Behaviors.setup { context =>
    context.log.info("Starting worker")

    Behaviors.receiveMessage {
      case DoLog(text) =>
        context.log.info("Got message {}", text)
        Behaviors.same
    }
  }
}



Java




copy
source
class Worker {
  interface Command {}

  static class DoLog implements Command {
    public final String text;

    public DoLog(String text) {
      this.text = text;
    }
  }

  static final Behavior<Command> create() {
    return Behaviors.setup(
        context -> {
          context.getLog().info("Starting worker");

          return Behaviors.receive(Command.class)
              .onMessage(DoLog.class, doLog -> onDoLog(context, doLog))
              .build();
        });
  }

  private static Behavior<Command> onDoLog(ActorContext<Command> context, DoLog doLog) {
    context.getLog().info("Got message {}", doLog.text);
    return Behaviors.same();
  }
}

static class Proxy {

  public final ServiceKey<Message> registeringKey =
      ServiceKey.create(Message.class, "aggregator-key");

  public String mapping(Message message) {
    return message.getId();
  }

  static class Message {

    public Message(String id, String content) {
      this.id = id;
      this.content = content;
    }

    private String content;
    private String id;

    public final String getContent() {
      return content;
    }

    public final String getId() {
      return id;
    }
  }

  static Behavior<Message> create(ActorRef<String> monitor) {
    return Behaviors.receive(Message.class)
        .onMessage(Message.class, in -> onMyMessage(monitor, in))
        .build();
  }

  private static Behavior<Message> onMyMessage(ActorRef<String> monitor, Message message) {
    monitor.tell(message.getId());
    return Behaviors.same();
  }
}





After having defined the routee, we can now concentrate on configuring the router itself. Note again the the router is an Actor in itself:




Scala




copy
source
import akka.actor.testkit.typed.scaladsl.{ LogCapturing, ScalaTestWithActorTestKit }
import akka.actor.typed.{ ActorRef, Behavior, SupervisorStrategy }
import akka.actor.typed.receptionist.{ Receptionist, ServiceKey }
import akka.actor.typed.scaladsl.{ Behaviors, Routers }

// This would be defined within your actor object
Behaviors.setup[Unit] { ctx =>
  val pool = Routers.pool(poolSize = 4) {
    // make sure the workers are restarted if they fail
    Behaviors.supervise(Worker()).onFailure[Exception](SupervisorStrategy.restart)
  }
  val router = ctx.spawn(pool, "worker-pool")

  (0 to 10).foreach { n =>
    router ! Worker.DoLog(s"msg $n")
  }

  val poolWithBroadcast = pool.withBroadcastPredicate(_.isInstanceOf[DoBroadcastLog])
  val routerWithBroadcast = ctx.spawn(poolWithBroadcast, "pool-with-broadcast")
  //this will be sent to all 4 routees
  routerWithBroadcast ! DoBroadcastLog("msg")
  Behaviors.empty
}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.SupervisorStrategy;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.GroupRouter;
import akka.actor.typed.javadsl.PoolRouter;
import akka.actor.typed.javadsl.Routers;
import akka.actor.typed.receptionist.Receptionist;
import akka.actor.typed.receptionist.ServiceKey;

import akka.actor.testkit.typed.javadsl.TestKitJunitResource;
import akka.actor.testkit.typed.javadsl.TestProbe;

import org.junit.ClassRule;
import org.junit.Test;
import org.scalatestplus.junit.JUnitSuite;

// This would be defined within your actor class
Behaviors.setup(
    context -> {
      int poolSize = 4;
      PoolRouter<Worker.Command> pool =
          Routers.pool(
              poolSize,
              // make sure the workers are restarted if they fail
              Behaviors.supervise(Worker.create()).onFailure(SupervisorStrategy.restart()));
      ActorRef<Worker.Command> router = context.spawn(pool, "worker-pool");

      for (int i = 0; i < 10; i++) {
        router.tell(new Worker.DoLog("msg " + i));
      }
    });




Configuring Dispatchers


Since the router itself is spawned as an actor the dispatcher used for it can be configured directly in the call to 
spawn
. The routees, however, are spawned by the router. Therefore, the 
PoolRouter
 has a property to configure the 
Props
 of its routees:




Scala




copy
source
// make sure workers use the default blocking IO dispatcher
val blockingPool = pool.withRouteeProps(routeeProps = DispatcherSelector.blocking())
// spawn head router using the same executor as the parent
val blockingRouter = ctx.spawn(blockingPool, "blocking-pool", DispatcherSelector.sameAsParent())


Java




copy
source
// make sure workers use the default blocking IO dispatcher
PoolRouter<Worker.Command> blockingPool =
    pool.withRouteeProps(DispatcherSelector.blocking());
// spawn head router using the same executor as the parent
ActorRef<Worker.Command> blockingRouter =
    context.spawn(blockingPool, "blocking-pool", DispatcherSelector.sameAsParent());




Broadcasting a message to all routees


Pool routers can be configured to identify messages intended to be broad-casted to all routees. Therefore, the 
PoolRouter
 has a property to configure its 
broadcastPredicate
:




Scala




copy
source
val poolWithBroadcast = pool.withBroadcastPredicate(_.isInstanceOf[DoBroadcastLog])
val routerWithBroadcast = ctx.spawn(poolWithBroadcast, "pool-with-broadcast")
//this will be sent to all 4 routees
routerWithBroadcast ! DoBroadcastLog("msg")
Behaviors.empty


Java




copy
source
PoolRouter<Worker.Command> broadcastingPool =
    pool.withBroadcastPredicate(msg -> msg instanceof DoBroadcastLog);




Group Router


The group router is created with a 
ServiceKey
 and uses the receptionist (see 
Receptionist
) to discover available actors for that key and routes messages to one of the currently known registered actors for a key.


Since the receptionist is used this means the group router is cluster-aware out of the box. The router sends messages to registered actors on any node in the cluster that is reachable. If no reachable actor exists the router will fallback and route messages to actors on nodes marked as unreachable.


That the receptionist is used also means that the set of routees is eventually consistent, and that immediately when the group router is started the set of routees it knows about is empty, until it has seen a listing from the receptionist it stashes incoming messages and forwards them as soon as it gets a listing from the receptionist. 


When the router has received a listing from the receptionist and the set of registered actors is empty the router will drop them (publish them to the event stream as 
akka.actor.Dropped
).




Scala




copy
source
val serviceKey = ServiceKey[Worker.Command]("log-worker")

Behaviors.setup[Unit] { ctx =>
  // this would likely happen elsewhere - if we create it locally we
  // can just as well use a pool
  val worker = ctx.spawn(Worker(), "worker")
  ctx.system.receptionist ! Receptionist.Register(serviceKey, worker)

  val group = Routers.group(serviceKey)
  val router = ctx.spawn(group, "worker-group")

  // the group router will stash messages until it sees the first listing of registered
  // services from the receptionist, so it is safe to send messages right away
  (0 to 10).foreach { n =>
    router ! Worker.DoLog(s"msg $n")
  }

  Behaviors.empty
}


Java




copy
source
ServiceKey<Worker.Command> serviceKey = ServiceKey.create(Worker.Command.class, "log-worker");

Behaviors.setup(
    context -> {

      // this would likely happen elsewhere - if we create it locally we
      // can just as well use a pool
      ActorRef<Worker.Command> worker = context.spawn(Worker.create(), "worker");
      context.getSystem().receptionist().tell(Receptionist.register(serviceKey, worker));

      GroupRouter<Worker.Command> group = Routers.group(serviceKey);
      ActorRef<Worker.Command> router = context.spawn(group, "worker-group");

      // the group router will stash messages until it sees the first listing of
      // registered
      // services from the receptionist, so it is safe to send messages right away
      for (int i = 0; i < 10; i++) {
        router.tell(new Worker.DoLog("msg " + i));
      }

      return Behaviors.empty();
    });




Routing strategies


There are three different strategies for selecting which routee a message is forwarded to that can be selected from the router before spawning it:




Scala




copy
source
val alternativePool = pool.withPoolSize(2).withRoundRobinRouting()


Java




copy
source
PoolRouter<Worker.Command> alternativePool = pool.withPoolSize(2).withRoundRobinRouting();




Round Robin


Rotates over the set of routees making sure that if there are 
n
 routees, then for 
n
 messages sent through the router, each actor is forwarded one message.


Round robin gives fair routing where every available routee gets the same amount of messages as long as the set of routees stays relatively stable, but may be unfair if the set of routees changes a lot.


This is the default for pool routers as the pool of routees is expected to remain the same.


An optional parameter 
preferLocalRoutees
 can be used for this strategy. Routers will only use routees located in local actor system if 
preferLocalRoutees
 is true and local routees do exist. The default value for this parameter is false.


Random


Randomly selects a routee when a message is sent through the router.


This is the default for group routers as the group of routees is expected to change as nodes join and leave the cluster.


An optional parameter 
preferLocalRoutees
 can be used for this strategy. Routers will only use routees located in local actor system if 
preferLocalRoutees
 is true and local routees do exist. The default value for this parameter is false.


Consistent Hashing


Uses 
consistent hashing
 to select a routee based on the sent message. This 
article
 gives good insight into how consistent hashing is implemented.


Currently you have to define hashMapping of the router to map incoming messages to their consistent hash key. This makes the decision transparent for the sender.


Consistent hashing delivers messages with the same hash to the same routee as long as the set of routees stays the same. When the set of routees changes, consistent hashing tries to make sure, but does not guarantee, that messages with the same hash are routed to the same routee.




Scala




copy
source
val router = spawn(Routers.group(Proxy.RegisteringKey).withConsistentHashingRouting(10, Proxy.mapping))

router ! Proxy.Message("123", "Text1")
router ! Proxy.Message("123", "Text2")

router ! Proxy.Message("zh3", "Text3")
router ! Proxy.Message("zh3", "Text4")
// the hash is calculated over the Proxy.Message first parameter obtained through the Proxy.mapping function


Java




copy
source
ActorRef<Proxy.Message> router =
    testKit.spawn(
        Routers.group(proxy.registeringKey)
            .withConsistentHashingRouting(10, command -> proxy.mapping(command)));

router.tell(new Proxy.Message("123", "Text1"));
router.tell(new Proxy.Message("123", "Text2"));

router.tell(new Proxy.Message("zh3", "Text3"));
router.tell(new Proxy.Message("zh3", "Text4"));
// the hash is calculated over the Proxy.Message first parameter obtained through the
// Proxy.mapping function




See also 
Akka Cluster Sharding
 which provides stable routing and rebalancing of the routee actors.


Routers and performance


Note that if the routees are sharing a resource, the resource will determine if increasing the number of actors will actually give higher throughput or faster answers. For example if the routees are CPU bound actors it will not give better performance to create more routees than there are threads to execute the actors.


Since the router itself is an actor and has a mailbox this means that messages are routed sequentially to the routees where it can be processed in parallel (depending on the available threads in the dispatcher). In a high throughput use cases the sequential routing could become a bottle neck. Akka Typed does not provide an optimized tool for this.














 
Actor discovery






Stash 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/serialization-jackson.html
Serialization with Jackson • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson




Dependency


Introduction


Usage


Security


Annotations


Schema Evolution


Rolling updates


Jackson Modules


Using Akka Serialization for embedded types


Additional configuration


Additional features




Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson




Dependency


Introduction


Usage


Security


Annotations


Schema Evolution


Rolling updates


Jackson Modules


Using Akka Serialization for embedded types


Additional configuration


Additional features




Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Serialization with Jackson


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Jackson Serialization, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-serialization-jackson" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-serialization-jackson_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-serialization-jackson_${versions.ScalaBinary}"
}


Introduction


You find general concepts for Akka serialization in the 
Serialization
 section. This section describes how to use the Jackson serializer for application specific messages and persistent events and snapshots.


Jackson
 has support for both text based JSON and binary formats.


In many cases ordinary classes can be serialized by Jackson without any additional hints, but sometimes annotations are needed to specify how to convert the objects to JSON/bytes.


Usage


To enable Jackson serialization for a class there needs to be a serialization binding for it or one of its super classes in serialization-bindings configuration. 


You can use one of the two predefined marker 
traits
interfaces
 
akka.serialization.jackson.JsonSerializable
 or 
akka.serialization.jackson.CborSerializable
.




Scala




copy
source
import akka.serialization.jackson.JsonSerializable

final case class Message(name: String, nr: Int) extends JsonSerializable


Java




copy
source
import akka.serialization.jackson.JsonSerializable;

class MyMessage implements JsonSerializable {
  public final String name;
  public final int nr;

  public MyMessage(String name, int nr) {
    this.name = name;
    this.nr = nr;
  }
}




If the pre-defined markers are not suitable for your project it is also possible to define your own marker 
trait
interface
 and let the messages 
extend
implement
 that. You will then have to add serialization binding configuration for your own marker in config, see 
Serialization
 for more details.


That is all that is needed for basic classes where Jackson understands the structure. A few cases that requires annotations are described below.
Note


Add the 
-parameters
 Java compiler option for usage by the 
ParameterNamesModule
. It reduces the need for some annotations.


Security


For security reasons it is disallowed to bind the Jackson serializers to open-ended types that might be a target for 
serialization gadgets
, such as:




java.lang.Object


java.io.Serializable


java.lang.Comparable
.




The deny list of possible serialization gadget classes defined by Jackson databind are checked and disallowed for deserialization.
Warning


Don’t use 
@JsonTypeInfo(use = Id.CLASS)
 or 
ObjectMapper.enableDefaultTyping
 since that is a security risk when using 
polymorphic types
.


Formats


The following formats are supported, and you select which one to use in the 
serialization-bindings
 configuration as described above.




jackson-json
 - ordinary text based JSON


jackson-cbor
 - binary 
CBOR data format




The binary format is more compact, with slightly better performance than the JSON format.


Annotations


Polymorphic types


A polymorphic type is when a certain base type has multiple alternative implementations. When nested fields or collections are of polymorphic type the concrete implementations of the type must be listed with 
@JsonTypeInfo
 and 
@JsonSubTypes
 annotations.


Example:




Scala




copy
source
final case class Zoo(primaryAttraction: Animal) extends JsonSerializable

@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
@JsonSubTypes(
  Array(
    new JsonSubTypes.Type(value = classOf[Lion], name = "lion"),
    new JsonSubTypes.Type(value = classOf[Elephant], name = "elephant")))
sealed trait Animal

final case class Lion(name: String) extends Animal

final case class Elephant(name: String, age: Int) extends Animal


Java




copy
source
public class Zoo implements JsonSerializable {
  public final Animal primaryAttraction;

  public Zoo(Animal primaryAttraction) {
    this.primaryAttraction = primaryAttraction;
  }
}

@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
@JsonSubTypes({
  @JsonSubTypes.Type(value = Lion.class, name = "lion"),
  @JsonSubTypes.Type(value = Elephant.class, name = "elephant")
})
interface Animal {}

public final class Lion implements Animal {
  public final String name;

  public Lion(String name) {
    this.name = name;
  }
}

public final class Elephant implements Animal {
  public final String name;
  public final int age;

  public Elephant(String name, int age) {
    this.name = name;
    this.age = age;
  }
}




If you haven’t defined the annotations you will see an exception like this:


InvalidDefinitionException: Cannot construct instance of `...` (no Creators, like default construct, exist): abstract types either need to be mapped to concrete types, have custom deserializer, or contain additional type information



Note that this is not needed for a top level class, but for fields inside it. In this example 
Animal
 is used inside of 
Zoo
, which is sent as a message or persisted. If 
Animal
 was sent or persisted standalone the annotations are not needed because then it is the concrete subclasses 
Lion
 or 
Elephant
 that are serialized.


When specifying allowed subclasses with those annotations the class names will not be included in the serialized representation and that is important for 
preventing loading of malicious serialization gadgets
 when deserializing.
Warning


Don’t use 
@JsonTypeInfo(use = Id.CLASS)
 or 
ObjectMapper.enableDefaultTyping
 since that is a security risk when using polymorphic types.


ADT with trait and case object


It’s common in Scala to use a sealed trait and case objects to represent enums. If the values are case classes the 
@JsonSubTypes
 annotation as described above works, but if the values are case objects it will not. The annotation requires a 
Class
 and there is no way to define that in an annotation for a 
case object
.


The easiest workaround is to define the case objects as case class without any field. 


Alternatively, you can define an intermediate trait for the case object and a custom deserializer for it. The example below builds on the previous 
Animal
 sample by adding a fictitious, single instance, new animal, an 
Unicorn
. 




Scala




copy
source
final case class Zoo(primaryAttraction: Animal) extends JsonSerializable

@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type")
@JsonSubTypes(
  Array(
    new JsonSubTypes.Type(value = classOf[Lion], name = "lion"),
    new JsonSubTypes.Type(value = classOf[Elephant], name = "elephant"),
    new JsonSubTypes.Type(value = classOf[Unicorn], name = "unicorn")))
sealed trait Animal

final case class Lion(name: String) extends Animal
final case class Elephant(name: String, age: Int) extends Animal

@JsonDeserialize(`using` = classOf[UnicornDeserializer])
sealed trait Unicorn extends Animal
@JsonTypeName("unicorn")
case object Unicorn extends Unicorn

class UnicornDeserializer extends StdDeserializer[Unicorn](Unicorn.getClass) {
  // whenever we need to deserialize an instance of Unicorn trait, we return the object Unicorn
  override def deserialize(p: JsonParser, ctxt: DeserializationContext): Unicorn = Unicorn
}




The case object 
Unicorn
 can’t be used in a 
@JsonSubTypes
 annotation, but its trait can. When serializing the case object we need to know which type tag to use, hence the 
@JsonTypeName
 annotation on the object. When deserializing, Jackson will only know about the trait variant therefore we need a custom deserializer that returns the case object. 


On the other hand, if the ADT only has case objects, you can solve it by implementing a custom serialization for the enums. Annotate the 
trait
 with 
@JsonSerialize
 and 
@JsonDeserialize
 and implement the serialization with 
StdSerializer
 and 
StdDeserializer
.




Scala




copy
source
import com.fasterxml.jackson.core.JsonGenerator
import com.fasterxml.jackson.core.JsonParser
import com.fasterxml.jackson.databind.DeserializationContext
import com.fasterxml.jackson.databind.SerializerProvider
import com.fasterxml.jackson.databind.annotation.JsonDeserialize
import com.fasterxml.jackson.databind.annotation.JsonSerialize
import com.fasterxml.jackson.databind.deser.std.StdDeserializer
import com.fasterxml.jackson.databind.ser.std.StdSerializer

@JsonSerialize(`using` = classOf[DirectionJsonSerializer])
@JsonDeserialize(`using` = classOf[DirectionJsonDeserializer])
sealed trait Direction

object Direction {
  case object North extends Direction
  case object East extends Direction
  case object South extends Direction
  case object West extends Direction
}

class DirectionJsonSerializer extends StdSerializer[Direction](classOf[Direction]) {
  import Direction._

  override def serialize(value: Direction, gen: JsonGenerator, provider: SerializerProvider): Unit = {
    val strValue = value match {
      case North => "N"
      case East  => "E"
      case South => "S"
      case West  => "W"
    }
    gen.writeString(strValue)
  }
}

class DirectionJsonDeserializer extends StdDeserializer[Direction](classOf[Direction]) {
  import Direction._

  override def deserialize(p: JsonParser, ctxt: DeserializationContext): Direction = {
    p.getText match {
      case "N" => North
      case "E" => East
      case "S" => South
      case "W" => West
    }
  }
}

final case class Compass(currentDirection: Direction) extends JsonSerializable




Enumerations


Jackson support for Scala Enumerations defaults to serializing a 
Value
 as a 
JsonObject
 that includes a field with the 
"value"
 and a field with the 
"type"
 whose value is the FQCN of the enumeration. Jackson includes the 
@JsonScalaEnumeration
 to statically specify the type information to a field. When using the 
@JsonScalaEnumeration
 annotation the enumeration value is serialized as a JsonString.




Scala




copy
source
object Planet extends Enumeration {
  type Planet = Value
  val Mercury, Venus, Earth, Mars, Krypton = Value
}

// Uses default Jackson serialization format for Scala Enumerations
final case class Alien(name: String, planet: Planet.Planet) extends TestMessage

// Serializes planet values as a JsonString
class PlanetType extends TypeReference[Planet.type] {}
final case class Superhero(name: String, @JsonScalaEnumeration(classOf[PlanetType]) planet: Planet.Planet)
    extends TestMessage




Schema Evolution


When using Event Sourcing, but also for rolling updates, schema evolution becomes an important aspect of developing your application. The requirements as well as our own understanding of the business domain may (and will) change over time.


The Jackson serializer provides a way to perform transformations of the JSON tree model during deserialization. This is working in the same way for the textual and binary formats.


We will look at a few scenarios of how the classes may be evolved.


Remove Field


Removing a field can be done without any migration code. The Jackson serializer will ignore properties that does not exist in the class.


Add Optional Field


Adding an optional field can be done without any migration code. The default value will be 
None
Optional.empty
.


Old class:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;
  public final String productId;
  public final int quantity;

  public ItemAdded(String shoppingCartId, String productId, int quantity) {
    this.shoppingCartId = shoppingCartId;
    this.productId = productId;
    this.quantity = quantity;
  }
}




New class with a new optional 
discount
 property and a new 
note
 field with default value:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, productId: String, quantity: Int, discount: Option[Double], note: String)
    extends JsonSerializable {

  // alternative constructor because `note` should have default value "" when not defined in json
  @JsonCreator
  def this(shoppingCartId: String, productId: String, quantity: Int, discount: Option[Double], note: Option[String]) =
    this(shoppingCartId, productId, quantity, discount, note.getOrElse(""))
}


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;
  public final String productId;
  public final int quantity;
  public final Optional<Double> discount;
  public final String note;

  @JsonCreator
  public ItemAdded(
      String shoppingCartId,
      String productId,
      int quantity,
      Optional<Double> discount,
      String note) {
    this.shoppingCartId = shoppingCartId;
    this.productId = productId;
    this.quantity = quantity;
    this.discount = discount;

    // default for note is "" if not included in json
    if (note == null) this.note = "";
    else this.note = note;
  }

  public ItemAdded(
      String shoppingCartId, String productId, int quantity, Optional<Double> discount) {
    this(shoppingCartId, productId, quantity, discount, "");
  }
}




Add Mandatory Field


Let’s say we want to have a mandatory 
discount
 property without default value instead:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, productId: String, quantity: Int, discount: Double)
    extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;
  public final String productId;
  public final int quantity;
  public final double discount;

  public ItemAdded(String shoppingCartId, String productId, int quantity, double discount) {
    this.shoppingCartId = shoppingCartId;
    this.productId = productId;
    this.quantity = quantity;
    this.discount = discount;
  }
}




To add a new mandatory field we have to use a 
JacksonMigration
JacksonMigration
 class and set the default value in the migration code.


This is how a migration class would look like for adding a 
discount
 field:




Scala




copy
source
import com.fasterxml.jackson.databind.JsonNode
import com.fasterxml.jackson.databind.node.DoubleNode
import com.fasterxml.jackson.databind.node.ObjectNode
import akka.serialization.jackson.JacksonMigration

class ItemAddedMigration extends JacksonMigration {

  override def currentVersion: Int = 2

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {
    val root = json.asInstanceOf[ObjectNode]
    if (fromVersion <= 1) {
      root.set("discount", DoubleNode.valueOf(0.0))
    }
    root
  }
}


Java




copy
source
import akka.serialization.jackson.JacksonMigration;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.DoubleNode;
import com.fasterxml.jackson.databind.node.ObjectNode;

public class ItemAddedMigration extends JacksonMigration {

  @Override
  public int currentVersion() {
    return 2;
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    ObjectNode root = (ObjectNode) json;
    if (fromVersion <= 1) {
      root.set("discount", DoubleNode.valueOf(0.0));
    }
    return root;
  }
}




Override the 
currentVersion
currentVersion()
 method to define the version number of the current (latest) version. The first version, when no migration was used, is always 1. Increase this version number whenever you perform a change that is not backwards compatible without migration code.


Implement the transformation of the old JSON structure to the new JSON structure in the 
transform(fromVersion, jsonNode)
transform(fromVersion, jsonNode)
 method. The 
JsonNode
 is mutable, so you can add and remove fields, or change values. Note that you have to cast to specific sub-classes such as 
ObjectNode
 and 
ArrayNode
 to get access to mutators.


The migration class must be defined in configuration file:


copy
source
akka.serialization.jackson.migrations {
  "com.myservice.event.ItemAdded" = "com.myservice.event.ItemAddedMigration"
}


The same thing could have been done for the 
note
 field, adding a default value of 
""
 in the 
ItemAddedMigration
.


Rename Field


Let’s say that we want to rename the 
productId
 field to 
itemId
 in the previous example.




Scala




copy
source
case class ItemAdded(shoppingCartId: String, itemId: String, quantity: Int) extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;

  public final String itemId;

  public final int quantity;

  public ItemAdded(String shoppingCartId, String itemId, int quantity) {
    this.shoppingCartId = shoppingCartId;
    this.itemId = itemId;
    this.quantity = quantity;
  }
}




The migration code would look like:




Scala




copy
source
import akka.serialization.jackson.JacksonMigration
import com.fasterxml.jackson.databind.JsonNode
import com.fasterxml.jackson.databind.node.ObjectNode

class ItemAddedMigration extends JacksonMigration {

  override def currentVersion: Int = 2

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {
    val root = json.asInstanceOf[ObjectNode]
    if (fromVersion <= 1) {
      root.set[JsonNode]("itemId", root.get("productId"))
      root.remove("productId")
    }
    root
  }
}


Java




copy
source
import akka.serialization.jackson.JacksonMigration;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;

public class ItemAddedMigration extends JacksonMigration {

  @Override
  public int currentVersion() {
    return 2;
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    ObjectNode root = (ObjectNode) json;
    if (fromVersion <= 1) {
      root.set("itemId", root.get("productId"));
      root.remove("productId");
    }
    return root;
  }
}




Structural Changes


In a similar way we can do arbitrary structural changes.


Old class:




Scala




copy
source
case class Customer(name: String, street: String, city: String, zipCode: String, country: String)
    extends JsonSerializable


Java




copy
source
public class Customer implements JsonSerializable {
  public final String name;
  public final String street;
  public final String city;
  public final String zipCode;
  public final String country;

  public Customer(String name, String street, String city, String zipCode, String country) {
    this.name = name;
    this.street = street;
    this.city = city;
    this.zipCode = zipCode;
    this.country = country;
  }
}




New class:




Scala




copy
source
case class Customer(name: String, shippingAddress: Address, billingAddress: Option[Address]) extends JsonSerializable


Java




copy
source
public class Customer implements JsonSerializable {
  public final String name;
  public final Address shippingAddress;
  public final Optional<Address> billingAddress;

  public Customer(String name, Address shippingAddress, Optional<Address> billingAddress) {
    this.name = name;
    this.shippingAddress = shippingAddress;
    this.billingAddress = billingAddress;
  }
}




with the 
Address
 class:




Scala




copy
source
case class Address(street: String, city: String, zipCode: String, country: String) extends JsonSerializable


Java




copy
source
public class Address {
  public final String street;
  public final String city;
  public final String zipCode;
  public final String country;

  public Address(String street, String city, String zipCode, String country) {
    this.street = street;
    this.city = city;
    this.zipCode = zipCode;
    this.country = country;
  }
}




The migration code would look like:




Scala




copy
source
import akka.serialization.jackson.JacksonMigration
import com.fasterxml.jackson.databind.JsonNode
import com.fasterxml.jackson.databind.node.ObjectNode

class CustomerMigration extends JacksonMigration {

  override def currentVersion: Int = 2

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {
    val root = json.asInstanceOf[ObjectNode]
    if (fromVersion <= 1) {
      val shippingAddress = root.withObject("/shippingAddress")
      shippingAddress.set[JsonNode]("street", root.get("street"))
      shippingAddress.set[JsonNode]("city", root.get("city"))
      shippingAddress.set[JsonNode]("zipCode", root.get("zipCode"))
      shippingAddress.set[JsonNode]("country", root.get("country"))
      root.remove("street")
      root.remove("city")
      root.remove("zipCode")
      root.remove("country")
    }
    root
  }
}


Java




copy
source
import akka.serialization.jackson.JacksonMigration;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;

public class CustomerMigration extends JacksonMigration {

  @Override
  public int currentVersion() {
    return 2;
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    ObjectNode root = (ObjectNode) json;
    if (fromVersion <= 1) {
      ObjectNode shippingAddress = root.withObject("/shippingAddress");
      shippingAddress.set("street", root.get("street"));
      shippingAddress.set("city", root.get("city"));
      shippingAddress.set("zipCode", root.get("zipCode"));
      shippingAddress.set("country", root.get("country"));
      root.remove("street");
      root.remove("city");
      root.remove("zipCode");
      root.remove("country");
    }
    return root;
  }
}




Rename Class


It is also possible to rename the class. For example, let’s rename 
OrderAdded
 to 
OrderPlaced
.


Old class:




Scala




copy
source
case class OrderAdded(shoppingCartId: String) extends JsonSerializable


Java




copy
source
public class OrderAdded implements JsonSerializable {
  public final String shoppingCartId;

  public OrderAdded(String shoppingCartId) {
    this.shoppingCartId = shoppingCartId;
  }
}




New class:




Scala




copy
source
case class OrderPlaced(shoppingCartId: String) extends JsonSerializable


Java




copy
source
public class OrderPlaced implements JsonSerializable {
  public final String shoppingCartId;

  public OrderPlaced(String shoppingCartId) {
    this.shoppingCartId = shoppingCartId;
  }
}




The migration code would look like:




Scala




copy
source
class OrderPlacedMigration extends JacksonMigration {

  override def currentVersion: Int = 2

  override def transformClassName(fromVersion: Int, className: String): String =
    classOf[OrderPlaced].getName

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = json
}


Java




copy
source
public class OrderPlacedMigration extends JacksonMigration {

  @Override
  public int currentVersion() {
    return 2;
  }

  @Override
  public String transformClassName(int fromVersion, String className) {
    return OrderPlaced.class.getName();
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    return json;
  }
}




Note the override of the 
transformClassName(fromVersion, className)
transformClassName(fromVersion, className)
 method to define the new class name.


That type of migration must be configured with the old class name as key. The actual class can be removed.


copy
source
akka.serialization.jackson.migrations {
  "com.myservice.event.OrderAdded" = "com.myservice.event.OrderPlacedMigration"
}


Remove from serialization-bindings


When a class is not used for serialization anymore it can be removed from 
serialization-bindings
 but to still allow deserialization it must then be listed in the 
allowed-class-prefix
 configuration. This is useful for example during rolling update with serialization changes, or when reading old stored data. It can also be used when changing from Jackson serializer to another serializer (e.g. Protobuf) and thereby changing the serialization binding, but it should still be possible to deserialize old data with Jackson.


copy
source
akka.serialization.jackson.allowed-class-prefix =
  ["com.myservice.event.OrderAdded", "com.myservice.command"]


It’s a list of class names or prefixes of class names.


Rolling updates


When doing a rolling update, for a period of time there are two different binaries running in production. If the schema has evolved requiring a new schema version, the data serialized by the new binary will be unreadable from the old binary. This situation causes transient errors on the processes running the old binary. This service degradation is usually fine since the rolling update will eventually complete and all old processes will be replaced with the new binary. To avoid this service degradation you can also use forward-one support in your schema evolutions.


To complete a no-degradation rolling update, you need to make two deployments. First, deploy a new binary which can read the new schema but still uses the old schema. Then, deploy a second binary which serializes data using the new schema and drops the downcasting code from the migration. 


Let’s take, for example, the case above where we 
renamed a field
.


The starting schema is:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;
  public final String productId;
  public final int quantity;

  public ItemAdded(String shoppingCartId, String productId, int quantity) {
    this.shoppingCartId = shoppingCartId;
    this.productId = productId;
    this.quantity = quantity;
  }
}




In a first deployment, we still don’t make any change to the event class:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;
  public final String productId;
  public final int quantity;

  public ItemAdded(String shoppingCartId, String productId, int quantity) {
    this.shoppingCartId = shoppingCartId;
    this.productId = productId;
    this.quantity = quantity;
  }
}




but we introduce a migration that can read the newer schema which is versioned 
2
:




Scala




copy
source
import akka.serialization.jackson.JacksonMigration
import com.fasterxml.jackson.databind.JsonNode
import com.fasterxml.jackson.databind.node.ObjectNode

class ItemAddedMigration extends JacksonMigration {

  // Data produced in this node is still produced using the version 1 of the schema
  override def currentVersion: Int = 1

  override def supportedForwardVersion: Int = 2

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {
    val root = json.asInstanceOf[ObjectNode]
    if (fromVersion == 2) {
      // When receiving an event of version 2 we down-cast it to the version 1 of the schema
      root.set[JsonNode]("productId", root.get("itemId"))
      root.remove("itemId")
    }
    root
  }
}


Java




copy
source
import akka.serialization.jackson.JacksonMigration;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;

public class ItemAddedMigration extends JacksonMigration {

  // Data produced in this node is still produced using the version 1 of the schema
  @Override
  public int currentVersion() {
    return 1;
  }

  @Override
  public int supportedForwardVersion() {
    return 2;
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    ObjectNode root = (ObjectNode) json;
    if (fromVersion == 2) {
      // When receiving an event of version 2 we down-cast it to the version 1 of the schema
      root.set("productId", root.get("itemId"));
      root.remove("itemId");
    }
    return root;
  }
}




Once all running nodes have the new migration code which can read version 
2
 of 
ItemAdded
 we can proceed with the second step. So, we deploy the updated event:




Scala




copy
source
case class ItemAdded(shoppingCartId: String, itemId: String, quantity: Int) extends JsonSerializable


Java




copy
source
public class ItemAdded implements JsonSerializable {
  public final String shoppingCartId;

  public final String itemId;

  public final int quantity;

  public ItemAdded(String shoppingCartId, String itemId, int quantity) {
    this.shoppingCartId = shoppingCartId;
    this.itemId = itemId;
    this.quantity = quantity;
  }
}




and the final migration code which no longer needs forward-compatibility code:




Scala




copy
source
import akka.serialization.jackson.JacksonMigration
import com.fasterxml.jackson.databind.JsonNode
import com.fasterxml.jackson.databind.node.ObjectNode

class ItemAddedMigration extends JacksonMigration {

  override def currentVersion: Int = 2

  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {
    val root = json.asInstanceOf[ObjectNode]
    if (fromVersion <= 1) {
      root.set[JsonNode]("itemId", root.get("productId"))
      root.remove("productId")
    }
    root
  }
}


Java




copy
source
import akka.serialization.jackson.JacksonMigration;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.node.ObjectNode;

public class ItemAddedMigration extends JacksonMigration {

  @Override
  public int currentVersion() {
    return 2;
  }

  @Override
  public JsonNode transform(int fromVersion, JsonNode json) {
    ObjectNode root = (ObjectNode) json;
    if (fromVersion <= 1) {
      root.set("itemId", root.get("productId"));
      root.remove("productId");
    }
    return root;
  }
}




Jackson Modules


The following Jackson modules are enabled by default:


copy
source
akka.serialization.jackson {

  # The Jackson JSON serializer will register these modules.
  jackson-modules += "akka.serialization.jackson.AkkaJacksonModule"
  # AkkaTypedJacksonModule optionally included if akka-actor-typed is in classpath
  jackson-modules += "akka.serialization.jackson.AkkaTypedJacksonModule"
  # AkkaStreamsModule optionally included if akka-streams is in classpath
  jackson-modules += "akka.serialization.jackson.AkkaStreamJacksonModule"
  jackson-modules += "com.fasterxml.jackson.module.paramnames.ParameterNamesModule"
  jackson-modules += "com.fasterxml.jackson.datatype.jdk8.Jdk8Module"
  jackson-modules += "com.fasterxml.jackson.datatype.jsr310.JavaTimeModule"
  jackson-modules += "com.fasterxml.jackson.module.scala.DefaultScalaModule"
}


You can amend the configuration 
akka.serialization.jackson.jackson-modules
 to enable other modules.


The 
ParameterNamesModule
 requires that the 
-parameters
 Java compiler option is enabled.


Compression


JSON can be rather verbose and for large messages it can be beneficial to compress large payloads. For the 
jackson-json
 binding the default configuration is:


copy
source
# Compression settings for the jackson-json binding
akka.serialization.jackson.jackson-json.compression {
  # Compression algorithm.
  # - off  : no compression
  # - gzip : using common java gzip
  # - lz4 : using lz4-java
  algorithm = gzip

  # If compression is enabled with the `algorithm` setting the payload is compressed
  # when it's larger than this value.
  compress-larger-than = 32 KiB
}


Supported compression algorithms are: gzip, lz4. Use ‘off’ to disable compression. Gzip is generally slower than lz4. Messages larger than the 
compress-larger-than
 property are compressed.


Compression can be disabled by setting the 
algorithm
 property to 
off
. It will still be able to decompress payloads that were compressed when serialized, e.g. if this configuration is changed.


For the 
jackson-cbor
 and custom bindings other than 
jackson-json
 compression is by default disabled, but can be enabled in the same way as the configuration shown above but replacing 
jackson-json
 with the binding name (for example 
jackson-cbor
).


Using Akka Serialization for embedded types


For types that already have an Akka Serializer defined that are embedded in types serialized with Jackson the 
AkkaSerializationSerializer
AkkaSerializationSerializer
 and 
AkkaSerializationDeserializer
AkkaSerializationDeserializer
 can be used to Akka Serialization for individual fields. 


The serializer/deserializer are not enabled automatically. The 
@JsonSerialize
 and 
@JsonDeserialize
 annotation needs to be added to the fields containing the types to be serialized with Akka Serialization.


The type will be embedded as an object with the fields:




serId - the serializer id


serManifest - the manifest for the type


payload - base64 encoded bytes




Additional configuration


Configuration per binding


By default, the configuration for the Jackson serializers and their 
ObjectMapper
s is defined in the 
akka.serialization.jackson
 section. It is possible to override that configuration in a more specific 
akka.serialization.jackson.<binding name>
 section.


copy
source
akka.serialization.jackson.jackson-json {
  serialization-features {
    WRITE_DATES_AS_TIMESTAMPS = off
  }
}
akka.serialization.jackson.jackson-cbor {
  serialization-features {
    WRITE_DATES_AS_TIMESTAMPS = on
  }
}


It’s also possible to define several bindings and use different configuration for them. For example, different settings for remote messages and persisted events.


copy
source
akka.actor {
  serializers {
    jackson-json-message = "akka.serialization.jackson.JacksonJsonSerializer"
    jackson-json-event   = "akka.serialization.jackson.JacksonJsonSerializer"
  }
  serialization-identifiers {
    jackson-json-message = 9001
    jackson-json-event = 9002
  }
  serialization-bindings {
    "com.myservice.MyMessage" = jackson-json-message
    "com.myservice.MyEvent" = jackson-json-event
  }
}
akka.serialization.jackson {
  jackson-json-message {
    serialization-features {
      WRITE_DATES_AS_TIMESTAMPS = on
    }
  }
  jackson-json-event {
    serialization-features {
      WRITE_DATES_AS_TIMESTAMPS = off
    }
  }
}


Manifest-less serialization


When using the Jackson serializer for persistence, given that the fully qualified class name is stored in the manifest, this can result in a lot of wasted disk and IO used, especially when the events are small. To address this, a 
type-in-manifest
 flag can be turned off, which will result in the class name not appearing in the manifest.


When deserializing, the Jackson serializer will use the type defined in 
deserialization-type
, if present, otherwise it will look for exactly one serialization binding class, and use that. For this to be useful, generally that single type must be a 
Polymorphic type
, with all type information necessary to deserialize to the various sub types contained in the JSON message.


When switching serializers, for example, if doing a rolling update as described 
here
, there will be periods of time when you may have no serialization bindings declared for the type. In such circumstances, you must use the 
deserialization-type
 configuration attribute to specify which type should be used to deserialize messages.


Since this configuration can only be applied to a single root type, you will usually only want to apply it to a per binding configuration, not to the regular 
jackson-json
 or 
jackson-cbor
 configurations.


copy
source
akka.actor {
  serializers {
    jackson-json-event = "akka.serialization.jackson.JacksonJsonSerializer"
  }
  serialization-identifiers {
    jackson-json-event = 9001
  }
  serialization-bindings {
    "com.myservice.MyEvent" = jackson-json-event
  }
}
akka.serialization.jackson {
  jackson-json-event {
    type-in-manifest = off
    # Since there is exactly one serialization binding declared for this
    # serializer above, this is optional, but if there were none or many,
    # this would be mandatory.
    deserialization-type = "com.myservice.MyEvent"
  }
}


Note that Akka remoting already implements manifest compression, and so this optimization will have no significant impact for messages sent over remoting. It’s only useful for messages serialized for other purposes, such as persistence or distributed data.


Additional features


Additional Jackson serialization features can be enabled/disabled in configuration. The default values from Jackson are used aside from the following that are changed in Akka’s default configuration.


copy
source
akka.serialization.jackson {
  # Configuration of the ObjectMapper serialization features.
  # See com.fasterxml.jackson.databind.SerializationFeature
  # Enum values corresponding to the SerializationFeature and their boolean value.
  serialization-features {
    # Date/time in ISO-8601 (rfc3339) yyyy-MM-dd'T'HH:mm:ss.SSSZ format
    # as defined by com.fasterxml.jackson.databind.util.StdDateFormat
    # For interoperability it's better to use the ISO format, i.e. WRITE_DATES_AS_TIMESTAMPS=off,
    # but WRITE_DATES_AS_TIMESTAMPS=on has better performance.
    WRITE_DATES_AS_TIMESTAMPS = off
    WRITE_DURATIONS_AS_TIMESTAMPS = off
    FAIL_ON_EMPTY_BEANS = off
  }

  # Configuration of the ObjectMapper deserialization features.
  # See com.fasterxml.jackson.databind.DeserializationFeature
  # Enum values corresponding to the DeserializationFeature and their boolean value.
  deserialization-features {
    FAIL_ON_UNKNOWN_PROPERTIES = off
  }

  # Configuration of the ObjectMapper mapper features.
  # See com.fasterxml.jackson.databind.MapperFeature
  # Enum values corresponding to the MapperFeature and their
  # boolean values, for example:
  #
  # mapper-features {
  #   SORT_PROPERTIES_ALPHABETICALLY = on
  # }
  mapper-features {}

  # Allowed values USE_PROPERTIES_BASED, USE_DELEGATING, DEFAULT, EXPLICIT_ONLY
  # see com.fasterxml.jackson.databind.cfg.ConstructorDetector for details for each option
  # The default USE_PROPERTIES_BASED allows for detecting single argument constructors without @JsonCreator annotations
  # while DEAFULT provides the old Jackson behavior of requiring such an annotation.
  constructor-detector-mode = "USE_PROPERTIES_BASED"

  # Configuration of the ObjectMapper JsonParser features.
  # See com.fasterxml.jackson.core.JsonParser.Feature
  # Enum values corresponding to the JsonParser.Feature and their
  # boolean value, for example:
  #
  # json-parser-features {
  #   ALLOW_SINGLE_QUOTES = on
  # }
  json-parser-features {}

  # Configuration of the ObjectMapper JsonParser features.
  # See com.fasterxml.jackson.core.JsonGenerator.Feature
  # Enum values corresponding to the JsonGenerator.Feature and
  # their boolean value, for example:
  #
  # json-generator-features {
  #   WRITE_NUMBERS_AS_STRINGS = on
  # }
  json-generator-features {}

  # Configuration of the JsonFactory StreamReadFeature.
  # See com.fasterxml.jackson.core.StreamReadFeature
  # Enum values corresponding to the StreamReadFeatures and
  # their boolean value, for example:
  #
  # stream-read-features {
  #   STRICT_DUPLICATE_DETECTION = on
  # }
  stream-read-features {}

  # Configuration of the JsonFactory StreamWriteFeature.
  # See com.fasterxml.jackson.core.StreamWriteFeature
  # Enum values corresponding to the StreamWriteFeatures and
  # their boolean value, for example:
  #
  # stream-write-features {
  #   WRITE_BIGDECIMAL_AS_PLAIN = on
  # }
  stream-write-features {}

  # Configuration of the JsonFactory JsonReadFeature.
  # See com.fasterxml.jackson.core.json.JsonReadFeature
  # Enum values corresponding to the JsonReadFeatures and
  # their boolean value, for example:
  #
  # json-read-features {
  #   ALLOW_SINGLE_QUOTES = on
  # }
  json-read-features {}

  # Configuration of the JsonFactory JsonWriteFeature.
  # See com.fasterxml.jackson.core.json.JsonWriteFeature
  # Enum values corresponding to the JsonWriteFeatures and
  # their boolean value, for example:
  #
  # json-write-features {
  #   WRITE_NUMBERS_AS_STRINGS = on
  # }
  json-write-features {}

  # Configuration of the JsonFactory Visibility.
  # See com.fasterxml.jackson.annotation.PropertyAccessor
  # and com.fasterxml.jackson.annotation.JsonAutoDetect.Visibility
  # Enum values. For example, to serialize only public fields
  # overwrite the default values with:
  #
  # visibility {
  #   FIELD = PUBLIC_ONLY
  # }
  # Default: all fields (including private and protected) are serialized.
  visibility {
    FIELD = ANY
  }

  # Deprecated, use `allowed-class-prefix` instead
  whitelist-class-prefix = []

  # Additional classes that are allowed even if they are not defined in `serialization-bindings`.
  # This is useful when a class is not used for serialization any more and therefore removed
  # from `serialization-bindings`, but should still be possible to deserialize.
  allowed-class-prefix = ${akka.serialization.jackson.whitelist-class-prefix}


  # settings for compression of the payload
  compression {
    # Compression algorithm.
    # - off  : no compression
    # - gzip : using common java gzip
    algorithm = off

    # If compression is enabled with the `algorithm` setting the payload is compressed
    # when it's larger than this value.
    compress-larger-than = 0 KiB
  }

  # Whether the type should be written to the manifest.
  # If this is off, then either deserialization-type must be defined, or there must be exactly
  # one serialization binding declared for this serializer, and the type in that binding will be
  # used as the deserialization type. This feature will only work if that type either is a
  # concrete class, or if it is a supertype that uses Jackson polymorphism (ie, the
  # @JsonTypeInfo annotation) to store type information in the JSON itself. The intention behind
  # disabling this is to remove extraneous type information (ie, fully qualified class names) when
  # serialized objects are persisted in Akka persistence or replicated using Akka distributed
  # data. Note that Akka remoting already has manifest compression optimizations that address this,
  # so for types that just get sent over remoting, this offers no optimization.
  type-in-manifest = on

  # The type to use for deserialization.
  # This is only used if type-in-manifest is disabled. If set, this type will be used to
  # deserialize all messages. This is useful if the binding configuration you want to use when
  # disabling type in manifest cannot be expressed as a single type. Examples of when you might
  # use this include when changing serializers, so you don't want this serializer used for
  # serialization and you haven't declared any bindings for it, but you still want to be able to
  # deserialize messages that were serialized with this serializer, as well as situations where
  # you only want some sub types of a given Jackson polymorphic type to be serialized using this
  # serializer.
  deserialization-type = ""

  # Specific settings for jackson-json binding can be defined in this section to
  # override the settings in 'akka.serialization.jackson'
  jackson-json {}

  # Specific settings for jackson-cbor binding can be defined in this section to
  # override the settings in 'akka.serialization.jackson'
  jackson-cbor {}

  # Issue #28918 for compatibility with data serialized with JacksonCborSerializer in
  # Akka 2.6.4 or earlier, which was plain JSON format.
  jackson-cbor-264 = ${akka.serialization.jackson.jackson-cbor}

}


Date/time format


WRITE_DATES_AS_TIMESTAMPS
 and 
WRITE_DURATIONS_AS_TIMESTAMPS
 are by default disabled, which means that date/time fields are serialized in ISO-8601 (rfc3339) 
yyyy-MM-dd'T'HH:mm:ss.SSSZ
 format instead of numeric arrays. This is better for interoperability but it is slower. If you don’t need the ISO format for interoperability with external systems you can change the following configuration for better performance of date/time fields.


copy
source
akka.serialization.jackson.serialization-features {
  WRITE_DATES_AS_TIMESTAMPS = on
  WRITE_DURATIONS_AS_TIMESTAMPS = on
}


Jackson is still able to deserialize the other format independent of this setting.














 
Serialization






Multi JVM Testing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/modules.html
Overview of Akka libraries and modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules




Actor library


Remoting


Cluster


Cluster Sharding


Cluster Singleton


Persistence


Projections


Distributed Data


Streams


Alpakka


HTTP


gRPC


Example of module use




Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules




Actor library


Remoting


Cluster


Cluster Sharding


Cluster Singleton


Persistence


Projections


Distributed Data


Streams


Alpakka


HTTP


gRPC


Example of module use




Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Overview of Akka libraries and modules


Before delving into some best practices for writing actors, it will be helpful to preview the most commonly used Akka libraries. This will help you start thinking about the functionality you want to use in your system. All core Akka functionality is source available. Lightbend sponsors Akka development but can also help you with 
commercial offerings 
 such as training, consulting, support, and 
Enterprise capabilities
 — a comprehensive set of tools for managing Akka systems.


The following capabilities are included with Akka and are introduced later on this page:




Actor library


Remoting


Cluster


Cluster Sharding


Cluster Singleton


Persistence


Projections


Distributed Data


Streams


Alpakka


HTTP


gRPC


Other Akka modules




This page does not list all available modules, but overviews the main functionality and gives you an idea of the level of sophistication you can reach when you start building systems on top of Akka.


Actor library


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


The core Akka library is 
akka-actor-typed
, but actors are used across Akka libraries, providing a consistent, integrated model that relieves you from individually solving the challenges that arise in concurrent or distributed system design. From a birds-eye view, actors are a programming paradigm that takes encapsulation, one of the pillars of OOP, to its extreme. Unlike objects, actors encapsulate not only their state but their execution. Communication with actors is not via method calls but by passing messages. While this difference may seem minor, it is actually what allows us to break clean from the limitations of OOP when it comes to concurrency and remote communication. Donât worry if this description feels too high level to fully grasp yet, in the next chapter we will explain actors in detail. For now, the important point is that this is a model that handles concurrency and distribution at the fundamental level instead of ad hoc patched attempts to bring these features to OOP.


Challenges that actors solve include the following:




How to build and design high-performance, concurrent applications.


How to handle errors in a multi-threaded environment.


How to protect my project from the pitfalls of concurrency.




Remoting


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-remote" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-remote_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-remote_${versions.ScalaBinary}"
}


Remoting enables actors that live on different computers to seamlessly exchange messages. While distributed as a JAR artifact, Remoting resembles a module more than it does a library. You enable it mostly with configuration and it has only a few APIs. Thanks to the actor model, a remote and local message send looks exactly the same. The patterns that you use on local systems translate directly to remote systems. You will rarely need to use Remoting directly, but it provides the foundation on which the Cluster subsystem is built.


Challenges Remoting solves include the following:




How to address actor systems living on remote hosts.


How to address individual actors on remote actor systems.


How to turn messages to bytes on the wire.


How to manage low-level, network connections (and reconnections) between hosts, detect crashed actor systems and hosts,  all transparently.


How to multiplex communications from an unrelated set of actors on the same network connection, all transparently.




Cluster


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}


If you have a set of actor systems that cooperate to solve some business problem, then you likely want to manage these set of systems in a disciplined way. While Remoting solves the problem of addressing and communicating with components of remote systems, Clustering gives you the ability to organize these into a “meta-system” tied together by a membership protocol. 
In most cases, you want to use the Cluster module instead of using Remoting directly.
 Clustering provides an additional set of services on top of Remoting that most real world applications need.


Challenges the Cluster module solves include the following:




How to maintain a set of actor systems (a cluster) that can communicate with each other and consider each other as part of the cluster.


How to introduce a new system safely to the set of already existing members.


How to reliably detect systems that are temporarily unreachable.


How to remove failed hosts/systems (or scale down the system) so that all remaining members agree on the remaining subset of the cluster.


How to distribute computations among the current set of members.


How to designate members of the cluster to a certain role, in other words, to provide certain services and not others.




Cluster Sharding


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-sharding-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-sharding-typed_${versions.ScalaBinary}"
}


Sharding helps to solve the problem of distributing a set of actors among members of an Akka cluster. Sharding is a pattern that mostly used together with Persistence to balance a large set of persistent entities (backed by actors) to members of a cluster and also migrate them to other nodes when members crash or leave.


Challenges that Sharding solves include the following:




How to model and scale out a large set of stateful entities on a set of systems.


How to ensure that entities in the cluster are distributed properly so that load is properly balanced across the machines.


How to ensure migrating entities from a crashed system without losing the state.


How to ensure that an entity does not exist on multiple systems at the same time and hence keeps consistent.




Cluster Singleton


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-singleton" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-singleton_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-singleton_${versions.ScalaBinary}"
}


A common (in fact, a bit too common) use case in distributed systems is to have a single entity responsible for a given task which is shared among other members of the cluster and migrated if the host system fails. While this undeniably introduces a common bottleneck for the whole cluster that limits scaling, there are scenarios where the use of this pattern is unavoidable. Cluster singleton allows a cluster to select an actor system which will host a particular actor while other systems can always access said service independently from where it is.


The Singleton module can be used to solve these challenges:




How to ensure that only one instance of a service is running in the whole cluster.


How to ensure that the service is up even if the system hosting it currently crashes or shuts down during the process of scaling down.


How to reach this instance from any member of the cluster assuming that it can migrate to other systems over time.




Persistence


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}"
}


Just like objects in OOP, actors keep their state in volatile memory. Once the system is shut down, gracefully or because of a crash, all data that was in memory is lost. Persistence provides patterns to enable actors to persist events that lead to their current state. Upon startup, events can be replayed to restore the state of the entity hosted by the actor. The event stream can be queried and fed into additional processing pipelines (an external Big Data cluster for example) or alternate views (like reports).


Persistence tackles the following challenges:




How to restore the state of an entity/actor when system restarts or crashes.


How to implement a 
CQRS system
.


How to ensure reliable delivery of messages in face of network errors and system crashes.


How to introspect domain events that have led an entity to its current state.


How to leverage 
Event Sourcing
 in your application to support long-running processes while the project continues to evolve.




Projections


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-projection-core" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-projection-core_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-projection-core_${versions.ScalaBinary}"
}


Projections provides a simple API for consuming a stream of events for projection into a variety of downstream options. The core dependency provides only the API and other provider dependencies are required for different source and sink implementations.


Challenges Projections solve include the following:




Constructing alternate or aggregate views over an event stream.


Propagating an event stream onto another downstream medium such as a Kafka topic.


A simple way of building read-side projections in the context of 
Event Sourcing
 and 
CQRS system




Distributed Data


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}


In situations where eventual consistency is acceptable, it is possible to share data between nodes in an Akka Cluster and accept both reads and writes even in the face of cluster partitions. This can be achieved using 
Conflict Free Replicated Data Types
 (CRDTs), where writes on different nodes can happen concurrently and are merged in a predictable way afterward. The Distributed Data module provides infrastructure to share data and a number of useful data types.


Distributed Data is intended to solve the following challenges:




How to accept writes even in the face of cluster partitions.


How to share data while at the same time ensuring low-latency local read and write access.




Streams


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream-typed_${versions.ScalaBinary}"
}


Actors are a fundamental model for concurrency, but there are common patterns where their use requires the user to implement the same pattern over and over. Very common is the scenario where a chain, or graph, of actors, need to process a potentially large, or infinite, stream of sequential events and properly coordinate resource usage so that faster processing stages do not overwhelm slower ones in the chain or graph. Streams provide a higher-level abstraction on top of actors that simplifies writing such processing networks, handling all the fine details in the background and providing a safe, typed, composable programming model. Streams is also an implementation of the 
Reactive Streams standard
 which enables integration with all third party implementations of that standard.


Streams solve the following challenges:




How to handle streams of events or large datasets with high performance, exploiting concurrency and keeping resource usage tight.


How to assemble reusable pieces of event/data processing into flexible pipelines.


How to connect asynchronous services in a flexible way to each other with high performance.


How to provide or consume Reactive Streams compliant interfaces to interface with a third party library.




Alpakka


Alpakka
 is a separate module from Akka.


Alpakka is collection of modules built upon the Streams API to provide Reactive Stream connector implementations for a variety of technologies common in the cloud and infrastructure landscape.
See the 
Alpakka overview page
 for more details on the API and the implementation modules available.


Alpakka helps solve the following challenges:




Connecting various infrastructure or persistence components to Stream based flows.


Connecting to legacy systems in a manner that adheres to a Reactive Streams API.




HTTP


Akka HTTP
 is a separate module from Akka.


The de facto standard for providing APIs remotely, internal or external, is 
HTTP
. Akka provides a library to construct or consume such HTTP services by giving a set of tools to create HTTP services (and serve them) and a client that can be used to consume other services. These tools are particularly suited to streaming in and out a large set of data or real-time events by leveraging the underlying model of Akka Streams.


Some of the challenges that HTTP tackles:




How to expose services of a system or cluster to the external world via an HTTP API in a performant way.


How to stream large datasets in and out of a system using HTTP.


How to stream live events in and out of a system using HTTP.




gRPC


Akka gRPC
 is a separate module from Akka.


This library provides an implementation of gRPC that integrates nicely with the 
HTTP
 and 
Streams
 modules. It is capable of generating both client and server-side artifacts from protobuf service definitions, which can then be exposed using Akka HTTP, and handled using Streams.


Some of the challenges that Akka gRPC tackles:




Exposing services with all the benefits of gRPC & protobuf:


Schema-first contract


Schema evolution support


Efficient binary protocol


First-class streaming support


Wide interoperability


Use of HTTP/2 connection multiplexing




Example of module use


Akka modules integrate together seamlessly. For example, think of a large set of stateful business objects, such as documents or shopping carts, that website users access. If you model these as sharded entities, using Sharding and Persistence, they will be balanced across a cluster that you can scale out on-demand. They will be available during spikes that come from advertising campaigns or before holidays will be handled, even if some systems crash. You can also take the real-time stream of domain events with Persistence Query and use Streams to pipe them into a streaming Fast Data engine. Then, take the output of that engine as a Stream, manipulate it using Akka Streams operators and expose it as web socket connections served by a load balanced set of HTTP servers hosted by your cluster to power your real-time business analytics tool.


We hope this preview caught your interest! The next topic introduces the example application we will build in the tutorial portion of this guide.














 
How the Actor Model Meets the Needs of Modern, Distributed Systems






Introduction to the Example 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/durable-state/persistence-query.html
Persistence Query • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query




Dependency


Introduction


Using query with Akka Projections




Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query




Dependency


Introduction


Using query with Akka Projections




Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence Query


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Persistence Query, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-query" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-query_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-query_${versions.ScalaBinary}"
}


This will also add dependency on the 
Akka Persistence
 module.


Introduction


Akka persistence query provides a query interface to 
Durable State Behaviors
. These queries are based on asynchronous streams. These streams are similar to the ones offered in the 
Event Sourcing
 based implementation. Various state store plugins can implement these interfaces to expose their query capabilities.


One of the rationales behind having a separate query module for Akka Persistence is for implementing the so-called query side or read side in the popular CQRS architecture pattern - in which the writing side of the application implemented using Akka persistence, is completely separated from the query side.
Alternative


When using the R2DBC plugin an alternative to using Akka persistence query or Projection is to 
store the query representation
 directly from the write side.


Using query with Akka Projections


Akka Persistence and Akka Projections together can be used to develop a CQRS application. In the application the durable state is stored in a database and fetched as an asynchronous stream to the user. Currently queries on durable state, provided by the 
DurableStateStoreQuery
 interface, is used to implement tag based searches in Akka Projections. 


At present the query is based on 
tags
. So if you have not tagged your objects, this query cannot be used.


The example below shows how to get the 
DurableStateStoreQuery
 from the 
DurableStateStoreRegistry
 extension.




Scala




copy
source
import akka.persistence.state.DurableStateStoreRegistry
import akka.persistence.query.scaladsl.DurableStateStoreQuery
import akka.persistence.query.DurableStateChange
import akka.persistence.query.UpdatedDurableState

val durableStateStoreQuery =
  DurableStateStoreRegistry(system).durableStateStoreFor[DurableStateStoreQuery[Record]](pluginId)
val source: Source[DurableStateChange[Record], NotUsed] = durableStateStoreQuery.changes("tag", offset)
source.map {
  case UpdatedDurableState(persistenceId, revision, value, offset, timestamp) => Some(value)
  case _: DeletedDurableState[_]                                              => None
}


Java




copy
source
import akka.persistence.state.DurableStateStoreRegistry;
import akka.persistence.query.javadsl.DurableStateStoreQuery;
import akka.persistence.query.DurableStateChange;
import akka.persistence.query.UpdatedDurableState;

DurableStateStoreQuery<Record> durableStateStoreQuery =
    DurableStateStoreRegistry.get(system)
        .getDurableStateStoreFor(DurableStateStoreQuery.class, pluginId);
Source<DurableStateChange<Record>, NotUsed> source =
    durableStateStoreQuery.changes("tag", offset);
source.map(
    chg -> {
      if (chg instanceof UpdatedDurableState) {
        UpdatedDurableState<Record> upd = (UpdatedDurableState<Record>) chg;
        return upd.value();
      } else {
        throw new IllegalArgumentException("Unexpected DurableStateChange " + chg.getClass());
      }
    });




The 
DurableStateChange
DurableStateChange
 elements can be 
UpdatedDurableState
 or 
DeletedDurableState
.














 
CQRS






Building a storage backend for Durable State 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-insights/current/
Akka Insights


You are being redirected. If nothing happens, please 
follow this link
.

URL: https://doc.akka.io/libraries/index.html
Akka Libraries :: Akka Documentation


























 














 


























Developers








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












SDK 








SDK


Key Value Entities


Event Sourced Entities


Views






 


Timers


Workflows










 


Contact Us


Sign In




Get Started











                    Try Akka for free
                    


Deploy and scale a multi-region app
No credit card required







                    Develop your own Akka app
                    


Akka account not required
Free and open SDK for offline dev







                    Request demo
                    


Personalized demo
Akka app design consultation












 


































Akka














Understanding






Architecture model






Deployment model






Development process






Declarative effects






Entity state models






Multi-region operations






Saga patterns






















Developing






Author your first service










Components






Event Sourced Entities






Key Value Entities






HTTP Endpoints






Views






Workflows






Timers






Consumers














Integrations






Component and service calls






Configure message brokers






Streaming














Setup and configuration






Setup and dependency injection






Serialization






Errors and failures






Access Control Lists (ACLs)






JSON Web Tokens (JWT)






Run a service locally










Developer best practices










Samples






Shopping cart quickstart


























Operating










Organizations






Manage users






Regions






Billing














Projects






Create






Manage users










Configure a container registry






Configure an external container registry














Configure message brokers






Aiven for Kafka






AWS MSK Kafka






Confluent Cloud






Google Pub/Sub










Manage secrets














Services






Deploy and manage services






Invoking Akka services






Viewing data






Data migration










Regions










Observability and monitoring






View logs






View metrics






View traces






Exporting metrics, logs, and traces














Integrating with CI/CD tools






CI/CD with GitHub Actions










Operator best practices






















Securing






Access Control Lists (ACLs)






TLS certificates






JSON Web Tokens (JWTs)






















Support






Community Forum






Email






Frequently Asked Questions






Paid Plans






Status






Request a Demo






Troubleshooting






















Reference






Service descriptor






Route descriptor






Observability descriptor






Glossary of terms






Security announcements






Release notes






Migration guide






API documentation






View query language










CLI






Install the Akka CLI






Using the Akka CLI






Enable CLI command completion










CLI command reference






akka






akka auth






akka auth container-registry






akka auth container-registry clear-cached-token






akka auth container-registry configure






akka auth container-registry credentials






akka auth container-registry install-helper






akka auth current-login






akka auth login






akka auth logout






akka auth signup






akka auth tokens






akka auth tokens create






akka auth tokens list






akka auth tokens revoke






akka auth use-token






akka completion






akka config






akka config clear-cache






akka config clear






akka config current-context






akka config delete-context






akka config get-organization






akka config get-project






akka config get






akka config list-contexts






akka config list






akka config rename-context






akka config set






akka config use-context






akka container-registry






akka container-registry delete-image






akka container-registry list-images






akka container-registry list-tags






akka container-registry list






akka container-registry print






akka container-registry push






akka docker






akka docker add-credentials






akka docker list-credentials






akka docker remove-credentials






akka docs






akka local






akka local console






akka local services






akka local services components






akka local services components get-state






akka local services components get-workflow






akka local services components list-events






akka local services components list-ids






akka local services components list-timers






akka local services components list






akka local services connectivity






akka local services list






akka local services views






akka local services views describe






akka local services views drop






akka local services views list






akka logs






akka organizations






akka organizations auth






akka organizations auth add






akka organizations auth add openid






akka organizations auth list






akka organizations auth remove






akka organizations auth show






akka organizations auth update






akka organizations auth update openid






akka organizations get






akka organizations invitations






akka organizations invitations cancel






akka organizations invitations create






akka organizations invitations list






akka organizations list






akka organizations users






akka organizations users add-binding






akka organizations users delete-binding






akka organizations users list-bindings






akka projects






akka projects config






akka projects config get






akka projects config get broker






akka projects config set






akka projects config set broker






akka projects config unset






akka projects config unset broker






akka projects delete






akka projects get






akka projects hostnames






akka projects hostnames add






akka projects hostnames list






akka projects hostnames remove






akka projects list






akka projects new






akka projects observability






akka projects observability apply






akka projects observability config






akka projects observability config traces






akka projects observability edit






akka projects observability export






akka projects observability get






akka projects observability set






akka projects observability set default






akka projects observability set default akka-console






akka projects observability set default google-cloud






akka projects observability set default otlp






akka projects observability set default splunk-hec






akka projects observability set logs






akka projects observability set logs google-cloud






akka projects observability set logs otlp






akka projects observability set logs splunk-hec






akka projects observability set metrics






akka projects observability set metrics google-cloud






akka projects observability set metrics otlp






akka projects observability set metrics prometheus






akka projects observability set metrics splunk-hec






akka projects observability set traces






akka projects observability set traces google-cloud






akka projects observability set traces otlp






akka projects observability unset






akka projects observability unset default






akka projects observability unset logs






akka projects observability unset metrics






akka projects observability unset traces






akka projects open






akka projects regions






akka projects regions add






akka projects regions list






akka projects regions set-primary






akka projects tokens






akka projects tokens create






akka projects tokens list






akka projects tokens revoke






akka projects update






akka quickstart






akka quickstart download






akka quickstart list






akka regions






akka regions list






akka roles






akka roles add-binding






akka roles delete-binding






akka roles invitations






akka roles invitations delete






akka roles invitations invite-user






akka roles invitations list






akka roles list-bindings






akka roles list






akka routes






akka routes create






akka routes delete






akka routes edit






akka routes export






akka routes get






akka routes list






akka routes update






akka secrets






akka secrets create






akka secrets create asymmetric






akka secrets create generic






akka secrets create symmetric






akka secrets create tls-ca






akka secrets create tls






akka secrets delete






akka secrets get






akka secrets list






akka services






akka services apply






akka services components






akka services components get-state






akka services components get-workflow






akka services components list-events






akka services components list-ids






akka services components list-timers






akka services components list






akka services connectivity






akka services data






akka services data cancel-task






akka services data export






akka services data get-task






akka services data import






akka services data list-tasks






akka services data watch-task






akka services delete






akka services deploy






akka services edit






akka services export






akka services expose






akka services get






akka services jwts






akka services jwts add






akka services jwts generate






akka services jwts list-algorithms






akka services jwts list






akka services jwts remove






akka services jwts update






akka services list






akka services logging






akka services logging list






akka services logging set-level






akka services logging unset-level






akka services pause






akka services proxy






akka services restart






akka services restore






akka services resume






akka services unexpose






akka services views






akka services views describe






akka services views drop






akka services views list






akka version


























Akka Libraries


















Akka


default








Akka
























Akka


Akka Libraries












Akka Libraries








Akka provides a set of libraries for building and running responsive applications. Key libraries and resources are outlined to help you get started.

















Find the latest Akka library versions at 
Akka library versions
.

















Start Learning








First example
: Create a simple "Hello world!" example.






Get started guide
: Learn Akka fundamentals with an IoT project.






Microservices guide
: Build resilient and scalable microservices.
















Foundations








Actors
: A model for concurrency and distribution without all the pain of threading primitives.






Streams
: An intuitive and safe way to do asynchronous, non-blocking, backpressured stream processing.






Event Sourcing
: Enables stateful actors to persist their state changes, so that the state can be recovered when an actor is restarted.






Durable State
: Enables stateful actors to persist their latest state, so that it can be recovered when an actor is restarted.






Projections
: Build a projected model out of streams of events.
















Communication and Integrations








HTTP
: Modern, fast, asynchronous, streaming-first HTTP server and client.






gRPC
: Build typed, streaming gRPC services.






Alpakka
: Streaming integrations with 
Kafka
 and other technologies.






Akka Persistence R2DBC
: Use Postgres-compatible databases with Akka Persistence.






Cassandra Persistence
: Use Cassandra databases with Akka Persistence.
















Resilience and Distribution








Cluster
: Boost resilience with distribution across multiple nodes.






Cluster Sharding
: Distribute actors across your cluster.






Distributed Data
: Eventually consistent, low latency data.
















Architecture








Distributed Cluster
: Connect Akka services built on Akka libraries across geographical locations for lower latency and higher availability.






Edge
: Move your endpoints to the edge of the cloud for lower latency and higher availability. Akka Edge Rust extends the power of Akka’s event-driven model to resource-constrained devices.
















Operations and Observability








Management
: Extensions for operating Akka on cloud providers and Kubernetes.






Diagnostics
: Identify configuration and operational issues in your Akka application.






Insights
: Intelligent monitoring and observability purpose-built for Akka.




















akka version












































© 2011 - 
, Lightbend, Inc. All rights reserved. | 
Licenses
 | 
Terms
 | 
Privacy Policy
 | 
Cookie Listing
 | 
Cookie Settings
 | 
RSS

URL: https://doc.akka.io/libraries/akka/snapshot/persistence-schema-evolution.html
Schema Evolution for Event Sourced Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors




Dependency


Introduction


Schema evolution in event-sourced systems


Picking the right serialization format


Schema evolution in action




Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors




Dependency


Introduction


Schema evolution in event-sourced systems


Picking the right serialization format


Schema evolution in action




Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Schema Evolution for Event Sourced Actors


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



This documentation page touches upon 
Akka Persistence
, so to follow those examples you will want to depend on:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-persistence" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}


Introduction


When working on long running projects using 
Persistence
, or any kind of 
Event Sourcing
 architectures, schema evolution becomes one of the more important technical aspects of developing your application. The requirements as well as our own understanding of the business domain may (and will) change in time.


In fact, if a project matures to the point where you need to evolve its schema to adapt to changing business requirements you can view this as first signs of its success â if you wouldn’t need to adapt anything over an apps lifecycle that could mean that no-one is really using it actively.


In this chapter we will investigate various schema evolution strategies and techniques from which you can pick and choose the ones that match your domain and challenge at hand.
Note


This page proposes a number of possible solutions to the schema evolution problem and explains how some of the utilities Akka provides can be used to achieve this, it is by no means a complete (closed) set of solutions.


Sometimes, based on the capabilities of your serialization formats, you may be able to evolve your schema in different ways than outlined in the sections below. If you discover useful patterns or techniques for schema evolution feel free to submit Pull Requests to this page to extend it.


Schema evolution in event-sourced systems


In recent years we have observed a tremendous move towards immutable append-only datastores, with event-sourcing being the prime technique successfully being used in these settings. For an excellent overview why and how immutable data makes scalability and systems design much simpler you may want to read Pat Helland’s excellent 
Immutability Changes Everything
 whitepaper.


Since with 
Event Sourcing
 the 
events are immutable
 and usually never deleted â the way schema evolution is handled differs from how one would go about it in a mutable database setting (e.g. in typical CRUD database applications).


The system needs to be able to continue to work in the presence of “old” events which were stored under the “old” schema. We also want to limit complexity in the business logic layer, exposing a consistent view over all of the events of a given type to 
PersistentActor
AbstractPersistentActor
 s and 
persistence queries
. This allows the business logic layer to focus on solving business problems instead of having to explicitly deal with different schemas.


In summary, schema evolution in event sourced systems exposes the following characteristics:




Allow the system to continue operating without large scale migrations to be applied,


Allow the system to read “old” events from the underlying storage, however present them in a “new” view to the application logic,


Transparently promote events to the latest versions during recovery (or queries) such that the business logic need not consider multiple versions of events




Types of schema evolution


Before we explain the various techniques that can be used to safely evolve the schema of your persistent events over time, we first need to define what the actual problem is, and what the typical styles of changes are.


Since events are never deleted, we need to have a way to be able to replay (read) old events, in such way that does not force the 
PersistentActor
AbstractPersistentActor
 to be aware of all possible versions of an event that it may have persisted in the past. Instead, we want the Actors to work on some form of “latest” version of the event and provide some means of either converting old “versions” of stored events into this “latest” event type, or constantly evolve the event definition - in a backwards compatible way - such that the new deserialization code can still read old events.


The most common schema changes you will likely are:




adding a field to an event type
,


remove or rename field in event type
,


remove event type
,


split event into multiple smaller events
.




The following sections will explain some patterns which can be used to safely evolve your schema when facing those changes.


Picking the right serialization format


Picking the serialization format is a very important decision you will have to make while building your application. It affects which kind of evolutions are simple (or hard) to do, how much work is required to add a new datatype, and, last but not least, serialization performance.


If you find yourself realising you have picked “the wrong” serialization format, it is always possible to change the format used for storing new events, however you would have to keep the old deserialization code in order to be able to replay events that were persisted using the old serialization scheme. It is possible to “rebuild” an event-log from one serialization format to another one, however it may be a more involved process if you need to perform this on a live system.


Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference. It also has support for 
Schema Evolution
.


Google Protocol Buffers
 is good if you want more control over the schema evolution of your messages, but it requires more work to develop and maintain the mapping between serialized representation and domain representation.


Binary serialization formats that we have seen work well for long-lived applications include the very flexible IDL based: 
Google Protocol Buffers
, 
Apache Thrift
 or 
Apache Avro
. Avro schema evolution is more “entire schema” based, instead of single fields focused like in protobuf or thrift, and usually requires using some kind of schema registry.


There are plenty excellent blog posts explaining the various trade-offs between popular serialization formats, one post we would like to highlight is the very well illustrated 
Schema evolution in Avro, Protocol Buffers and Thrift
 by Martin Kleppmann.


Provided default serializers


Akka Persistence provides 
Google Protocol Buffers
 based serializers (using 
Akka Serialization
) for its own message types such as 
PersistentRepr
PersistentRepr
, 
AtomicWrite
AtomicWrite
 and snapshots. Journal plugin implementations 
may
 choose to use those provided serializers, or pick a serializer which suits the underlying database better.
Note


Serialization is 
NOT
 handled automatically by Akka Persistence itself. Instead, it only provides the above described serializers, and in case a 
AsyncWriteJournal
AsyncWriteJournal
 plugin implementation chooses to use them directly, the above serialization scheme will be used.


Please refer to your write journal’s documentation to learn more about how it handles serialization!


For example, some journals may choose to not use Akka Serialization 
at all
 and instead store the data in a format that is more “native” for the underlying datastore, e.g. using JSON or some other kind of format that the target datastore understands directly.


The below figure explains how the default serialization scheme works, and how it fits together with serializing the user provided message itself, which we will from here on refer to as the 
payload
 (highlighted in yellow):




Akka Persistence provided serializers wrap the user payload in an envelope containing all persistence-relevant information. 
If the Journal uses provided Protobuf serializers for the wrapper types (e.g. PersistentRepr), then the payload will be serialized using the user configured serializer, and if none is provided explicitly, Java serialization will be used for it.


The blue colored regions of the 
PersistentMessage
 indicate what is serialized using the generated protocol buffers serializers, and the yellow payload indicates the user provided event (by calling 
persist(payload)(...)
persist(payload,...)
. As you can see, the 
PersistentMessage
 acts as an envelope around the payload, adding various fields related to the origin of the event (
persistenceId
, 
sequenceNr
 and more).


More advanced techniques (e.g. 
Remove event class and ignore events
) will dive into using the manifests for increasing the flexibility of the persisted vs. exposed types even more. However for now we will focus on the simpler evolution techniques, concerning only configuring the payload serializers.


By default the 
payload
 will be serialized using Java Serialization. This is fine for testing and initial phases of your development (while you’re still figuring out things, and the data will not need to stay persisted forever). However, once you move to production you should really 
pick a different serializer for your payloads
.
Warning


Do not rely on Java serialization for 
serious
 application development! It does not lean itself well to evolving schemas over long periods of time, and its performance is also not very high (it never was designed for high-throughput scenarios).


Configuring payload serializers


This section aims to highlight the complete basics on how to define custom serializers using 
Akka Serialization
. Many journal plugin implementations use Akka Serialization, thus it is tremendously important to understand how to configure it to work with your event classes.
Note


Read the 
Akka Serialization
 docs to learn more about defining custom serializers.


The below snippet explains in the minimal amount of lines how a custom serializer can be registered. For more in-depth explanations on how serialization picks the serializer to use etc, please refer to its documentation.


First we start by defining our domain model class, here representing a person:




Scala




copy
source
final case class Person(name: String, surname: String)


Java




copy
source
static class Person {
  public final String name;
  public final String surname;

  public Person(String name, String surname) {
    this.name = name;
    this.surname = surname;
  }
}




Next we implement a serializer (or extend an existing one to be able to handle the new 
Person
 class):




Scala




copy
source
/**
 * Simplest possible serializer, uses a string representation of the Person class.
 *
 * Usually a serializer like this would use a library like:
 * protobuf, kryo, avro, cap'n proto, flatbuffers, SBE or some other dedicated serializer backend
 * to perform the actual to/from bytes marshalling.
 */
class SimplestPossiblePersonSerializer extends SerializerWithStringManifest {
  val Utf8 = Charset.forName("UTF-8")

  val PersonManifest = classOf[Person].getName

  // unique identifier of the serializer
  def identifier = 1234567

  // extract manifest to be stored together with serialized object
  override def manifest(o: AnyRef): String = o.getClass.getName

  // serialize the object
  override def toBinary(obj: AnyRef): Array[Byte] = obj match {
    case p: Person => s"""${p.name}|${p.surname}""".getBytes(Utf8)
    case _         => throw new IllegalArgumentException(s"Unable to serialize to bytes, clazz was: ${obj.getClass}!")
  }

  // deserialize the object, using the manifest to indicate which logic to apply
  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =
    manifest match {
      case PersonManifest =>
        val nameAndSurname = new String(bytes, Utf8)
        val Array(name, surname) = nameAndSurname.split("[|]")
        Person(name, surname)
      case _ =>
        throw new NotSerializableException(
          s"Unable to deserialize from bytes, manifest was: $manifest! Bytes length: " +
          bytes.length)
    }

}



Java




copy
source
/**
 * Simplest possible serializer, uses a string representation of the Person class.
 *
 * <p>Usually a serializer like this would use a library like: protobuf, kryo, avro, cap'n
 * proto, flatbuffers, SBE or some other dedicated serializer backend to perform the actual
 * to/from bytes marshalling.
 */
static class SimplestPossiblePersonSerializer extends SerializerWithStringManifest {
  private final Charset utf8 = StandardCharsets.UTF_8;

  private final String personManifest = Person.class.getName();

  // unique identifier of the serializer
  @Override
  public int identifier() {
    return 1234567;
  }

  // extract manifest to be stored together with serialized object
  @Override
  public String manifest(Object o) {
    return o.getClass().getName();
  }

  // serialize the object
  @Override
  public byte[] toBinary(Object obj) {
    if (obj instanceof Person) {
      Person p = (Person) obj;
      return (p.name + "|" + p.surname).getBytes(utf8);
    } else {
      throw new IllegalArgumentException(
          "Unable to serialize to bytes, clazz was: " + obj.getClass().getName());
    }
  }

  // deserialize the object, using the manifest to indicate which logic to apply
  @Override
  public Object fromBinary(byte[] bytes, String manifest) throws NotSerializableException {
    if (personManifest.equals(manifest)) {
      String nameAndSurname = new String(bytes, utf8);
      String[] parts = nameAndSurname.split("[|]");
      return new Person(parts[0], parts[1]);
    } else {
      throw new NotSerializableException(
          "Unable to deserialize from bytes, manifest was: "
              + manifest
              + "! Bytes length: "
              + bytes.length);
    }
  }
}




And finally we register the serializer and bind it to handle the 
docs.persistence.Person
 class:


copy
source
# application.conf
akka {
  actor {
    serializers {
      person = "docs.persistence.SimplestPossiblePersonSerializer"
    }

    serialization-bindings {
      "docs.persistence.Person" = person
    }
  }
}


Deserialization will be performed by the same serializer which serialized the message initially because of the 
identifier
 being stored together with the message.


Please refer to the 
Akka Serialization
 documentation for more advanced use of serializers, especially the 
Serializer with String Manifest
 section since it is very useful for Persistence based applications dealing with schema evolutions, as we will see in some of the examples below.


Schema evolution in action


In this section we will discuss various schema evolution techniques using concrete examples and explaining some of the various options one might go about handling the described situation. The list below is by no means a complete guide, so feel free to adapt these techniques depending on your serializer’s capabilities and/or other domain specific limitations.
Note


Serialization with Jackson
 has good support for 
Schema Evolution
 and many of the scenarios described here can be solved with that Jackson transformation technique instead.




Add fields


Situation:
 You need to add a field to an existing message type. For example, a 
SeatReserved(letter:String, row:Int)
SeatReserved(String letter, int row)
 now needs to have an associated code which indicates if it is a window or aisle seat.


Solution:
 Adding fields is the most common change you’ll need to apply to your messages so make sure the serialization format you picked for your payloads can handle it appropriately, i.e. such changes should be 
binary compatible
. This is achieved using the right serializer toolkit. In the following examples we will be using protobuf. See also 
how to add fields with Jackson
.


While being able to read messages with missing fields is half of the solution, you also need to deal with the missing values somehow. This is usually modeled as some kind of default value, or by representing the field as an 
Option[T]
Optional<T>
 See below for an example how reading an optional field from a serialized protocol buffers message might look like.




Scala




copy
source
sealed abstract class SeatType { def code: String }
object SeatType {
  def fromString(s: String) = s match {
    case Window.code => Window
    case Aisle.code  => Aisle
    case Other.code  => Other
    case _           => Unknown
  }
  case object Window extends SeatType { override val code = "W" }
  case object Aisle extends SeatType { override val code = "A" }
  case object Other extends SeatType { override val code = "O" }
  case object Unknown extends SeatType { override val code = "" }

}

case class SeatReserved(letter: String, row: Int, seatType: SeatType)


Java




copy
source
static enum SeatType {
  Window("W"),
  Aisle("A"),
  Other("O"),
  Unknown("");

  private final String code;

  private SeatType(String code) {
    this.code = code;
  }

  public static SeatType fromCode(String c) {
    if (Window.code.equals(c)) return Window;
    else if (Aisle.code.equals(c)) return Aisle;
    else if (Other.code.equals(c)) return Other;
    else return Unknown;
  }
}
static class SeatReserved {
  public final String letter;
  public final int row;
  public final SeatType seatType;

  public SeatReserved(String letter, int row, SeatType seatType) {
    this.letter = letter;
    this.row = row;
    this.seatType = seatType;
  }
}




Next we prepare a protocol definition using the protobuf Interface Description Language, which we’ll use to generate the serializer code to be used on the Akka Serialization layer (notice that the schema approach allows us to rename fields, as long as the numeric identifiers of the fields do not change):


copy
source
// FlightAppModels.proto
option java_package = "docs.persistence.proto";
option optimize_for = SPEED;

message SeatReserved {
  required string letter   = 1;
  required uint32 row      = 2;
  optional string seatType = 3; // the new field
}


The serializer implementation uses the protobuf generated classes to marshall the payloads. Optional fields can be handled explicitly or missing values by calling the 
has...
 methods on the protobuf object, which we do for 
seatType
 in order to use a 
Unknown
 type in case the event was stored before we had introduced the field to this event type:




Scala




copy
source
/**
 * Example serializer impl which uses protocol buffers generated classes (proto.*)
 * to perform the to/from binary marshalling.
 */
class AddedFieldsSerializerWithProtobuf extends SerializerWithStringManifest {
  override def identifier = 67876

  final val SeatReservedManifest = classOf[SeatReserved].getName

  override def manifest(o: AnyRef): String = o.getClass.getName

  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =
    manifest match {
      case SeatReservedManifest =>
        // use generated protobuf serializer
        seatReserved(FlightAppModels.SeatReserved.parseFrom(bytes))
      case _ =>
        throw new NotSerializableException("Unable to handle manifest: " + manifest)
    }

  override def toBinary(o: AnyRef): Array[Byte] = o match {
    case s: SeatReserved =>
      FlightAppModels.SeatReserved.newBuilder
        .setRow(s.row)
        .setLetter(s.letter)
        .setSeatType(s.seatType.code)
        .build()
        .toByteArray
  }

  // -- fromBinary helpers --

  private def seatReserved(p: FlightAppModels.SeatReserved): SeatReserved =
    SeatReserved(p.getLetter, p.getRow, seatType(p))

  // handle missing field by assigning "Unknown" value
  private def seatType(p: FlightAppModels.SeatReserved): SeatType =
    if (p.hasSeatType) SeatType.fromString(p.getSeatType) else SeatType.Unknown

}


Java




copy
source
/**
 * Example serializer impl which uses protocol buffers generated classes (proto.*) to perform the
 * to/from binary marshalling.
 */
static class AddedFieldsSerializerWithProtobuf extends SerializerWithStringManifest {
  @Override
  public int identifier() {
    return 67876;
  }

  private final String seatReservedManifest = SeatReserved.class.getName();

  @Override
  public String manifest(Object o) {
    return o.getClass().getName();
  }

  @Override
  public Object fromBinary(byte[] bytes, String manifest) throws NotSerializableException {
    if (seatReservedManifest.equals(manifest)) {
      // use generated protobuf serializer
      try {
        return seatReserved(FlightAppModels.SeatReserved.parseFrom(bytes));
      } catch (InvalidProtocolBufferException e) {
        throw new IllegalArgumentException(e.getMessage());
      }
    } else {
      throw new NotSerializableException("Unable to handle manifest: " + manifest);
    }
  }

  @Override
  public byte[] toBinary(Object o) {
    if (o instanceof SeatReserved) {
      SeatReserved s = (SeatReserved) o;
      return FlightAppModels.SeatReserved.newBuilder()
          .setRow(s.row)
          .setLetter(s.letter)
          .setSeatType(s.seatType.code)
          .build()
          .toByteArray();

    } else {
      throw new IllegalArgumentException("Unable to handle: " + o);
    }
  }

  // -- fromBinary helpers --

  private SeatReserved seatReserved(FlightAppModels.SeatReserved p) {
    return new SeatReserved(p.getLetter(), p.getRow(), seatType(p));
  }

  // handle missing field by assigning "Unknown" value
  private SeatType seatType(FlightAppModels.SeatReserved p) {
    if (p.hasSeatType()) return SeatType.fromCode(p.getSeatType());
    else return SeatType.Unknown;
  }
}






Rename fields


Situation:
 When first designing the system the 
SeatReserved
 event featured a 
code
 field. After some time you discover that what was originally called 
code
 actually means 
seatNr
, thus the model should be changed to reflect this concept more accurately.


Solution 1 - using IDL based serializers:
 First, we will discuss the most efficient way of dealing with such kinds of schema changes â IDL based serializers.


IDL stands for Interface Description Language, and means that the schema of the messages that will be stored is based on this description. Most IDL based serializers also generate the serializer / deserializer code so that using them is not too hard. Examples of such serializers are protobuf or thrift.


Using these libraries rename operations are “free”, because the field name is never actually stored in the binary representation of the message. This is one of the advantages of schema based serializers, even though that they add the overhead of having to maintain the schema. When using serializers like this, no additional code change (except renaming the field and method used during serialization) is needed to perform such evolution:




This is how such a rename would look in protobuf:


copy
source
// protobuf message definition, BEFORE:
message SeatReserved {
  required string code = 1;
}

// protobuf message definition, AFTER:
message SeatReserved {
  required string seatNr = 1; // field renamed, id remains the same
}


It is important to learn about the strengths and limitations of your serializers, in order to be able to move swiftly and refactor your models fearlessly as you go on with the project.
Note


Learn in-depth about the serialization engine you’re using as it will impact how you can approach schema evolution.


Some operations are “free” in certain serialization formats (more often than not: removing/adding optional fields, sometimes renaming fields etc.), while some other operations are strictly not possible.


Solution 2 - by manually handling the event versions:
 Another solution, in case your serialization format does not support renames like the above mentioned formats, is versioning your schema. For example, you could have made your events carry an additional field called 
_version
 which was set to 
1
 (because it was the initial schema), and once you change the schema you bump this number to 
2
, and write an adapter which can perform the rename.


This approach is popular when your serialization format is something like JSON, where renames can not be performed automatically by the serializer. See also 
how to rename fields with Jackson
, which is using this kind of versioning approach.




The following snippet showcases how one could apply renames if working with plain JSON (using 
spray.json.JsObject
a 
JsObject
 as an example JSON representation
):




Scala




copy
source
class JsonRenamedFieldAdapter extends EventAdapter {
  val marshaller = new ExampleJsonMarshaller

  val V1 = "v1"
  val V2 = "v2"

  // this could be done independently for each event type
  override def manifest(event: Any): String = V2

  override def toJournal(event: Any): JsObject =
    marshaller.toJson(event)

  override def fromJournal(event: Any, manifest: String): EventSeq = event match {
    case json: JsObject =>
      EventSeq(marshaller.fromJson(manifest match {
        case V1      => rename(json, "code", "seatNr")
        case V2      => json // pass-through
        case unknown => throw new IllegalArgumentException(s"Unknown manifest: $unknown")
      }))
    case _ =>
      val c = event.getClass
      throw new IllegalArgumentException("Can only work with JSON, was: %s".format(c))
  }

  def rename(json: JsObject, from: String, to: String): JsObject = {
    val value = json.fields(from)
    val withoutOld = json.fields - from
    JsObject(withoutOld + (to -> value))
  }

}


Java




copy
source
static class JsonRenamedFieldAdapter implements EventAdapter {
  // use your favorite json library
  private final ExampleJsonMarshaller marshaller = new ExampleJsonMarshaller();

  private final String V1 = "v1";
  private final String V2 = "v2";

  // this could be done independently for each event type
  @Override
  public String manifest(Object event) {
    return V2;
  }

  @Override
  public JsObject toJournal(Object event) {
    return marshaller.toJson(event);
  }

  @Override
  public EventSeq fromJournal(Object event, String manifest) {
    if (event instanceof JsObject) {
      JsObject json = (JsObject) event;
      if (V1.equals(manifest)) json = rename(json, "code", "seatNr");
      return EventSeq.single(json);
    } else {
      throw new IllegalArgumentException(
          "Can only work with JSON, was: " + event.getClass().getName());
    }
  }

  private JsObject rename(JsObject json, String from, String to) {
    // use your favorite json library to rename the field
    JsObject renamed = json;
    return renamed;
  }
}




As you can see, manually handling renames induces some boilerplate onto the EventAdapter, however much of it you will find is common infrastructure code that can be either provided by an external library (for promotion management) or put together in a simple helper 
trait
class
.
Note


The technique of versioning events and then promoting them to the latest version using JSON transformations can be applied to more than just field renames â it also applies to adding fields and all kinds of changes in the message format.




Remove event class and ignore events


Situation:
 While investigating app performance you notice that unreasonable amounts of 
CustomerBlinked
 events are being stored for every customer each time he/she blinks. Upon investigation, you decide that the event does not add any value and should be deleted. You still have to be able to replay from a journal which contains those old CustomerBlinked events though.


Naive solution - drop events in EventAdapter:


The problem of removing an event type from the domain model is not as much its removal, as the implications for the recovery mechanisms that this entails. For example, a naive way of filtering out certain kinds of events from being delivered to a recovering 
PersistentActor
 is pretty simple, as one can filter them out in an 
EventAdapter
:




The 
EventAdapter
EventAdapter
 can drop old events (**O**) by emitting an empty 
EventSeq
EventSeq
. Other events can be passed through (**E**).


This however does not address the underlying cost of having to deserialize all the events during recovery, even those which will be filtered out by the adapter. In the next section we will improve the above explained mechanism to avoid deserializing events which would be filtered out by the adapter anyway, thus allowing to save precious time during a recovery containing lots of such events (without actually having to delete them).


Improved solution - deserialize into tombstone:


In the just described technique we have saved the PersistentActor from receiving un-wanted events by filtering them out in the 
EventAdapter
EventAdapter
, however the event itself still was deserialized and loaded into memory. This has two notable 
downsides
:




first, that the deserialization was actually performed, so we spent some of our time budget on the deserialization, even though the event does not contribute anything to the persistent actors state.


second, that we are 
unable to remove the event class
 from the system â since the serializer still needs to create the actual instance of it, as it does not know it will not be used.




The solution to these problems is to use a serializer that is aware of that event being no longer needed, and can notice this before starting to deserialize the object.


This approach allows us to 
remove the original class from our classpath
, which makes for less “old” classes lying around in the project. This can for example be implemented by using an 
SerializerWithStringManifest
SerializerWithStringManifest
 (documented in depth in 
Serializer with String Manifest
). By looking at the string manifest, the serializer can notice that the type is no longer needed, and skip the deserialization all-together:




The serializer is aware of the old event types that need to be skipped (**O**), and can skip deserializing them altogether by returning a “tombstone” (**T**), which the EventAdapter converts into an empty EventSeq. Other events (**E**) can just be passed through.


The serializer detects that the string manifest points to a removed event type and skips attempting to deserialize it:




Scala




copy
source
case object EventDeserializationSkipped

class RemovedEventsAwareSerializer extends SerializerWithStringManifest {
  val utf8 = Charset.forName("UTF-8")
  override def identifier: Int = 8337

  val SkipEventManifestsEvents = Set("docs.persistence.CustomerBlinked" // ...
  )

  override def manifest(o: AnyRef): String = o.getClass.getName

  override def toBinary(o: AnyRef): Array[Byte] = o match {
    case _ => o.toString.getBytes(utf8) // example serialization
  }

  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =
    manifest match {
      case m if SkipEventManifestsEvents.contains(m) =>
        EventDeserializationSkipped

      case _ => new String(bytes, utf8)
    }
}


Java




copy
source
static class EventDeserializationSkipped {
  public static EventDeserializationSkipped instance = new EventDeserializationSkipped();

  private EventDeserializationSkipped() {}
}

static class RemovedEventsAwareSerializer extends SerializerWithStringManifest {
  private final Charset utf8 = StandardCharsets.UTF_8;
  private final String customerBlinkedManifest = "blinked";

  // unique identifier of the serializer
  @Override
  public int identifier() {
    return 8337;
  }

  // extract manifest to be stored together with serialized object
  @Override
  public String manifest(Object o) {
    if (o instanceof CustomerBlinked) return customerBlinkedManifest;
    else return o.getClass().getName();
  }

  @Override
  public byte[] toBinary(Object o) {
    return o.toString().getBytes(utf8); // example serialization
  }

  @Override
  public Object fromBinary(byte[] bytes, String manifest) {
    if (customerBlinkedManifest.equals(manifest)) return EventDeserializationSkipped.instance;
    else return new String(bytes, utf8);
  }
}




The EventAdapter we implemented is aware of 
EventDeserializationSkipped
 events (our “Tombstones”), and emits and empty 
EventSeq
 whenever such object is encountered:




Scala




copy
source
class SkippedEventsAwareAdapter extends EventAdapter {
  override def manifest(event: Any) = ""
  override def toJournal(event: Any) = event

  override def fromJournal(event: Any, manifest: String) = event match {
    case EventDeserializationSkipped => EventSeq.empty
    case _                           => EventSeq(event)
  }
}


Java




copy
source
static class SkippedEventsAwareAdapter implements EventAdapter {
  @Override
  public String manifest(Object event) {
    return "";
  }

  @Override
  public Object toJournal(Object event) {
    return event;
  }

  @Override
  public EventSeq fromJournal(Object event, String manifest) {
    if (event == EventDeserializationSkipped.instance) return EventSeq.empty();
    else return EventSeq.single(event);
  }
}






Detach domain model from data model


Situation:
 You want to separate the application model (often called the “
domain model
”) completely from the models used to persist the corresponding events (the “
data model
”). For example because the data representation may change independently of the domain model.


Another situation where this technique may be useful is when your serialization tool of choice requires generated classes to be used for serialization and deserialization of objects, like for example 
Google Protocol Buffers
 do, yet you do not want to leak this implementation detail into the domain model itself, which you’d like to model as plain 
Scala case
Java
 classes.


Solution:
 In order to detach the domain model, which is often represented using pure 
Scala (case)
Java
 classes, from the data model classes which very often may be less user-friendly yet highly optimised for throughput and schema evolution (like the classes generated by protobuf for example), it is possible to use a simple EventAdapter which maps between these types in a 1:1 style as illustrated below:




Domain events (**A**) are adapted to the data model events (**D**) by the 
EventAdapter
. The data model can be a format natively understood by the journal, such that it can store it more efficiently or include additional data for the event (e.g. tags), for ease of later querying.


We will use the following domain and data models to showcase how the separation can be implemented by the adapter:




Scala




copy
source
/** Domain model - highly optimised for domain language and maybe "fluent" usage */
object DomainModel {
  final case class Customer(name: String)
  final case class Seat(code: String) {
    def bookFor(customer: Customer): SeatBooked = SeatBooked(code, customer)
  }

  final case class SeatBooked(code: String, customer: Customer)
}

/** Data model - highly optimised for schema evolution and persistence */
object DataModel {
  final case class SeatBooked(code: String, customerName: String)
}


Java




copy
source
// Domain model - highly optimised for domain language and maybe "fluent" usage
static class Customer {
  public final String name;

  public Customer(String name) {
    this.name = name;
  }
}

static class Seat {
  public final String code;

  public Seat(String code) {
    this.code = code;
  }

  public SeatBooked bookFor(Customer customer) {
    return new SeatBooked(code, customer);
  }
}

static class SeatBooked {
  public final String code;
  public final Customer customer;

  public SeatBooked(String code, Customer customer) {
    this.code = code;
    this.customer = customer;
  }
}

// Data model - highly optimised for schema evolution and persistence
static class SeatBookedData {
  public final String code;
  public final String customerName;

  public SeatBookedData(String code, String customerName) {
    this.code = code;
    this.customerName = customerName;
  }
}




The 
EventAdapter
EventAdapter
 takes care of converting from one model to the other one (in both directions), allowing the models to be completely detached from each other, such that they can be optimised independently as long as the mapping logic is able to convert between them:




Scala




copy
source
class DetachedModelsAdapter extends EventAdapter {
  override def manifest(event: Any): String = ""

  override def toJournal(event: Any): Any = event match {
    case DomainModel.SeatBooked(code, customer) =>
      DataModel.SeatBooked(code, customer.name)
  }
  override def fromJournal(event: Any, manifest: String): EventSeq = event match {
    case DataModel.SeatBooked(code, customerName) =>
      EventSeq(DomainModel.SeatBooked(code, DomainModel.Customer(customerName)))
  }
}


Java




copy
source
class DetachedModelsAdapter implements EventAdapter {
  @Override
  public String manifest(Object event) {
    return "";
  }

  @Override
  public Object toJournal(Object event) {
    if (event instanceof SeatBooked) {
      SeatBooked s = (SeatBooked) event;
      return new SeatBookedData(s.code, s.customer.name);
    } else {
      throw new IllegalArgumentException("Unsupported: " + event.getClass());
    }
  }

  @Override
  public EventSeq fromJournal(Object event, String manifest) {
    if (event instanceof SeatBookedData) {
      SeatBookedData d = (SeatBookedData) event;
      return EventSeq.single(new SeatBooked(d.code, new Customer(d.customerName)));
    } else {
      throw new IllegalArgumentException("Unsupported: " + event.getClass());
    }
  }
}




The same technique could also be used directly in the Serializer if the end result of marshalling is bytes. Then the serializer can simply convert the bytes do the domain object by using the generated protobuf builders.




Store events as human-readable data model


Situation:
 You want to keep your persisted events in a human-readable format, for example JSON.


Solution:
 This is a special case of the 
Detach domain model from data model
 pattern, and thus requires some co-operation from the Journal implementation to achieve this.


An example of a Journal which may implement this pattern is MongoDB, however other databases such as PostgreSQL and Cassandra could also do it because of their built-in JSON capabilities.


In this approach, the 
EventAdapter
EventAdapter
 is used as the marshalling layer: it serializes the events to/from JSON. The journal plugin notices that the incoming event type is JSON (for example by performing a 
match
 on the incoming event) and stores the incoming object directly.




Scala




copy
source
class JsonDataModelAdapter extends EventAdapter {
  override def manifest(event: Any): String = ""

  val marshaller = new ExampleJsonMarshaller

  override def toJournal(event: Any): JsObject =
    marshaller.toJson(event)

  override def fromJournal(event: Any, manifest: String): EventSeq = event match {
    case json: JsObject =>
      EventSeq(marshaller.fromJson(json))
    case _ =>
      throw new IllegalArgumentException("Unable to fromJournal a non-JSON object! Was: " + event.getClass)
  }
}


Java




copy
source
static class JsonDataModelAdapter implements EventAdapter {

  // use your favorite json library
  private final ExampleJsonMarshaller marshaller = new ExampleJsonMarshaller();

  @Override
  public String manifest(Object event) {
    return "";
  }

  @Override
  public JsObject toJournal(Object event) {
    return marshaller.toJson(event);
  }

  @Override
  public EventSeq fromJournal(Object event, String manifest) {
    if (event instanceof JsObject) {
      JsObject json = (JsObject) event;
      return EventSeq.single(marshaller.fromJson(json));
    } else {
      throw new IllegalArgumentException(
          "Unable to fromJournal a non-JSON object! Was: " + event.getClass());
    }
  }
}


Note


This technique only applies if the Akka Persistence plugin you are using provides this capability. Check the documentation of your favourite plugin to see if it supports this style of persistence.


If it doesn’t, you may want to skim the 
list of existing journal plugins
, just in case some other plugin for your favourite datastore 
does
 provide this capability.


Alternative solution:


In fact, an AsyncWriteJournal implementation could natively decide to not use binary serialization at all, and 
always
 serialize the incoming messages as JSON - in which case the 
toJournal
 implementation of the 
EventAdapter
EventAdapter
 would be an identity function, and the 
fromJournal
 would need to de-serialize messages from JSON.
Note


If in need of human-readable events on the 
write-side
 of your application reconsider whether preparing materialized views using 
Persistence Query
 would not be an efficient way to go about this, without compromising the write-side’s throughput characteristics.


If indeed you want to use a human-readable representation on the write-side, pick a Persistence plugin that provides that functionality, or â implement one yourself.




Split large event into fine-grained events


Situation:
 While refactoring your domain events, you find that one of the events has become too large (coarse-grained) and needs to be split up into multiple fine-grained events.


Solution:
 Let us consider a situation where an event represents “user details changed”. After some time we discover that this event is too coarse, and needs to be split into “user name changed” and “user address changed”, because somehow users keep changing their usernames a lot and we’d like to keep this as a separate event.


The write side change is very simple, we persist 
UserNameChanged
 or 
UserAddressChanged
 depending on what the user actually intended to change (instead of the composite 
UserDetailsChanged
 that we had in version 1 of our model).




The 
EventAdapter
 splits the incoming event into smaller more fine-grained events during recovery.


During recovery however, we now need to convert the old 
V1
 model into the 
V2
 representation of the change. Depending if the old event contains a name change, we either emit the 
UserNameChanged
 or we don’t, and the address change is handled similarly:




Scala




copy
source
trait Version1
trait Version2

// V1 event:
final case class UserDetailsChanged(name: String, address: String) extends Version1

// corresponding V2 events:
final case class UserNameChanged(name: String) extends Version2
final case class UserAddressChanged(address: String) extends Version2

// event splitting adapter:
class UserEventsAdapter extends EventAdapter {
  override def manifest(event: Any): String = ""

  override def fromJournal(event: Any, manifest: String): EventSeq = event match {
    case UserDetailsChanged(null, address) => EventSeq(UserAddressChanged(address))
    case UserDetailsChanged(name, null)    => EventSeq(UserNameChanged(name))
    case UserDetailsChanged(name, address) =>
      EventSeq(UserNameChanged(name), UserAddressChanged(address))
    case event: Version2 => EventSeq(event)
  }

  override def toJournal(event: Any): Any = event
}


Java




copy
source
interface Version1 {};

interface Version2 {}

// V1 event:
static class UserDetailsChanged implements Version1 {
  public final String name;
  public final String address;

  public UserDetailsChanged(String name, String address) {
    this.name = name;
    this.address = address;
  }
}

// corresponding V2 events:
static class UserNameChanged implements Version2 {
  public final String name;

  public UserNameChanged(String name) {
    this.name = name;
  }
}
static class UserAddressChanged implements Version2 {
  public final String address;

  public UserAddressChanged(String address) {
    this.address = address;
  }
}

// event splitting adapter:
static class UserEventsAdapter implements EventAdapter {
  @Override
  public String manifest(Object event) {
    return "";
  }

  @Override
  public EventSeq fromJournal(Object event, String manifest) {
    if (event instanceof UserDetailsChanged) {
      UserDetailsChanged c = (UserDetailsChanged) event;
      if (c.name == null) return EventSeq.single(new UserAddressChanged(c.address));
      else if (c.address == null) return EventSeq.single(new UserNameChanged(c.name));
      else return EventSeq.create(new UserNameChanged(c.name), new UserAddressChanged(c.address));
    } else {
      return EventSeq.single(event);
    }
  }

  @Override
  public Object toJournal(Object event) {
    return event;
  }
}




By returning an 
EventSeq
 from the event adapter, the recovered event can be converted to multiple events before being delivered to the persistent actor.














 
Testing






Persistence Query 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-quickstart.html
Streams Quickstart Guide • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide




Dependency


First steps


Reusable Pieces


Time-Based Processing


Reactive Tweets




Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide




Dependency


First steps


Reusable Pieces


Time-Based Processing


Reactive Tweets




Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Streams Quickstart Guide


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}
Note


Both the Java and Scala DSLs of Akka Streams are bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting 
javadsl
 imports when working in Scala, or viceversa. See 
IDE Tips
. 


First steps


A stream usually begins at a source, so this is also how we start an Akka Stream. Before we create one, we import the full complement of streaming tools:




Scala




copy
source
import akka.stream._
import akka.stream.scaladsl._

import scala.annotation.nowarn


Java




copy
source
import akka.stream.*;
import akka.stream.javadsl.*;




If you want to execute the code samples while you read through the quick start guide, you will also need the following imports:




Scala




copy
source
import akka.{ Done, NotUsed }
import akka.actor.ActorSystem
import akka.util.ByteString
import scala.concurrent._
import scala.concurrent.duration._
import java.nio.file.Paths


Java




copy
source
import akka.Done;
import akka.NotUsed;
import akka.actor.ActorSystem;
import akka.util.ByteString;

import java.nio.file.Paths;
import java.math.BigInteger;
import java.time.Duration;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.ExecutionException;




And 
an object
a class
 to start an Akka 
ActorSystem
ActorSystem
 and hold your code 
. Making the 
ActorSystem
 implicit makes it available to the streams without manually passing it when running them
:




Scala




copy
source
object Main extends App {
  implicit val system: ActorSystem = ActorSystem("QuickStart")
  // Code here
}


Java




copy
source
public class Main {
  public static void main(String[] argv) {
    final ActorSystem system = ActorSystem.create("QuickStart");
    // Code here
  }
}




Now we will start with a rather simple source, emitting the integers 1 to 100:




Scala




copy
source
val source: Source[Int, NotUsed] = Source(1 to 100)


Java




copy
source
final Source<Integer, NotUsed> source = Source.range(1, 100);




The 
Source
Source
 type is parameterized with two types: the first one is the type of element that this source emits and the second one, the “materialized value”, allows running the source to produce some auxiliary value (e.g. a network source may provide information about the bound port or the peerâs address). Where no auxiliary information is produced, the type 
NotUsed
NotUsed
 is used. A simple range of integers falls into this category - running our stream produces a 
NotUsed
.


Having created this source means that we have a description of how to emit the first 100 natural numbers, but this source is not yet active. In order to get those numbers out we have to run it:




Scala




copy
source
source.runForeach(i => println(i))


Java




copy
source
source.runForeach(i -> System.out.println(i), system);




This line will complement the source with a consumer functionâin this example we print out the numbers to the consoleâand pass this little stream setup to an Actor that runs it. This activation is signaled by having ârunâ be part of the method name; there are other methods that run Akka Streams, and they all follow this pattern.


When running this 
source in a 
scala.App
program
 you might notice it does not terminate, because the 
ActorSystem
ActorSystem
 is never terminated. Luckily 
runForeach
runForeach
 returns a 
Future
[
Done
Done
]
CompletionStage
<
Done
Done
>
 which resolves when the stream finishes:




Scala




copy
source
val done: Future[Done] = source.runForeach(i => println(i))

implicit val ec = system.dispatcher
done.onComplete(_ => system.terminate())


Java




copy
source
final CompletionStage<Done> done = source.runForeach(i -> System.out.println(i), system);

done.thenRun(() -> system.terminate());




The nice thing about Akka Streams is that the 
Source
 is a description of what you want to run, and like an architectâs blueprint it can be reused, incorporated into a larger design. We may choose to transform the source of integers and write it to a file instead:




Scala




copy
source
val factorials = source.scan(BigInt(1))((acc, next) => acc * next)

val result: Future[IOResult] =
  factorials.map(num => ByteString(s"$num\n")).runWith(FileIO.toPath(Paths.get("factorials.txt")))


Java




copy
source
final Source<BigInteger, NotUsed> factorials =
    source.scan(BigInteger.ONE, (acc, next) -> acc.multiply(BigInteger.valueOf(next)));

final CompletionStage<IOResult> result =
    factorials
        .map(num -> ByteString.fromString(num.toString() + "\n"))
        .runWith(FileIO.toPath(Paths.get("factorials.txt")), system);




First we use the 
scan
scan
 operator to run a computation over the whole stream: starting with the number 1 (
BigInt(1)
BigInteger.ONE
) we multiply by each of the incoming numbers, one after the other; the scan operation emits the initial value and then every calculation result. This yields the series of factorial numbers which we stash away as a 
Source
 for later reuseâit is important to keep in mind that nothing is actually computed yet, this is a description of what we want to have computed once we run the stream. Then we convert the resulting series of numbers into a stream of 
ByteString
ByteString
 objects describing lines in a text file. This stream is then run by attaching a file as the receiver of the data. In the terminology of Akka Streams this is called a 
Sink
Sink
. 
IOResult
IOResult
 is a type that IO operations return in Akka Streams in order to tell you how many bytes or elements were consumed and whether the stream terminated normally or exceptionally.


Reusable Pieces


One of the nice parts of Akka Streamsâand something that other stream libraries do not offerâis that not only sources can be reused like blueprints, all other elements can be as well. We can take the file-writing 
Sink
Sink
, prepend the processing steps necessary to get the 
ByteString
ByteString
 elements from incoming strings and package that up as a reusable piece as well. Since the language for writing these streams always flows from left to right (just like plain English), we need a starting point that is like a source but with an âopenâ input. In Akka Streams this is called a 
Flow
Flow
:




Scala




copy
source
def lineSink(filename: String): Sink[String, Future[IOResult]] =
  Flow[String].map(s => ByteString(s + "\n")).toMat(FileIO.toPath(Paths.get(filename)))(Keep.right)


Java




copy
source
public Sink<String, CompletionStage<IOResult>> lineSink(String filename) {
  return Flow.of(String.class)
      .map(s -> ByteString.fromString(s.toString() + "\n"))
      .toMat(FileIO.toPath(Paths.get(filename)), Keep.right());
}




Starting from a flow of strings we convert each to 
ByteString
 and then feed to the already known file-writing 
Sink
. The resulting blueprint is a 
Sink[String, Future[IOResult]]
Sink<String, CompletionStage<IOResult>>
, which means that it accepts strings as its input and when materialized it will create auxiliary information of type 
Future
[
IOResult
IOResult
]
CompletionStage
<
IOResult
IOResult
>
 (when chaining operations on a 
Source
 or 
Flow
 the type of the auxiliary informationâcalled the âmaterialized valueââis given by the leftmost starting point; since we want to retain what the 
FileIO.toPath
FileIO.toPath
 sink has to offer, we need to say 
Keep.right
Keep.right()
.


We can use the new and shiny 
Sink
Sink
 we just created by attaching it to our 
factorials
 sourceâafter a small adaptation to turn the numbers into strings:




Scala




copy
source
factorials.map(_.toString).runWith(lineSink("factorial2.txt"))


Java




copy
source
factorials.map(BigInteger::toString).runWith(lineSink("factorial2.txt"), system);




Time-Based Processing


Before we start looking at a more involved example we explore the streaming nature of what Akka Streams can do. Starting from the 
factorials
 source we transform the stream by zipping it together with another stream, represented by a 
Source
Source
 that emits the number 0 to 100: the first number emitted by the 
factorials
 source is the factorial of zero, the second is the factorial of one, and so on. We combine these two by forming strings like 
"3! = 6"
.




Scala




copy
source
factorials
  .zipWith(Source(0 to 100))((num, idx) => s"$idx! = $num")
  .throttle(1, 1.second)
  .runForeach(println)


Java




copy
source
factorials
    .zipWith(Source.range(0, 99), (num, idx) -> String.format("%d! = %s", idx, num))
    .throttle(1, Duration.ofSeconds(1))
    .runForeach(s -> System.out.println(s), system);




All operations so far have been time-independent and could have been performed in the same fashion on strict collections of elements. The next line demonstrates that we are in fact dealing with streams that can flow at a certain speed: we use the 
throttle
throttle
 operator to slow down the stream to 1 element per second.


If you run this program you will see one line printed per second. One aspect that is not immediately visible deserves mention, though: if you try and set the streams to produce a billion numbers each then you will notice that your JVM does not crash with an OutOfMemoryError, even though you will also notice that running the streams happens in the background, asynchronously (this is the reason for the auxiliary information to be provided as a 
Future
CompletionStage
, in the future). The secret that makes this work is that Akka Streams implicitly implement pervasive flow control, all operators respect back-pressure. This allows the throttle operator to signal to all its upstream sources of data that it can only accept elements at a certain rateâwhen the incoming rate is higher than one per second the throttle operator will assert 
back-pressure
 upstream.


This is all there is to Akka Streams in a nutshellâglossing over the fact that there are dozens of sources and sinks and many more stream transformation operators to choose from, see also 
operator index
.


Reactive Tweets


A typical use case for stream processing is consuming a live stream of data that we want to extract or aggregate some other data from. In this example we’ll consider consuming a stream of tweets and extracting information concerning Akka from them.


We will also consider the problem inherent to all non-blocking streaming solutions: 
“What if the subscriber is too slow to consume the live stream of data?”
. Traditionally the solution is often to buffer the elements, but this canâand usually willâcause eventual buffer overflows and instability of such systems. Instead Akka Streams depend on internal backpressure signals that allow to control what should happen in such scenarios.


Here’s the data model we’ll be working with throughout the quickstart examples:




Scala




copy
source
final case class Author(handle: String)

final case class Hashtag(name: String)

final case class Tweet(author: Author, timestamp: Long, body: String) {
  def hashtags: Set[Hashtag] =
    body
      .split(" ")
      .collect {
        case t if t.startsWith("#") => Hashtag(t.replaceAll("[^#\\w]", ""))
      }
      .toSet
}

val akkaTag = Hashtag("#akka")


Java




copy
source
public static class Author {
  public final String handle;

  public Author(String handle) {
    this.handle = handle;
  }

  // ...

}

public static class Hashtag {
  public final String name;

  public Hashtag(String name) {
    this.name = name;
  }

  // ...
}

public static class Tweet {
  public final Author author;
  public final long timestamp;
  public final String body;

  public Tweet(Author author, long timestamp, String body) {
    this.author = author;
    this.timestamp = timestamp;
    this.body = body;
  }

  public Set<Hashtag> hashtags() {
    return Arrays.asList(body.split(" ")).stream()
        .filter(a -> a.startsWith("#"))
        .map(a -> new Hashtag(a))
        .collect(Collectors.toSet());
  }

  // ...
}

public static final Hashtag AKKA = new Hashtag("#akka");


Note


If you would like to get an overview of the used vocabulary first instead of diving head-first into an actual example you can have a look at the 
Core concepts
 and 
Defining and running streams
 sections of the docs, and then come back to this quickstart to see it all pieced together into a simple example application.


Transforming and consuming simple streams


The example application we will be looking at is a simple Twitter feed stream from which we’ll want to extract certain information, like for example finding all twitter handles of users who tweet about 
#akka
.


In order to prepare our environment by creating an 
ActorSystem
ActorSystem
 which will be responsible for running the streams we are about to create:




Scala




copy
source
implicit val system: ActorSystem = ActorSystem("reactive-tweets")


Java




copy
source
final ActorSystem system = ActorSystem.create("reactive-tweets");




Let’s assume we have a stream of tweets readily available. In Akka this is expressed as a 
Source[Out, M]
Source<Out, M>
:




Scala




copy
source
val tweets: Source[Tweet, NotUsed]


Java




copy
source
Source<Tweet, NotUsed> tweets;




Streams always start flowing from a 
Source[Out,M1]
Source<Out,M1>
 then can continue through 
Flow[In,Out,M2]
Flow<In,Out,M2>
 elements or more advanced operators to finally be consumed by a 
Sink[In,M3]
Sink<In,M3>
 
(ignore the type parameters 
M1
, 
M2
 and 
M3
 for now, they are not relevant to the types of the elements produced/consumed by these classes â they are “materialized types”, which we’ll talk about 
below
)
. The first type parameterâ
Tweet
 in this caseâdesignates the kind of elements produced by the source while the 
M
 type parameters describe the object that is created during materialization (
see below
)â
NotUsed
 (from the 
scala.runtime
 package) means that no value is produced, it is the generic equivalent of 
void
.


The operations should look familiar to anyone who has used the Scala Collections library, however they operate on streams and not collections of data (which is a very important distinction, as some operations only make sense in streaming and vice versa):




Scala




copy
source
val authors: Source[Author, NotUsed] =
  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)


Java




copy
source
final Source<Author, NotUsed> authors =
    tweets.filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);




Finally in order to 
materialize
 and run the stream computation we need to attach the Flow to a 
Sink
Sink<T, M>
 that will get the Flow running. The simplest way to do this is to call 
runWith(sink)
runWith(sink)
 on a 
Source
Source<Out, M>
. For convenience a number of common Sinks are predefined and collected as 
static
 methods on the 
Sink
 companion object
Sink class
. For now let’s print each author:




Scala




copy
source
authors.runWith(Sink.foreach(println))


Java




copy
source
authors.runWith(Sink.foreach(a -> System.out.println(a)), system);




or by using the shorthand version (which are defined only for the most popular Sinks such as 
Sink.fold
Sink.fold
 and 
Sink.foreach
Sink.foreach
):




Scala




copy
source
authors.runForeach(println)


Java




copy
source
authors.runForeach(a -> System.out.println(a), system);




Materializing and running a stream always requires an 
ActorSystem
 to be 
in implicit scope (or passed in explicitly, like this: 
.runWith(sink)(system)
)
passed in explicitly, like this: 
runWith(sink, system)
.


The complete snippet looks like this:




Scala




copy
source
implicit val system: ActorSystem = ActorSystem("reactive-tweets")

val authors: Source[Author, NotUsed] =
  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)

authors.runWith(Sink.foreach(println))


Java




copy
source
final ActorSystem system = ActorSystem.create("reactive-tweets");

final Source<Author, NotUsed> authors =
    tweets.filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);

authors.runWith(Sink.foreach(a -> System.out.println(a)), system);




Flattening sequences in streams


In the previous section we were working on 1:1 relationships of elements which is the most common case, but sometimes we might want to map from one element to a number of elements and receive a “flattened” stream, similarly like 
flatMap
 works on Scala Collections. In order to get a flattened stream of hashtags from our stream of tweets we can use the 
mapConcat
mapConcat
 operator:




Scala




copy
source
val hashtags: Source[Hashtag, NotUsed] = tweets.mapConcat(_.hashtags.toList)


Java




copy
source
final Source<Hashtag, NotUsed> hashtags =
    tweets.mapConcat(t -> new ArrayList<Hashtag>(t.hashtags()));


Note


The name 
flatMap
 was consciously avoided due to its proximity with for-comprehensions and monadic composition. It is problematic for two reasons: 
first
firstly
, flattening by concatenation is often undesirable in bounded stream processing due to the risk of deadlock (with merge being the preferred strategy), and 
second
secondly
, the monad laws would not hold for our implementation of flatMap (due to the liveness issues).


Please note that the 
mapConcat
mapConcat
 requires the supplied function to return 
an iterable (
f: Out => immutable.Iterable[T]
a strict collection (
Out f -> java.util.List<T>
)
, whereas 
flatMap
 would have to operate on streams all the way through.


Broadcasting a stream


Now let’s say we want to persist all hashtags, as well as all author names from this one live stream. For example we’d like to write all author handles into one file, and all hashtags into another file on disk. This means we have to split the source stream into two streams which will handle the writing to these different files.


Elements that can be used to form such “fan-out” (or “fan-in”) structures are referred to as “junctions” in Akka Streams. One of these that we’ll be using in this example is called 
Broadcast
Broadcast
, and it emits elements from its input port to all of its output ports.


Akka Streams intentionally separate the linear stream structures (Flows) from the non-linear, branching ones (Graphs) in order to offer the most convenient API for both of these cases. Graphs can express arbitrarily complex stream setups at the expense of not reading as familiarly as collection transformations.


Graphs are constructed using 
GraphDSL
GraphDSL
 like this:




Scala




copy
source
val writeAuthors: Sink[Author, NotUsed] = ???
val writeHashtags: Sink[Hashtag, NotUsed] = ???
val g = RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val bcast = b.add(Broadcast[Tweet](2))
  tweets ~> bcast.in
  bcast.out(0) ~> Flow[Tweet].map(_.author) ~> writeAuthors
  bcast.out(1) ~> Flow[Tweet].mapConcat(_.hashtags.toList) ~> writeHashtags
  ClosedShape
})
g.run()


Java




copy
source
Sink<Author, NotUsed> writeAuthors;
Sink<Hashtag, NotUsed> writeHashtags;
RunnableGraph.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanOutShape<Tweet, Tweet> bcast = b.add(Broadcast.create(2));
              final FlowShape<Tweet, Author> toAuthor =
                  b.add(Flow.of(Tweet.class).map(t -> t.author));
              final FlowShape<Tweet, Hashtag> toTags =
                  b.add(
                      Flow.of(Tweet.class)
                          .mapConcat(t -> new ArrayList<Hashtag>(t.hashtags())));
              final SinkShape<Author> authors = b.add(writeAuthors);
              final SinkShape<Hashtag> hashtags = b.add(writeHashtags);

              b.from(b.add(tweets)).viaFanOut(bcast).via(toAuthor).to(authors);
              b.from(bcast).via(toTags).to(hashtags);
              return ClosedShape.getInstance();
            }))
    .run(system);




As you can see, 
inside the 
GraphDSL
 we use an implicit graph builder 
b
 to mutably construct the graph using the 
~>
 “edge operator” (also read as “connect” or “via” or “to”). The operator is provided implicitly by importing 
GraphDSL.Implicits._
we use graph builder 
b
 to construct the graph using 
UniformFanOutShape
UniformFanOutShape
 and 
Flow
Flow
 s
.


GraphDSL.create
GraphDSL.create
 returns a 
Graph
Graph
, in this example a 
Graph[ClosedShape, NotUsed]
Graph<ClosedShape,NotUsed>
 where 
ClosedShape
ClosedShape
 means that it is 
a fully connected graph
 or “closed” - there are no unconnected inputs or outputs. Since it is closed it is possible to transform the graph into a 
RunnableGraph
RunnableGraph
 using 
RunnableGraph.fromGraph
RunnableGraph.fromGraph
. The 
RunnableGraph
 can then be 
run()
run()
 to materialize a stream out of it.


Both 
Graph
 and 
RunnableGraph
 are 
immutable, thread-safe, and freely shareable
.


A graph can also have one of several other shapes, with one or more unconnected ports. Having unconnected ports expresses a graph that is a 
partial graph
. Concepts around composing and nesting graphs in large structures are explained in detail in 
Modularity, Composition and Hierarchy
. It is also possible to wrap complex computation graphs as Flows, Sinks or Sources, which will be explained in detail in 
Constructing Sources, Sinks and Flows from Partial Graphs
Constructing and combining Partial Graphs
.


Back-pressure in action


One of the main advantages of Akka Streams is that they 
always
 propagate back-pressure information from stream Sinks (Subscribers) to their Sources (Publishers). It is not an optional feature, and is enabled at all times. To learn more about the back-pressure protocol used by Akka Streams and all other Reactive Streams compatible implementations read 
Back-pressure explained
.


A typical problem applications (not using Akka Streams) like this often face is that they are unable to process the incoming data fast enough, either temporarily or by design, and will start buffering incoming data until there’s no more space to buffer, resulting in either 
OutOfMemoryError
 s or other severe degradations of service responsiveness. With Akka Streams buffering can and must be handled explicitly. For example, if we are only interested in the “
most recent tweets, with a buffer of 10 elements
” this can be expressed using the 
buffer
buffer
 element:




Scala




copy
source
tweets.buffer(10, OverflowStrategy.dropHead).map(slowComputation).runWith(Sink.ignore)


Java




copy
source
tweets
    .buffer(10, OverflowStrategy.dropHead())
    .map(t -> slowComputation(t))
    .runWith(Sink.ignore(), system);




The 
buffer
 element takes an explicit and required 
OverflowStrategy
OverflowStrategy
, which defines how the buffer should react when it receives another element while it is full. Strategies provided include dropping the oldest element (
dropHead
), dropping the entire buffer, signalling 
errors
failures
 etc. Be sure to pick and choose the strategy that fits your use case best.




Materialized values


So far we’ve been only processing data using Flows and consuming it into some kind of external Sink - be it by printing values or storing them in some external system. However sometimes we may be interested in some value that can be obtained from the materialized processing pipeline. For example, we want to know how many tweets we have processed. While this question is not as obvious to give an answer to in case of an infinite stream of tweets (one way to answer this question in a streaming setting would be to create a stream of counts described as “
up until now
, we’ve processed N tweets”), but in general it is possible to deal with finite streams and come up with a nice result such as a total count of elements.


First, let’s write such an element counter using 
Sink.fold
 and
Flow.of(Class)
 and 
Sink.fold
 to
 see how the types look like:




Scala




copy
source
val count: Flow[Tweet, Int, NotUsed] = Flow[Tweet].map(_ => 1)

val sumSink: Sink[Int, Future[Int]] = Sink.fold[Int, Int](0)(_ + _)

val counterGraph: RunnableGraph[Future[Int]] =
  tweets.via(count).toMat(sumSink)(Keep.right)

val sum: Future[Int] = counterGraph.run()

sum.foreach(c => println(s"Total tweets processed: $c"))


Java




copy
source
final Sink<Integer, CompletionStage<Integer>> sumSink =
    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);

final RunnableGraph<CompletionStage<Integer>> counter =
    tweets.map(t -> 1).toMat(sumSink, Keep.right());

final CompletionStage<Integer> sum = counter.run(system);

sum.thenAcceptAsync(
    c -> System.out.println("Total tweets processed: " + c), system.dispatcher());




First we prepare a reusable 
Flow
 that will change each incoming tweet into an integer of value 
1
. We’ll use this in order to combine those with a 
Sink.fold
 that will sum all 
Int
 elements of the stream and make its result available as a 
Future[Int]
. Next we connect the 
tweets
 stream to 
count
 with 
via
. Finally we connect the Flow to the previously prepared Sink using 
toMat
Sink.fold
 will sum all 
Integer
 elements of the stream and make its result available as a 
CompletionStage<Integer>
. Next we use the 
map
 method of 
tweets
 
Source
 which will change each incoming tweet into an integer value 
1
. Finally we connect the Flow to the previously prepared Sink using 
toMat
.


Remember those mysterious 
Mat
 type parameters on 
Source
[+Out, +Mat], 
Flow
[-In, +Out, +Mat] and 
Sink
[-In, +Mat]
Source
<Out, Mat>, 
Flow
<In, Out, Mat> and 
Sink
<In, Mat>
? They represent the type of values these processing parts return when materialized. When you chain these together, you can explicitly combine their materialized values. In our example we used the 
Keep.right
Keep.right()
 predefined function, which tells the implementation to only care about the materialized type of the operator currently appended to the right. The materialized type of 
sumSink
 is 
Future
[Int]
CompletionStage
 and because of using 
Keep.right
Keep.right()
, the resulting 
RunnableGraph
RunnableGraph
 has also a type parameter of 
Future[Int]
CompletionStage<Integer>
.


This step does 
not
 yet materialize the processing pipeline, it merely prepares the description of the Flow, which is now connected to a Sink, and therefore can be 
run()
, as indicated by its type: 
RunnableGraph[Future[Int]]
RunnableGraph<CompletionStage<Integer>>
. Next we call 
run()
run()
 which materializes and runs the Flow. The value returned by calling 
run()
 on a 
RunnableGraph[T]
RunnableGraph<T>
 is of type 
T
. In our case this type is 
Future[Int]
CompletionStage<Integer>
 which, when completed, will contain the total length of our 
tweets
 stream. In case of the stream failing, this future would complete with a Failure.


A 
RunnableGraph
RunnableGraph
 may be reused and materialized multiple times, because it is only the “blueprint” of the stream. This means that if we materialize a stream, for example one that consumes a live stream of tweets within a minute, the materialized values for those two materializations will be different, as illustrated by this example:




Scala




copy
source
val sumSink = Sink.fold[Int, Int](0)(_ + _)
val counterRunnableGraph: RunnableGraph[Future[Int]] =
  tweetsInMinuteFromNow.filter(_.hashtags contains akkaTag).map(t => 1).toMat(sumSink)(Keep.right)

// materialize the stream once in the morning
val morningTweetsCount: Future[Int] = counterRunnableGraph.run()
// and once in the evening, reusing the flow
val eveningTweetsCount: Future[Int] = counterRunnableGraph.run()



Java




copy
source
final Sink<Integer, CompletionStage<Integer>> sumSink =
    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);
final RunnableGraph<CompletionStage<Integer>> counterRunnableGraph =
    tweetsInMinuteFromNow
        .filter(t -> t.hashtags().contains(AKKA))
        .map(t -> 1)
        .toMat(sumSink, Keep.right());

// materialize the stream once in the morning
final CompletionStage<Integer> morningTweetsCount = counterRunnableGraph.run(system);
// and once in the evening, reusing the blueprint
final CompletionStage<Integer> eveningTweetsCount = counterRunnableGraph.run(system);




Many elements in Akka Streams provide materialized values which can be used for obtaining either results of computation or steering these elements which will be discussed in detail in 
Stream Materialization
. Summing up this section, now we know what happens behind the scenes when we run this one-liner, which is equivalent to the multi line version above:




Scala




copy
source
val sum: Future[Int] = tweets.map(t => 1).runWith(sumSink)


Java




copy
source
final CompletionStage<Integer> sum = tweets.map(t -> 1).runWith(sumSink, system);


Note


runWith()
 is a convenience method that automatically ignores the materialized value of any other operators except those appended by the 
runWith()
 itself. In the above example it translates to using 
Keep.right
Keep.right()
 as the combiner for materialized values.














 
Introduction






Design Principles behind Akka Streams 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-network.html
Classic Networking • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking




I/O


Using TCP


Using UDP


DNS Extension




Classic Utilities




















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking




I/O


Using TCP


Using UDP


DNS Extension




Classic Utilities






















Classic Networking
Note


Akka Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Akka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see 
coexistence
. For new projects we recommend using 
the new Actor API
.


FIXME 
https://github.com/akka/akka/issues/27263






I/O




Dependency


Introduction


Terminology, Concepts


Architecture in-depth


I/O Layer Design




Using TCP




Dependency


Introduction


Connecting


Accepting connections


Closing connections


Writing to a connection


Throttling Reads and Writes


ACK-Based Write Back-Pressure


NACK-Based Write Back-Pressure with Suspending


Read Back-Pressure with Pull Mode




Using UDP




Dependency


Introduction


Unconnected UDP


Connected UDP


UDP Multicast




DNS Extension




SRV Records




















 
Classic Serialization






I/O 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/reliable-delivery.html
Reliable delivery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery




Module info


Introduction


Point-to-point


Work pulling


Sharding


Durable producer


Ask from the producer


Only flow control


Chunk large messages


Configuration




Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery




Module info


Introduction


Point-to-point


Work pulling


Sharding


Durable producer


Ask from the producer


Only flow control


Chunk large messages


Configuration




Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Reliable delivery


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic At-Least-Once Delivery
.
Warning


This module is currently marked as 
may change
 because it is a new feature that needs feedback from real usage before finalizing the API. This means that API or semantics can change without warning or deprecation period. It is also not recommended to use this module in production just yet.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use reliable delivery, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


Normal 
message delivery reliability
 is at-most-once delivery, which means that messages may be lost. That should be rare, but still possible.


For interactions between some actors, that is not acceptable and at-least-once delivery or effectively-once processing is needed. The tools for reliable delivery described here help with implementing that. It can’t be achieved automatically under the hood without collaboration from the application. This is because confirming when a message has been fully processed is a business level concern. Only ensuring that it was transferred over the network or delivered to the mailbox of the actor would not be enough, since the actor may crash right before being able to process the message.


Lost messages are detected, resent and deduplicated as needed. In addition, it also includes flow control for the sending of messages to avoid that a fast producer overwhelms a slower consumer or sends messages at a higher rate than what can be transferred over the network. This can be a common problem in interactions between actors, resulting in fatal errors like 
OutOfMemoryError
 because too many messages are queued in the mailboxes of the actors. The detection of lost messages and the flow control is driven by the consumer side, which means that the producer side will not send faster than the demand requested by the consumer side. The producer side will not push resends unless requested by the consumer side.


There are 3 supported patterns, which are described in the following sections:




Point-to-point


Work pulling


Sharding




The Point-to-Point pattern has support for automatically 
splitting up large messages
 and assemble them again on the consumer side. This feature is useful for avoiding head of line blocking from serialization and transfer of large messages.


Point-to-point


This pattern implements point-to-point reliable delivery between a single producer actor sending messages and a single consumer actor receiving the messages.


Messages are sent from the producer to 
ProducerController
ProducerController
 and via 
ConsumerController
ConsumerController
 actors, which handle the delivery and confirmation of the processing in the destination consumer actor.




The producer actor will start the flow by sending a 
ProducerController.Start
 message to the 
ProducerController
.


The 
ProducerController
 sends 
RequestNext
 to the producer, which is then allowed to send one message to the 
ProducerController
. Thereafter the producer will receive a new 
RequestNext
 when it’s allowed to send one more message.


The producer and 
ProducerController
 actors are required to be local so that message delivery is both fast and guaranteed. This requirement is enforced by a runtime check.


Similarly, on the consumer side the destination consumer actor will start the flow by sending an initial 
ConsumerController.Start
 message to the 
ConsumerController
. 


For the 
ProducerController
 to know where to send the messages, it must be connected with the 
ConsumerController
. This can be done with the 
ProducerController.RegisterConsumer
 or 
ConsumerController.RegisterToProducerController
 messages. When using the point-to-point pattern, it is the application’s responsibility to connect them together. For example, this can be done by sending the 
ActorRef
 in an ordinary message to the other side, or by registering the 
ActorRef
 in the 
Receptionist
 so it can be found on the other side.


You must also take measures to reconnect them if any of the sides crashes, for example by watching it for termination.


Messages sent by the producer are wrapped in 
ConsumerController.Delivery
 when received by a consumer and the consumer should reply with 
ConsumerController.Confirmed
 when it has processed the message.


The next message is not delivered until the previous one is confirmed. Any messages from the producer that arrive while waiting for the confirmation are stashed by the 
ConsumerController
 and delivered when the previous message is confirmed.


Similar to the producer side, the consumer and the 
ConsumerController
 actors are required to be local so that message delivery is both fast and guaranteed. This requirement is enforced by a runtime check.


Many unconfirmed messages can be in flight between the 
ProducerController
 and 
ConsumerController
, but their number is limited by a flow control window. The flow control is driven by the consumer side, which means that the 
ProducerController
 will not send faster than the demand requested by the 
ConsumerController
.


Point-to-point example


An example of a fibonacci number generator (producer):




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.delivery.ProducerController
import akka.actor.typed.scaladsl.Behaviors

object FibonacciProducer {
  sealed trait Command

  private case class WrappedRequestNext(r: ProducerController.RequestNext[FibonacciConsumer.Command]) extends Command

  def apply(
      producerController: ActorRef[ProducerController.Command[FibonacciConsumer.Command]]): Behavior[Command] = {
    Behaviors.setup { context =>
      val requestNextAdapter =
        context.messageAdapter[ProducerController.RequestNext[FibonacciConsumer.Command]](WrappedRequestNext(_))
      producerController ! ProducerController.Start(requestNextAdapter)

      fibonacci(0, 1, 0)
    }
  }

  private def fibonacci(n: Long, b: BigInt, a: BigInt): Behavior[Command] = {
    Behaviors.receive {
      case (context, WrappedRequestNext(next)) =>
        context.log.info("Generated fibonacci {}: {}", n, a)
        next.sendNextTo ! FibonacciConsumer.FibonacciNumber(n, a)

        if (n == 1000)
          Behaviors.stopped
        else
          fibonacci(n + 1, a + b, b)
    }
  }
}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.delivery.ProducerController;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.math.BigInteger;
import java.util.Optional;

public class FibonacciProducer extends AbstractBehavior<FibonacciProducer.Command> {

  private long n = 0;
  private BigInteger b = BigInteger.ONE;
  private BigInteger a = BigInteger.ZERO;

  interface Command {}

  private static class WrappedRequestNext implements Command {
    final ProducerController.RequestNext<FibonacciConsumer.Command> next;

    private WrappedRequestNext(ProducerController.RequestNext<FibonacciConsumer.Command> next) {
      this.next = next;
    }
  }

  private FibonacciProducer(ActorContext<Command> context) {
    super(context);
  }

  public static Behavior<Command> create(
      ActorRef<ProducerController.Command<FibonacciConsumer.Command>> producerController) {
    return Behaviors.setup(
        context -> {
          ActorRef<ProducerController.RequestNext<FibonacciConsumer.Command>> requestNextAdapter =
              context.messageAdapter(
                  ProducerController.requestNextClass(), WrappedRequestNext::new);
          producerController.tell(new ProducerController.Start<>(requestNextAdapter));

          return new FibonacciProducer(context);
        });
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(WrappedRequestNext.class, w -> onWrappedRequestNext(w))
        .build();
  }

  private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {
    getContext().getLog().info("Generated fibonacci {}: {}", n, a);
    w.next.sendNextTo().tell(new FibonacciConsumer.FibonacciNumber(n, a));

    if (n == 1000) {
      return Behaviors.stopped();
    } else {
      n = n + 1;
      b = a.add(b);
      a = b;
      return this;
    }
  }
}




and consumer of the fibonacci numbers:




Scala




copy
source
import akka.actor.typed.delivery.ConsumerController

object FibonacciConsumer {
  sealed trait Command

  final case class FibonacciNumber(n: Long, value: BigInt) extends Command

  private case class WrappedDelivery(d: ConsumerController.Delivery[Command]) extends Command

  def apply(
      consumerController: ActorRef[ConsumerController.Command[FibonacciConsumer.Command]]): Behavior[Command] = {
    Behaviors.setup { context =>
      val deliveryAdapter =
        context.messageAdapter[ConsumerController.Delivery[FibonacciConsumer.Command]](WrappedDelivery(_))
      consumerController ! ConsumerController.Start(deliveryAdapter)

      Behaviors.receiveMessagePartial {
        case WrappedDelivery(ConsumerController.Delivery(FibonacciNumber(n, value), confirmTo)) =>
          context.log.info("Processed fibonacci {}: {}", n, value)
          confirmTo ! ConsumerController.Confirmed
          Behaviors.same
      }
    }
  }
}


Java




copy
source
import akka.actor.typed.delivery.ConsumerController;

public class FibonacciConsumer extends AbstractBehavior<FibonacciConsumer.Command> {

  interface Command {}

  public static class FibonacciNumber implements Command {
    public final long n;
    public final BigInteger value;

    public FibonacciNumber(long n, BigInteger value) {
      this.n = n;
      this.value = value;
    }
  }

  private static class WrappedDelivery implements Command {
    final ConsumerController.Delivery<Command> delivery;

    private WrappedDelivery(ConsumerController.Delivery<Command> delivery) {
      this.delivery = delivery;
    }
  }

  public static Behavior<Command> create(
      ActorRef<ConsumerController.Command<FibonacciConsumer.Command>> consumerController) {
    return Behaviors.setup(
        context -> {
          ActorRef<ConsumerController.Delivery<FibonacciConsumer.Command>> deliveryAdapter =
              context.messageAdapter(ConsumerController.deliveryClass(), WrappedDelivery::new);
          consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));

          return new FibonacciConsumer(context);
        });
  }

  private FibonacciConsumer(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder().onMessage(WrappedDelivery.class, this::onDelivery).build();
  }

  private Behavior<Command> onDelivery(WrappedDelivery w) {
    FibonacciNumber number = (FibonacciNumber) w.delivery.message();
    getContext().getLog().info("Processed fibonacci {}: {}", number.n, number.value);
    w.delivery.confirmTo().tell(ConsumerController.confirmed());
    return this;
  }
}




The 
FibonacciProducer
 sends the messages to a 
ProducerController
. The 
FibonacciConsumer
 receives the messages from a 
ConsumerController
. Note how the 
ActorRef
 in the 
Start
 messages are constructed as message adapters to map the 
RequestNext
 and 
Delivery
 to the protocol of the producer and consumer actors respectively.


The 
ConsumerController
 and 
ProducerController
 are connected via the 
ConsumerController.RegisterToProducerController
 message. The 
ActorRef
 of the 
ProducerController
 can be shared between producer and consumer sides with ordinary messages, or by using the 
Receptionist
. Alternatively, they can be connected in the other direction by sending 
ProducerController.RegisterConsumer
 to the 
ProducerController
.




Scala




copy
source
val consumerController = context.spawn(ConsumerController[FibonacciConsumer.Command](), "consumerController")
context.spawn(FibonacciConsumer(consumerController), "consumer")

val producerId = s"fibonacci-${UUID.randomUUID()}"
val producerController = context.spawn(
  ProducerController[FibonacciConsumer.Command](producerId, durableQueueBehavior = None),
  "producerController")
context.spawn(FibonacciProducer(producerController), "producer")

consumerController ! ConsumerController.RegisterToProducerController(producerController)


Java




copy
source
ActorRef<ConsumerController.Command<FibonacciConsumer.Command>> consumerController =
    context.spawn(ConsumerController.create(), "consumerController");
context.spawn(FibonacciConsumer.create(consumerController), "consumer");

String producerId = "fibonacci-" + UUID.randomUUID();
ActorRef<ProducerController.Command<FibonacciConsumer.Command>> producerController =
    context.spawn(
        ProducerController.create(
            FibonacciConsumer.Command.class, producerId, Optional.empty()),
        "producerController");
context.spawn(FibonacciProducer.create(producerController), "producer");

consumerController.tell(
    new ConsumerController.RegisterToProducerController<>(producerController));




Point-to-point delivery semantics


As long as neither producer nor consumer crash, the messages are delivered to the consumer actor in the same order as they were sent to the 
ProducerController
, without loss or duplicates. This means effectively-once processing without any business level deduplication.


Unconfirmed messages may be lost if the producer crashes. To avoid that, you need to enable the 
durable queue
 on the producer side. By doing so, any stored unconfirmed messages will be redelivered when the corresponding producer is started again. Even if the same 
ConsumerController
 instance is used, there may be delivery of messages that had already been processed but the fact that they were confirmed had not been stored yet. This means that we have at-least-once delivery.


If the consumer crashes, a new 
ConsumerController
 can be connected to the original 
ProducerConsumer
 without restarting it. The 
ProducerConsumer
 will then redeliver all unconfirmed messages. In that case the unconfirmed messages will be delivered to the new consumer and some of these may already have been processed by the previous consumer. Again, this means that we have at-least-once delivery.


Work pulling


Work pulling is a pattern where several worker actors pull tasks at their own pace from a shared work manager instead of that the manager pushes work to the workers blindly without knowing their individual capacity and current availability.


One important property is that the order of the messages should not matter, because each message is routed randomly to one of the workers with demand. In other words, two subsequent messages may be routed to two different workers and processed independent of each other.


Messages are sent from the producer to 
WorkPullingProducerController
WorkPullingProducerController
 and via 
ConsumerController
ConsumerController
 actors, which handle the delivery and confirmation of the processing in the destination worker (consumer) actor.




and adding another worker




A worker actor (consumer) and its 
ConsumerController
 is dynamically registered to the 
WorkPullingProducerController
 via a 
ServiceKey
. It will register itself to the 
Receptionist
, and the 
WorkPullingProducerController
 subscribes to the same key to find active workers. In this way workers can be dynamically added or removed from any node in the cluster.


The work manager (producer) actor will start the flow by sending a 
WorkPullingProducerController.Start
 message to the 
WorkPullingProducerController
.


The 
WorkPullingProducerController
 sends 
RequestNext
 to the producer, which is then allowed to send one message to the 
WorkPullingProducerController
. Thereafter the producer will receive a new 
RequestNext
 when it’s allowed to send one more message. 
WorkPullingProducerController
 will send a new 
RequestNext
 when there is a demand from any worker. It’s possible that all workers with demand are deregistered after the 
RequestNext
 is sent and before the actual messages is sent to the 
WorkPullingProducerController
. In that case the message is buffered and will be delivered when a new worker is registered or when there is a new demand.


The producer and 
WorkPullingProducerController
 actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.


Similarly, on the consumer side the destination consumer actor will start the flow by sending an initial 
ConsumerController.Start
 message to the 
ConsumerController
.


Received messages from the producer are wrapped in 
ConsumerController.Delivery
 when sent to the consumer, which is supposed to reply with 
ConsumerController.Confirmed
 when it has processed the message. Next message is not delivered until the previous is confirmed. More messages from the producer that arrive while waiting for the confirmation are stashed by the 
ConsumerController
 and delivered when the previous message is confirmed.


The consumer and the 
ConsumerController
 actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.


Many unconfirmed messages can be in flight between the 
WorkPullingProducerController
 and each 
ConsumerController
, but it is limited by a flow control window. The flow control is driven by the consumer side, which means that the 
WorkPullingProducerController
 will not send faster than the demand requested by the workers.


Work pulling example


Example of image converter worker (consumer):




Scala




copy
source
import akka.actor.typed.scaladsl.Behaviors
import akka.actor.typed.Behavior
import akka.actor.typed.delivery.ConsumerController
import akka.actor.typed.receptionist.ServiceKey

object ImageConverter {
  sealed trait Command
  final case class ConversionJob(resultId: UUID, fromFormat: String, toFormat: String, image: Array[Byte])
  private case class WrappedDelivery(d: ConsumerController.Delivery[ConversionJob]) extends Command

  val serviceKey = ServiceKey[ConsumerController.Command[ConversionJob]]("ImageConverter")

  def apply(): Behavior[Command] = {
    Behaviors.setup { context =>
      val deliveryAdapter =
        context.messageAdapter[ConsumerController.Delivery[ConversionJob]](WrappedDelivery(_))
      val consumerController =
        context.spawn(ConsumerController(serviceKey), "consumerController")
      consumerController ! ConsumerController.Start(deliveryAdapter)

      Behaviors.receiveMessage {
        case WrappedDelivery(delivery) =>
          val image = delivery.message.image
          val fromFormat = delivery.message.fromFormat
          val toFormat = delivery.message.toFormat
          // convert image...
          // store result with resultId key for later retrieval

          // and when completed confirm
          delivery.confirmTo ! ConsumerController.Confirmed

          Behaviors.same
      }

    }
  }

}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.delivery.ConsumerController;
import akka.actor.typed.delivery.DurableProducerQueue;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;

import java.time.Duration;
import java.util.Optional;
import java.util.UUID;

public class ImageConverter {
  interface Command {}

  public static class ConversionJob {
    public final UUID resultId;
    public final String fromFormat;
    public final String toFormat;
    public final byte[] image;

    public ConversionJob(UUID resultId, String fromFormat, String toFormat, byte[] image) {
      this.resultId = resultId;
      this.fromFormat = fromFormat;
      this.toFormat = toFormat;
      this.image = image;
    }
  }

  private static class WrappedDelivery implements Command {
    final ConsumerController.Delivery<ConversionJob> delivery;

    private WrappedDelivery(ConsumerController.Delivery<ConversionJob> delivery) {
      this.delivery = delivery;
    }
  }

  public static ServiceKey<ConsumerController.Command<ConversionJob>> serviceKey =
      ServiceKey.create(ConsumerController.serviceKeyClass(), "ImageConverter");

  public static Behavior<Command> create() {
    return Behaviors.setup(
        context -> {
          ActorRef<ConsumerController.Delivery<ConversionJob>> deliveryAdapter =
              context.messageAdapter(ConsumerController.deliveryClass(), WrappedDelivery::new);
          ActorRef<ConsumerController.Command<ConversionJob>> consumerController =
              context.spawn(ConsumerController.create(serviceKey), "consumerController");
          consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));

          return Behaviors.receive(Command.class)
              .onMessage(WrappedDelivery.class, ImageConverter::onDelivery)
              .build();
        });
  }

  private static Behavior<Command> onDelivery(WrappedDelivery w) {
    byte[] image = w.delivery.message().image;
    String fromFormat = w.delivery.message().fromFormat;
    String toFormat = w.delivery.message().toFormat;
    // convert image...
    // store result with resultId key for later retrieval

    // and when completed confirm
    w.delivery.confirmTo().tell(ConsumerController.confirmed());

    return Behaviors.same();
  }
}




and image converter job manager (producer):




Scala




copy
source
import akka.actor.typed.delivery.WorkPullingProducerController
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.StashBuffer

object ImageWorkManager {
  sealed trait Command
  final case class Convert(fromFormat: String, toFormat: String, image: Array[Byte]) extends Command
  private case class WrappedRequestNext(r: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob])
      extends Command

  final case class GetResult(resultId: UUID, replyTo: ActorRef[Option[Array[Byte]]]) extends Command

  def apply(): Behavior[Command] = {
    Behaviors.setup { context =>
      val requestNextAdapter =
        context.messageAdapter[WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]](
          WrappedRequestNext(_))
      val producerController = context.spawn(
        WorkPullingProducerController(
          producerId = "workManager",
          workerServiceKey = ImageConverter.serviceKey,
          durableQueueBehavior = None),
        "producerController")
      producerController ! WorkPullingProducerController.Start(requestNextAdapter)

      Behaviors.withStash(1000) { stashBuffer =>
        new ImageWorkManager(context, stashBuffer).waitForNext()
      }
    }
  }

}


final class ImageWorkManager(
    context: ActorContext[ImageWorkManager.Command],
    stashBuffer: StashBuffer[ImageWorkManager.Command]) {
  import ImageWorkManager._

  private def waitForNext(): Behavior[Command] = {
    Behaviors.receiveMessagePartial {
      case WrappedRequestNext(next) =>
        stashBuffer.unstashAll(active(next))
      case c: Convert =>
        if (stashBuffer.isFull) {
          context.log.warn("Too many Convert requests.")
          Behaviors.same
        } else {
          stashBuffer.stash(c)
          Behaviors.same
        }
      case GetResult(resultId, replyTo) =>
        // TODO retrieve the stored result and reply
        Behaviors.same
    }
  }

  private def active(
      next: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]): Behavior[Command] = {
    Behaviors.receiveMessagePartial {
      case Convert(from, to, image) =>
        val resultId = UUID.randomUUID()
        next.sendNextTo ! ImageConverter.ConversionJob(resultId, from, to, image)
        waitForNext()
      case GetResult(resultId, replyTo) =>
        // TODO retrieve the stored result and reply
        Behaviors.same
      case _: WrappedRequestNext =>
        throw new IllegalStateException("Unexpected RequestNext")
    }
  }
}


Java




copy
source
import akka.actor.typed.delivery.WorkPullingProducerController;
import akka.Done;

public class ImageWorkManager {

  interface Command {}

  public static class Convert implements Command {
    public final String fromFormat;
    public final String toFormat;
    public final byte[] image;

    public Convert(String fromFormat, String toFormat, byte[] image) {
      this.fromFormat = fromFormat;
      this.toFormat = toFormat;
      this.image = image;
    }
  }

  public static class GetResult implements Command {
    public final UUID resultId;
    public final ActorRef<Optional<byte[]>> replyTo;

    public GetResult(UUID resultId, ActorRef<Optional<byte[]>> replyTo) {
      this.resultId = resultId;
      this.replyTo = replyTo;
    }
  }

  private static class WrappedRequestNext implements Command {
    final WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next;

    private WrappedRequestNext(
        WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {
      this.next = next;
    }
  }


  private final ActorContext<Command> context;
  private final StashBuffer<Command> stashBuffer;

  private ImageWorkManager(ActorContext<Command> context, StashBuffer<Command> stashBuffer) {
    this.context = context;
    this.stashBuffer = stashBuffer;
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(
        context -> {
          ActorRef<WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob>>
              requestNextAdapter =
                  context.messageAdapter(
                      WorkPullingProducerController.requestNextClass(), WrappedRequestNext::new);
          ActorRef<WorkPullingProducerController.Command<ImageConverter.ConversionJob>>
              producerController =
                  context.spawn(
                      WorkPullingProducerController.create(
                          ImageConverter.ConversionJob.class,
                          "workManager",
                          ImageConverter.serviceKey,
                          Optional.empty()),
                      "producerController");
          producerController.tell(new WorkPullingProducerController.Start<>(requestNextAdapter));

          return Behaviors.withStash(
              1000, stashBuffer -> new ImageWorkManager(context, stashBuffer).waitForNext());
        });
  }

  private Behavior<Command> waitForNext() {
    return Behaviors.receive(Command.class)
        .onMessage(WrappedRequestNext.class, this::onWrappedRequestNext)
        .onMessage(Convert.class, this::onConvertWait)
        .onMessage(GetResult.class, this::onGetResult)
        .build();
  }

  private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {
    return stashBuffer.unstashAll(active(w.next));
  }

  private Behavior<Command> onConvertWait(Convert convert) {
    if (stashBuffer.isFull()) {
      context.getLog().warn("Too many Convert requests.");
      return Behaviors.same();
    } else {
      stashBuffer.stash(convert);
      return Behaviors.same();
    }
  }

  private Behavior<Command> onGetResult(GetResult get) {
    // TODO retrieve the stored result and reply
    return Behaviors.same();
  }

  private Behavior<Command> active(
      WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {
    return Behaviors.receive(Command.class)
        .onMessage(Convert.class, c -> onConvert(c, next))
        .onMessage(GetResult.class, this::onGetResult)
        .onMessage(WrappedRequestNext.class, this::onUnexpectedWrappedRequestNext)
        .build();
  }

  private Behavior<Command> onUnexpectedWrappedRequestNext(WrappedRequestNext w) {
    throw new IllegalStateException("Unexpected RequestNext");
  }

  private Behavior<Command> onConvert(
      Convert convert,
      WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {
    UUID resultId = UUID.randomUUID();
    next.sendNextTo()
        .tell(
            new ImageConverter.ConversionJob(
                resultId, convert.fromFormat, convert.toFormat, convert.image));
    return waitForNext();
  }
}




Note how the 
ActorRef
 in the 
Start
 messages are constructed as message adapters to map the 
RequestNext
 and 
Delivery
 to the protocol of the producer and consumer actors respectively.


See also the corresponding 
example that is using ask from the producer
.


Work pulling delivery semantics


For work pulling the order of the messages should not matter, because each message is routed randomly to one of the workers with demand and can therefore be processed in any order.


As long as neither producers nor workers crash (or workers being removed for other reasons) the messages are delivered to the workers without loss or duplicates. Meaning effectively-once processing without any business level deduplication.


Unconfirmed messages may be lost if the producer crashes. To avoid that you need to enable the 
durable queue
 on the producer side. The stored unconfirmed messages will be redelivered when the corresponding producer is started again. Those messages may be routed to different workers than before and some of them may have already been processed but the fact that they were confirmed had not been stored yet. Meaning at-least-once delivery.


If a worker crashes or is stopped gracefully the unconfirmed messages will be redelivered to other workers. In that case some of these may already have been processed by the previous worker. Meaning at-least-once delivery.


Sharding


To use reliable delivery with Cluster Sharding, add the following module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-sharding-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-sharding-typed_${versions.ScalaBinary}"
}


Reliable delivery between a producer actor sending messages to 
sharded
 consumer actor receiving the messages.




and sending to another entity




and sending from another producer (different node)




The 
ShardingProducerController
ShardingProducerController
 should be used together with 
ShardingConsumerController
ShardingConsumerController
.


A producer can send messages via a 
ShardingProducerController
 to any 
ShardingConsumerController
 identified by an 
entityId
. A single 
ShardingProducerController
 per 
ActorSystem
 (node) can be shared for sending to all entities of a certain entity type. No explicit registration is needed between the 
ShardingConsumerController
 and 
ShardingProducerController
.


The producer actor will start the flow by sending a 
ShardingProducerController.Start
 message to the 
ShardingProducerController
.


The 
ShardingProducerController
 sends 
RequestNext
 to the producer, which is then allowed to send one message to the 
ShardingProducerController
. Thereafter the producer will receive a new 
RequestNext
 when it’s allowed to send one more message.


In the 
ShardingProducerController.RequestNext
ShardingProducerController.RequestNext
 message there is information about which entities that have demand. It is allowed to send to a new 
entityId
 that is not included in the 
RequestNext.entitiesWithDemand
. If sending to an entity that doesn’t have demand the message will be buffered. This support for buffering means that it is even allowed to send several messages in response to one 
RequestNext
 but it’s recommended to only send one message and wait for next 
RequestNext
 before sending more messages.


The producer and 
ShardingProducerController
 actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.


Similarly, on the consumer side the destination consumer actor will start the flow by sending an initial 
ConsumerController.Start
 message to the 
ConsumerController
.


There will be one 
ShardingConsumerController
 for each entity. Many unconfirmed messages can be in flight between the 
ShardingProducerController
 and each 
ShardingConsumerController
, but it is limited by a flow control window. The flow control is driven by the consumer side, which means that the 
ShardingProducerController
 will not send faster than the demand requested by the consumers.


Sharding example


The sharded entity is a todo list which uses an async database call to store its entire state on each change, and first when that completes replies to reliable delivery that the message was consumed.


Example of 
TodoList
 entity (consumer):




Scala




copy
source
import scala.concurrent.Future
import scala.concurrent.duration._
import scala.util.Failure
import scala.util.Success

import akka.Done
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.delivery.ConsumerController
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors
import akka.cluster.sharding.typed.delivery.ShardingConsumerController
import akka.util.Timeout

trait DB {
  def save(id: String, value: TodoList.State): Future[Done]
  def load(id: String): Future[TodoList.State]
}

object TodoList {

  sealed trait Command

  final case class AddTask(item: String) extends Command
  final case class CompleteTask(item: String) extends Command

  private final case class InitialState(state: State) extends Command
  private final case class SaveSuccess(confirmTo: ActorRef[ConsumerController.Confirmed]) extends Command
  private final case class DBError(cause: Throwable) extends Command

  private final case class CommandDelivery(command: Command, confirmTo: ActorRef[ConsumerController.Confirmed])
      extends Command

  final case class State(tasks: Vector[String])

  def apply(
      id: String,
      db: DB,
      consumerController: ActorRef[ConsumerController.Start[Command]]): Behavior[Command] = {
    Behaviors.setup[Command] { context =>
      new TodoList(context, id, db).start(consumerController)
    }
  }

}

class TodoList(context: ActorContext[TodoList.Command], id: String, db: DB) {
  import TodoList._

  private def start(consumerController: ActorRef[ConsumerController.Start[Command]]): Behavior[Command] = {
    context.pipeToSelf(db.load(id)) {
      case Success(value) => InitialState(value)
      case Failure(cause) => DBError(cause)
    }

    Behaviors.receiveMessagePartial {
      case InitialState(state) =>
        val deliveryAdapter: ActorRef[ConsumerController.Delivery[Command]] = context.messageAdapter { delivery =>
          CommandDelivery(delivery.message, delivery.confirmTo)
        }
        consumerController ! ConsumerController.Start(deliveryAdapter)
        active(state)
      case DBError(cause) =>
        throw cause
    }
  }

  private def active(state: State): Behavior[Command] = {
    Behaviors.receiveMessagePartial {
      case CommandDelivery(AddTask(item), confirmTo) =>
        val newState = state.copy(tasks = state.tasks :+ item)
        save(newState, confirmTo)
        active(newState)
      case CommandDelivery(CompleteTask(item), confirmTo) =>
        val newState = state.copy(tasks = state.tasks.filterNot(_ == item))
        save(newState, confirmTo)
        active(newState)
      case SaveSuccess(confirmTo) =>
        confirmTo ! ConsumerController.Confirmed
        Behaviors.same
      case DBError(cause) =>
        throw cause
    }
  }

  private def save(newState: State, confirmTo: ActorRef[ConsumerController.Confirmed]): Unit = {
    context.pipeToSelf(db.save(id, newState)) {
      case Success(_)     => SaveSuccess(confirmTo)
      case Failure(cause) => DBError(cause)
    }
  }
}


Java




copy
source
import akka.Done;
import akka.actor.Address;
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.delivery.ConsumerController;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;

import java.time.Duration;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Optional;
import java.util.concurrent.CompletionStage;


interface DB {
  CompletionStage<Done> save(String id, TodoList.State state);

  CompletionStage<TodoList.State> load(String id);
}

public class TodoList {
  interface Command {}

  public static class AddTask implements Command {
    public final String item;

    public AddTask(String item) {
      this.item = item;
    }
  }

  public static class CompleteTask implements Command {
    public final String item;

    public CompleteTask(String item) {
      this.item = item;
    }
  }

  private static class InitialState implements Command {
    final State state;

    private InitialState(State state) {
      this.state = state;
    }
  }

  private static class SaveSuccess implements Command {
    final ActorRef<ConsumerController.Confirmed> confirmTo;

    private SaveSuccess(ActorRef<ConsumerController.Confirmed> confirmTo) {
      this.confirmTo = confirmTo;
    }
  }

  private static class DBError implements Command {
    final Exception cause;

    private DBError(Throwable cause) {
      if (cause instanceof Exception) this.cause = (Exception) cause;
      else this.cause = new RuntimeException(cause.getMessage(), cause);
    }
  }

  private static class CommandDelivery implements Command {
    final Command command;
    final ActorRef<ConsumerController.Confirmed> confirmTo;

    private CommandDelivery(Command command, ActorRef<ConsumerController.Confirmed> confirmTo) {
      this.command = command;
      this.confirmTo = confirmTo;
    }
  }

  public static class State {
    public final List<String> tasks;

    public State(List<String> tasks) {
      this.tasks = Collections.unmodifiableList(tasks);
    }

    public State add(String task) {
      ArrayList<String> copy = new ArrayList<>(tasks);
      copy.add(task);
      return new State(copy);
    }

    public State remove(String task) {
      ArrayList<String> copy = new ArrayList<>(tasks);
      copy.remove(task);
      return new State(copy);
    }
  }

  public static Behavior<Command> create(
      String id, DB db, ActorRef<ConsumerController.Start<Command>> consumerController) {
    return Init.create(id, db, consumerController);
  }

  private static Behavior<Command> onDBError(DBError error) throws Exception {
    throw error.cause;
  }

  static class Init extends AbstractBehavior<Command> {

    private final String id;
    private final DB db;
    private final ActorRef<ConsumerController.Start<Command>> consumerController;

    private Init(
        ActorContext<Command> context,
        String id,
        DB db,
        ActorRef<ConsumerController.Start<Command>> consumerController) {
      super(context);
      this.id = id;
      this.db = db;
      this.consumerController = consumerController;
    }

    static Behavior<Command> create(
        String id, DB db, ActorRef<ConsumerController.Start<Command>> consumerController) {
      return Behaviors.setup(
          context -> {
            context.pipeToSelf(
                db.load(id),
                (state, exc) -> {
                  if (exc == null) return new InitialState(state);
                  else return new DBError(exc);
                });

            return new Init(context, id, db, consumerController);
          });
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(InitialState.class, this::onInitialState)
          .onMessage(DBError.class, TodoList::onDBError)
          .build();
    }

    private Behavior<Command> onInitialState(InitialState initial) {
      ActorRef<ConsumerController.Delivery<Command>> deliveryAdapter =
          getContext()
              .messageAdapter(
                  ConsumerController.deliveryClass(),
                  d -> new CommandDelivery(d.message(), d.confirmTo()));
      consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));

      return Active.create(id, db, initial.state);
    }
  }

  static class Active extends AbstractBehavior<Command> {

    private final String id;
    private final DB db;
    private State state;

    private Active(ActorContext<Command> context, String id, DB db, State state) {
      super(context);
      this.id = id;
      this.db = db;
      this.state = state;
    }

    static Behavior<Command> create(String id, DB db, State state) {
      return Behaviors.setup(context -> new Active(context, id, db, state));
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(CommandDelivery.class, this::onDelivery)
          .onMessage(SaveSuccess.class, this::onSaveSuccess)
          .onMessage(DBError.class, TodoList::onDBError)
          .build();
    }

    private Behavior<Command> onDelivery(CommandDelivery delivery) {
      if (delivery.command instanceof AddTask) {
        AddTask addTask = (AddTask) delivery.command;
        state = state.add(addTask.item);
        save(state, delivery.confirmTo);
        return this;
      } else if (delivery.command instanceof CompleteTask) {
        CompleteTask completeTask = (CompleteTask) delivery.command;
        state = state.remove(completeTask.item);
        save(state, delivery.confirmTo);
        return this;
      } else {
        return Behaviors.unhandled();
      }
    }

    private void save(State newState, ActorRef<ConsumerController.Confirmed> confirmTo) {
      getContext()
          .pipeToSelf(
              db.save(id, newState),
              (state, exc) -> {
                if (exc == null) return new SaveSuccess(confirmTo);
                else return new DBError(exc);
              });
    }

    private Behavior<Command> onSaveSuccess(SaveSuccess success) {
      success.confirmTo.tell(ConsumerController.confirmed());
      return this;
    }
  }
}




and 
TodoService
 (producer):




Scala




copy
source
import akka.cluster.sharding.typed.delivery.ShardingProducerController

object TodoService {
  sealed trait Command

  final case class UpdateTodo(listId: String, item: String, completed: Boolean, replyTo: ActorRef[Response])
      extends Command

  sealed trait Response
  case object Accepted extends Response
  case object Rejected extends Response
  case object MaybeAccepted extends Response

  private final case class WrappedRequestNext(requestNext: ShardingProducerController.RequestNext[TodoList.Command])
      extends Command
  private final case class Confirmed(originalReplyTo: ActorRef[Response]) extends Command
  private final case class TimedOut(originalReplyTo: ActorRef[Response]) extends Command

  def apply(producerController: ActorRef[ShardingProducerController.Command[TodoList.Command]]): Behavior[Command] = {
    Behaviors.setup { context =>
      new TodoService(context).start(producerController)
    }
  }

}

class TodoService(context: ActorContext[TodoService.Command]) {
  import TodoService._

  private implicit val askTimeout: Timeout = 5.seconds

  private def start(
      producerController: ActorRef[ShardingProducerController.Start[TodoList.Command]]): Behavior[Command] = {
    val requestNextAdapter: ActorRef[ShardingProducerController.RequestNext[TodoList.Command]] =
      context.messageAdapter(WrappedRequestNext.apply)
    producerController ! ShardingProducerController.Start(requestNextAdapter)

    Behaviors.receiveMessagePartial {
      case WrappedRequestNext(next) =>
        active(next)
      case UpdateTodo(_, _, _, replyTo) =>
        // not hooked up with shardingProducerController yet
        replyTo ! Rejected
        Behaviors.same
    }
  }

  private def active(requestNext: ShardingProducerController.RequestNext[TodoList.Command]): Behavior[Command] = {
    Behaviors.receiveMessage {
      case WrappedRequestNext(next) =>
        active(next)

      case UpdateTodo(listId, item, completed, replyTo) =>
        if (requestNext.bufferedForEntitiesWithoutDemand.getOrElse(listId, 0) >= 100)
          replyTo ! Rejected
        else {
          val requestMsg = if (completed) TodoList.CompleteTask(item) else TodoList.AddTask(item)
          context.ask[ShardingProducerController.MessageWithConfirmation[TodoList.Command], Done](
            requestNext.askNextTo,
            askReplyTo => ShardingProducerController.MessageWithConfirmation(listId, requestMsg, askReplyTo)) {
            case Success(Done) => Confirmed(replyTo)
            case Failure(_)    => TimedOut(replyTo)
          }
        }
        Behaviors.same

      case Confirmed(originalReplyTo) =>
        originalReplyTo ! Accepted
        Behaviors.same

      case TimedOut(originalReplyTo) =>
        originalReplyTo ! MaybeAccepted
        Behaviors.same
    }
  }

}


Java




copy
source
import akka.cluster.sharding.typed.delivery.ShardingProducerController;

public class TodoService {

  interface Command {}

  public static class UpdateTodo implements Command {
    public final String listId;
    public final String item;
    public final boolean completed;
    public final ActorRef<Response> replyTo;

    public UpdateTodo(String listId, String item, boolean completed, ActorRef<Response> replyTo) {
      this.listId = listId;
      this.item = item;
      this.completed = completed;
      this.replyTo = replyTo;
    }
  }

  public enum Response {
    ACCEPTED,
    REJECTED,
    MAYBE_ACCEPTED
  }

  private static class Confirmed implements Command {
    final ActorRef<Response> originalReplyTo;

    private Confirmed(ActorRef<Response> originalReplyTo) {
      this.originalReplyTo = originalReplyTo;
    }
  }

  private static class TimedOut implements Command {
    final ActorRef<Response> originalReplyTo;

    private TimedOut(ActorRef<Response> originalReplyTo) {
      this.originalReplyTo = originalReplyTo;
    }
  }

  private static class WrappedRequestNext implements Command {
    final ShardingProducerController.RequestNext<TodoList.Command> next;

    private WrappedRequestNext(ShardingProducerController.RequestNext<TodoList.Command> next) {
      this.next = next;
    }
  }

  public static Behavior<Command> create(
      ActorRef<ShardingProducerController.Command<TodoList.Command>> producerController) {
    return Init.create(producerController);
  }

  static class Init extends AbstractBehavior<TodoService.Command> {

    static Behavior<Command> create(
        ActorRef<ShardingProducerController.Command<TodoList.Command>> producerController) {
      return Behaviors.setup(
          context -> {
            ActorRef<ShardingProducerController.RequestNext<TodoList.Command>>
                requestNextAdapter =
                    context.messageAdapter(
                        ShardingProducerController.requestNextClass(), WrappedRequestNext::new);
            producerController.tell(new ShardingProducerController.Start<>(requestNextAdapter));

            return new Init(context);
          });
    }

    private Init(ActorContext<Command> context) {
      super(context);
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(WrappedRequestNext.class, w -> Active.create(w.next))
          .onMessage(
              UpdateTodo.class,
              command -> {
                // not hooked up with shardingProducerController yet
                command.replyTo.tell(Response.REJECTED);
                return this;
              })
          .build();
    }
  }

  static class Active extends AbstractBehavior<TodoService.Command> {

    private ShardingProducerController.RequestNext<TodoList.Command> requestNext;

    static Behavior<Command> create(
        ShardingProducerController.RequestNext<TodoList.Command> requestNext) {
      return Behaviors.setup(context -> new Active(context, requestNext));
    }

    private Active(
        ActorContext<Command> context,
        ShardingProducerController.RequestNext<TodoList.Command> requestNext) {
      super(context);
      this.requestNext = requestNext;
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(WrappedRequestNext.class, this::onRequestNext)
          .onMessage(UpdateTodo.class, this::onUpdateTodo)
          .onMessage(Confirmed.class, this::onConfirmed)
          .onMessage(TimedOut.class, this::onTimedOut)
          .build();
    }

    private Behavior<Command> onRequestNext(WrappedRequestNext w) {
      requestNext = w.next;
      return this;
    }

    private Behavior<Command> onUpdateTodo(UpdateTodo command) {
      Integer buffered = requestNext.getBufferedForEntitiesWithoutDemand().get(command.listId);
      if (buffered != null && buffered >= 100) {
        command.replyTo.tell(Response.REJECTED);
      } else {
        TodoList.Command requestMsg;
        if (command.completed) requestMsg = new TodoList.CompleteTask(command.item);
        else requestMsg = new TodoList.AddTask(command.item);
        getContext()
            .ask(
                Done.class,
                requestNext.askNextTo(),
                Duration.ofSeconds(5),
                askReplyTo ->
                    new ShardingProducerController.MessageWithConfirmation<>(
                        command.listId, requestMsg, askReplyTo),
                (done, exc) -> {
                  if (exc == null) return new Confirmed(command.replyTo);
                  else return new TimedOut(command.replyTo);
                });
      }
      return this;
    }

    private Behavior<Command> onConfirmed(Confirmed confirmed) {
      confirmed.originalReplyTo.tell(Response.ACCEPTED);
      return this;
    }

    private Behavior<Command> onTimedOut(TimedOut timedOut) {
      timedOut.originalReplyTo.tell(Response.MAYBE_ACCEPTED);
      return this;
    }
  }
}




Note how the 
ActorRef
 in the 
Start
 messages are constructed as message adapters to map the 
RequestNext
 and 
Delivery
 to the protocol of the producer and consumer actors respectively.


Those are initialized with sharding like this (from the guardian):




Java




copy
source
import akka.cluster.sharding.typed.delivery.ShardingConsumerController;
import akka.cluster.sharding.typed.ShardingEnvelope;
import akka.cluster.sharding.typed.javadsl.ClusterSharding;
import akka.cluster.sharding.typed.javadsl.Entity;
import akka.cluster.sharding.typed.javadsl.EntityTypeKey;
import akka.cluster.typed.Cluster;
import akka.actor.typed.ActorSystem;

final DB db = theDatabaseImplementation();

ActorSystem<Void> system = context.getSystem();

EntityTypeKey<ConsumerController.SequencedMessage<TodoList.Command>> entityTypeKey =
    EntityTypeKey.create(ShardingConsumerController.entityTypeKeyClass(), "todo");

ActorRef<ShardingEnvelope<ConsumerController.SequencedMessage<TodoList.Command>>> region =
    ClusterSharding.get(system)
        .init(
            Entity.of(
                entityTypeKey,
                entityContext ->
                    ShardingConsumerController.create(
                        start ->
                            TodoList.create(entityContext.getEntityId(), db, start))));

Address selfAddress = Cluster.get(system).selfMember().address();
String producerId = "todo-producer-" + selfAddress.hostPort();

ActorRef<ShardingProducerController.Command<TodoList.Command>> producerController =
    context.spawn(
        ShardingProducerController.create(
            TodoList.Command.class, producerId, region, Optional.empty()),
        "producerController");

context.spawn(TodoService.create(producerController), "producer");




Sharding delivery semantics


As long as neither producer nor consumer crash the messages are delivered to the consumer actor in the same order as they were sent to the 
ShardingProducerController
, without loss or duplicates. Meaning effectively-once processing without any business level deduplication.


Unconfirmed messages may be lost if the producer crashes. To avoid that you need to enable the 
durable queue
 on the producer side. The stored unconfirmed messages will be redelivered when the corresponding producer is started again. In that case there may be delivery of messages that had already been processed but the fact that they were confirmed had not been stored yet. Meaning at-least-once delivery.


If the consumer crashes or the shard is rebalanced the unconfirmed messages will be redelivered. In that case some of these may already have been processed by the previous consumer.


Durable producer


Until sent messages have been confirmed the producer side keeps them in memory to be able to resend them. If the JVM of the producer side crashes those unconfirmed messages are lost. To make sure the messages can be delivered also in that scenario a 
DurableProducerQueue
DurableProducerQueue
 can be used. Then the unconfirmed messages are stored in a durable way so that they can be redelivered when the producer is started again. An implementation of the 
DurableProducerQueue
 is provided by 
EventSourcedProducerQueue
EventSourcedProducerQueue
 in 
akka-persistence-typed
.


Be aware of that a 
DurableProducerQueue
 will add a substantial performance overhead. 


When using the 
EventSourcedProducerQueue
 the following dependency is needed:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}"
}


You also have to select journal plugin and snapshot store plugin, see 
Persistence Plugins
.


Example of the image converter work manager from the 
Work pulling example
 with 
EventSourcedProducerQueue
 enabled:




Scala




copy
source
import akka.persistence.typed.delivery.EventSourcedProducerQueue
import akka.persistence.typed.PersistenceId

val durableQueue =
  EventSourcedProducerQueue[ImageConverter.ConversionJob](PersistenceId.ofUniqueId("ImageWorkManager"))
val durableProducerController = context.spawn(
  WorkPullingProducerController(
    producerId = "workManager",
    workerServiceKey = ImageConverter.serviceKey,
    durableQueueBehavior = Some(durableQueue)),
  "producerController")


Java




copy
source
import akka.persistence.typed.PersistenceId;
import akka.persistence.typed.delivery.EventSourcedProducerQueue;

Behavior<DurableProducerQueue.Command<ImageConverter.ConversionJob>> durableQueue =
    EventSourcedProducerQueue.create(PersistenceId.ofUniqueId("ImageWorkManager"));
ActorRef<WorkPullingProducerController.Command<ImageConverter.ConversionJob>>
    durableProducerController =
        context.spawn(
            WorkPullingProducerController.create(
                ImageConverter.ConversionJob.class,
                "workManager",
                ImageConverter.serviceKey,
                Optional.of(durableQueue)),
            "producerController");




It’s important to note that the 
EventSourcedProducerQueue
 requires a 
PersistenceId
, which must be unique. The same 
PersistenceId
 must not be used for different producers at the same time. A 
Cluster Singleton
 hosting the producer would satisfy that requirement, or one producer per node and a naming scheme to ensure that different nodes use different 
PersistenceId
. 


To deliver unconfirmed messages after a crash the producer must be started again with same 
PersistenceId
 as before the crash.


Ask from the producer


Instead of using 
tell
 with the 
sendNextTo
 in the 
RequestNext
 the producer can use 
context.ask
 with the 
askNextTo
 in the 
RequestNext
. The difference is that a reply is sent back when the message has been handled. To include the 
replyTo
 
ActorRef
 the message must be wrapped in a 
MessageWithConfirmation
. If a 
DurableProducerQueue
 is used then the reply is sent when the message has been stored successfully, but it might not have been processed by the consumer yet. Otherwise the reply is sent after the consumer has processed and confirmed the message.


Example of using 
ask
 in the image converter work manager from the 
Work pulling example
:




Scala




copy
source
  final case class ConvertRequest(
      fromFormat: String,
      toFormat: String,
      image: Array[Byte],
      replyTo: ActorRef[ConvertResponse])
      extends Command

  sealed trait ConvertResponse
  final case class ConvertAccepted(resultId: UUID) extends ConvertResponse
  case object ConvertRejected extends ConvertResponse
  final case class ConvertTimedOut(resultId: UUID) extends ConvertResponse

  private final case class AskReply(resultId: UUID, originalReplyTo: ActorRef[ConvertResponse], timeout: Boolean)
      extends Command

final class ImageWorkManager(
    context: ActorContext[ImageWorkManager.Command],
    stashBuffer: StashBuffer[ImageWorkManager.Command]) {

    import WorkPullingProducerController.MessageWithConfirmation
    import akka.util.Timeout

    implicit val askTimeout: Timeout = 5.seconds

    private def waitForNext(): Behavior[Command] = {
      Behaviors.receiveMessagePartial {
        case WrappedRequestNext(next) =>
          stashBuffer.unstashAll(active(next))
        case c: ConvertRequest =>
          if (stashBuffer.isFull) {
            c.replyTo ! ConvertRejected
            Behaviors.same
          } else {
            stashBuffer.stash(c)
            Behaviors.same
          }
        case AskReply(resultId, originalReplyTo, timeout) =>
          val response = if (timeout) ConvertTimedOut(resultId) else ConvertAccepted(resultId)
          originalReplyTo ! response
          Behaviors.same
        case GetResult(resultId, replyTo) =>
          // TODO retrieve the stored result and reply
          Behaviors.same
      }
    }

    private def active(
        next: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]): Behavior[Command] = {
      Behaviors.receiveMessagePartial {
        case ConvertRequest(from, to, image, originalReplyTo) =>
          val resultId = UUID.randomUUID()
          context.ask[MessageWithConfirmation[ImageConverter.ConversionJob], Done](
            next.askNextTo,
            askReplyTo =>
              MessageWithConfirmation(ImageConverter.ConversionJob(resultId, from, to, image), askReplyTo)) {
            case Success(done) => AskReply(resultId, originalReplyTo, timeout = false)
            case Failure(_)    => AskReply(resultId, originalReplyTo, timeout = true)
          }
          waitForNext()
        case AskReply(resultId, originalReplyTo, timeout) =>
          val response = if (timeout) ConvertTimedOut(resultId) else ConvertAccepted(resultId)
          originalReplyTo ! response
          Behaviors.same
        case GetResult(resultId, replyTo) =>
          // TODO retrieve the stored result and reply
          Behaviors.same
        case _: WrappedRequestNext =>
          throw new IllegalStateException("Unexpected RequestNext")
      }
    }

}


Java




copy
source
public static class ConvertRequest implements Command {
  public final String fromFormat;
  public final String toFormat;
  public final byte[] image;
  public final ActorRef<ConvertResponse> replyTo;

  public ConvertRequest(
      String fromFormat, String toFormat, byte[] image, ActorRef<ConvertResponse> replyTo) {
    this.fromFormat = fromFormat;
    this.toFormat = toFormat;
    this.image = image;
    this.replyTo = replyTo;
  }
}

interface ConvertResponse {}

public static class ConvertAccepted implements ConvertResponse {
  public final UUID resultId;

  public ConvertAccepted(UUID resultId) {
    this.resultId = resultId;
  }
}

enum ConvertRejected implements ConvertResponse {
  INSTANCE
}

public static class ConvertTimedOut implements ConvertResponse {
  public final UUID resultId;

  public ConvertTimedOut(UUID resultId) {
    this.resultId = resultId;
  }
}

private static class AskReply implements Command {
  final UUID resultId;
  final ActorRef<ConvertResponse> originalReplyTo;
  final boolean timeout;

  private AskReply(UUID resultId, ActorRef<ConvertResponse> originalReplyTo, boolean timeout) {
    this.resultId = resultId;
    this.originalReplyTo = originalReplyTo;
    this.timeout = timeout;
  }
}

      private Behavior<Command> waitForNext() {
        return Behaviors.receive(Command.class)
            .onMessage(WrappedRequestNext.class, this::onWrappedRequestNext)
            .onMessage(ConvertRequest.class, this::onConvertRequestWait)
            .onMessage(AskReply.class, this::onAskReply)
            .onMessage(GetResult.class, this::onGetResult)
            .build();
      }

      private Behavior<Command> onConvertRequestWait(ConvertRequest convert) {
        if (stashBuffer.isFull()) {
          convert.replyTo.tell(ConvertRejected.INSTANCE);
          return Behaviors.same();
        } else {
          stashBuffer.stash(convert);
          return Behaviors.same();
        }
      }

      private Behavior<Command> onAskReply(AskReply reply) {
        if (reply.timeout) reply.originalReplyTo.tell(new ConvertTimedOut(reply.resultId));
        else reply.originalReplyTo.tell(new ConvertAccepted(reply.resultId));
        return Behaviors.same();
      }

      private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {
        return stashBuffer.unstashAll(active(w.next));
      }

      private Behavior<Command> onGetResult(GetResult get) {
        // TODO retrieve the stored result and reply
        return Behaviors.same();
      }

      private Behavior<Command> active(
          WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {
        return Behaviors.receive(Command.class)
            .onMessage(ConvertRequest.class, c -> onConvertRequest(c, next))
            .onMessage(AskReply.class, this::onAskReply)
            .onMessage(GetResult.class, this::onGetResult)
            .onMessage(WrappedRequestNext.class, this::onUnexpectedWrappedRequestNext)
            .build();
      }

      private Behavior<Command> onConvertRequest(
          ConvertRequest convert,
          WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {
        UUID resultId = UUID.randomUUID();

        context.ask(
            Done.class,
            next.askNextTo(),
            Duration.ofSeconds(5),
            askReplyTo ->
                new WorkPullingProducerController.MessageWithConfirmation<>(
                    new ImageConverter.ConversionJob(
                        resultId, convert.fromFormat, convert.toFormat, convert.image),
                    askReplyTo),
            (done, exc) -> {
              if (exc == null) return new AskReply(resultId, convert.replyTo, false);
              else return new AskReply(resultId, convert.replyTo, true);
            });

        return waitForNext();
      }

      private Behavior<Command> onUnexpectedWrappedRequestNext(WrappedRequestNext w) {
        throw new IllegalStateException("Unexpected RequestNext");
      }





Only flow control


It’s possible to use this without resending lost messages, but the flow control is still used. This can for example be useful when both consumer and producer are know to be located in the same local 
ActorSystem
. This can be more efficient since messages don’t have to be kept in memory in the 
ProducerController
 until they have been confirmed, but the drawback is that lost messages will not be delivered. See configuration 
only-flow-control
 of the 
ConsumerController
.


Chunk large messages


To avoid head of line blocking from serialization and transfer of large messages the 
Point-to-Point
 pattern has support for automatically 
splitting up large messages
 and assemble them again on the consumer side.


Serialization and deserialization is performed by the 
ProducerController
 and 
ConsumerController
 respectively instead of in the remote transport layer.


This is enabled by configuration 
akka.reliable-delivery.producer-controller.chunk-large-messages
 and defines the maximum size in bytes of the chunked pieces. Messages smaller than the configured size are not chunked, but serialization still takes place in the 
ProducerController
 and 
ConsumerController
. 


Aside from the configuration the API is the same as the 
Point-to-point
 pattern. If 
Durable producer
 is enabled the chunked pieces are stored rather than the full large message.


This feature is not implemented for 
Work pulling
 and 
Sharding
 yet.


Configuration


There are several configuration properties, please refer to 
akka.reliable-delivery
 config section in the reference configuration:




akka-actor-typed reference configuration


akka-persistence-typed reference configuration


akka-cluster-sharding-typed reference configuration
















 
Distributed Publish Subscribe in Cluster






Serialization 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/stream/stream-configuration.html
Configuration • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Configuration


copy
source
#####################################
# Akka Stream Reference Config File #
#####################################

# eager creation of the system wide materializer
akka.library-extensions += "akka.stream.SystemMaterializer$"
akka {
  stream {

    # Default materializer settings
    materializer {

      # Initial size of buffers used in stream elements
      initial-input-buffer-size = 4
      # Maximum size of buffers used in stream elements
      max-input-buffer-size = 16

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by ActorMaterializer when creating Actors.
      dispatcher = "akka.actor.default-dispatcher"

      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      # defaults to the single consumer mailbox for better performance.
      mailbox {
        mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
      }

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by stream operators that
      # perform blocking operations
      blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # Cleanup leaked publishers and subscribers when they are not used within a given
      # deadline
      subscription-timeout {
        # when the subscription timeout is reached one of the following strategies on
        # the "stale" publisher:
        # cancel - cancel it (via `onError` or subscribing to the publisher and
        #          `cancel()`ing the subscription right away
        # warn   - log a warning statement about the stale element (then drop the
        #          reference to it)
        # noop   - do nothing (not recommended)
        mode = cancel

        # time after which a subscriber / publisher is considered stale and eligible
        # for cancelation (see `akka.stream.subscription-timeout.mode`)
        timeout = 5s
      }

      # Enable additional troubleshooting logging at DEBUG log level
      debug-logging = off

      # Log any stream stage error at the specified log level: "error", "warning", "info", "debug" or "off".
      # If there is an `akka.stream.Attributes.LogLevels` attribute defined for a specific stream this value is ignored
      # and the `onFailure` value of the attribute is applied instead.
      stage-errors-default-log-level = error

      # Maximum number of elements emitted in batch if downstream signals large demand
      output-burst-limit = 1000

      # Enable automatic fusing of all graphs that are run. For short-lived streams
      # this may cause an initial runtime overhead, but most of the time fusing is
      # desirable since it reduces the number of Actors that are created.
      # Deprecated, since Akka 2.5.0, setting does not have any effect.
      auto-fusing = on

      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
      # buffer upon stream materialization if the requested buffer size is less than this
      # configuration parameter. The default is very high because failing early is better
      # than failing under load.
      #
      # Buffers sized larger than this will dynamically grow/shrink and consume more memory
      # per element than the fixed size buffers.
      max-fixed-buffer-size = 1000000000

      # Maximum number of sync messages that actor can process for stream to substream communication.
      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.
      # Allows to accelerate message processing that happening within same actor but keep system responsive.
      sync-processing-limit = 1000

      debug {
        # Enables the fuzzing mode which increases the chance of race conditions
        # by aggressively reordering events and making certain operations more
        # concurrent than usual.
        # This setting is for testing purposes, NEVER enable this in a production
        # environment!
        # To get the best results, try combining this setting with a throughput
        # of 1 on the corresponding dispatchers.
        fuzzing-mode = off
      }

      io.tcp {
        # The outgoing bytes are accumulated in a buffer while waiting for acknowledgment
        # of pending write. This improves throughput for small messages (frames) without
        # sacrificing latency. While waiting for the ack the stage will eagerly pull
        # from upstream until the buffer exceeds this size. That means that the buffer may hold
        # slightly more bytes than this limit (at most one element more). It can be set to 0
        # to disable the usage of the buffer.
        write-buffer-size = 16 KiB

        # In addition to the buffering described for property write-buffer-size, try to collect
        # more consecutive writes from the upstream stream producers.
        #
        # The rationale is to increase write efficiency by avoiding separate small 
        # writes to the network which is expensive to do. Merging those writes together
        # (up to `write-buffer-size`) improves throughput for small writes.
        #
        # The idea is that a running stream may produce multiple small writes consecutively
        # in one go without waiting for any external input. To probe the stream for
        # data, this features delays sending a write immediately by probing the stream
        # for more writes. This works by rescheduling the TCP connection stage via the
        # actor mailbox of the underlying actor. Thus, before the stage is reactivated
        # the upstream gets another opportunity to emit writes.
        #
        # When the stage is reactivated and if new writes are detected another round-trip
        # is scheduled. The loop repeats until either the number of round trips given in this
        # setting is reached, the buffer reaches `write-buffer-size`, or no new writes
        # were detected during the last round-trip.
        #
        # This mechanism ensures that a write is guaranteed to be sent when the remaining stream
        # becomes idle waiting for external signals.
        #
        # In most cases, the extra latency this mechanism introduces should be negligible,
        # but depending on the stream setup it may introduce a noticeable delay,
        # if the upstream continuously produces small amounts of writes in a
        # blocking (CPU-bound) way.
        #
        # In that case, the feature can either be disabled, or the producing CPU-bound
        # work can be taken off-stream to avoid excessive delays (e.g. using `mapAsync` instead of `map`).
        #
        # A value of 0 disables this feature.
        coalesce-writes = 10
      }

      # Time to wait for async materializer creation before throwing an exception
      creation-timeout = 20 seconds

      //#stream-ref
      # configure defaults for SourceRef and SinkRef
      stream-ref {
        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref
        #
        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
        # because the delay of requesting over network boundaries is much higher.
        buffer-capacity = 32

        # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
        #
        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
        #
        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
        demand-redelivery-interval = 1 second

        # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
        # This timeout does not have to be very low in normal situations, since the remote side may also need to
        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
        # in-active streams which are never subscribed to.
        subscription-timeout = 30 seconds

        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed
        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.
        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the
        # other side of the stream ref would never send the "final" terminal message.
        #
        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef
        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.
        final-termination-signal-deadline = 2 seconds
      }
      //#stream-ref
    }

    # Deprecated, left here to not break Akka HTTP which refers to it
    blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

    # Deprecated, will not be used unless user code refer to it, use 'akka.stream.materializer.blocking-io-dispatcher'
    # instead, or if from code, prefer the 'ActorAttributes.IODispatcher' attribute
    default-blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"
  }

  # configure overrides to ssl-configuration here (to be used by akka-streams, and akka-http â i.e. when serving https connections)
  ssl-config {
    protocol = "TLSv1.2"
  }

  actor {

    serializers {
      akka-stream-ref = "akka.stream.serialization.StreamRefSerializer"
    }

    serialization-bindings {
      "akka.stream.SinkRef"                           = akka-stream-ref
      "akka.stream.SourceRef"                         = akka-stream-ref
      "akka.stream.impl.streamref.StreamRefsProtocol" = akka-stream-ref
    }

    serialization-identifiers {
      "akka.stream.serialization.StreamRefSerializer" = 30
    }
  }
}

# ssl configuration
# folded in from former ssl-config-akka module
ssl-config {
  logger = "com.typesafe.sslconfig.akka.util.AkkaLoggerBridge"
}














 
Streams Cookbook






Operators 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/deploy.html
Deploying • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying




Deploying to Kubernetes


Deploying to Docker containers




Rolling Updates


Building Native Images




Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying




Deploying to Kubernetes


Deploying to Docker containers




Rolling Updates


Building Native Images




Project Information


Akka Classic




















Deploying


Deploying to Kubernetes


You can deploy to Kubernetes according to the guide and example project for 
Deploying Akka Cluster to Kubernetes
.


Cluster bootstrap


To take advantage of running inside Kubernetes while forming a cluster, 
Akka Cluster Bootstrap
 helps forming or joining a cluster using Akka Discovery to discover peer nodes. with the Kubernetes API or Kubernetes via DNS. 


Rolling updates


Enable the 
Kubernetes Rolling Updates
 and 
app-version from Deployment
 features from Akka Management for smooth rolling updates.


Resource management


CPU requests and limits


To avoid CFS scheduler limits, it is best not to use 
resources.limits.cpu
 limits, but use 
resources.requests.cpu
 configuration instead.
Note


Some resource sizing for Akka and other library dependencies, such as for thread pools or direct memory pools, is based on the detected number of available processors. This will be the CPU limit, if configured, or otherwise all available CPU on the underlying Kubernetes node. While it’s recommended to not set a CPU limit, this can lead to over-sized resource allocation. The available processors detected by the JVM can be configured directly using the 
-XX:ActiveProcessorCount
 option.




Example
: Akka applications are being deployed to Kubernetes on 16 CPU nodes. Workloads are variable, so to schedule several pods on each node, a CPU request of 2 is being used. No CPU limit is set, so that pods can burst to more CPU usage as needed and when available. 
-XX:ActiveProcessorCount=4
 is added to the JVM options so that thread pools are sized appropriately for 4 CPU — rather than the full 16 CPU as detected automatically, and more than the 2 CPU request, for when the application is active and able to use more resources.


Memory requests and limits


For memory, it’s recommended to set both 
resources.requests.memory
 and 
resources.limits.memory
 to the same value. The 
-XX:InitialRAMPercentage
 and 
-XX:MaxRAMPercentage
 JVM options can be used to set the heap size relative to the memory limit.
Note


To account for non-heap memory areas (such as class metadata, threads, code cache, symbols, garbage collection, and direct memory), it’s recommended to set the heap percentage to 
70%
 of the available memory. This may need to be a smaller percentage for lower memory limits, or can be a higher percentage for higher memory limits.


Deploying to Docker containers


You can use both Akka remoting and Akka Cluster inside Docker containers. Note that you will need to take special care with the network configuration when using Docker, described here: 
Akka behind NAT or in a Docker container


For the JVM to run well in a Docker container, there are some general (not Akka specific) parameters that might need tuning:


Resource constraints


Docker allows 
constraining each containers’ resource usage
.


Memory


Any memory limits for the Docker container are detected automatically by current JVMs by default. The 
-XX:InitialRAMPercentage
 and 
-XX:MaxRAMPercentage
 JVM options can be used to set the heap size relative to the memory limit.
Note


To account for non-heap memory areas (such as class metadata, threads, code cache, symbols, garbage collection, and direct memory), it’s recommended to set the heap percentage to 
70%
 of the available memory. This may need to be a smaller percentage for lower memory limits, or can be a higher percentage for higher memory limits.


CPU


For multithreaded applications such as the JVM, the CFS scheduler limits are an ill fit, because they will restrict the allowed CPU usage even when more CPU cycles are available from the host system. This means your application may be starved of CPU time, but your system appears idle.


For this reason, it is best to avoid 
--cpus
 and 
--cpu-quota
 entirely, and instead specify relative container weights using 
--cpu-shares
 instead.
Note


Some resource sizing for Akka and other library dependencies, such as for thread pools or direct memory pools, is based on the detected number of available processors. This will be the CPU quota, if configured, or otherwise all CPU available to Docker. While it’s recommended to not set a CPU quota, this can lead to over-sized resource allocation. The available processors detected by the JVM can be configured directly using the 
-XX:ActiveProcessorCount
 option.














 
Operating a Cluster






Rolling Updates 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-substream.html
Substreams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams




Dependency


Introduction


Nesting operators


Flattening operators




Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams




Dependency


Introduction


Nesting operators


Flattening operators




Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Substreams


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


Substreams are represented as 
SubSource
 or
 
SubFlow
SubFlow
 instances, on which you can multiplex a single 
Source
Source
 or
 
Flow
Flow
 into a stream of streams.


SubFlows cannot contribute to the super-flowâs materialized value since they are materialized later, during the runtime of the stream processing.


operators that create substreams are listed on 
Nesting and flattening operators


Nesting operators


groupBy


A typical operation that generates substreams is 
groupBy
groupBy
.




Scala




copy
source
val source = Source(1 to 10).groupBy(3, _ % 3)


Java




copy
source
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).groupBy(3, elem -> elem % 3);






This operation splits the incoming stream into separate output streams, one for each element key. The key is computed for each element using the given function, which is 
f
 in the above diagram. When a new key is encountered for the first time a new substream is opened and subsequently fed with all elements belonging to that key. If 
allowClosedSubstreamRecreation
 is set to 
true
 a substream belonging to a specific key will be recreated if it was closed before, otherwise elements belonging to that key will be dropped.


If you add a 
Sink
Sink
 or 
Flow
Flow
 right after the 
groupBy
 operator, all transformations are applied to all encountered substreams in the same fashion. So, if you add the following 
Sink
, that is added to each of the substreams as in the below diagram.




Scala




copy
source
Source(1 to 10).groupBy(3, _ % 3).to(Sink.ignore).run()


Java




copy
source
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
    .groupBy(3, elem -> elem % 3)
    .to(Sink.ignore())
    .run(system);






Also substreams, more precisely, 
SubFlow
SubFlow
 
and 
SubSource
 have methods that allow you to merge or concat substreams into the main stream again.


The 
mergeSubstreams
mergeSubstreams
 method merges an unbounded number of substreams back to the main stream.




Scala




copy
source
Source(1 to 10).groupBy(3, _ % 3).mergeSubstreams.runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
    .groupBy(3, elem -> elem % 3)
    .mergeSubstreams()
    .runWith(Sink.ignore(), system);






You can limit the number of active substreams running and being merged at a time, with either the 
mergeSubstreamsWithParallelism
mergeSubstreamsWithParallelism
 or 
concatSubstreams
concatSubstreams
 method.




Scala




copy
source
Source(1 to 10).groupBy(3, _ % 3).mergeSubstreamsWithParallelism(2).runWith(Sink.ignore)

//concatSubstreams is equivalent to mergeSubstreamsWithParallelism(1)
Source(1 to 10).groupBy(3, _ % 3).concatSubstreams.runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
    .groupBy(3, elem -> elem % 3)
    .mergeSubstreamsWithParallelism(2)
    .runWith(Sink.ignore(), system);
// concatSubstreams is equivalent to mergeSubstreamsWithParallelism(1)
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
    .groupBy(3, elem -> elem % 3)
    .concatSubstreams()
    .runWith(Sink.ignore(), system);




However, since the number of running (i.e. not yet completed) substreams is capped, be careful so that these methods do not cause deadlocks with back pressure like in the below diagram.


Element one and two leads to two created substreams, but since the number of substreams are capped to 2 when element 3 comes in it cannot lead to creation of a new substream until one of the previous two are completed and this leads to the stream being deadlocked.




splitWhen and splitAfter


splitWhen
splitWhen
 and 
splitAfter
splitAfter
 are two other operations which generate substreams.


The difference from 
groupBy
groupBy
 is that, if the predicate for 
splitWhen
 and 
splitAfter
 returns true, a new substream is generated, and the succeeding elements after split will flow into the new substream.


splitWhen
 flows the element on which the predicate returned true to a new substream,  whereas 
splitAfter
 flows the next element to the new substream after the element on which predicate returned true.




Scala




copy
source
Source(1 to 10).splitWhen(SubstreamCancelStrategy.drain)(_ == 3)

Source(1 to 10).splitAfter(SubstreamCancelStrategy.drain)(_ == 3)


Java




copy
source
Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).splitWhen(elem -> elem == 3);

Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).splitAfter(elem -> elem == 3);




These are useful when you scanned over something and you don’t need to care about anything behind it. A typical example is counting the number of characters for each line like below.




Scala




copy
source
val text =
  "This is the first line.\n" +
  "The second line.\n" +
  "There is also the 3rd line\n"

val charCount = Source(text.toList)
  .splitAfter { _ == '\n' }
  .filter(_ != '\n')
  .map(_ => 1)
  .reduce(_ + _)
  .to(Sink.foreach(println))
  .run()


Java




copy
source
String text =
    "This is the first line.\n" + "The second line.\n" + "There is also the 3rd line\n";

Source.from(Arrays.asList(text.split("")))
    .map(x -> x.charAt(0))
    .splitAfter(x -> x == '\n')
    .filter(x -> x != '\n')
    .map(x -> 1)
    .reduce((x, y) -> x + y)
    .to(Sink.foreach(x -> System.out.println(x)))
    .run(system);




This prints out the following output.


23
16
26





Flattening operators


flatMapConcat


flatMapConcat
flatMapConcat
 and 
flatMapMerge
flatMapMerge
 are substream operations different from 
groupBy
groupBy
 and 
splitWhen/After
.


flatMapConcat
 takes a function, which is 
f
 in the following diagram. The function 
f
 of 
flatMapConcat
 transforms each input element into a 
Source
Source
 that is then flattened into the output stream by concatenation.




Scala




copy
source
Source(1 to 2).flatMapConcat(i => Source(List.fill(3)(i))).runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(1, 2))
    .flatMapConcat(i -> Source.from(Arrays.asList(i, i, i)))
    .runWith(Sink.ignore(), system);






Like the 
concat
 operation on 
Flow
Flow
, it fully consumes one 
Source
Source
 after the other. So, there is only one substream actively running at a given time.


Then once the active substream is fully consumed, the next substream can start running. Elements from all the substreams are concatenated to the sink.




flatMapMerge


flatMapMerge
 is similar to 
flatMapConcat
, but it doesn’t wait for one 
Source
 to be fully consumed.  Instead, up to 
breadth
 number of streams emit elements at any given time.




Scala




copy
source
Source(1 to 2).flatMapMerge(2, i => Source(List.fill(3)(i))).runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(1, 2))
    .flatMapMerge(2, i -> Source.from(Arrays.asList(i, i, i)))
    .runWith(Sink.ignore(), system);


















 
Testing streams






Streams Cookbook 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-classic.html
Akka Classic • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities




















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities






















Akka Classic
Note


Akka Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Akka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see 
coexistence
. For new projects we recommend using 
the new Actor API
.






Classic Actors




Dependency


Classic Actors


Classic Supervision


Classic Fault Tolerance


Classic Dispatchers


Classic Mailboxes


Classic Routing


Classic FSM


Classic Persistence


Testing Classic Actors




Classic Clustering




Classic Cluster Usage


Classic Cluster Aware Routers


Classic Cluster Singleton


Classic Distributed Publish Subscribe in Cluster


Classic Cluster Sharding


Classic Cluster Metrics Extension


Classic Distributed Data


Classic Serialization




Classic Networking




I/O


Using TCP


Using UDP


DNS Extension




Classic Utilities




Dependency


Classic Event Bus


Classic Logging


Classic Scheduler


Classic Akka Extensions




















 
Project






Classic Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/index.html#receiving-security-advisories
Security Announcements • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Security Announcements
Note


Security announcements has moved to a shared page for all Akka projects and can now be found at 
akka.io/security


Receiving Security Advisories


The best way to receive any and all security announcements is to subscribe to the 
Akka security list
.


The mailing list is very low traffic, and receives notifications only after security reports have been managed by the core team and fixes are publicly available.


Reporting Vulnerabilities


We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.


Following best practice, we strongly encourage anyone to report potential security vulnerabilities to 
[email protected]
 before disclosing them in a public forum like the mailing list or as a GitHub issue.


Reports to this email address will be handled by our security team, who will work together with you to ensure that a fix can be provided without delay.


Security Related Documentation




Java Serialization


Remote deployment allow list


Remote Security




Fixed Security Vulnerabilities






Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


















 
Akka Documentation






Java Serialization, Fixed in Akka 2.4.17 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/replicated-eventsourcing-db-transport.html
Replicated Event Sourcing replication via direct access to replica databases • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases




Sharded Replicated Event Sourced entities


Direct Replication of Events


Hot Standby


Examples


Journal Support




Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases




Sharded Replicated Event Sourced entities


Direct Replication of Events


Hot Standby


Examples


Journal Support




Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Replicated Event Sourcing replication via direct access to replica databases
Note


Since Akka 2.8.0 a gRPC based transport is the recommended way to set up the replication of events between the replicas.


It is possible to consume events with a direct connection to the database backing each replica. Such a setup is generally harder to set up and secure, and is less feasible unless the replication is over a private network.


To enable an entity for Replicated Event Sourcing 
let it extend 
ReplicatedEventSourcedBehavior
 instead of 
EventSourcedBehavior
 and
 use the factory methods on 
akka.persistence.typed.scaladsl.ReplicatedEventSourcing
akka.persistence.typed.javadsl.ReplicatedEventSourcing
. 


All replicas need to be known up front:




Scala




copy
source
val DCA = ReplicaId("DC-A")
val DCB = ReplicaId("DC-B")
val AllReplicas = Set(DCA, DCB)


Java




copy
source
public static final ReplicaId DCA = new ReplicaId("DCA");
public static final ReplicaId DCB = new ReplicaId("DCB");

public static final Set<ReplicaId> ALL_REPLICAS =
    Collections.unmodifiableSet(new HashSet<>(Arrays.asList(DCA, DCB)));




Then to enable replication create the event sourced behavior with the factory method:




Scala




copy
source
def apply(
    system: ActorSystem[_],
    entityId: String,
    replicaId: ReplicaId): EventSourcedBehavior[Command, State, Event] = {
  ReplicatedEventSourcing.perReplicaJournalConfig(
    ReplicationId("MyReplicatedEntity", entityId, replicaId),
    Map(DCA -> "journalForDCA", DCB -> "journalForDCB")) { replicationContext =>
    EventSourcedBehavior[Command, State, Event](???, ???, ???, ???)
  }
}



Java




copy
source
public class MyReplicatedBehavior
    extends ReplicatedEventSourcedBehavior<
        MyReplicatedBehavior.Command, MyReplicatedBehavior.Event, MyReplicatedBehavior.State> {
  public static Behavior<Command> create(String entityId, ReplicaId replicaId) {
    Map<ReplicaId, String> allReplicasAndQueryPlugins = new HashMap<>();
    allReplicasAndQueryPlugins.put(DCA, "journalForDCA");
    allReplicasAndQueryPlugins.put(DCB, "journalForDCB");

    return ReplicatedEventSourcing.perReplicaJournalConfig(
        new ReplicationId("MyReplicatedEntity", entityId, replicaId),
        allReplicasAndQueryPlugins,
        MyReplicatedBehavior::new);
  }

  private MyReplicatedBehavior(ReplicationContext replicationContext) {
    super(replicationContext);
  }




The factory takes in:




entityId
: this will be used as part of the underlying persistenceId


replicaId
: Which replica this instance is


allReplicasAndQueryPlugins
: All Replicas and the query plugin used to read their events


A factory function to create an instance of the 
EventSourcedBehavior
ReplicatedEventSourcedBehavior




In this scenario each replica reads from each other’s database effectively providing cross region replication for any database that has an Akka Persistence plugin. Alternatively if all the replicas use the same journal, e.g. for testing or if it is a distributed database such as Cassandra, the 
withSharedJournal
 factory can be used. 




Scala




copy
source
def apply(
    system: ActorSystem[_],
    entityId: String,
    replicaId: ReplicaId): EventSourcedBehavior[Command, State, Event] = {
  ReplicatedEventSourcing.commonJournalConfig(
    ReplicationId("MyReplicatedEntity", entityId, replicaId),
    AllReplicas,
    queryPluginId) { replicationContext =>
    EventSourcedBehavior[Command, State, Event](???, ???, ???, ???)
  }
}


Java




copy
source
public static Behavior<Command> create(
    String entityId, ReplicaId replicaId, String queryPluginId) {
  return ReplicatedEventSourcing.commonJournalConfig(
      new ReplicationId("MyReplicatedEntity", entityId, replicaId),
      ALL_REPLICAS,
      queryPluginId,
      MyReplicatedBehavior::new);
}




The function passed to both factory methods return an 
EventSourcedBehavior
 and provide access to the 
ReplicationContext
ReplicationContext
 that has the following methods:




entityId


replicaId


allReplicas


persistenceId
 - to provide to the 
EventSourcedBehavior
 factory. This 
must be used
.




As well as methods that 
can only be
 used in the event handler. The values these methods return relate to the event that is being processed.


The function passed to both factory methods is invoked with a special 
ReplicationContext
ReplicationContext
 that needs to be passed to the concrete 
ReplicatedEventSourcedBehavior
 and on to the super constructor.


The context gives access to: 




entityId


replicaId


allReplicas


persistenceId




As well as methods that 
can only be
 used in the event handler, accessed through 
getReplicationContext
. The values these methods return relate to the event that is being processed.




origin
: The ReplicaId that originally created the event


concurrent
: Whether the event was concurrent with another event as in the second diagram above


recoveryRunning
: Whether a recovery is running. Can be used to send commands back to self for side effects that should only happen once.


currentTimeMillis
: similar to 
System.currentTimeMillis
 but guaranteed never to go backwards




The factory returns a 
Behavior
 that can be spawned like any other behavior.


Sharded Replicated Event Sourced entities


There are two ways to integrate replicated event sourced entities with sharding:




Ensure that each replica has a unique entity id by using the replica id as part of the entity id


Use roles to run a full copy of sharding per replica




To simplify, the 
ReplicatedShardingExtension
ReplicatedShardingExtension
 is available from the 
akka-cluster-sharding-typed
 module.




Scala




copy
source
ReplicatedEntityProvider[Command]("MyEntityType", Set(ReplicaId("DC-A"), ReplicaId("DC-B"))) {
  (entityTypeKey, replicaId) =>
    ReplicatedEntity(replicaId, Entity(entityTypeKey) { entityContext =>
      // the sharding entity id contains the business entityId, entityType, and replica id
      // which you'll need to create a ReplicatedEventSourcedBehavior
      val replicationId = ReplicationId.fromString(entityContext.entityId)
      MyEventSourcedBehavior(replicationId)
    })
}


Java




copy
source
return ReplicatedEntityProvider.create(
    Command.class,
    "MyReplicatedType",
    ALL_REPLICAS,
    (entityTypeKey, replicaId) ->
        ReplicatedEntity.create(
            replicaId,
            Entity.of(
                entityTypeKey,
                entityContext ->
                    myEventSourcedBehavior(
                        ReplicationId.fromString(entityContext.getEntityId())))));





This will run an instance of sharding and per replica and each entity id contains the replica id and the type name. Replicas could be on the same node if they end up in the same shard or if the shards get allocated to the same node.


To prevent this roles can be used. You could for instance add a cluster role per availability zone / rack and have a replica per rack.




Scala




copy
source
val provider = ReplicatedEntityProvider.perRole("MyEntityType", Set(ReplicaId("DC-A"), ReplicaId("DC-B"))) {
  replicationId =>
    MyEventSourcedBehavior(replicationId)
}


Java




copy
source
return ReplicatedEntityProvider.create(
    Command.class,
    "MyReplicatedType",
    ALL_REPLICAS,
    (entityTypeKey, replicaId) ->
        ReplicatedEntity.create(
            replicaId,
            Entity.of(
                    entityTypeKey,
                    entityContext ->
                        myEventSourcedBehavior(
                            ReplicationId.fromString(entityContext.getEntityId())))
                .withRole(replicaId.id())));





Regardless of which replication strategy you use sending messages to the replicated entities is the same.


init
 returns an 
ReplicatedSharding
ReplicatedSharding
 instance which gives access to 
EntityRef
EntityRef
s for each of the replicas for arbitrary routing logic:




Scala




copy
source
val myReplicatedSharding: ReplicatedSharding[Command] =
  ReplicatedShardingExtension(system).init(provider)

val entityRefs: Map[ReplicaId, EntityRef[Command]] = myReplicatedSharding.entityRefsFor("myEntityId")


Java




copy
source
ReplicatedShardingExtension extension = ReplicatedShardingExtension.get(system);

ReplicatedSharding<Command> replicatedSharding = extension.init(provider());

Map<ReplicaId, EntityRef<Command>> myEntityId =
    replicatedSharding.getEntityRefsFor("myEntityId");




More advanced routing among the replicas is currently left as an exercise for the reader (or may be covered in a future release 
#29281
, 
#29319
).


Direct Replication of Events


Each replica will read the events from all the other copies from the database. When used with Cluster Sharding, and to make the sharing of events with other replicas more efficient, each replica publishes the events across the Akka cluster directly to other replicas. The delivery of events across the cluster is not guaranteed so the query to the journal is still needed but can be configured to poll the database less often since most events will arrive at the replicas through the cluster.


The direct replication of events feature is enabled by default when using Cluster Sharding. To disable this feature you first need to:




disable event publishing 
on the 
EventSourcedBehavior
 with 
withEventPublishing(false)
overriding 
withEventPublishing
 from 
ReplicatedEventSourcedBehavior
 to return 
false
 , and then


disable direct replication through 
withDirectReplication(false)
 on 
ReplicatedEntityProvider
ReplicatedEntityProvider




The “event publishing” feature publishes each event to the local system event bus as a side effect after it has been written.


Hot Standby


If all writes occur to one replica and the other replicas are not started there might be many replicated events to catch up with when they are later started. Therefore it can be good to activate all replicas when there is some activity.


This can be achieved automatically when direct access to replica databases and 
ReplicatedSharding
 is used and direct replication of events is enabled as described in 
Direct Replication of Events
. When each written event is forwarded to the other replicas it will trigger them to start if they are not already started.


Examples


More examples can be found in 
Replicated Event Sourcing Examples


Journal Support


For a journal plugin to support replication it needs to store and read metadata for each event if it is defined in the   
metadata
 field. To attach the metadata after writing it, 
PersistentRepr.withMetadata
 is used. The 
JournalSpec
JournalSpec
 in the Persistence TCK provides  a capability flag 
supportsMetadata
 to toggle verification that metadata is handled correctly.


For a snapshot plugin to support replication it needs to store and read metadata for the snapshot if it is defined in the  
metadata
 field. To attach the metadata when reading the snapshot the 
akka.persistence.SnapshotMetadata.apply
 factory overload taking a 
metadata
 parameter is used. The 
SnapshotStoreSpec
SnapshotStoreSpec
 in the Persistence TCK provides a capability flag 
supportsMetadata
 to toggle verification that metadata is handled correctly.


The following plugins support Replicated Event Sourcing:




Akka Persistence Cassandra
 versions 1.0.3+


Akka Persistence R2DBC
 versions 1.0.0+


Akka Persistence JDBC
 versions 5.0.0+
















 
Persistence - Building a storage backend






Replicated Event Sourcing Examples 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#discovery-method-aggregate-multiple-discovery-methods
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/durable-state/state-store-plugin.html
Building a storage backend for Durable State • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




State Store plugin API


State Store provider


Configure the State Store






Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




State Store plugin API


State Store provider


Configure the State Store






Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Building a storage backend for Durable State


Storage backends for state stores are pluggable in the Akka persistence extension. This documentation described how to build a new storage backend for durable state.


Applications can provide their own plugins by implementing a plugin API and activating them by configuration. Plugin development requires the following imports:




Scala




copy
source
import akka.persistence._
import akka.persistence.state.scaladsl.DurableStateUpdateStore
import akka.persistence.state.scaladsl.GetObjectResult


Java




copy
source
import akka.Done;
import akka.actor.ExtendedActorSystem;
import akka.persistence.state.javadsl.DurableStateUpdateStore;
import akka.persistence.state.javadsl.GetObjectResult;
import com.typesafe.config.Config;
import java.util.concurrent.CompletionStage;





State Store plugin API


A durable state store plugin extends 
DurableStateUpdateStore
. 


DurableStateUpdateStore
 is an interface and the methods to be implemented are:




Scala




copy
source
class MyStateStore[A](system: ExtendedActorSystem, config: Config, cfgPath: String) extends DurableStateUpdateStore[A] {

  /**
   * Will persist the latest state. If itâs a new persistence id, the record will be inserted.
   *
   * In case of an existing persistence id, the record will be updated only if the revision
   * number of the incoming record is 1 more than the already existing record. Otherwise persist will fail.
   */
  override def upsertObject(persistenceId: String, revision: Long, value: A, tag: String): Future[Done] = ???

  /**
   * Deprecated. Use the deleteObject overload with revision instead.
   */
  override def deleteObject(persistenceId: String): Future[Done] = deleteObject(persistenceId, 0)

  /**
   * Will delete the state by setting it to the empty state and the revision number will be incremented by 1.
   */
  override def deleteObject(persistenceId: String, revision: Long): Future[Done] = ???

  /**
   * Returns the current state for the given persistence id.
   */
  override def getObject(persistenceId: String): Future[GetObjectResult[A]] = ???
}


Java




copy
source
class MyJavaStateStore<A> implements DurableStateUpdateStore<A> {

  private ExtendedActorSystem system;
  private Config config;
  private String cfgPath;

  public MyJavaStateStore(ExtendedActorSystem system, Config config, String cfgPath) {
    this.system = system;
    this.config = config;
    this.cfgPath = cfgPath;
  }

  /** Returns the current state for the given persistence id. */
  @Override
  public CompletionStage<GetObjectResult<A>> getObject(String persistenceId) {
    // implement getObject here
    return null;
  }

  /**
   * Will persist the latest state. If itâs a new persistence id, the record will be inserted.
   *
   * <p>In case of an existing persistence id, the record will be updated only if the revision
   * number of the incoming record is 1 more than the already existing record. Otherwise persist
   * will fail.
   */
  @Override
  public CompletionStage<Done> upsertObject(
      String persistenceId, long revision, Object value, String tag) {
    // implement upsertObject here
    return null;
  }

  /** Deprecated. Use the deleteObject overload with revision instead. */
  @Override
  public CompletionStage<Done> deleteObject(String persistenceId) {
    return deleteObject(persistenceId, 0);
  }

  /**
   * Will delete the state by setting it to the empty state and the revision number will be
   * incremented by 1.
   */
  @Override
  public CompletionStage<Done> deleteObject(String persistenceId, long revision) {
    // implement deleteObject here
    return null;
  }
}




A durable state store plugin may also extend 
DurableStateUpdateWithChangeEventStore
 to store additional change event.


DurableStateUpdateWithChangeEventStore
 is an interface and the methods to be implemented are:




Scala




copy
source
class MyChangeEventStateStore[A](system: ExtendedActorSystem, config: Config, cfgPath: String)
    extends DurableStateUpdateWithChangeEventStore[A] {

  /**
   * The `changeEvent` is written to the event journal.
   * Same `persistenceId` is used in the journal and the `revision` is used as `sequenceNr`.
   *
   * @param revision sequence number for optimistic locking. starts at 1.
   */
  override def upsertObject(
      persistenceId: String,
      revision: Long,
      value: A,
      tag: String,
      changeEvent: Any): Future[Done] = ???

  override def deleteObject(persistenceId: String, revision: Long, changeEvent: Any): Future[Done] = ???

  /**
   * Will persist the latest state. If itâs a new persistence id, the record will be inserted.
   *
   * In case of an existing persistence id, the record will be updated only if the revision
   * number of the incoming record is 1 more than the already existing record. Otherwise persist will fail.
   */
  override def upsertObject(persistenceId: String, revision: Long, value: A, tag: String): Future[Done] = ???

  /**
   * Deprecated. Use the deleteObject overload with revision instead.
   */
  override def deleteObject(persistenceId: String): Future[Done] = deleteObject(persistenceId, 0)

  /**
   * Will delete the state by setting it to the empty state and the revision number will be incremented by 1.
   */
  override def deleteObject(persistenceId: String, revision: Long): Future[Done] = ???

  /**
   * Returns the current state for the given persistence id.
   */
  override def getObject(persistenceId: String): Future[GetObjectResult[A]] = ???

}


Java




copy
source
class MyChangeEventJavaStateStore<A> implements DurableStateUpdateWithChangeEventStore<A> {

  private ExtendedActorSystem system;
  private Config config;
  private String cfgPath;

  public MyChangeEventJavaStateStore(ExtendedActorSystem system, Config config, String cfgPath) {
    this.system = system;
    this.config = config;
    this.cfgPath = cfgPath;
  }

  /**
   * Will delete the state by setting it to the empty state and the revision number will be
   * incremented by 1.
   */
  @Override
  public CompletionStage<Done> deleteObject(String persistenceId, long revision) {
    // implement deleteObject here
    return null;
  }

  @Override
  public CompletionStage<Done> deleteObject(
      String persistenceId, long revision, Object changeEvent) {
    // implement deleteObject here
    return null;
  }

  /** Returns the current state for the given persistence id. */
  @Override
  public CompletionStage<GetObjectResult<A>> getObject(String persistenceId) {
    // implement getObject here
    return null;
  }

  /**
   * Will persist the latest state. If itâs a new persistence id, the record will be inserted.
   *
   * <p>In case of an existing persistence id, the record will be updated only if the revision
   * number of the incoming record is 1 more than the already existing record. Otherwise persist
   * will fail.
   */
  @Override
  public CompletionStage<Done> upsertObject(
      String persistenceId, long revision, Object value, String tag) {
    // implement upsertObject here
    return null;
  }

  /** Deprecated. Use the deleteObject overload with revision instead. */
  @Override
  public CompletionStage<Done> deleteObject(String persistenceId) {
    return deleteObject(persistenceId, 0);
  }

  @Override
  public CompletionStage<Done> upsertObject(
      String persistenceId, long revision, A value, String tag, Object changeEvent) {
    // implement deleteObject here
    return null;
  }
}




State Store provider


A 
DurableStateStoreProvider
 needs to be implemented to be able to create the plugin itself:




Scala




copy
source
class MyStateStoreProvider(system: ExtendedActorSystem, config: Config, cfgPath: String)
    extends DurableStateStoreProvider {

  /**
   * The `DurableStateStore` implementation for the Scala API.
   * This corresponds to the instance that is returned by [[DurableStateStoreRegistry#durableStateStoreFor]].
   */
  override def scaladslDurableStateStore(): DurableStateStore[Any] = new MyStateStore(system, config, cfgPath)

  /**
   * The `DurableStateStore` implementation for the Java API.
   * This corresponds to the instance that is returned by [[DurableStateStoreRegistry#getDurableStateStoreFor]].
   */
  override def javadslDurableStateStore(): JDurableStateStore[AnyRef] = new MyJavaStateStore(system, config, cfgPath)
}


Java




copy
source
class MyJavaStateStoreProvider implements DurableStateStoreProvider {

  private ExtendedActorSystem system;
  private Config config;
  private String cfgPath;

  public MyJavaStateStoreProvider(ExtendedActorSystem system, Config config, String cfgPath) {
    this.system = system;
    this.config = config;
    this.cfgPath = cfgPath;
  }

  @Override
  public DurableStateStore<Object> scaladslDurableStateStore() {
    return new MyStateStore<>(this.system, this.config, this.cfgPath);
  }

  @Override
  public akka.persistence.state.javadsl.DurableStateStore<Object> javadslDurableStateStore() {
    return new MyJavaStateStore<>(this.system, this.config, this.cfgPath);
  }
}




Configure the State Store


A durable state store plugin can be activated with the following minimal configuration:




Scala




copy
source
# Path to the state store plugin to be used
akka.persistence.state.plugin = "my-state-store"

# My custom state store plugin
my-state-store {
  # Class name of the plugin.
  class = "docs.persistence.state.MyStateStoreProvider"
}


Java




copy
source
# Path to the state store plugin to be used
akka.persistence.state.plugin = "my-java-state-store"

# My custom state store plugin
my-java-state-store {
  # Class name of the plugin.
  class = "docs.persistence.state.MyJavaStateStoreProvider"
}
















 
Persistence Query






Streams 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/immutable.html
Immutability using Lombok • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok




Adding Lombok to your project


Using lombok




Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok




Adding Lombok to your project


Using lombok




Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Immutability using Lombok


A preferred best practice in Akka is to have immutable messages. Scala provides case class which makes it extremely easy to have short and clean classes for creating immutable objects, but no such facility is easily available in Java. We can make use of several third party libraries which help is achieving this. One good example is Lombok.


Project Lombok is a java library that automatically plugs into your editor and build tools, and helps get rid of much of the boilerplate code for java development.


Lombok handles the following details for you. It:




modifies fields to be 
private
 and 
final


creates getters for each field


creates correct 
equals
, 
hashCode
 and a human-friendly 
toString


creates a constructor requiring all fields.




Adding Lombok to your project


To add Lombok to a Maven project, declare it as a simple dependency:
Maven
<dependencies>
  <dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>1.18.10</version>
  </dependency>
</dependencies>
Gradle
dependencies {
  implementation "org.projectlombok:lombok:1.18.10"
}


Using lombok


@Value
public class LombokUser {

  String name;

  String email;
}



The example does not demonstrate other useful Lombok features like 
@Builder
 or 
@With
 which will help you create builder and copy methods. Be aware that Lombok is not an immutability library but a code generation library which means some setups might not create immutable objects. For example, Lombokâs 
@Data
 is equivalent to Lombokâs 
@Value
 but will also synthesize mutable methods. Donât use Lombokâs 
@Data
 when creating immutable classes.


Using Lombok for creating a message class for actors is quite simple. In following example, Message class just defines the member variable and Lombok annotation 
@Value
 takes care of creating methods like getter, toString, hashCode, equals.


public class MyActor extends AbstractActor {
        private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);

        public Receive createReceive() {
            return receiveBuilder()
            .match(Message.class, message -> {
                System.out.println(message.getMessage());
            })
            .matchAny(o -> log.info("Received unknown message"))
            .build();
        }

        @Value
        public static class Message {
            private String message;

            public Message(String message) {
                this.message = message;
            }
        }
}



Integrating Lombok with an IDE


Lombok integrates with popular IDEs:




To use Lombok in IntelliJ IDEA you’ll need the 
Lombok Plugin for IntelliJ IDEA
 and you’ll also need to enable Annotation Processing (
Settings / Build,Execution,Deployment / Compiler / Annotation Processors
 and tick 
Enable annotation processing
)


To Use Lombok in Eclipse, run 
java -jar lombok.jar
 (see the video at 
Project Lombok
).
















 
IDE Tips






Migration Guides 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/split-brain-resolver.html
Split Brain Resolver • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver




Module info


Enable the Split Brain Resolver


The Problem


Strategies


Indirectly connected nodes


Down all when unstable


Cluster Singleton and Cluster Sharding




Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver




Module info


Enable the Split Brain Resolver


The Problem


Strategies


Indirectly connected nodes


Down all when unstable


Cluster Singleton and Cluster Sharding




Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Split Brain Resolver


When operating an Akka cluster you must consider how to handle 
network partitions
 (a.k.a. split brain scenarios) and machine crashes (including JVM and hardware failures). This is crucial for correct behavior if you use 
Cluster Singleton
 or 
Cluster Sharding
, especially together with Akka Persistence.


The 
Split Brain Resolver video
 is a good starting point for learning why it is important to use a correct downing provider and how the Split Brain Resolver works.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Split Brain Resolver is part of 
akka-cluster
 and you probably already have that dependency included. Otherwise, add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster_${versions.ScalaBinary}"
}




Project Info: Akka Cluster (classic)


Artifact
com.typesafe.akka


akka-cluster


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.2.0, 2013-07-09




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Enable the Split Brain Resolver


You need to enable the Split Brain Resolver by configuring it as downing provider in the configuration of the 
ActorSystem
 (
application.conf
):


akka.cluster.downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"



You should also consider the different available 
downing strategies
.


The Problem


A fundamental problem in distributed systems is that network partitions (split brain scenarios) and machine crashes are indistinguishable for the observer, i.e. a node can observe that there is a problem with another node, but it cannot tell if it has crashed and will never be available again or if there is a network issue that might or might not heal again after a while. Temporary and permanent failures are indistinguishable because decisions must be made in finite time, and there always exists a temporary failure that lasts longer than the time limit for the decision.


A third type of problem is if a process is unresponsive, e.g. because of overload, CPU starvation or long garbage collection pauses. This is also indistinguishable from network partitions and crashes. The only signal we have for decision is “no reply in given time for heartbeats” and this means that phenomena causing delays or lost heartbeats are indistinguishable from each other and must be handled in the same way.


When there is a crash, we would like to remove the affected node immediately from the cluster membership. When there is a network partition or unresponsive process we would like to wait for a while in the hope that it is a transient problem that will heal again, but at some point, we must give up and continue with the nodes on one side of the partition and shut down nodes on the other side. Also, certain features are not fully available during partitions so it might not matter that the partition is transient or not if it just takes too long. Those two goals are in conflict with each other and there is a trade-off between how quickly we can remove a crashed node and premature action on transient network partitions.


This is a difficult problem to solve given that the nodes on the different sides of the network partition cannot communicate with each other. We must ensure that both sides can make this decision by themselves and that they take the same decision about which part will keep running and which part will shut itself down.


Another type of problem that makes it difficult to see the “right” picture is when some nodes are not fully connected and cannot communicate directly to each other but information can be disseminated between them via other nodes.


The Akka cluster has a failure detector that will notice network partitions and machine crashes (but it cannot distinguish the two). It uses periodic heartbeat messages to check if other nodes are available and healthy. These observations by the failure detector are referred to as a node being 
unreachable
 and it may become 
reachable
 again if the failure detector observes that it can communicate with it again. 


The failure detector in itself is not enough for making the right decision in all situations. The naive approach is to remove an unreachable node from the cluster membership after a timeout. This works great for crashes and short transient network partitions, but not for long network partitions. Both sides of the network partition will see the other side as unreachable and after a while remove it from its cluster membership. Since this happens on both sides the result is that two separate disconnected clusters have been created.


If you use the timeout based auto-down feature in combination with Cluster Singleton or Cluster Sharding that would mean that two singleton instances or two sharded entities with the same identifier would be running. One would be running: one in each cluster. For example when used together with Akka Persistence that could result in that two instances of a persistent actor with the same 
persistenceId
 are running and writing concurrently to the same stream of persistent events, which will have fatal consequences when replaying these events.


The default setting in Akka Cluster is to not remove unreachable nodes automatically and the recommendation is that the decision of what to do should be taken by a human operator or an external monitoring system. This is a valid solution, but not very convenient if you do not have this staff or external system for other reasons.


If the unreachable nodes are not downed at all they will still be part of the cluster membership. Meaning that Cluster Singleton and Cluster Sharding will not failover to another node. While there are unreachable nodes new nodes that are joining the cluster will not be promoted to full worthy members (with status Up). Similarly, leaving members will not be removed until all unreachable nodes have been resolved. In other words, keeping unreachable members for an unbounded time is undesirable.


With that introduction of the problem domain, it is time to look at the provided strategies for handling network partition, unresponsive nodes and crashed nodes.


Strategies


By default the 
Keep Majority
 strategy will be used because it works well for most systems. However, it’s worth considering the other available strategies and pick a strategy that fits the characteristics of your system. For example, in a Kubernetes environment the 
Lease
 strategy can be a good choice.


Every strategy has a failure scenario where it makes a “wrong” decision. This section describes the different strategies and guidelines of when to use what.


When there is uncertainty it selects to down more nodes than necessary, or even downing of all nodes. Therefore Split Brain Resolver should always be combined with a mechanism to automatically start up nodes that have been shutdown, and join them to the existing cluster or form a new cluster again.


You enable a strategy with the configuration property 
akka.cluster.split-brain-resolver.active-strategy
.


Stable after


All strategies are inactive until the cluster membership and the information about unreachable nodes have been stable for a certain time period. Continuously adding more nodes while there is a network partition does not influence this timeout, since the status of those nodes will not be changed to Up while there are unreachable nodes. Joining nodes are not counted in the logic of the strategies. 


copy
source
# To enable the split brain resolver you first need to enable the provider in your application.conf:
# akka.cluster.downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

akka.cluster.split-brain-resolver {
  # Select one of the available strategies (see descriptions below):
  # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
  active-strategy = keep-majority

  # Time margin after which shards or singletons that belonged to a downed/removed
  # partition are created in surviving partition. The purpose of this margin is that
  # in case of a network partition the persistent actors in the non-surviving partitions
  # must be stopped before corresponding persistent actors are started somewhere else.
  # This is useful if you implement downing strategies that handle network partitions,
  # e.g. by keeping the larger side of the partition and shutting down the smaller side.
  # Decision is taken by the strategy when there has been no membership or
  # reachability changes for this duration, i.e. the cluster state is stable.
  stable-after = 20s

  # When reachability observations by the failure detector are changed the SBR decisions
  # are deferred until there are no changes within the 'stable-after' duration.
  # If this continues for too long it might be an indication of an unstable system/network
  # and it could result in delayed or conflicting decisions on separate sides of a network
  # partition.
  # As a precaution for that scenario all nodes are downed if no decision is made within
  # `stable-after + down-all-when-unstable` from the first unreachability event.
  # The measurement is reset if all unreachable have been healed, downed or removed.
  # The value can be on, off, or a duration.
  # By default it is 'on' and then it is derived to be 3/4 of stable-after, but not less than
  # 4 seconds.
  down-all-when-unstable = on

}


Set 
akka.cluster.split-brain-resolver.stable-after
 to a shorter duration to have quicker removal of crashed nodes, at the price of risking too early action on transient network partitions that otherwise would have healed. Do not set this to a shorter duration than the membership dissemination time in the cluster, which depends on the cluster size. Recommended minimum duration for different cluster sizes:








cluster size 


stable-after










5 


7 s 






10 


10 s






20 


13 s






50 


17 s






100 


20 s






1000 


30 s








The different strategies may have additional settings that are described below.
Note


It is important that you use the same configuration on all nodes.


The side of the split that decides to shut itself down will use the cluster 
down
 command to initiate the removal of a cluster member. When that has been spread among the reachable nodes it will be removed from the cluster membership.


It’s good to terminate the 
ActorSystem
 and exit the JVM when the node is removed from the cluster.


That is handled by 
Coordinated Shutdown
 but to exit the JVM it’s recommended that you enable:


akka.coordinated-shutdown.exit-jvm = on

Note


Some legacy containers may block calls to System.exit(..) and you may have to find an alternate way to shut the app down. For example, when running Akka on top of a Spring / Tomcat setup, you could replace the call to 
System.exit(..)
 with a call to Spring’s ApplicationContext .close() method (or with a HTTP call to Tomcat Manager’s API to un-deploy the app).


Keep Majority


The strategy named 
keep-majority
 will down the unreachable nodes if the current node is in the majority part based on the last known membership information. Otherwise down the reachable nodes, i.e. the own part. If the parts are of equal size the part containing the node with the lowest address is kept.


This strategy is a good choice when the number of nodes in the cluster change dynamically and you can therefore not use 
static-quorum
.


This strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of two members are changed to 
Up
 on one side but that information is not disseminated to the other side before the connection is broken. Then one side sees two more nodes and both sides might consider themselves having a majority. It will detect this situation and make the safe decision to down all nodes on the side that could be in minority if the joining nodes were changed to 
Up
 on the other side. Note that this has the drawback that if the joining nodes were not changed to 
Up
 and becoming a majority on the other side then each part will shut down itself, terminating the whole cluster.


Note that if there are more than two partitions and none is in majority each part will shut down itself, terminating the whole cluster.


If more than half of the nodes crash at the same time the other running nodes will down themselves because they think that they are not in majority, and thereby the whole cluster is terminated. 


The decision can be based on nodes with a configured 
role
 instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might for example have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.


Configuration:


akka.cluster.split-brain-resolver.active-strategy=keep-majority



copy
source
akka.cluster.split-brain-resolver.keep-majority {
  # if the 'role' is defined the decision is based only on members with that 'role'
  role = ""
}


Static Quorum


The strategy named 
static-quorum
 will down the unreachable nodes if the number of remaining nodes are greater than or equal to a configured 
quorum-size
. Otherwise, it will down the reachable nodes, i.e. it will shut down that side of the partition. In other words, the 
quorum-size
 defines the minimum number of nodes that the cluster must have to be operational.


This strategy is a good choice when you have a fixed number of nodes in the cluster, or when you can define a fixed number of nodes with a certain role.


For example, in a 9 node cluster you will configure the 
quorum-size
 to 5. If there is a network split of 4 and 5 nodes the side with 5 nodes will survive and the other 4 nodes will be downed. After that, in the 5 node cluster, no more failures can be handled, because the remaining cluster size would be less than 5. In the case of another failure in that 5 node cluster all nodes will be downed.


Therefore, it is important that you join new nodes when old nodes have been removed.


Another consequence of this is that if there are unreachable nodes when starting up the cluster, before reaching this limit, the cluster may shut itself down immediately. This is not an issue if you start all nodes at approximately the same time or use the 
akka.cluster.min-nr-of-members
 to define required number of members before the leader changes member status of ‘Joining’ members to ‘Up’. You can tune the timeout after which downing decisions are made using the 
stable-after
 setting.


You should not add more members to the cluster than 
quorum-size * 2 - 1
. A warning is logged if this recommendation is violated. If the exceeded cluster size remains when an SBR decision is needed it will down all nodes because otherwise there is a risk that both sides may down each other and thereby form two separate clusters.


For rolling updates, it’s best to leave the cluster gracefully via 
Coordinated Shutdown
 (SIGTERM). For successful leaving, SBR will not be used (no downing) but if there is an unreachability problem at the same time as the rolling update is in progress there could be an SBR decision. To avoid that the total number of members limit is not exceeded during the rolling update it’s recommended to leave and fully remove one node before adding a new one, when using 
static-quorum
.


If the cluster is split into 3 (or more) parts each part that is smaller than then configured 
quorum-size
 will down itself and possibly shutdown the whole cluster.


If more nodes than the configured 
quorum-size
 crash at the same time the other running nodes will down themselves because they think that they are not in the majority, and thereby the whole cluster is terminated.


The decision can be based on nodes with a configured 
role
 instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might, for example, have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.


There is another use of the 
role
 as well. By defining a 
role
 for a few (e.g. 7) stable nodes in the cluster and using that in the configuration of 
static-quorum
 you will be able to dynamically add and remove other nodes without this role and still have good decisions of what nodes to keep running and what nodes to shut down in the case of network partitions. The advantage of this approach compared to 
keep-majority
 (described below) is that you 
do not
 risk splitting the cluster into two separate clusters, i.e. 
a split brain
*. You must still obey the rule of not starting too many nodes with this 
role
 as described above. It also suffers the risk of shutting down all nodes if there is a failure when there are not enough nodes with this 
role
 remaining in the cluster, as described above.


Configuration:


akka.cluster.split-brain-resolver.active-strategy=static-quorum



copy
source
akka.cluster.split-brain-resolver.static-quorum {
  # minimum number of nodes that the cluster must have
  quorum-size = undefined

  # if the 'role' is defined the decision is based only on members with that 'role'
  role = ""
}


Keep Oldest


The strategy named 
keep-oldest
 will down the part that does not contain the oldest member. The oldest member is interesting because the active Cluster Singleton instance is running on the oldest member.


There is one exception to this rule if 
down-if-alone
 is configured to 
on
. Then, if the oldest node has partitioned from all other nodes the oldest will down itself and keep all other nodes running. The strategy will not down the single oldest node when it is the only remaining node in the cluster.


Note that if the oldest node crashes the others will remove it from the cluster when 
down-if-alone
 is 
on
, otherwise they will down themselves if the oldest node crashes, i.e. shut down the whole cluster together with the oldest node.


This strategy is good to use if you use Cluster Singleton and do not want to shut down the node where the singleton instance runs. If the oldest node crashes a new singleton instance will be started on the next oldest node. The drawback is that the strategy may keep only a few nodes in a large cluster. For example, if one part with the oldest consists of 2 nodes and the other part consists of 98 nodes then it will keep 2 nodes and shut down 98 nodes.


This strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of the oldest member is changed to 
Exiting
 on one side but that information is not disseminated to the other side before the connection is broken. It will detect this situation and make the safe decision to down all nodes on the side that sees the oldest as 
Leaving
. Note that this has the drawback that if the oldest was 
Leaving
 and not changed to 
Exiting
 then each part will shut down itself, terminating the whole cluster.


The decision can be based on nodes with a configured 
role
 instead of all nodes in the cluster, i.e. using the oldest member (singleton) within the nodes with that role.


Configuration:


akka.cluster.split-brain-resolver.active-strategy=keep-oldest



copy
source
akka.cluster.split-brain-resolver.keep-oldest {
  # Enable downing of the oldest node when it is partitioned from all other nodes
  down-if-alone = on

  # if the 'role' is defined the decision is based only on members with that 'role',
  # i.e. using the oldest member (singleton) within the nodes with that role
  role = ""
}


Down All


The strategy named 
down-all
 will down all nodes.


This strategy can be a safe alternative if the network environment is highly unstable with unreachability observations that can’t be fully trusted, and including frequent occurrences of 
indirectly connected nodes
. Due to the instability there is an increased risk of different information on different sides of partitions and therefore the other strategies may result in conflicting decisions. In such environments it can be better to shutdown all nodes and start up a new fresh cluster.


Shutting down all nodes means that the system will be completely unavailable until nodes have been restarted and formed a new cluster. This strategy is not recommended for large clusters (> 10 nodes) because any minor problem will shutdown all nodes, and that is more likely to happen in larger clusters since there are more nodes that may fail.


See also 
Down all when unstable
 and 
indirectly connected nodes
.


Lease


The strategy named 
lease-majority
 is using a distributed lease (lock) to decide what nodes that are allowed to survive. Only one SBR instance can acquire the lease and make the decision to remain up. The other side will not be able to aquire the lease and will therefore down itself.


Best effort is to keep the side that has most nodes, i.e. the majority side. This is achieved by adding a delay before trying to acquire the lease on the minority side.


There is currently one supported implementation of the lease which is backed by a 
Custom Resource Definition (CRD)
 in Kubernetes. It is described in the 
Kubernetes Lease
 documentation.


This strategy is very safe since coordination is added by an external arbiter. The trade-off compared to other strategies is that it requires additional infrastructure for implementing the lease and it reduces the availability of a decision to that of the system backing the lease store.


Similar to other strategies it is important that decisions are not deferred for too long because the nodes that couldn’t acquire the lease must decide to down themselves, see 
Down all when unstable
.


In some cases the lease will be unavailable when needed for a decision from all SBR instances, e.g. because it is on another side of a network partition, and then all nodes will be downed.


Configuration:


akka {
  cluster {
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
    split-brain-resolver {
      active-strategy = "lease-majority"
      lease-majority {
        lease-implementation = "akka.coordination.lease.kubernetes"
      }
    }
  }
}



copy
source
akka.cluster.split-brain-resolver.lease-majority {
  lease-implementation = ""

  # The recommended format for the lease name is "<service-name>-akka-sbr".
  # When lease-name is not defined, the name will be set to "<actor-system-name>-akka-sbr"
  lease-name = ""

  # This delay is used on the minority side before trying to acquire the lease,
  # as an best effort to try to keep the majority side.
  acquire-lease-delay-for-minority = 2s

  # Release the lease after this duration.
  release-after = 40s

  # If the 'role' is defined the majority/minority is based only on members with that 'role'.
  role = ""
}


See also configuration and additional dependency in 
Kubernetes Lease


Indirectly connected nodes


In a malfunctioning network there can be situations where nodes are observed as unreachable via some network links, but they are still indirectly connected via other nodes, i.e. it’s not a clean network partition (or node crash).


When this situation is detected the Split Brain Resolvers will keep fully connected nodes and down all the indirectly connected nodes.


If there is a combination of indirectly connected nodes and a clean network partition it will combine the above decision with the ordinary decision, e.g. keep majority, after excluding suspicious failure detection observations.


Down all when unstable


When reachability observations by the failure detector are changed the SBR decisions are deferred until there are no changes within the 
stable-after
 duration. If this continues for too long it might be an indication of an unstable system/network and it could result in delayed or conflicting decisions on separate sides of a network partition.


As a precaution for that scenario all nodes are downed if no decision is made within 
stable-after + down-all-when-unstable
 from the first unreachability event. The measurement is reset if all unreachable have been healed, downed or removed.


This is enabled by default for all strategies and by default the duration is derived to be 3/4 of 
stable-after
.


The below property can be defined as a duration of for how long the changes are acceptable to continue after the 
stable-after
 or it can be set to 
off
 to disable this feature.


akka.cluster.split-brain-resolver {
  down-all-when-unstable = 15s
  stable-after = 20s
}

Warning


It is recommended to keep 
down-all-when-unstable
 enabled and not set it to a longer duration than 
stable-after
 (
down-removal-margin
) because that can result in delayed decisions on the side that should have been downed, e.g. in the case of a clean network partition followed by continued instability on the side that should be downed. That could result in that members are removed from one side but are still running on the other side.


Cluster Singleton and Cluster Sharding


The purpose of Cluster Singleton and Cluster Sharding is to run at most one instance of a given actor at any point in time. When such an instance is shut down a new instance is supposed to be started elsewhere in the cluster. It is important that the new instance is not started before the old instance has been stopped. This is especially important when the singleton or the sharded instance is persistent, since there must only be one active writer of the journaled events of a persistent actor instance.


Since the strategies on different sides of a network partition cannot communicate with each other and they may take the decision at slightly different points in time there must be a time based margin that makes sure that the new instance is not started before the old has been stopped.


You would like to configure this to a short duration to have quick failover, but that will increase the risk of having multiple singleton/sharded instances running at the same time and it may take a different amount of time to act on the decision (dissemination of the down/removal). The duration is by default the same as the 
stable-after
 property (see 
Stable after
 above). It is recommended to leave this value as is, but it can also be separately overriden with the 
akka.cluster.down-removal-margin
 property.


Another concern for setting this 
stable-after
/
akka.cluster.down-removal-margin
 is dealing with JVM pauses e.g. garbage collection. When a node is unresponsive it is not known if it is due to a pause, overload, a crash or a network partition. If it is pause that lasts longer than 
stable-after
 * 2 it gives time for SBR to down the node and for singletons and shards to be started on other nodes. When the node un-pauses there will be a short time before it sees its self as down where singletons and sharded actors are still running. It is therefore important to understand the max pause time your application is likely to incur and make sure it is smaller than 
stable-margin
.


If you choose to set a separate value for 
down-removal-margin
, the recommended minimum duration for different cluster sizes are:








cluster size 


down-removal-margin










5 


7 s 






10 


10 s






20 


13 s






50 


17 s






100 


20 s






1000 


30 s








Expected Failover Time


As you have seen, there are several configured timeouts that add to the total failover latency. With default configuration those are:




failure detection 5 seconds


stable-after 20 seconds


down-removal-margin (by default the same as stable-after) 20 seconds




In total, you can expect the failover time of a singleton or sharded instance to be around 45 seconds with default configuration. The default configuration is sized for a cluster of 100 nodes. If you have around 10 nodes you can reduce the 
stable-after
 to around 10 seconds, resulting in an expected failover time of around 25 seconds. 














 
Remote Security






Coordination 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial_1.html
Part 1: Actor Architecture • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture




Dependency


Introduction


The Akka actor hierarchy


Summary




Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture




Dependency


Introduction


The Akka actor hierarchy


Summary




Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Part 1: Actor Architecture


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


Use of Akka relieves you from creating the infrastructure for an actor system and from writing the low-level code necessary to control basic behavior. To appreciate this, let’s look at the relationships between actors you create in your code and those that Akka creates and manages for you internally, the actor lifecycle, and failure handling.


The Akka actor hierarchy


An actor in Akka always belongs to a parent. You create an actor by calling 
ActorContext.spawn()
ActorContext.spawn()
. The creator actor becomes the 
parent
 of the newly created 
child
 actor. You might ask then, who is the parent of the 
first
 actor you create?


As illustrated below, all your actors have a common parent, the user guardian, which is defined and created when you start the 
ActorSystem
ActorSystem
. As we covered in the 
first Hello World example
, creation of an actor returns a reference that is a valid URL. So, for example, if we create an actor named 
someActor
 from the user guardian with 
context.spawn(someBehavior, "someActor")
, its reference will include the path 
/user/someActor
.




In fact, before your first actor - the user guardian - is started, Akka has already created two other guardian actors in the system: 
/
 and 
/system
. Thus, there are three 
guardian
 actors at the top of the tree:




/
 the so-called 
root guardian
. This is the parent of all actors in the system, and the last one to stop when the system itself is terminated.


/system
 the 
system guardian
. Akka or other libraries built on top of Akka may create actors in the 
system
 namespace.


/user
 the 
user guardian
. This is the top level actor that you provide to start all other actors in your application.




The easiest way to see the actor hierarchy in action is to print 
ActorRef
ActorRef
 instances. In this small experiment, we create an actor, print its reference, create a child of this actor, and print the child’s reference. We start with the Hello World project, which you can download from: 




Scala 
akka-quickstart-scala.zip


Java 
akka-quickstart-java.zip




In your Hello World project, navigate to the 
com.example
 package and create 
a new Scala file called 
ActorHierarchyExperiments.scala
 here. Copy and paste the code from the snippet below to this new source file
a Java file for each of the classes in the snippet below and copy the respective contents
. Save your 
file and run 
sbt "runMain com.example.ActorHierarchyExperiments"
files and run 
com.example.ActorHierarchyExperiments
 from your build tool or IDE
 to observe the output.




Scala




copy
source
package com.example

import akka.actor.typed.ActorSystem
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.AbstractBehavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object PrintMyActorRefActor {
  def apply(): Behavior[String] =
    Behaviors.setup(context => new PrintMyActorRefActor(context))
}

class PrintMyActorRefActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {

  override def onMessage(msg: String): Behavior[String] =
    msg match {
      case "printit" =>
        val secondRef = context.spawn(Behaviors.empty[String], "second-actor")
        println(s"Second: $secondRef")
        this
    }
}

object Main {
  def apply(): Behavior[String] =
    Behaviors.setup(context => new Main(context))

}

class Main(context: ActorContext[String]) extends AbstractBehavior[String](context) {
  override def onMessage(msg: String): Behavior[String] =
    msg match {
      case "start" =>
        val firstRef = context.spawn(PrintMyActorRefActor(), "first-actor")
        println(s"First: $firstRef")
        firstRef ! "printit"
        this
    }
}

object ActorHierarchyExperiments extends App {
  val testSystem = ActorSystem(Main(), "testSystem")
  testSystem ! "start"
}


Java




copy
source
package com.example;

import akka.actor.typed.ActorRef;
import akka.actor.typed.ActorSystem;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;

class PrintMyActorRefActor extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(PrintMyActorRefActor::new);
  }

  private PrintMyActorRefActor(ActorContext<String> context) {
    super(context);
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onMessageEquals("printit", this::printIt).build();
  }

  private Behavior<String> printIt() {
    ActorRef<String> secondRef = getContext().spawn(Behaviors.empty(), "second-actor");
    System.out.println("Second: " + secondRef);
    return this;
  }
}

class Main extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(Main::new);
  }

  private Main(ActorContext<String> context) {
    super(context);
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onMessageEquals("start", this::start).build();
  }

  private Behavior<String> start() {
    ActorRef<String> firstRef = getContext().spawn(PrintMyActorRefActor.create(), "first-actor");

    System.out.println("First: " + firstRef);
    firstRef.tell("printit");
    return Behaviors.same();
  }
}

public class ActorHierarchyExperiments {
  public static void main(String[] args) {
    ActorRef<String> testSystem = ActorSystem.create(Main.create(), "testSystem");
    testSystem.tell("start");
  }
}




Note the way a message asked the first actor to do its work. We sent the message by using the parent’s reference: 
firstRef ! "printit"
firstRef.tell("printit", ActorRef.noSender())
. When the code executes, the output includes the references for the first actor and the child it created as part of the 
printit
 case. Your output should look similar to the following:


First: Actor[akka://testSystem/user/first-actor#1053618476]
Second: Actor[akka://testSystem/user/first-actor/second-actor#-1544706041]



Notice the structure of the references:




Both paths start with 
akka://testSystem/
. Since all actor references are valid URLs, 
akka://
 is the value of the protocol field.


Next, just like on the World Wide Web, the URL identifies the system. In this example, the system is named 
testSystem
, but it could be any other name. If remote communication between multiple systems is enabled, this part of the URL includes the hostname so other systems can find it on the network.


Because the second actor’s reference includes the path 
/first-actor/
, it identifies it as a child of the first.


The last part of the actor reference, 
#1053618476
 or 
#-1544706041
 is a unique identifier that you can ignore in most cases.




Now that you understand what the actor hierarchy looks like, you might be wondering: 
Why do we need this hierarchy? What is it used for?


An important role of the hierarchy is to safely manage actor lifecycles. Let’s consider this next and see how that knowledge can help us write better code.


The actor lifecycle


Actors pop into existence when created, then later, at user requests, they are stopped. Whenever an actor is stopped, all of its children are 
recursively stopped
 too. This behavior greatly simplifies resource cleanup and helps avoid resource leaks such as those caused by open sockets and files. In fact, a commonly overlooked difficulty when dealing with low-level multi-threaded code is the lifecycle management of various concurrent resources.


To stop an actor, the recommended pattern is to return 
Behaviors.stopped
Behaviors.stopped
 inside the actor to stop itself, usually as a response to some user defined stop message or when the actor is done with its job. Stopping a child actor is technically possible by calling 
context.stop(childRef)
context.stop(childRef)
 from the parent, but it’s not possible to stop arbitrary (non-child) actors this way.


The Akka actor API exposes some lifecycle signals, for example 
PostStop
PostStop
 is sent just after the actor has been stopped. No messages are processed after this point.


Let’s use the 
PostStop
 lifecycle signal in a simple experiment to observe the behavior when we stop an actor. First, add the following 2 actor classes to your project:




Scala




copy
source
object StartStopActor1 {
  def apply(): Behavior[String] =
    Behaviors.setup(context => new StartStopActor1(context))
}

class StartStopActor1(context: ActorContext[String]) extends AbstractBehavior[String](context) {
  println("first started")
  context.spawn(StartStopActor2(), "second")

  override def onMessage(msg: String): Behavior[String] =
    msg match {
      case "stop" => Behaviors.stopped
    }

  override def onSignal: PartialFunction[Signal, Behavior[String]] = {
    case PostStop =>
      println("first stopped")
      this
  }

}

object StartStopActor2 {
  def apply(): Behavior[String] =
    Behaviors.setup(new StartStopActor2(_))
}

class StartStopActor2(context: ActorContext[String]) extends AbstractBehavior[String](context) {
  println("second started")

  override def onMessage(msg: String): Behavior[String] = {
    // no messages handled by this actor
    Behaviors.unhandled
  }

  override def onSignal: PartialFunction[Signal, Behavior[String]] = {
    case PostStop =>
      println("second stopped")
      this
  }

}


Java




copy
source
class StartStopActor1 extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(StartStopActor1::new);
  }

  private StartStopActor1(ActorContext<String> context) {
    super(context);
    System.out.println("first started");

    context.spawn(StartStopActor2.create(), "second");
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder()
        .onMessageEquals("stop", Behaviors::stopped)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private Behavior<String> onPostStop() {
    System.out.println("first stopped");
    return this;
  }
}

class StartStopActor2 extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(StartStopActor2::new);
  }

  private StartStopActor2(ActorContext<String> context) {
    super(context);
    System.out.println("second started");
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onSignal(PostStop.class, signal -> onPostStop()).build();
  }

  private Behavior<String> onPostStop() {
    System.out.println("second stopped");
    return this;
  }
}




And create a ‘main’ class like above to start the actors and then send them a 
"stop"
 message:




Scala




copy
source
val first = context.spawn(StartStopActor1(), "first")
first ! "stop"


Java




copy
source
ActorRef<String> first = context.spawn(StartStopActor1.create(), "first");
first.tell("stop");




You can again use 
sbt
 to start this program. The output should look like this:


first started
second started
second stopped
first stopped



When we stopped actor 
first
, it stopped its child actor, 
second
, before stopping itself. This ordering is strict, 
all
 
PostStop
PostStop
 signals of the children are processed before the 
PostStop
 signal of the parent is processed.


Failure handling


Parents and children are connected throughout their lifecycles. Whenever an actor fails (throws an exception or an unhandled exception bubbles out from 
onMessage
Receive
) the failure information is propagated to the supervision strategy, which then decides how to handle the exception caused by the actor. The supervision strategy is typically defined by the parent actor when it spawns a child actor. In this way, parents act as supervisors for their children. The default 
supervisor strategy
 is to stop the child. If you don’t define the strategy all failures result in a stop.


Let’s observe a restart supervision strategy in a simple experiment. Add the following classes to your project, just as you did with the previous ones:




Scala




copy
source
object SupervisingActor {
  def apply(): Behavior[String] =
    Behaviors.setup(context => new SupervisingActor(context))
}

class SupervisingActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {
  private val child = context.spawn(
    Behaviors.supervise(SupervisedActor()).onFailure(SupervisorStrategy.restart),
    name = "supervised-actor")

  override def onMessage(msg: String): Behavior[String] =
    msg match {
      case "failChild" =>
        child ! "fail"
        this
    }
}

object SupervisedActor {
  def apply(): Behavior[String] =
    Behaviors.setup(context => new SupervisedActor(context))
}

class SupervisedActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {
  println("supervised actor started")

  override def onMessage(msg: String): Behavior[String] =
    msg match {
      case "fail" =>
        println("supervised actor fails now")
        throw new Exception("I failed!")
    }

  override def onSignal: PartialFunction[Signal, Behavior[String]] = {
    case PreRestart =>
      println("supervised actor will be restarted")
      this
    case PostStop =>
      println("supervised actor stopped")
      this
  }

}


Java




copy
source
class SupervisingActor extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(SupervisingActor::new);
  }

  private final ActorRef<String> child;

  private SupervisingActor(ActorContext<String> context) {
    super(context);
    child =
        context.spawn(
            Behaviors.supervise(SupervisedActor.create()).onFailure(SupervisorStrategy.restart()),
            "supervised-actor");
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onMessageEquals("failChild", this::onFailChild).build();
  }

  private Behavior<String> onFailChild() {
    child.tell("fail");
    return this;
  }
}

class SupervisedActor extends AbstractBehavior<String> {

  static Behavior<String> create() {
    return Behaviors.setup(SupervisedActor::new);
  }

  private SupervisedActor(ActorContext<String> context) {
    super(context);
    System.out.println("supervised actor started");
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder()
        .onMessageEquals("fail", this::fail)
        .onSignal(PreRestart.class, signal -> preRestart())
        .onSignal(PostStop.class, signal -> postStop())
        .build();
  }

  private Behavior<String> fail() {
    System.out.println("supervised actor fails now");
    throw new RuntimeException("I failed!");
  }

  private Behavior<String> preRestart() {
    System.out.println("supervised will be restarted");
    return this;
  }

  private Behavior<String> postStop() {
    System.out.println("supervised stopped");
    return this;
  }
}




And run with:




Scala




copy
source
val supervisingActor = context.spawn(SupervisingActor(), "supervising-actor")
supervisingActor ! "failChild"


Java




copy
source
ActorRef<String> supervisingActor =
    context.spawn(SupervisingActor.create(), "supervising-actor");
supervisingActor.tell("failChild");




You should see output similar to the following:


supervised actor started
supervised actor fails now
supervised actor will be restarted
supervised actor started
[ERROR] [11/12/2018 12:03:27.171] [ActorHierarchyExperiments-akka.actor.default-dispatcher-2] [akka://ActorHierarchyExperiments/user/supervising-actor/supervised-actor] Supervisor akka.actor.typed.internal.RestartSupervisor@1c452254 saw failure: I failed!
java.lang.Exception: I failed!
	at typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:113)
	at typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:106)
	at akka.actor.typed.scaladsl.AbstractBehavior.receive(AbstractBehavior.scala:59)
	at akka.actor.typed.Behavior$.interpret(Behavior.scala:395)
	at akka.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)
	at akka.actor.typed.internal.InterceptorImpl$$anon$2.apply(InterceptorImpl.scala:49)
	at akka.actor.typed.internal.SimpleSupervisor.aroundReceive(Supervision.scala:85)
	at akka.actor.typed.internal.InterceptorImpl.receive(InterceptorImpl.scala:70)
	at akka.actor.typed.Behavior$.interpret(Behavior.scala:395)
	at akka.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)



We see that after failure the supervised actor is stopped and immediately restarted. We also see a log entry reporting the exception that was handled, in this case, our test exception. In this example we also used the 
PreRestart
PreRestart
 signal which is processed before restarts.


For the impatient, we also recommend looking into the 
fault tolerance reference page
 for more in-depth details.


Summary


We’ve learned about how Akka manages actors in hierarchies where parents supervise their children and handle exceptions. We saw how to create a very simple actor and child. Next, we’ll apply this knowledge to our example use case by modeling the communication necessary to get information from device actors. Later, we’ll deal with how to manage the actors in groups.














 
Introduction to the Example






Part 2: Creating the First Actor 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/message-delivery-reliability.html
Message Delivery Reliability • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability




The General Rules


The Rules for In-JVM (Local) Message Sends


Higher-level abstractions


Dead Letters




Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability




The General Rules


The Rules for In-JVM (Local) Message Sends


Higher-level abstractions


Dead Letters




Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Message Delivery Reliability


Akka helps you build reliable applications which make use of multiple processor cores in one machine (âscaling upâ) or distributed across a computer network (âscaling outâ). The key abstraction to make this work is that all interactions between your code unitsâactorsâhappen via message passing, which is why the precise semantics of how messages are passed between actors deserve their own chapter.


In order to give some context to the discussion below, consider an application which spans multiple network hosts. The basic mechanism for communication is the same whether sending to an actor on the local JVM or to a remote actor, but there will be observable differences in the latency of delivery (possibly also depending on the bandwidth of the network link and the message size) and the reliability. In case of a remote message send there are more steps involved which means that more can go wrong. Another aspect is that local sending will pass a reference to the message inside the same JVM, without any restrictions on the underlying object which is sent, whereas a remote transport will place a limit on the message size.


Writing your actors such that every interaction could possibly be remote is the safe, pessimistic bet. It means to only rely on those properties which are always guaranteed and which are discussed in detail below. This has some overhead in the actorâs implementation. If you are willing to sacrifice full location transparencyâfor example in case of a group of closely collaborating actorsâyou can place them always on the same JVM and enjoy stricter guarantees on message delivery. The details of this trade-off are discussed further below.


As a supplementary part we give a few pointers at how to build stronger reliability on top of the built-in ones. The chapter closes by discussing the role of the âDead Letter Officeâ.


The General Rules


These are the rules for message sends (i.e. the 
tell
 or 
!
tell
 method, which also underlies the 
ask
 pattern):




at-most-once delivery
, i.e. no guaranteed delivery


message ordering per senderâreceiver pair




The first rule is typically found also in other actor implementations while the second is specific to Akka.


Discussion: What does âat-most-onceâ mean?


When it comes to describing the semantics of a delivery mechanism, there are three basic categories:




at-most-once
 delivery means that for each message handed to the mechanism, that message is delivered once or not at all; in more casual terms it means that messages may be lost.


at-least-once
 delivery means that for each message handed to the mechanism potentially multiple attempts are made at delivering it, such that at least one succeeds; again, in more casual terms this means that messages may be duplicated but not lost.


exactly-once
 delivery means that for each message handed to the mechanism exactly one delivery is made to the recipient; the message can neither be lost nor duplicated.




The first one is the cheapestâhighest performance, least implementation overheadâbecause it can be done in a fire-and-forget fashion without keeping state at the sending end or in the transport mechanism. The second one requires retries to counter transport losses, which means keeping state at the sending end and having an acknowledgement mechanism at the receiving end. The third is most expensiveâand has consequently worst performanceâbecause in addition to the second it requires state to be kept at the receiving end in order to filter out duplicate deliveries.


Discussion: Why No Guaranteed Delivery?


At the core of the problem lies the question what exactly this guarantee shall mean:




The message is sent out on the network?


The message is received by the other host?


The message is put into the target actor’s mailbox?


The message is starting to be processed by the target actor?


The message is processed successfully by the target actor?




Each one of these have different challenges and costs, and it is obvious that there are conditions under which any message passing library would be unable to comply; think for example about configurable mailbox types and how a bounded mailbox would interact with the third point, or even what it would mean to decide upon the âsuccessfullyâ part of point five.


Along those same lines goes the reasoning in 
Nobody Needs Reliable Messaging
. The only meaningful way for a sender to know whether an interaction was successful is by receiving a business-level acknowledgement message, which is not something Akka could make up on its own (neither are we writing a âdo what I meanâ framework nor would you want us to).


Akka embraces distributed computing and makes the fallibility of communication explicit through message passing, therefore it does not try to lie and emulate a leaky abstraction. This is a model that has been used with great success in Erlang and requires the users to design their applications around it. You can read more about this approach in the 
Erlang documentation
 (section 10.8 and 10.9), Akka follows it closely.


Another angle on this issue is that by providing only basic guarantees those use cases which do not need stronger reliability do not pay the cost of their implementation; it is always possible to add stronger reliability on top of basic ones, but it is not possible to retroactively remove reliability in order to gain more performance.




Discussion: Message Ordering


The rule more specifically is that 
for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order.
 The word 
directly
 emphasizes that this guarantee only applies when sending with the 
tell
 operator to the final destination, not when employing mediators or other message dissemination features (unless stated otherwise).


The guarantee is illustrated in the following:




Actor 
A1
 sends messages 
M1
, 
M2
, 
M3
 to 
A2


Actor 
A3
 sends messages 
M4
, 
M5
, 
M6
 to 
A2




This means that:




If 
M1
 is delivered it must be delivered before 
M2
 and 
M3


If 
M2
 is delivered it must be delivered before 
M3


If 
M4
 is delivered it must be delivered before 
M5
 and 
M6


If 
M5
 is delivered it must be delivered before 
M6


A2
 can see messages from 
A1
 interleaved with messages from 
A3


Since there is no guaranteed delivery, any of the messages may be dropped, i.e. not arrive at 
A2


Note


It is important to note that Akkaâs guarantee applies to the order in which messages are enqueued into the recipientâs mailbox. If the mailbox implementation does not respect FIFO order (e.g. a 
PriorityMailbox
), then the order of processing by the actor can deviate from the enqueueing order.


Please note that this rule is 
not transitive
:




Actor 
A
 sends message 
M1
 to actor 
C


Actor 
A
 then sends message 
M2
 to actor 
B


Actor 
B
 forwards message 
M2
 to actor 
C


Actor 
C
 may receive 
M1
 and 
M2
 in any order




Causal transitive ordering would imply that 
M2
 is never received before 
M1
 at actor 
C
 (though any of them might be lost). This ordering can be violated due to different message delivery latencies when 
A
, 
B
 and 
C
 reside on different network hosts, see more below.
Note


Actor creation is treated as a message sent from the parent to the child, with the same semantics as discussed above. Sending a message to an actor in a way which could be reordered with this initial creation message means that the message might not arrive because the actor does not exist yet. An example where the message might arrive too early would be to create a remote-deployed actor R1, send its reference to another remote actor R2 and have R2 send a message to R1. An example of well-defined ordering is a parent which creates an actor and immediately sends a message to it.


Communication of failure


Please note, that the ordering guarantees discussed above only hold for user messages between actors. Failure of a child of an actor is communicated by special system messages that are not ordered relative to ordinary user messages. In particular:




Child actor 
C
 sends message 
M
 to its parent 
P


Child actor fails with failure 
F


Parent actor 
P
 might receive the two events either in order 
M
, 
F
 or 
F
, 
M




The reason for this is that internal system messages have their own mailboxes therefore the ordering of enqueue calls of a user and system messages cannot guarantee the ordering of their dequeue times.


The Rules for In-JVM (Local) Message Sends


Be careful what you do with this section!


Relying on the stronger reliability in this section is not recommended since it will bind your application to local-only deployment: an application may have to be designed differently (as opposed to just employing some message exchange patterns local to some actors) in order to be fit for running on a cluster of machines. Our credo is âdesign once, deploy any way you wishâ, and to achieve this you should only rely on 
The General Rules
.


Reliability of Local Message Sends


The Akka test suite relies on not losing messages in the local context (and for non-error condition tests also for remote deployment), meaning that we actually do apply the best effort to keep our tests stable. A local 
tell
tell
 operation can however fail for the same reasons as a normal method call can on the JVM:




StackOverflowError


OutOfMemoryError


other 
VirtualMachineError




In addition, local sends can fail in Akka-specific ways:




if the mailbox does not accept the message (e.g. full 
BoundedMailbox
BoundedMailbox
)


if the receiving actor fails while processing the message or is already terminated




While the first is a matter of configuration the second deserves some thought: the sender of a message does not get feedback if there was an exception while processing, that notification goes to the supervisor instead. This is in general not distinguishable from a lost message for an outside observer.


Ordering of Local Message Sends


Assuming strict FIFO mailboxes the aforementioned caveat of non-transitivity of the message ordering guarantee is eliminated under certain conditions. As you will note, these are quite subtle as it stands, and it is even possible that future performance optimizations will invalidate this whole paragraph. The possibly non-exhaustive list of counter-indications is:




Before receiving the first reply from a top-level actor, there is a lock which protects an internal interim queue, and this lock is not fair; the implication is that enqueue requests from different senders which arrive during the actorâs construction (figuratively, the details are more involved) may be reordered depending on low-level thread scheduling. Since completely fair locks do not exist on the JVM this is unfixable.


The same mechanism is used during the construction of a Router, more precisely the routed ActorRef, hence the same problem exists for actors deployed with Routers.


As mentioned above, the problem occurs anywhere a lock is involved during enqueueing, which may also apply to custom mailboxes.




This list has been compiled carefully, but other problematic scenarios may have escaped our analysis.


How does Local Ordering relate to Network Ordering


The rule that 
for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order
 holds for messages sent over the network with the TCP based Akka remote transport protocol.


As explained in the previous section local message sends obey transitive causal ordering under certain conditions. This ordering can be violated due to different message delivery latencies. For example:




Actor 
A
 on node-1 sends message 
M1
 to actor 
C
 on node-3


Actor 
A
 on node-1 then sends message 
M2
 to actor 
B
 on node-2


Actor 
B
 on node-2 forwards message 
M2
 to actor 
C
 on node-3


Actor 
C
 may receive 
M1
 and 
M2
 in any order




It might take longer time for 
M1
 to “travel” to node-3 than it takes for 
M2
 to “travel” to node-3 via node-2.


Higher-level abstractions


Based on a small and consistent tool set in Akka’s core, Akka also provides powerful, higher-level abstractions on top of it.


Messaging Patterns


As discussed above a straight-forward answer to the requirement of reliable delivery is an explicit ACKâRETRY protocol. In its simplest form this requires




a way to identify individual messages to correlate message with acknowledgement


a retry mechanism which will resend messages if not acknowledged in time


a way for the receiver to detect and discard duplicates




The third becomes necessary by virtue of the acknowledgements not being guaranteed to arrive either.


An ACK-RETRY protocol with business-level acknowledgements and de-duplication using identifiers is supported by the 
Reliable Delivery
 feature.


Another way of implementing the third part would be to make processing the messages idempotent on the level of the business logic.


Event Sourcing


Event Sourcing (and sharding) is what makes large websites scale to billions of users, and the idea is quite simple: when a component (think actor) processes a command it will generate a list of events representing the effect of the command. These events are stored in addition to being applied to the componentâs state. The nice thing about this scheme is that events only ever are appended to the storage, nothing is ever mutated; this enables perfect replication and scaling of consumers of this event stream (i.e. other components may consume the event stream as a means to replicate the componentâs state on a different continent or to react to changes). If the componentâs state is lostâdue to a machine failure or by being pushed out of a cacheâit can be reconstructed by replaying the event stream (usually employing snapshots to speed up the process). 
Event Sourcing
 is supported by Akka Persistence.


Mailbox with Explicit Acknowledgement


By implementing a custom mailbox type it is possible to retry message processing at the receiving actorâs end in order to handle temporary failures. This pattern is mostly useful in the local communication context where delivery guarantees are otherwise sufficient to fulfill the applicationâs requirements.


Please note that the caveats for 
The Rules for In-JVM (Local) Message Sends
 do apply.




Dead Letters


Messages which cannot be delivered (and for which this can be ascertained) will be delivered to a synthetic actor called 
/deadLetters
. This delivery happens on a best-effort basis; it may fail even within the local JVM (e.g. during actor termination). Messages sent via unreliable network transports will be lost without turning up as dead letters.


What Should I Use Dead Letters For?


The main use of this facility is for debugging, especially if an actor send does not arrive consistently (where usually inspecting the dead letters will tell you that the sender or recipient was set wrong somewhere along the way). In order to be useful for this purpose it is good practice to avoid sending to deadLetters where possible, i.e. run your application with a suitable dead letter logger (see more below) from time to time and clean up the log output. This exerciseâlike all elseârequires judicious application of common sense: it may well be that avoiding to send to a terminated actor complicates the senderâs code more than is gained in debug output clarity.


The dead letter service follows the same rules with respect to delivery guarantees as all other message sends, hence it cannot be used to implement guaranteed delivery.


How do I Receive Dead Letters?


An actor can subscribe to class 
akka.actor.DeadLetter
akka.actor.DeadLetter
 on the event stream, see 
Event Stream
 for how to do that. The subscribed actor will then receive all dead letters published in the (local) system from that point onwards. Dead letters are not propagated over the network, if you want to collect them in one place you will have to subscribe one actor per network node and forward them manually. Also consider that dead letters are generated at that node which can determine that a send operation is failed, which for a remote send can be the local system (if no network connection can be established) or the remote one (if the actor you are sending to does not exist at that point in time).


Dead Letters Which are (Usually) not Worrisome


Every time an actor does not terminate by its own decision, there is a chance that some messages which it sends to itself are lost. There is one which happens quite easily in complex shutdown scenarios that is usually benign: seeing instances of a graceful stop command for an actor being dropped means that two stop requests were given, but only one can succeed. In the same vein, you might see 
akka.actor.Terminated
akka.actor.Terminated
 messages from children while stopping a hierarchy of actors turning up in dead letters if the parent is still watching the child when the parent terminates.














 
Akka and the Java Memory Model






Configuration 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/choosing-cluster.html
Choosing Akka Cluster • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Microservices


Traditional distributed application


Distributed monolith






Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Microservices


Traditional distributed application


Distributed monolith






Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic






















Choosing Akka Cluster


An architectural choice you have to make is if you are going to use a microservices architecture or a traditional distributed application. This choice will influence how you should use Akka Cluster.


The 
Stateful or Stateless applications: to Akka Cluster or not
 video is a good starting point to understand the motivation to use Akka Cluster.


Microservices


Microservices has many attractive properties, such as the independent nature of microservices allows for multiple smaller and more focused teams that can deliver new functionality more frequently and can respond quicker to business opportunities. Reactive Microservices should be isolated, autonomous, and have a single responsibility as identified by Jonas BonÃ©r in the book 
Reactive Microsystems: The Evolution of Microservices at Scale
.


In a microservices architecture, you should consider communication within a service and between services.


In general we recommend against using Akka Cluster and actor messaging between 
different
 services because that would result in a too tight code coupling between the services and difficulties deploying these independent of each other, which is one of the main reasons for using a microservices architecture. See the discussion on 
Internal and External Communication
 for some background on this.


Nodes of a single service (collectively called a cluster) require less decoupling. They share the same code and are deployed together, as a set, by a single team or individual. There might be two versions running concurrently during a rolling deployment, but deployment of the entire set has a single point of control. For this reason, intra-service communication can take advantage of Akka Cluster, failure management and actor messaging, which is convenient to use and has great performance.


Between different services 
Akka HTTP
 or 
Akka gRPC
 can be used for synchronous (yet non-blocking) communication and 
Akka Streams Kafka
 or other 
Alpakka
 connectors for integration asynchronous communication. All those communication mechanisms work well with streaming of messages with end-to-end back-pressure, and the synchronous communication tools can also be used for single request response interactions. It is also important to note that when using these tools both sides of the communication do not have to be implemented with Akka, nor does the programming language matter.


Traditional distributed application


We acknowledge that microservices also introduce many new challenges and it’s not the only way to build applications. A traditional distributed application may have less complexity and work well in many cases. For example for a small startup, with a single team, building an application where time to market is everything. Akka Cluster can efficiently be used for building such distributed application.


In this case, you have a single deployment unit, built from a single code base (or using traditional binary dependency management to modularize) but deployed across many nodes using a single cluster. Tighter coupling is OK, because there is a central point of deployment and control. In some cases, nodes may have specialized runtime roles which means that the cluster is not totally homogenous (e.g., “front-end” and “back-end” nodes, or dedicated master/worker nodes) but if these are run from the same built artifacts this is just a runtime behavior and doesn’t cause the same kind of problems you might get from tight coupling of totally separate artifacts.


A tightly coupled distributed application has served the industry and many Akka users well for years and is still a valid choice.


Distributed monolith


There is also an anti-pattern that is sometimes called “distributed monolith”. You have multiple services that are built and deployed independently from each other, but they have a tight coupling that makes this very risky, such as a shared cluster, shared code and dependencies for service API calls, or a shared database schema. There is a false sense of autonomy because of the physical separation of the code and deployment units, but you are likely to encounter problems because of changes in the implementation of one service leaking into the behavior of others.


Organizations that find themselves in this situation often react by trying to centrally coordinate deployment of multiple services, at which point you have lost the principal benefit of microservices while taking on the costs. You are in a halfway state with things that aren’t really separable being built and deployed in a separate way. Some people do this, and some manage to make it work, but it’s not something we would recommend and it needs to be carefully managed.














 
Coordination






Persistence (Event Sourcing) 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/rolling-updates.html
Rolling Updates • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates




Serialization Compatibility


Cluster Sharding


Cluster Singleton


Cluster Shutdown


Configuration Compatibility Checks


Rolling Updates and Migrating Akka


When Shutdown Startup Is Required




Building Native Images




Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates




Serialization Compatibility


Cluster Sharding


Cluster Singleton


Cluster Shutdown


Configuration Compatibility Checks


Rolling Updates and Migrating Akka


When Shutdown Startup Is Required




Building Native Images




Project Information


Akka Classic




















Rolling Updates
Note


There are a few instances 
when a full cluster restart is required
 versus being able to do a rolling update.


A rolling update is the process of replacing one version of the system with another without downtime. The changes can be new code, changed dependencies such as new Akka version, or modified configuration.


In Akka, rolling updates are typically used for a stateful Akka Cluster where you can’t run two separate clusters in parallel during the update, for example in blue green deployments.


For rolling updates related to Akka dependency version upgrades and the migration guides, please see 
Rolling Updates and Akka versions


Serialization Compatibility


There are two parts of Akka that need careful consideration when performing an rolling update.




Compatibility of remote message protocols. Old nodes may send messages to new nodes and vice versa.


Serialization format of persisted events and snapshots. New nodes must be able to read old data, and  during the update old nodes must be able to read data stored by new nodes.




There are many more application specific aspects for serialization changes during rolling updates to consider. For example based on the use case and requirements, whether to allow dropped messages or tear down the TCP connection when the manifest is unknown. When some message loss during a rolling update is acceptable versus a full shutdown and restart, assuming the application recovers afterwards * If a 
java.io.NotSerializableException
 is thrown in 
fromBinary
 this is treated as a transient problem, the issue logged and the message is dropped * If other exceptions are thrown it can be an indication of corrupt bytes from the underlying transport, and the connection is broken


For more zero-impact rolling updates, it is important to consider a strategy for serialization that can be evolved. One approach to retiring a serializer without downtime is described in 
two rolling update steps to switch to the new serializer
. Additionally you can find advice on 
Persistence - Schema Evolution
 which also applies to remote messages when deploying with rolling updates.


Cluster Sharding


During a rolling update, sharded entities receiving traffic may be moved, based on the pluggable allocation strategy and settings. When an old node is stopped the shards that were running on it are moved to one of the other remaining nodes in the cluster when messages are sent to those shards.


To make rolling updates as smooth as possible there is a configuration property that defines the version of the application. This is used by rolling update features to distinguish between old and new nodes. For example, the default 
LeastShardAllocationStrategy
 avoids allocating shards to old nodes during a rolling update. The 
LeastShardAllocationStrategy
 sees that there is rolling update in progress when there are members with different configured 
app-version
.


To make use of this feature you need to define the 
app-version
 and increase it for each rolling update.


akka.cluster.app-version = 1.2.3



To understand which is old and new it compares the version numbers using normal conventions, see 
Version
Version
 for more details.


When using 
Kubernetes Deployments
 with 
RollingUpdate
 strategy you should enable the 
app-version from Deployment feature from Akka Management
 to automatically define the 
app-version
 from the Kubernetes 
deployment.kubernetes.io/revision
 annotation.


Rebalance is also disabled during rolling updates, since shards from stopped nodes are anyway supposed to be started on new nodes. Messages to shards that were stopped on the old nodes will allocate corresponding shards on the new nodes, without waiting for rebalance actions. 


You should also enable the 
health check for Cluster Sharding
 if you use Akka Management. The readiness check will delay incoming traffic to the node until Sharding has been initialized and can accept messages.


The 
ShardCoordinator
 is itself a cluster singleton. To minimize downtime of the shard coordinator, see the strategies about 
ClusterSingleton
 rolling updates below.


A few specific changes to sharding configuration require 
a full cluster restart
.


Cluster Singleton


Cluster singletons are always running on the oldest node. To avoid moving cluster singletons more than necessary during a rolling update, it is recommended to upgrade the oldest node last. This way cluster singletons are only moved once during a full rolling update. Otherwise, in the worst case cluster singletons may be migrated from node to node which requires coordination and initialization overhead several times.


When using 
Kubernetes Deployments
 with 
RollingUpdate
 strategy you should enable the 
Kubernetes Rolling Updates feature from Akka Management
 to delete pods in the preferred order.


Cluster Shutdown


Graceful shutdown


For rolling updates it is best to leave the Cluster gracefully via 
Coordinated Shutdown
, which will run automatically on SIGTERM, when the Cluster node sees itself as 
Exiting
. Environments such as Kubernetes send a SIGTERM, however if the JVM is wrapped with a script ensure that it forwards the signal. 
Graceful shutdown
 of Cluster Singletons and Cluster Sharding similarly happen automatically.


Ungraceful shutdown


In case of network failures it may still be necessary to set the node’s status to Down in order to complete the removal. 
Cluster Downing
 details downing nodes and downing providers. 
Split Brain Resolver
 can be used to ensure the cluster continues to function during network partitions and node failures. For example if there is an unreachability problem Split Brain Resolver would make a decision based on the configured downing strategy. 


Configuration Compatibility Checks


During rolling updates the configuration from existing nodes should pass the Cluster configuration compatibility checks. For example, it is possible to migrate Cluster Sharding from Classic to Typed Actors in a rolling update using a two step approach as of Akka version 
2.5.23
:




Deploy with the new nodes set to 
akka.cluster.configuration-compatibility-check.enforce-on-join = off
 and ensure all nodes are in this state


Deploy again and with the new nodes set to 
akka.cluster.configuration-compatibility-check.enforce-on-join = on
.




Full documentation about enforcing these checks on joining nodes and optionally adding custom checks can be found in
Akka Cluster configuration compatibility checks
.


Rolling Updates and Migrating Akka


From Java serialization to Jackson


If you are migrating from Akka 2.5 to 2.6, and use Java serialization you can replace it with, for example, the new 
Serialization with Jackson
 and still be able to perform a rolling updates without bringing down the entire cluster.


The procedure for changing from Java serialization to Jackson would look like:




Rolling update from 2.5.24 (or later) to 2.6.0
    


Use config 
akka.actor.allow-java-serialization=on
.


Roll out the change.


Java serialization will be used as before.


This step is optional and you could combine it with next step if you like, but could be good to  make one change at a time.






Rolling update to support deserialization but not enable serialization
    


Change message classes by adding the marker interface and possibly needed annotations as  described in 
Serialization with Jackson
.


Test the system with the new serialization in a new test cluster (no rolling update).


Remove the binding for the marker interface in 
akka.actor.serialization-bindings
, so that Jackson is not used for serialization (toBinary) yet.


Configure 
akka.serialization.jackson.allowed-class-prefix=["com.myapp"]




This is needed for Jackson deserialization when the 
serialization-bindings
 isn’t defined.


Replace 
com.myapp
 with the name of the root package of your application to trust all classes.






Roll out the change.


Java serialization is still used, but this version is prepared for next roll out.






Rolling update to enable serialization with Jackson.
    


Add the binding to the marker interface in 
akka.actor.serialization-bindings
 to the Jackson serializer.


Remove 
akka.serialization.jackson.allowed-class-prefix
.


Roll out the change.


Old nodes will still send messages with Java serialization, and that can still be deserialized by new nodes.


New nodes will send messages with Jackson serialization, and old node can deserialize those because they were  prepared in previous roll out.






Rolling update to disable Java serialization
    


Remove 
allow-java-serialization
 config, to use the default 
allow-java-serialization=off
.


Remove 
warn-about-java-serializer-usage
 config if you had changed that, to use the default 
warn-about-java-serializer-usage=on
.


Roll out the change.








A similar approach can be used when changing between other serializers, for example between Jackson and Protobuf. 


Akka Typed with Receptionist or Cluster Receptionist


If you are migrating from Akka 2.5 to 2.6, and use the 
Receptionist
 or 
Cluster Receptionist
 with Akka Typed, during a rolling update information will not be disseminated between 2.5 and 2.6 nodes. However once all old nodes have been phased out during the rolling update it will work properly again.


When Shutdown Startup Is Required


There are a few instances when a full shutdown and startup is required versus being able to do a rolling update.


Cluster Sharding configuration change


If you need to change any of the following aspects of sharding it will require a full cluster restart versus a rolling update:




The 
extractShardId
 function


The role that the shard regions run on


The persistence mode - It’s important to use the same mode on all nodes in the cluster


The 
number-of-shards
 - Note: changing the number  of nodes in the cluster does not require changing the number of shards.




Cluster configuration change




A full restart is required if you change the 
SBR strategy




Migrating from PersistentFSM to EventSourcedBehavior


If you’ve migrated from 
PersistentFSM
 to 
EventSourcedBehavior
 (See the 
Akka 2.8 migration guide
) and are using PersistenceFSM with Cluster Sharding, a full shutdown is required as shards can move between new and old nodes.


Migrating from classic remoting to Artery


If you’ve migrated from classic remoting to Artery which has a completely different protocol, a rolling update is not supported. For more details on this migration see 
the migration guide
.


Changing remoting transport


Rolling update is not supported when 
changing the remoting transport
.


Migrating from Classic Sharding to Typed Sharding


If you have been using classic sharding it is possible to do a rolling update to typed sharding using a 3 step procedure. The steps along with example commits are detailed in 
this sample PR
 














 
Deploying






Building Native Images 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-io.html
Working with streaming IO • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO




Dependency


Introduction


Streaming TCP


Streaming File IO




StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO




Dependency


Introduction


Streaming TCP


Streaming File IO




StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Working with streaming IO


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


Akka Streams provides a way of handling File IO and TCP connections with Streams. While the general approach is very similar to the 
Actor based TCP handling
 using Akka IO, by using Akka Streams you are freed of having to manually react to back-pressure signals, as the library does it transparently for you.


Streaming TCP


Accepting connections: Echo Server


In order to implement a simple EchoServer we 
bind
 to a given address, which returns a 
Source
Source
[
IncomingConnection
IncomingConnection
, 
Future
[
ServerBinding
ServerBinding
]
Source
Source
<
IncomingConnection
IncomingConnection
, 
CompletionStage
<
ServerBinding
ServerBinding
>>
, which will emit an 
IncomingConnection
 element for each new connection that the Server should handle:




Scala




copy
source
val binding: Future[ServerBinding] =
  Tcp(system).bind("127.0.0.1", 8888).to(Sink.ignore).run()

binding.map { b =>
  b.unbind().onComplete {
    case _ => // ...
  }
}


Java




copy
source
// IncomingConnection and ServerBinding imported from Tcp
final Source<IncomingConnection, CompletionStage<ServerBinding>> connections =
    Tcp.get(system).bind("127.0.0.1", 8888);






Next, we handle 
each
 incoming connection using a 
Flow
Flow
 which will be used as the operator to handle and emit 
ByteString
ByteString
 s from and to the TCP Socket. Since one 
ByteString
 does not have to necessarily correspond to exactly one line of text (the client might be sending the line in chunks) we use the 
Framing.delimiter
Framing.delimiter
 helper Flow to chunk the inputs up into actual lines of text. The last boolean argument indicates that we require an explicit line ending even for the last message before the connection is closed. In this example we add exclamation marks to each incoming text message and push it through the flow:




Scala




copy
source
import akka.stream.scaladsl.Framing

val connections: Source[IncomingConnection, Future[ServerBinding]] =
  Tcp(system).bind(host, port)
connections.runForeach { connection =>
  println(s"New connection from: ${connection.remoteAddress}")

  val echo = Flow[ByteString]
    .via(Framing.delimiter(ByteString("\n"), maximumFrameLength = 256, allowTruncation = true))
    .map(_.utf8String)
    .map(_ + "!!!\n")
    .map(ByteString(_))

  connection.handleWith(echo)
}


Java




copy
source
connections.runForeach(
    connection -> {
      System.out.println("New connection from: " + connection.remoteAddress());

      final Flow<ByteString, ByteString, NotUsed> echo =
          Flow.of(ByteString.class)
              .via(
                  Framing.delimiter(
                      ByteString.fromString("\n"), 256, FramingTruncation.DISALLOW))
              .map(ByteString::utf8String)
              .map(s -> s + "!!!\n")
              .map(ByteString::fromString);

      connection.handleWith(echo, system);
    },
    system);






Notice that while most building blocks in Akka Streams are reusable and freely shareable, this is 
not
 the case for the incoming connection Flow, since it directly corresponds to an existing, already accepted connection its handling can only ever be materialized 
once
.


Closing connections is possible by cancelling the 
incoming connection
 
Flow
Flow
 from your server logic (e.g. by connecting its downstream to a 
Sink.cancelled
Sink.cancelled
 and its upstream to a 
Source.empty
Source.empty
. It is also possible to shut down the server’s socket by cancelling the 
IncomingConnection
IncomingConnection
 source 
connections
.


We can then test the TCP server by sending data to the TCP Socket using 
netcat
:


$ echo -n "Hello World" | netcat 127.0.0.1 8888
Hello World!!!



Connecting: REPL Client


In this example we implement a rather naive Read Evaluate Print Loop client over TCP. Let’s say we know a server has exposed a simple command line interface over TCP, and would like to interact with it using Akka Streams over TCP. To open an outgoing connection socket we use the 
outgoingConnection
 method:




Scala




copy
source
val connection = Tcp(system).outgoingConnection("127.0.0.1", 8888)

val replParser =
  Flow[String].takeWhile(_ != "q").concat(Source.single("BYE")).map(elem => ByteString(s"$elem\n"))

val repl = Flow[ByteString]
  .via(Framing.delimiter(ByteString("\n"), maximumFrameLength = 256, allowTruncation = true))
  .map(_.utf8String)
  .map(text => println("Server: " + text))
  .map(_ => readLine("> "))
  .via(replParser)

val connected = connection.join(repl).run()


Java




copy
source
final Flow<ByteString, ByteString, CompletionStage<OutgoingConnection>> connection =
    Tcp.get(system).outgoingConnection("127.0.0.1", 8888);
final Flow<String, ByteString, NotUsed> replParser =
    Flow.<String>create()
        .takeWhile(elem -> !elem.equals("q"))
        .concat(Source.single("BYE")) // will run after the original flow completes
        .map(elem -> ByteString.fromString(elem + "\n"));

final Flow<ByteString, ByteString, NotUsed> repl =
    Flow.of(ByteString.class)
        .via(Framing.delimiter(ByteString.fromString("\n"), 256, FramingTruncation.DISALLOW))
        .map(ByteString::utf8String)
        .map(
            text -> {
              System.out.println("Server: " + text);
              return "next";
            })
        .map(elem -> readLine("> "))
        .via(replParser);

CompletionStage<OutgoingConnection> connectionCS = connection.join(repl).run(system);




The 
repl
 flow we use to handle the server interaction first prints the servers response, then awaits on input from the command line (this blocking call is used here for the sake of simplicity) and converts it to a 
ByteString
ByteString
 which is then sent over the wire to the server. Then we connect the TCP pipeline to this operatorâat this point it will be materialized and start processing data once the server responds with an 
initial message
.


A resilient REPL client would be more sophisticated than this, for example it should split out the input reading into a separate mapAsync step and have a way to let the server write more data than one ByteString chunk at any given time, these improvements however are left as exercise for the reader.


Avoiding deadlocks and liveness issues in back-pressured cycles


When writing such end-to-end back-pressured systems you may sometimes end up in a situation of a loop, in which 
either side is waiting for the other one to start the conversation
. One does not need to look far to find examples of such back-pressure loops. In the two examples shown previously, we always assumed that the side we are connecting to would start the conversation, which effectively means both sides are back-pressured and can not get the conversation started. There are multiple ways of dealing with this which are explained in depth in 
Graph cycles, liveness and deadlocks
, however in client-server scenarios it is often the simplest to make either side send an initial message.
Note


In case of back-pressured cycles (which can occur even between different systems) sometimes you have to decide which of the sides has start the conversation in order to kick it off. This can be often done by injecting an initial message from one of the sidesâa conversation starter.


To break this back-pressure cycle we need to inject some initial message, a “conversation starter”. First, we need to decide which side of the connection should remain passive and which active. Thankfully in most situations finding the right spot to start the conversation is rather simple, as it often is inherent to the protocol we are trying to implement using Streams. In chat-like applications, which our examples resemble, it makes sense to make the Server initiate the conversation by emitting a “hello” message:




Scala




copy
source
connections
  .to(Sink.foreach { connection =>
    // server logic, parses incoming commands
    val commandParser = Flow[String].takeWhile(_ != "BYE").map(_ + "!")

    import connection._
    val welcomeMsg = s"Welcome to: $localAddress, you are: $remoteAddress!"
    val welcome = Source.single(welcomeMsg)

    val serverLogic = Flow[ByteString]
      .via(Framing.delimiter(ByteString("\n"), maximumFrameLength = 256, allowTruncation = true))
      .map(_.utf8String)
      .via(commandParser)
      // merge in the initial banner after parser
      .merge(welcome)
      .map(_ + "\n")
      .map(ByteString(_))

    connection.handleWith(serverLogic)
  })
  .run()


Java




copy
source
connections
    .to(
        Sink.foreach(
            (IncomingConnection connection) -> {
              // server logic, parses incoming commands
              final Flow<String, String, NotUsed> commandParser =
                  Flow.<String>create()
                      .takeWhile(elem -> !elem.equals("BYE"))
                      .map(elem -> elem + "!");

              final String welcomeMsg =
                  "Welcome to: "
                      + connection.localAddress()
                      + " you are: "
                      + connection.remoteAddress()
                      + "!";

              final Source<String, NotUsed> welcome = Source.single(welcomeMsg);
              final Flow<ByteString, ByteString, NotUsed> serverLogic =
                  Flow.of(ByteString.class)
                      .via(
                          Framing.delimiter(
                              ByteString.fromString("\n"), 256, FramingTruncation.DISALLOW))
                      .map(ByteString::utf8String)
                      .via(commandParser)
                      .merge(welcome)
                      .map(s -> s + "\n")
                      .map(ByteString::fromString);

              connection.handleWith(serverLogic, system);
            }))
    .run(system);




To emit the initial message we merge a 
Source
Source
 with a single element, after the command processing but before the framing and transformation to 
ByteString
ByteString
 s this way we do not have to repeat such logic.


In this example both client and server may need to close the stream based on a parsed command - 
BYE
 in the case of the server, and 
q
 in the case of the client. This is implemented by 
taking from the stream until 
q
 and and concatenating a 
Source
 with a single 
BYE
 element which will then be sent after the original source completed
using a custom operator extending 
GraphStage
 which completes the stream once it encounters such command
.


Using framing in your protocol


Streaming transport protocols like TCP only pass streams of bytes, and does not know what is a logical chunk of bytes from the application’s point of view. Often when implementing network protocols you will want to introduce your own framing. This can be done in two ways: An end-of-frame marker, e.g. end line 
\n
, can do framing via 
Framing.delimiter
Framing.delimiter
. Or a length-field can be used to build a framing protocol. There is a bidi implementing this protocol provided by 
Framing.simpleFramingProtocol
Framing.simpleFramingProtocol
.


JsonFraming
JsonFraming
 separates valid JSON objects from incoming 
ByteString
ByteString
 objects:




Scala




copy
source
val input =
  """
    |[
    | { "name" : "john" },
    | { "name" : "Ãg get etiÃ° gler Ã¡n Ã¾ess aÃ° meiÃ°a mig" },
    | { "name" : "jack" },
    |]
    |""".stripMargin // also should complete once notices end of array

val result =
  Source.single(ByteString(input)).via(JsonFraming.objectScanner(Int.MaxValue)).runFold(Seq.empty[String]) {
    case (acc, entry) => acc ++ Seq(entry.utf8String)
  }


Java




copy
source
String input =
    "[{ \"name\" : \"john\" }, { \"name\" : \"Ãg get etiÃ° gler Ã¡n Ã¾ess aÃ° meiÃ°a mig\" }, { \"name\" : \"jack\" }]";
CompletionStage<ArrayList<String>> result =
    Source.single(ByteString.fromString(input))
        .via(JsonFraming.objectScanner(Integer.MAX_VALUE))
        .runFold(
            new ArrayList<String>(),
            (acc, entry) -> {
              acc.add(entry.utf8String());
              return acc;
            },
            system);




TLS


Similar factories as shown above for raw TCP but where the data is encrypted using TLS are available from 
Tcp
 through 
outgoingConnectionWithTls
, 
bindWithTls
 and 
bindAndHandleWithTls
, see the 
`Tcp Scaladoc`
`Tcp Javadoc`
 for details.


Using TLS requires a keystore and a truststore and then a somewhat involved dance of configuring the SSLEngine and the details for how the session should be negotiated:




Scala




copy
source
import java.security.KeyStore
import javax.net.ssl.KeyManagerFactory
import javax.net.ssl.SSLContext
import javax.net.ssl.SSLEngine
import javax.net.ssl.TrustManagerFactory

import akka.stream.TLSRole

// initialize SSLContext once
lazy val sslContext: SSLContext = {
  // Don't hardcode your password in actual code
  val password = "abcdef".toCharArray

  // trust store and keys in one keystore
  val keyStore = KeyStore.getInstance("PKCS12")
  keyStore.load(getClass.getResourceAsStream("/tcp-spec-keystore.p12"), password)

  val trustManagerFactory = TrustManagerFactory.getInstance("SunX509")
  trustManagerFactory.init(keyStore)

  val keyManagerFactory = KeyManagerFactory.getInstance("SunX509")
  keyManagerFactory.init(keyStore, password)

  // init ssl context
  val context = SSLContext.getInstance("TLSv1.2")
  context.init(keyManagerFactory.getKeyManagers, trustManagerFactory.getTrustManagers, new SecureRandom)
  context
}

// create new SSLEngine from the SSLContext, which was initialized once
def createSSLEngine(role: TLSRole): SSLEngine = {
  val engine = sslContext.createSSLEngine()

  engine.setUseClientMode(role == akka.stream.Client)
  engine.setEnabledCipherSuites(Array("TLS_RSA_WITH_AES_128_CBC_SHA"))
  engine.setEnabledProtocols(Array("TLSv1.2"))

  engine
}


Java




copy
source
// imports
import java.security.KeyStore;
import java.security.SecureRandom;
import javax.net.ssl.KeyManagerFactory;
import javax.net.ssl.SSLContext;
import javax.net.ssl.SSLEngine;
import javax.net.ssl.TrustManagerFactory;
import akka.stream.TLSRole;

  // initialize SSLContext once
  private final SSLContext sslContext;

  {
    try {
      // Don't hardcode your password in actual code
      char[] password = "abcdef".toCharArray();

      // trust store and keys in one keystore
      KeyStore keyStore = KeyStore.getInstance("PKCS12");
      keyStore.load(getClass().getResourceAsStream("/tcp-spec-keystore.p12"), password);

      TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance("SunX509");
      trustManagerFactory.init(keyStore);

      KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance("SunX509");
      keyManagerFactory.init(keyStore, password);

      // init ssl context
      SSLContext context = SSLContext.getInstance("TLSv1.2");
      context.init(
          keyManagerFactory.getKeyManagers(),
          trustManagerFactory.getTrustManagers(),
          new SecureRandom());

      sslContext = context;

    } catch (KeyStoreException
        | IOException
        | NoSuchAlgorithmException
        | CertificateException
        | UnrecoverableKeyException
        | KeyManagementException e) {
      throw new RuntimeException(e);
    }
  }

  // create new SSLEngine from the SSLContext, which was initialized once
  public SSLEngine createSSLEngine(TLSRole role) {
    SSLEngine engine = sslContext.createSSLEngine();

    engine.setUseClientMode(role.equals(akka.stream.TLSRole.client()));
    engine.setEnabledCipherSuites(new String[] {"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384"});
    engine.setEnabledProtocols(new String[] {"TLSv1.2"});

    return engine;
  }




The 
SSLEngine
 instance can then be used with the binding or outgoing connection factory methods.


Streaming File IO


Akka Streams provide simple Sources and Sinks that can work with 
ByteString
ByteString
 instances to perform IO operations on files.


Streaming data from a file is as easy as creating a 
FileIO.fromPath
 given a target path, and an optional 
chunkSize
 which determines the buffer size determined as one “element” in such stream:




Scala




copy
source
import akka.stream.scaladsl._
val file = Paths.get("example.csv")

val foreach: Future[IOResult] = FileIO.fromPath(file).to(Sink.ignore).run()


Java




copy
source
final Path file = Paths.get("example.csv");
  Sink<ByteString, CompletionStage<Done>> printlnSink =
      Sink.<ByteString>foreach(chunk -> System.out.println(chunk.utf8String()));

  CompletionStage<IOResult> ioResult = FileIO.fromPath(file).to(printlnSink).run(system);




Please note that these operators are backed by Actors and by default are configured to run on a pre-configured threadpool-backed dispatcher dedicated for File IO. This is very important as it isolates the blocking file IO operations from the rest of the ActorSystem allowing each dispatcher to be utilised in the most efficient way. If you want to configure a custom dispatcher for file IO operations globally, you can do so by changing the 
akka.stream.materializer.blocking-io-dispatcher
, or for a specific operator by specifying a custom Dispatcher in code, like this:




Scala




copy
source
FileIO.fromPath(file).withAttributes(ActorAttributes.dispatcher("custom-blocking-io-dispatcher"))


Java




copy
source
FileIO.toPath(file)
    .withAttributes(ActorAttributes.dispatcher("custom-blocking-io-dispatcher"));
















 
Error Handling in Streams






StreamRefs - Reactive Streams over the network 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/ide.html
IDE Tips • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips




Configure the auto-importer in IntelliJ / Eclipse




Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips




Configure the auto-importer in IntelliJ / Eclipse




Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















IDE Tips


Configure the auto-importer in IntelliJ / Eclipse


For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting 
javadsl
 imports when working in Scala, or viceversa.


In IntelliJ, the auto-importer settings are under “Editor” / “General” / “Auto Import”. Use a name mask such as 
akka.stream.javadsl*
 or 
akka.stream.scaladsl*
 or 
*javadsl*
 or 
*scaladsl*
 to indicate the DSL you want to exclude from import/completion. See screenshot below: 




Eclipse users can configure this aspect of the IDE by going to “Window” / “Preferences” / “Java” / “Appearance” / “Type Filters”. 














 
Modules marked May Change






Immutability using Lombok 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://www.reactiveprinciples.org/
The Reactive Principles :: The Reactive Principles




























 








The 
Reactive
 Principles




















The 
Reactive
 Principles












About this Document






Design Principles for Cloud Native Applications






Design Principles for Edge Native Applications








The Reactive Principles






I. Stay Responsive






II. Accept Uncertainty






III. Embrace Failure






IV. Assert Autonomy






V. Tailor Consistency






VI. Decouple Time






VII. Decouple Space






VIII. Handle Dynamics












The Reactive Patterns






1. Partition State






2. Communicate Facts






3. Isolate Mutations






4. Coordinate Dataflow






5. Localize State






6. Observe Communications










How to Contribute to this Document











      DOWNLOAD AS PDF
      






The 
Reactive
 Manifesto
















master














master
































The Reactive Principles




Design Principles for Distributed Applications














This document provides guidance and techniques established among experienced Reactive practitioners for building individual services, applications, and whole systems. As a companion to the 
Reactive Manifesto
, 
 it incorporates the ideas, paradigms, methods, and patterns from both Reactive Programming and Reactive Systems into a set of practical principles that software architects and developers can apply in their transformative work.






Our purpose is to help businesses realize the efficiencies inherent to using Reactive. Our collective experience shows that these principles enable the design and implementation of  highly concurrent and distributed software that is performant, scalable, and resilient, while at the same time conserving resources when deploying, operating, and maintaining it.  Further application of Reactive principles will allow us as a society to depend on software for making our diverse and distributed civilization more robust.














Written by Jonas Bonér—with help (roughly in order of contribution level) from: Roland Kuhn, Ben Christensen, Peter Vlugter, Josh Long, Sergey Bykov, Ben Hindman, Vaughn Vernon, Clement Escoffier, James Roper, Michael Behrendt, Kresten Krab Thorup, Colin Breck, Allard Buijze, Derek Collison, Viktor Klang, Ben Hale, Steve Gury, Tyler Jewell, James Ward, Stephan Ewen, and Ryland Degnan.




















Design Principles for Cloud Native Applications






Learn










How the Reactive approach to Cloud Native provides benefits






Why Cloud Native infrastructure alone isn’t enough to achieve elasticity, scalability, and resilience
















Read More →












Design Principles for Edge Native Applications






Learn










The constraints Edge Native imposes on design






How to avoid impedance mismatch between Edge Native and Cloud Native applications
















Read More →












The Reactive Principles






Learn










What makes a system Reactive






How to:








Stay responsive






Accept uncertainty






Embrace failure






Assert autonomy






Tailor consistency






Decouple time






Decouple space






Handle dynamics






















Read More →












The Reactive Patterns






Learn how the following patterns contribute to Reactive design










Partition State






Communicate Facts






Isolate Mutations






Coordinate Dataflow






Localize State






Observe Communications
















Read More →














Version 1.0
 
Published 2022-06-15






Design Principles for Cloud Native Applications












 










Authors


Written by Jonas Bonér—with help (roughly in order of contribution level) from: Roland Kuhn, Ben Christensen,
      Peter Vlugter, Josh Long, Sergey Bykov, Ben Hindman, Vaughn Vernon, Clement Escoffier, James Roper, Michael
      Behrendt, Kresten Krab Thorup, Colin Breck, Allard Buijze, Derek Collison, Viktor Klang, Ben Hale, Steve Gury,
      Tyler Jewell, James Ward, Stephan Ewen, and Ryland Degnan.










The 
Reactive
 Principles
 
a
        companion to
 
The
        
Reactive
 Manifesto







    © 2024
    |
    
Privacy Policy

    |
    
Cookie Policy

    |
    
Cookie Settings

URL: https://akkademy.akka.io/learn/public/catalog/view/3
Loading

URL: https://doc.akka.io/libraries/akka/snapshot/persistence-plugins.html
Persistence Plugins • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins




R2DBC plugin


Cassandra plugin


AWS DynamoDB plugin


JDBC plugin


Feature limitations


Enabling a plugin


Eager initialization of persistence plugin


Pre-packaged plugins




Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins




R2DBC plugin


Cassandra plugin


AWS DynamoDB plugin


JDBC plugin


Feature limitations


Enabling a plugin


Eager initialization of persistence plugin


Pre-packaged plugins




Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence Plugins


Storage backends for journals, snapshot stores, durable state stores and persistence queries are pluggable in the Akka persistence extension. The following plugins are maintained by the Akka team.


R2DBC plugin


The Reactive database drivers (R2DBC) support relational databases like PostgreSQL, H2 (As a minimal in-process memory or file based database) and Yugabyte.


The 
Akka Persistence R2DBC plugin
 supports the latest feature additions of Akka Persistence and is generally recommended over the JDBC-based plugin.


Cassandra plugin


Akka supports Cassandra’s data model through 
Akka Persistence Cassandra
.


Some later Akka Persistence feature additions (including 
Durable State
) are not supported by the Cassandra plugin (see below).


AWS DynamoDB plugin


AWS DynamoDB can be used as backend for Akka Persistence with the 
Akka Persistence DynamoDB plugin
.


Durable State
 is not supported by the DynamoDB plugin.


Recovery from only last event is not supported by the DynamoDB plugin.


JDBC plugin


Relational databases with JDBC-drivers are supported through 
Akka Persistence JDBC
. For new projects, the 
R2DBC plugin
 is recommended.


Some later Akka Persistence feature additions are not supported by the Akka Persistence JDBC plugin (see below).


Feature limitations


Example of concrete features 
not
 supported by the Cassandra and JDBC plugins:




eventsBySlices
 query


Projections over gRPC


Replicated Event Sourcing over gRPC


Dynamic scaling of number of Projection instances


Low latency Projections


Projections starting from snapshots


Scalability of many Projections


Durable State entities (partly supported by JDBC plugin)


Recovery from only last event




Enabling a plugin


Plugins can be selected either by “default” for all persistent actors, or “individually”, when a persistent actor defines its own set of plugins.


When a persistent actor does NOT override the 
journalPluginId
 and 
snapshotPluginId
 methods, the persistence extension will use the “default” journal, snapshot-store and durable-state plugins configured in 
reference.conf
:


akka.persistence.journal.plugin = ""
akka.persistence.snapshot-store.plugin = ""
akka.persistence.state.plugin = ""



However, these entries are provided as empty "", and require explicit user configuration via override in the user 
application.conf
.




For an example of a journal plugin which writes messages to LevelDB see 
Local LevelDB journal
.


For an example of a snapshot store plugin which writes snapshots as individual files to the local filesystem see 
Local snapshot store
.


The state store is relatively new, one available implementation is the 
akka-persistence-jdbc-plugin
.




Eager initialization of persistence plugin


By default, persistence plugins are started on-demand, as they are used. In some case, however, it might be beneficial to start a certain plugin eagerly. In order to do that, you should first add 
akka.persistence.Persistence
 under the 
akka.extensions
 key. Then, specify the IDs of plugins you wish to start automatically under 
akka.persistence.journal.auto-start-journals
 and 
akka.persistence.snapshot-store.auto-start-snapshot-stores
.


For example, if you want eager initialization for the leveldb journal plugin and the local snapshot store plugin, your configuration should look like this: 


akka {

  extensions = [akka.persistence.Persistence]

  persistence {

    journal {
      plugin = "akka.persistence.journal.leveldb"
      auto-start-journals = ["akka.persistence.journal.leveldb"]
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    }

  }

}



Pre-packaged plugins


The Akka Persistence module comes with few built-in persistence plugins, but none of these are suitable for production usage in an Akka Cluster. 


Local LevelDB journal


This plugin writes events to a local LevelDB instance.
Warning


The LevelDB plugin cannot be used in an Akka Cluster since the storage is in a local file system.


The LevelDB journal is deprecated and it is not advised to build new applications with it. As a replacement we recommend using 
Akka Persistence JDBC
.


The LevelDB journal plugin config entry is 
akka.persistence.journal.leveldb
. Enable this plugin by defining config property:


copy
source
# Path to the journal plugin to be used
akka.persistence.journal.plugin = "akka.persistence.journal.leveldb"


LevelDB based plugins will also require the following additional dependency declaration:
sbt
libraryDependencies += "org.fusesource.leveldbjni" % "leveldbjni-all" % "1.8"
Maven
<dependencies>
  <dependency>
    <groupId>org.fusesource.leveldbjni</groupId>
    <artifactId>leveldbjni-all</artifactId>
    <version>1.8</version>
  </dependency>
</dependencies>
Gradle
dependencies {
  implementation "org.fusesource.leveldbjni:leveldbjni-all:1.8"
}


The default location of LevelDB files is a directory named 
journal
 in the current working directory. This location can be changed by configuration where the specified path can be relative or absolute:


copy
source
akka.persistence.journal.leveldb.dir = "target/journal"


With this plugin, each actor system runs its own private LevelDB instance.


One peculiarity of LevelDB is that the deletion operation does not remove messages from the journal, but adds a “tombstone” for each deleted message instead. In the case of heavy journal usage, especially one including frequent deletes, this may be an issue as users may find themselves dealing with continuously increasing journal sizes. To this end, LevelDB offers a special journal compaction function that is exposed via the following configuration:


copy
source
# Number of deleted messages per persistence id that will trigger journal compaction
akka.persistence.journal.leveldb.compaction-intervals {
  persistence-id-1 = 100
  persistence-id-2 = 200
  # ...
  persistence-id-N = 1000
  # use wildcards to match unspecified persistence ids, if any
  "*" = 250
}


Shared LevelDB journal


The LevelDB journal is deprecated and will be removed from a future Akka version, it is not advised to build new applications with it. For testing in a multi node environment the “inmem” journal together with the 
proxy plugin
 can be used, but the actual journal used in production of applications is also a good choice.
Note


This plugin has been supplanted by 
Persistence Plugin Proxy
.


A shared LevelDB instance is started by instantiating the 
SharedLeveldbStore
 actor.




Scala




copy
source
import akka.persistence.journal.leveldb.SharedLeveldbStore

val store = system.actorOf(Props[SharedLeveldbStore](), "store")


Java




copy
source
final ActorRef store = system.actorOf(Props.create(SharedLeveldbStore.class), "store");




By default, the shared instance writes journaled messages to a local directory named 
journal
 in the current working directory. The storage location can be changed by configuration:


copy
source
akka.persistence.journal.leveldb-shared.store.dir = "target/shared"


Actor systems that use a shared LevelDB store must activate the 
akka.persistence.journal.leveldb-shared
 plugin.


copy
source
akka.persistence.journal.plugin = "akka.persistence.journal.leveldb-shared"


This plugin must be initialized by injecting the (remote) 
SharedLeveldbStore
 actor reference. Injection is done by calling the 
SharedLeveldbJournal.setStore
 method with the actor reference as argument.




Scala




copy
source
trait SharedStoreUsage extends Actor {
  override def preStart(): Unit = {
    context.actorSelection("akka://
[email protected]
:2552/user/store") ! Identify(1)
  }

  def receive = {
    case ActorIdentity(1, Some(store)) =>
      SharedLeveldbJournal.setStore(store, context.system)
  }
}


Java




copy
source
class SharedStorageUsage extends AbstractActor {
  @Override
  public void preStart() throws Exception {
    String path = "akka://
[email protected]
:2552/user/store";
    ActorSelection selection = getContext().actorSelection(path);
    selection.tell(new Identify(1), getSelf());
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            ActorIdentity.class,
            ai -> {
              if (ai.correlationId().equals(1)) {
                Optional<ActorRef> store = ai.getActorRef();
                if (store.isPresent()) {
                  SharedLeveldbJournal.setStore(store.get(), getContext().getSystem());
                } else {
                  throw new RuntimeException("Couldn't identify store");
                }
              }
            })
        .build();
  }
}




Internal journal commands (sent by persistent actors) are buffered until injection completes. Injection is idempotent i.e. only the first injection is used.


Local snapshot store


This plugin writes snapshot files to the local filesystem.
Warning


The local snapshot store plugin cannot be used in an Akka Cluster since the storage is in a local file system.


The local snapshot store plugin config entry is 
akka.persistence.snapshot-store.local
. Enable this plugin by defining config property:


copy
source
# Path to the snapshot store plugin to be used
akka.persistence.snapshot-store.plugin = "akka.persistence.snapshot-store.local"


The default storage location is a directory named 
snapshots
 in the current working directory. This can be changed by configuration where the specified path can be relative or absolute:


copy
source
akka.persistence.snapshot-store.local.dir = "target/snapshots"


Note that it is not mandatory to specify a snapshot store plugin. If you don’t use snapshots you don’t have to configure it.


Persistence Plugin Proxy


For testing purposes a persistence plugin proxy allows sharing of a journal and snapshot store on a single node across multiple actor systems (on the same or on different nodes). This, for example, allows persistent actors to failover to a backup node and continue using the shared journal instance from the backup node. The proxy works by forwarding all the journal/snapshot store messages to a single, shared, persistence plugin instance, and therefore supports any use case supported by the proxied plugin.
Warning


A shared journal/snapshot store is a single point of failure and should only be used for testing purposes.


The journal and snapshot store proxies are controlled via the 
akka.persistence.journal.proxy
 and 
akka.persistence.snapshot-store.proxy
 configuration entries, respectively. Set the 
target-journal-plugin
 or 
target-snapshot-store-plugin
 keys to the underlying plugin you wish to use (for example: 
akka.persistence.journal.inmem
). The 
start-target-journal
 and 
start-target-snapshot-store
 keys should be set to 
on
 in exactly one actor system - this is the system that will instantiate the shared persistence plugin. Next, the proxy needs to be told how to find the shared plugin. This can be done by setting the 
target-journal-address
 and 
target-snapshot-store-address
 configuration keys, or programmatically by calling the 
PersistencePluginProxy.setTargetLocation
 method.
Note


Akka starts extensions lazily when they are required, and this includes the proxy. This means that in order for the proxy to work, the persistence plugin on the target node must be instantiated. This can be done by instantiating the 
PersistencePluginProxyExtension
 
extension
, or by calling the 
PersistencePluginProxy.start
 method.
Note


The proxied persistence plugin can (and should) be configured using its original configuration keys.














 
Persistence Query for LevelDB






Persistence - Building a storage backend 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster.html
Cluster Usage • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage




Module info


Cluster API Extension


Cluster Membership API


Node Roles


Failure Detector


How to test


Configuration


Higher level Cluster tools


Example project




Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage




Module info


Cluster API Extension


Cluster Membership API


Node Roles


Failure Detector


How to test


Configuration


Higher level Cluster tools


Example project




Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Usage


This document describes how to use Akka Cluster and the Cluster APIs. The 
Stateful or Stateless Applications: To Akka Cluster or not
 video is a good starting point to understand the motivation to use Akka Cluster.


For specific documentation topics see: 




When and where to use Akka Cluster


Cluster Specification


Cluster Membership Service


Higher level Cluster tools


Rolling Updates


Operating, Managing, Observability




You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Cluster
.


You have to enable 
serialization
 to send messages between ActorSystems (nodes) in the Cluster. 
Serialization with Jackson
 is a good choice in many cases, and our recommendation if you don’t have other preferences or constraints.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Cluster add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}




Project Info: Akka Cluster (typed)


Artifact
com.typesafe.akka


akka-cluster-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Cluster API Extension


The Cluster extension gives you access to management tasks such as 
Joining, Leaving and Downing
 and subscription of cluster membership events such as 
MemberUp, MemberRemoved and UnreachableMember
, which are exposed as event APIs. 


It does this through these references on the 
Cluster
Cluster
 extension:




manager
: An 
ActorRef
ActorRef
[
akka.cluster.typed.ClusterCommand
akka.cluster.typed.ClusterCommand
]
ActorRef
ActorRef
<
akka.cluster.typed.ClusterCommand
akka.cluster.typed.ClusterCommand
>
 where a 
ClusterCommand
 is a command such as: 
Join
Join
, 
Leave
Leave
 and 
Down
Down


subscriptions
: An 
ActorRef
ActorRef
[
akka.cluster.typed.ClusterStateSubscription
akka.cluster.typed.ClusterStateSubscription
]
ActorRef
ActorRef
<
akka.cluster.typed.ClusterStateSubscription
akka.cluster.typed.ClusterStateSubscription
>
 where a 
ClusterStateSubscription
 is one of 
GetCurrentState
GetCurrentState
 or 
Subscribe
Subscribe
 and 
Unsubscribe
Unsubscribe
 to cluster events like 
MemberRemoved
MemberRemoved


state
: The current 
CurrentClusterState
CurrentClusterState




All of the examples below assume the following imports:




Scala




copy
source
import akka.actor.typed._
import akka.actor.typed.scaladsl._
import akka.cluster.ClusterEvent._
import akka.cluster.MemberStatus
import akka.cluster.typed._


Java




copy
source
import akka.actor.typed.*;
import akka.actor.typed.javadsl.*;
import akka.cluster.ClusterEvent;
import akka.cluster.typed.*;






The minimum configuration required is to set a host/port for remoting and the 
akka.actor.provider = "cluster"
.


copy
source
akka {
  actor {
    provider = "cluster"
  }
  remote.artery {
    canonical {
      hostname = "127.0.0.1"
      port = 2551
    }
  }

  cluster {
    seed-nodes = [
      "akka://
[email protected]
:2551",
      "akka://
[email protected]
:2552"]
    
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }
}


Accessing the 
Cluster
Cluster
 extension on each node:




Scala




copy
source
val cluster = Cluster(system)


Java




copy
source
Cluster cluster = Cluster.get(system);


Note


The name of the cluster’s 
ActorSystem
ActorSystem
 must be the same for all members, which is passed in when you start the 
ActorSystem
.


Joining and Leaving a Cluster


If not using configuration to specify 
seed nodes to join
, joining the cluster can be done programmatically via the 
manager
manager()
.




Scala




copy
source
cluster.manager ! Join(cluster.selfMember.address)


Java




copy
source
cluster.manager().tell(Join.create(cluster.selfMember().address()));




Leaving
 the cluster and 
downing
 a node are similar:




Scala




copy
source
cluster2.manager ! Leave(cluster2.selfMember.address)


Java




copy
source
cluster2.manager().tell(Leave.create(cluster2.selfMember().address()));




Cluster Subscriptions


Cluster 
subscriptions
subscriptions()
 can be used to receive messages when cluster state changes. For example, registering for all 
MemberEvent
MemberEvent
’s, then using the 
manager
 to have a node leave the cluster will result in events for the node going through the 
Membership Lifecycle
.


This example subscribes to a 
subscriber: ActorRef[MemberEvent]
ActorRef<MemberEvent> subscriber
:




Scala




copy
source
cluster.subscriptions ! Subscribe(subscriber, classOf[MemberEvent])


Java




copy
source
cluster.subscriptions().tell(Subscribe.create(subscriber, ClusterEvent.MemberEvent.class));




Then asking a node to leave:




Scala




copy
source
cluster.manager ! Leave(anotherMemberAddress)
// subscriber will receive events MemberLeft, MemberExited and MemberRemoved


Java




copy
source
cluster.manager().tell(Leave.create(anotherMemberAddress));
// subscriber will receive events MemberLeft, MemberExited and MemberRemoved




Cluster State


Instead of subscribing to cluster events it can sometimes be convenient to only get the full membership state with 
Cluster(system).state
Cluster.get(system).state()
. Note that this state is not necessarily in sync with the events published to a cluster subscription.


See 
Cluster Membership
 more information on member events specifically. There are more types of change events, consult the API documentation of classes that extends 
akka.cluster.ClusterEvent.ClusterDomainEvent
akka.cluster.ClusterEvent.ClusterDomainEvent
 for details about the events.


Cluster Membership API


Joining


The seed nodes are initial contact points for joining a cluster, which can be done in different ways:




automatically with Cluster Bootstrap


with configuration of seed-nodes


programatically




After the joining process the seed nodes are not special and they participate in the cluster in exactly the same way as other nodes.


Joining automatically to seed nodes with Cluster Bootstrap


Automatic discovery of nodes for the joining process is available using the Akka Management project’s module, 
Cluster Bootstrap
. Please refer to its documentation for more details.


Joining configured seed nodes


When a new node is started it sends a message to all seed nodes and then sends a join command to the one that answers first. If none of the seed nodes replies (might not be started yet) it retries this procedure until success or shutdown.


You can define the seed nodes in the 
configuration
 file (application.conf):


akka.cluster.seed-nodes = [
  "akka://ClusterSystem@host1:2552",
  "akka://ClusterSystem@host2:2552"]



This can also be defined as Java system properties when starting the JVM using the following syntax:


-Dakka.cluster.seed-nodes.0=akka://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka://ClusterSystem@host2:2552



When a new node is started it sends a message to all configured 
seed-nodes
 and then sends a join command to the one that answers first. If none of the seed nodes replies (might not be started yet) it retries this procedure until successful or shutdown.


The seed nodes can be started in any order. It is not necessary to have all seed nodes running, but the node configured as the 
first element
 in the 
seed-nodes
 list must be started when initially starting a cluster. If it is not, the other seed-nodes will not become initialized, and no other node can join the cluster. The reason for the special first seed node is to avoid forming separated islands when starting from an empty cluster. It is quickest to start all configured seed nodes at the same time (order doesn’t matter), otherwise it can take up to the configured 
seed-node-timeout
 until the nodes can join.


As soon as more than two seed nodes have been started, it is no problem to shut down the first seed node. If the first seed node is restarted, it will first try to join the other seed nodes in the existing cluster. Note that if you stop all seed nodes at the same time and restart them with the same 
seed-nodes
 configuration they will join themselves and form a new cluster, instead of joining remaining nodes of the existing cluster. That is likely not desired and can be avoided by listing several nodes as seed nodes for redundancy, and don’t stop all of them at the same time.


If you are going to start the nodes on different machines you need to specify the ip-addresses or host names of the machines in 
application.conf
 instead of 
127.0.0.1


Joining programmatically to seed nodes


Joining programmatically is useful when 
dynamically discovering
 other nodes at startup through an external tool or API.




Scala




copy
source
import akka.actor.Address
import akka.actor.AddressFromURIString
import akka.cluster.typed.JoinSeedNodes

val seedNodes: List[Address] =
  List("akka://
[email protected]
:2551", "akka://
[email protected]
:2552").map(AddressFromURIString.parse)
Cluster(system).manager ! JoinSeedNodes(seedNodes)


Java




copy
source
import akka.actor.Address;
import akka.actor.AddressFromURIString;
import akka.cluster.Member;
import akka.cluster.typed.JoinSeedNodes;

List<Address> seedNodes = new ArrayList<>();
seedNodes.add(AddressFromURIString.parse("akka://
[email protected]
:2551"));
seedNodes.add(AddressFromURIString.parse("akka://
[email protected]
:2552"));

Cluster.get(system).manager().tell(new JoinSeedNodes(seedNodes));




The seed node address list has the same semantics as the configured 
seed-nodes
, and the the underlying implementation of the process is the same, see 
Joining configured seed nodes
.


When joining to seed nodes you should not include the node itself, except for the node that is supposed to be the first seed node bootstrapping the cluster. The desired initial seed node address should be placed first in the parameter to the programmatic join.


Tuning joins


Unsuccessful attempts to contact seed nodes are automatically retried after the time period defined in configuration property 
seed-node-timeout
. Unsuccessful attempts to join a specific seed node are automatically retried after the configured 
retry-unsuccessful-join-after
. Retrying means that it tries to contact all seed nodes, then joins the node that answers first. The first node in the list of seed nodes will join itself if it cannot contact any of the other seed nodes within the configured 
seed-node-timeout
.


The joining of given seed nodes will, by default, be retried indefinitely until a successful join. That process can be aborted if unsuccessful by configuring a timeout. When aborted it will run 
Coordinated Shutdown
, which will terminate the ActorSystem by default. CoordinatedShutdown can also be configured to exit the JVM. If the 
seed-nodes
 are assembled dynamically, it is useful to define this timeout, and a restart with new seed-nodes should be tried after unsuccessful attempts.


akka.cluster.shutdown-after-unsuccessful-join-seed-nodes = 20s
akka.coordinated-shutdown.exit-jvm = on



If you don’t configure seed nodes or use one of the join seed node functions, you need to join the cluster manually by using 
JMX
 or 
HTTP
.


You can join to any node in the cluster. It does not have to be configured as a seed node. Note that you can only join to an existing cluster member, which for bootstrapping means a node must join itself and subsequent nodes could join them to make up a cluster.


An actor system can only join a cluster once, additional attempts will be ignored. Once an actor system has successfully joined a cluster, it would have to be restarted to join the same cluster again. It can use the same host name and port after the restart. When it come up as a new incarnation of an existing member in the cluster and attempts to join, the existing member will be removed and its new incarnation allowed to join.


Leaving


There are a few ways to remove a member from the cluster.




The recommended way to leave a cluster is a graceful exit, informing the cluster that a node shall leave.  This is performed by 
Coordinated Shutdown
 when the 
ActorSystem
ActorSystem
  is terminated and also when a SIGTERM is sent from the environment to stop the JVM process.


Graceful exit can also be performed using 
HTTP
 or 
JMX
.


When a graceful exit is not possible, for example in case of abrupt termination of the the JVM process, the node  will be detected as unreachable by other nodes and removed after 
Downing
.




Graceful leaving offers faster hand off to peer nodes during node shutdown than abrupt termination and downing.


The 
Coordinated Shutdown
 will also run when the cluster node sees itself as 
Exiting
, i.e. leaving from another node will trigger the shutdown process on the leaving node. Tasks for graceful leaving of cluster, including graceful shutdown of Cluster Singletons and Cluster Sharding, are added automatically when Akka Cluster is used. For example, running the shutdown process will also trigger the graceful leaving if not already in progress.


Normally this is handled automatically, but in case of network failures during this process it may still be necessary to set the nodeâs status to 
Down
 in order to complete the removal, see 
Downing
.


Downing


In many cases a member can gracefully exit from the cluster, as described in 
Leaving
, but there are scenarios when an explicit downing decision is needed before it can be removed. For example in case of abrupt termination of the the JVM process, system overload that doesn’t recover, or network partitions that don’t heal. In such cases, the node(s) will be detected as unreachable by other nodes, but they must also be marked as 
Down
 before they are removed.


When a member is considered by the failure detector to be 
unreachable
 the leader is not allowed to perform its duties, such as changing status of new joining members to ‘Up’. The node must first become 
reachable
 again, or the status of the unreachable member must be changed to 
Down
. Changing status to 
Down
 can be performed automatically or manually.


We recommend that you enable the 
Split Brain Resolver
 that is part of the Akka Cluster module. You enable it with configuration:


akka.cluster.downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"



You should also consider the different available 
downing strategies
.


If a downing provider is not configured downing must be performed manually using 
HTTP
 or 
JMX
.


Note that 
Cluster Singleton
 or 
Cluster Sharding entities
 that are running on a crashed (unreachable) node will not be started on another node until the previous node has been removed from the Cluster. Removal of crashed (unreachable) nodes is performed after a downing decision.


Downing can also be performed programmatically with 
Cluster(system).manager ! Down(address)
Cluster.get(system).manager().tell(Down(address))
, but that is mostly useful from tests and when implementing a 
DowningProvider
DowningProvider
.


If a crashed node is restarted and joining the cluster again with the same hostname and port, the previous incarnation of that member will first be downed and removed. The new join attempt with same hostname and port is used as evidence that the previous is no longer alive.


If a node is still running and sees its self as 
Down
 it will shutdown. 
Coordinated Shutdown
 will automatically run if 
run-coordinated-shutdown-when-down
 is set to 
on
 (the default) however the node will not try and leave the cluster gracefully.


Node Roles


Not all nodes of a cluster need to perform the same function. For example, there might be one sub-set which runs the web front-end, one which runs the data access layer and one for the number-crunching. Choosing which actors to start on each node, for example cluster-aware routers, can take node roles into account to achieve this distribution of responsibilities.


The node roles are defined in the configuration property named 
akka.cluster.roles
 and typically defined in the start script as a system property or environment variable.


The roles are part of the membership information in 
MemberEvent
MemberEvent
 that you can subscribe to. The roles of the own node are available from the 
selfMember
selfMember()
 and that can be used for conditionally starting certain actors:




Scala




copy
source
val selfMember = Cluster(context.system).selfMember
if (selfMember.hasRole("backend")) {
  context.spawn(Backend(), "back")
} else if (selfMember.hasRole("frontend")) {
  context.spawn(Frontend(), "front")
}


Java




copy
source
Member selfMember = Cluster.get(context.getSystem()).selfMember();
if (selfMember.hasRole("backend")) {
  context.spawn(Backend.create(), "back");
} else if (selfMember.hasRole("front")) {
  context.spawn(Frontend.create(), "front");
}




Failure Detector


The nodes in the cluster monitor each other by sending heartbeats to detect if a node is unreachable from the rest of the cluster. Please see:




Failure Detector specification


Phi Accrual Failure Detector
 implementation


Using the Failure Detector




Using the Failure Detector


Cluster uses the 
akka.remote.PhiAccrualFailureDetector
akka.remote.PhiAccrualFailureDetector
 failure detector by default, or you can provide your by implementing the 
akka.remote.FailureDetector
akka.remote.FailureDetector
 and configuring it:


akka.cluster.implementation-class = "com.example.CustomFailureDetector"



In the 
Cluster Configuration
 you may want to adjust these depending on you environment:




When a 
phi
 value is considered to be a failure 
akka.cluster.failure-detector.threshold


Margin of error for sudden abnormalities 
akka.cluster.failure-detector.acceptable-heartbeat-pause




How to test


Akka comes with and uses several types of testing strategies:




Testing


Multi Node Testing


Multi JVM Testing




Configuration


There are several configuration properties for the cluster. Refer to the 
reference configuration
 for full configuration descriptions, default values and options.


How To Startup when a Cluster size is reached


A common use case is to start actors after the cluster has been initialized, members have joined, and the cluster has reached a certain size.


With a configuration option you can define the required number of members before the leader changes member status of ‘Joining’ members to ‘Up’.:


akka.cluster.min-nr-of-members = 3



In a similar way you can define the required number of members of a certain role before the leader changes member status of ‘Joining’ members to ‘Up’.:


akka.cluster.role {
  frontend.min-nr-of-members = 1
  backend.min-nr-of-members = 2
}



Cluster Info Logging


You can silence the logging of cluster events at info level with configuration property:


akka.cluster.log-info = off



You can enable verbose logging of cluster events at info level, e.g. for temporary troubleshooting, with configuration property:


akka.cluster.log-info-verbose = on



Cluster Dispatcher


The Cluster extension is implemented with actors. To protect them against disturbance from user actors they are by default run on the internal dispatcher configured under 
akka.actor.internal-dispatcher
. The cluster actors can potentially be isolated even further, onto their own dispatcher using the setting 
akka.cluster.use-dispatcher
 or made run on the same dispatcher to keep the number of threads down.


Configuration Compatibility Check


Creating a cluster is about deploying two or more nodes and having them behave as if they were a single application. Therefore it’s extremely important that all nodes in a cluster are configured with compatible settings. 


The Configuration Compatibility Check feature ensures that all nodes in a cluster have a compatible configuration. Whenever a new node is joining an existing cluster, a subset of its configuration settings (only those that are required to be checked) is sent to the nodes in the cluster for verification. Once the configuration is checked on the cluster side, the cluster sends back its own set of required configuration settings. The joining node will then verify if it’s compliant with the cluster configuration. The joining node will only proceed if all checks pass, on both sides. 


New custom checkers can be added by extending 
akka.cluster.JoinConfigCompatChecker
akka.cluster.JoinConfigCompatChecker
 and including them in the configuration. Each checker must be associated with a unique key:


akka.cluster.configuration-compatibility-check.checkers {
  my-custom-config = "com.company.MyCustomJoinConfigCompatChecker"
}

Note


Configuration Compatibility Check is enabled by default, but can be disabled by setting 
akka.cluster.configuration-compatibility-check.enforce-on-join = off
. This is specially useful when performing rolling updates. Obviously this should only be done if a complete cluster shutdown isn’t an option. A cluster with nodes with different configuration settings may lead to data loss or data corruption. 


This setting should only be disabled on the joining nodes. The checks are always performed on both sides, and warnings are logged. In case of incompatibilities, it is the responsibility of the joining node to decide if the process should be interrupted or not. 


If you are performing a rolling update on cluster using Akka 2.5.9 or prior (thus, not supporting this feature), the checks will not be performed because the running cluster has no means to verify the configuration sent by the joining node, nor to send back its own configuration. 


Higher level Cluster tools
Cluster Singleton


For some use cases it is convenient or necessary to ensure only one actor of a certain type is running somewhere in the cluster. This can be implemented by subscribing to member events, but there are several corner cases to consider. Therefore, this specific use case is covered by the Cluster Singleton.


See 
Cluster Singleton
.
Cluster Sharding


Distributes actors across several nodes in the cluster and supports interaction with the actors using their logical identifier, but without having to care about their physical location in the cluster.


See 
Cluster Sharding
.
Distributed Data


Distributed Data is useful when you need to share data between nodes in an Akka Cluster. The data is accessed with an actor providing a key-value store like API.


See 
Distributed Data
.
Distributed Publish Subscribe


Publish-subscribe messaging between actors in the cluster based on a topic, i.e. the sender does not have to know on which node the destination actor is running.


See 
Distributed Publish Subscribe
.
Cluster aware routers


Distribute messages to actors on different nodes in the cluster with routing strategies like round-robin and consistent hashing.


See 
Group Routers
. 
Reliable Delivery


Reliable delivery and flow control of messages between actors in the Cluster.


See 
Reliable Delivery


Example project


Cluster example project
 
(sources zip)
 
Cluster example project
 
(sources zip)
 is an example project that can be downloaded, and with instructions of how to run.


This project contains samples illustrating different Cluster features, such as subscribing to cluster membership events, and sending messages to actors running on nodes in the cluster with Cluster aware routers.














 
Cluster






Cluster Specification 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-persistence-cassandra/current/
Akka Persistence Cassandra









































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Persistence Cassandra





Version 1.3.0





Java
Scala










Overview


Journal


Query Plugin


Events by tag


Snapshots


Serialization


Testing


Event sourcing and CQRS


Health check


Configuration


Cassandra server support


Migration


Database Cleanup


















Akka Persistence Cassandra





Version 1.3.0





Java
Scala












Overview


Journal


Query Plugin


Events by tag


Snapshots


Serialization


Testing


Event sourcing and CQRS


Health check


Configuration


Cassandra server support


Migration


Database Cleanup




















Akka Persistence Cassandra


The Akka Persistence Cassandra plugin allows for using 
Apache Cassandra
 as a backend for 
Akka Persistence
 and 
Akka Persistence Query
. It uses 
Alpakka Cassandra
 for Cassandra access which is based on the 
Datastax Java Driver
.






Overview




Project Info


Dependencies


Supported features


History


Contributing




Journal




Features


Schema


Configuration


Event deletion and retention




Query Plugin




Configuration




Events by tag




First time bucket


Consistency


Back tracking


Tuning for lower latency


Missing searches and gap detection


Events by tag reconciliation


Other tuning


Cleanup of tag_views table


How it works




Snapshots




Features


Schema


Configuration


Limitations


Delete all snapshots




Serialization


Testing


Event sourcing and CQRS


Health check


Configuration




Default configuration


Cassandra driver configuration


Contact points configuration




Cassandra server support




More details


Amazon Keyspaces


CosmosDB


DataStax Astra


ScyllaDB




Migration




Migrating from 0.80+ to 1.0


Migrations to 0.101 and later in 0.x series


Migrations to 0.80 and later in 0.x series


Migrations from 0.54 to 0.59


Migrations from 0.51 to 0.52


Migrations from 0.23 to 0.50


Migrations from 0.11 to 0.12


Migrations from 0.9 to 0.10


Migrations from 0.6 to 0.7


Migrating from 0.3.x (Akka 2.3.x)




Database Cleanup






















Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka Persistence plugin for Cassandra is available under the 
Business Source License 1.1
.



© 2011-2024 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Listing
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-persistence-jdbc/current/
Akka Persistence JDBC









































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Persistence JDBC





Version 5.5.0





Java
Scala










Overview


Configuration


Migration


Persistence Query


Custom DAO Implementation


Snapshots


DurableStateStore


















Akka Persistence JDBC





Version 5.5.0





Java
Scala












Overview


Configuration


Migration


Persistence Query


Custom DAO Implementation


Snapshots


DurableStateStore




















Akka Persistence JDBC


The Akka Persistence JDBC plugin allows for using JDBC-compliant databases as backend for 
Akka Persistence
 and 
Akka Persistence Query
.






Overview




Version history


Module info


Contribution policy


Code of Conduct


License




Configuration




Database Schema


Reference Configuration


Explicitly shutting down the database connections


Tuning for Lower Latency




Migration




Migrating to version 5.4.0


Migrating to version 5.2.0


Migrating to version 5.0.0




Persistence Query




How to get the ReadJournal


Persistence Query Plugin


AllPersistenceIdsQuery and CurrentPersistenceIdsQuery


EventsByPersistenceIdQuery and CurrentEventsByPersistenceIdQuery


EventsByTag and CurrentEventsByTag




Custom DAO Implementation


Snapshots




Configure repository


Documentation




DurableStateStore




How to get the DurableStateStore


APIs supported by DurableStateStore
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka Persistence plugin for JDBC is available under the 
Business Source License 1.1
.



© 2011-2024 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Listing
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/licenses.html
Licenses • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses




Akka License


Documentation and test sources license


Akka Committer License Agreement


Licenses for Dependency Libraries




Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses




Akka License


Documentation and test sources license


Akka Committer License Agreement


Licenses for Dependency Libraries




Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Licenses


Akka License


copy
source
Business Source License 1.1

Parameters

Licensor:             Lightbend, Inc.
Licensed Work:        Akka 2.10.1
                      This license applies to all sub directories and files
                      UNLESS another license file is present in a sub
                      directory, then that other license applies to all files
                      in its directory and sub directories.
                      The Licensed Work is (c) 2025 Lightbend Inc.
Additional Use Grant:
    If you develop an application using a version of Play Framework that
    utilizes binary versions of akka-streams and its dependencies, you may
    use such binary versions of akka-streams and its dependencies in the
    development of your application only as they are incorporated into
    Play Framework and solely to implement the functionality provided by
    Play Framework; provided that, they are only used in the following way:
    Connecting to a Play Framework websocket and/or Play Framework
    request/response bodies for server and play-ws client.

Change Date:          2028-01-28

Change License:       Apache License, Version 2.0

For information about alternative licensing arrangements for the Software,
please visit: https://akka.io/

-----------------------------------------------------------------------------

Business Source License 1.1

License text copyright (c) 2017 MariaDB Corporation Ab, All Rights Reserved.
âBusiness Source Licenseâ is a trademark of MariaDB Corporation Ab.

Terms

The Licensor hereby grants you the right to copy, modify, create derivative
works, redistribute, and make non-production use of the Licensed Work. The
Licensor may make an Additional Use Grant, above, permitting limited
production use.

Effective on the Change Date, or the fourth anniversary of the first publicly
available distribution of a specific version of the Licensed Work under this
License, whichever comes first, the Licensor hereby grants you rights under
the terms of the Change License, and the rights granted in the paragraph
above terminate.

If your use of the Licensed Work does not comply with the requirements
currently in effect as described in this License, you must purchase a
commercial license from the Licensor, its affiliated entities, or authorized
resellers, or you must refrain from using the Licensed Work.

All copies of the original and modified Licensed Work, and derivative works
of the Licensed Work, are subject to this License. This License applies
separately for each version of the Licensed Work and the Change Date may vary
for each version of the Licensed Work released by Licensor.

You must conspicuously display this License on each original or modified copy
of the Licensed Work. If you receive the Licensed Work in original or
modified form from a third party, the terms and conditions set forth in this
License apply to your use of that work.

Any use of the Licensed Work in violation of this License will automatically
terminate your rights under this License for the current and all other
versions of the Licensed Work.

This License does not grant you any right in any trademark or logo of
Licensor or its affiliates (provided that you may use a trademark or logo of
Licensor as expressly required by this License).

TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE LICENSED WORK IS PROVIDED ON
AN âAS ISâ BASIS. LICENSOR HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS,
EXPRESS OR IMPLIED, INCLUDING (WITHOUT LIMITATION) WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, AND
TITLE.

MariaDB hereby grants you permission to use this Licenseâs text to license
your works, and to refer to it using the trademark âBusiness Source Licenseâ,
as long as you comply with the Covenants of Licensor below.

Covenants of Licensor

In consideration of the right to use this Licenseâs text and the âBusiness
Source Licenseâ name and trademark, Licensor covenants to MariaDB, and to all
other recipients of the licensed work to be provided by Licensor:

1. To specify as the Change License the GPL Version 2.0 or any later version,
   or a license that is compatible with GPL Version 2.0 or a later version,
   where âcompatibleâ means that software provided under the Change License can
   be included in a program with software provided under GPL Version 2.0 or a
   later version. Licensor may specify additional Change Licenses without
   limitation.

2. To either: (a) specify an additional grant of rights to use that does not
   impose any additional restriction on the right granted in this License, as
   the Additional Use Grant; or (b) insert the text âNoneâ.

3. To specify a Change Date.

4. Not to modify this License in any other way.


Documentation and test sources license


copy
source
ï»¿LIGHTBEND COMMERCIAL SOFTWARE LICENSE AGREEMENT

THIS LIGHTBEND COMMERCIAL SOFTWARE LICENSE AGREEMENT (THIS "AGREEMENT") IS A LEGAL AGREEMENT BETWEEN YOU ("USER") AND LIGHTBEND, INC. ("LICENSOR"). 
BY CLICKING THE "I ACCEPT" BUTTON, OR INSTALLING, COPYING OR OTHERWISE USING LIGHTBENDâS SOFTWARE (THE âSOFTWAREâ) AND ITS ASSOCIATED USER DOCUMENTATION, USER ACKNOWLEDGES THAT USER HAS REVIEWED AND ACCEPTS THIS AGREEMENT AND AGREES TO BE BOUND BY ALL OF ITS TERMS. 
IF YOU ARE AGREEING TO THIS AGREEMENT AS AN INDIVIDUAL, âUSERâ REFERS TO YOU INDIVIDUALLY.  IF YOU ARE AGREEING TO THIS AGREEMENT AS A REPRESENTATIVE OF AN ENTITY, YOU REPRESENT THAT YOU HAVE THE AUTHORITY TO BIND THAT ENTITY AND âUSERâ REFERS TO THAT ENTITY AND ALL THE USERS ACCESSING THE SOFTWARE BY, THROUGH OR ON BEHALF OF THAT ENTITY.  
IF USER DOES NOT AGREE WITH ALL OF THE TERMS OF THIS AGREEMENT, DO NOT INSTALL, COPY OR OTHERWISE USE THE SOFTWARE OR ITS DOCUMENTATION. 

1. DEFINITIONS. 
   1. âUser Systemâ means Userâs website(s), computers, servers and other equipment and software upon and with which the Software is run.
   2. âDocumentationâ means the user instructions and help files made available by Licensor for use with the Software, as may be updated from time to time by Licensor.
   3. âIntellectual Property Rightsâ means all intellectual property rights or similar proprietary rights, including 
	(a) patent rights and utility models, 
	(b) copyrights and database rights, 
	(c) trademarks, trade names, domain names and trade dress and the goodwill associated therewith, 
	(d) trade secrets, 
	(e) mask works, and 
	(f) industrial design rights; in each case, including any registrations of, applications to register, and renewals and extensions of, any of the foregoing in any jurisdiction in the world.
   4. âOpen Source Softwareâ means all software that is available under the GNU Affero General Public License (AGPL), GNU General Public License (GPL), GNU Lesser General Public License (LGPL), Mozilla Public License (MPL), Apache License, BSD licenses, or any other license that is approved by or similar to those approved by the Open Source Initiative (www.opensource.org).

2. LICENSES AND RESTRICTIONS.  
   1. License.  Subject to Userâs compliance with the terms and conditions of this Agreement, Licensor hereby grants to User, during the term of this Agreement, a limited, non-exclusive, non-transferable and non-sublicensable right to 
	(i) install and execute one (1) copy of the Software in accordance with the Documentation, solely in binary form, and not for the benefit of any other person or entity, and 
	(ii) access and use the Documentation, solely for Userâs own internally purposes in support of End Userâs permitted use of the Software.  
   2. Restrictions.  User shall not, directly or indirectly, or permit any User or third party to: 
	(a) reverse engineer, decompile, disassemble or otherwise attempt to discover the source code or underlying ideas or algorithms of the Software;  
	(b) modify, translate, or create derivative works based on any element of the Software or any related Documentation (except to the extent applicable laws specifically prohibit such restriction for interoperability purposes, in which case you agree to first contact Licensor and provide Licensor an opportunity to create such changes as are needed for interoperability purposes); 
	(c) use, rent, lease, distribute, sell, resell, assign, or otherwise transfer the Software or any copy thereof; 
	(d) use the Software for timesharing purposes or otherwise for the benefit of any person or entity other than for the benefit of User and Users; 
	(e) remove any proprietary notices from the Software or the Documentation or attempt to defeat any copy protection device included with the Software; or 
	(f) use the Software for any purpose other than its intended purpose.
   3. Reservation of Rights.  Nothing in this Agreement shall be construed to give User a right to use or otherwise obtain access to any source code from which the Software is compiled or interpreted.  Except as expressly granted in this Agreement, there are no other licenses granted to User, express, implied or by way of estoppel.  All rights not granted in this Agreement are reserved by Licensor.
   4. Open Source Software.  Notwithstanding the foregoing, certain items of software included with the Software are Open Source Software and remains subject Open Source Software licenses.  Such Open Source Software is not subject to the terms and conditions of this Agreement.  
Instead, each such item of Open Source Software is licensed under the terms of the end user license that accompanies such Open Source Software and nothing in this Agreement limits your rights under, or grants you rights that supersede, the terms and conditions of any applicable end user license for such Open Source Software.  If required by any license for particular Open Source Software, Licensor makes such Open Source Software, and any Licensor modifications to that Open Source Software, available as further described in the Documentation.
USE OF THE SOFTWARE IN ANY MANNER OTHER THAN AS PROVIDED IN THIS AGREEMENT IS STRICTLY PROHIBITED AND MAY INFRINGE ON THE INTELLECTUAL PROPERTY RIGHTS OF LICENSOR AND/OR ITS LICENSOR(S), SUBJECTING USER TO CIVIL AND CRIMINAL PENALTIES, INCLUDING WITHOUT LIMITATION MONETARY DAMAGES AND IMPRISONMENT FOR COPYRIGHT INFRINGEMENT.

3. USER OBLIGATIONS.
   1. User System.  User is responsible for 
	(a) obtaining, deploying and maintaining the User System, and all computer hardware, software, modems, routers and other communications equipment necessary for User and its Users to install and use the Software; and 
	(b) paying all third party fees and access charges incurred in connection with the foregoing.  Licensor shall not be responsible for supplying any hardware, software or other equipment to User under this Agreement.
   2. Compliance with Laws.  User agrees to use the Software in compliance with all applicable laws, including local laws of the country or region in which User resides, and in compliance with all United States export laws and regulations.  User shall not use the Software for any purpose prohibited by applicable law.  
   3. Trademarks and Tradenames.  With regard to all copies of the Software permitted herein, User shall reproduce on such copies all Licensor copyright notices, and other proprietary notices appearing on and in the original copy of the software received from Licensor. Except as set forth in the foregoing sentence, User will not, during the term of this Agreement or thereafter, use any trademark of Licensor, or any word and/or symbol likely to be confused with any Licensor trademark, either alone or in any combination with other words and/or symbols.

4. SUPPORT AND MAINTENANCE.
   1. Support.  Licensor is not responsible for maintenance or support of the Software, or the equipment on which the Software resides or is used, under this Agreement. By accepting the license granted under this Agreement, User agrees that Licensor will be under no obligation to provide any support, maintenance or service in connection with the Software or such equipment.  
   2. Upgrades and Updates.  Licensor may from time to time in its sole discretion develop and provide updates for the Software, which may include upgrades, bug fixes, patches, other error corrections, and/or new features (collectively, including related documentation, âUpdatesâ). Updates may also modify or delete in their entirety certain features and functionality. You agree that Licensor has no obligation to provide any Updates or to continue to provide or enable any particular features or functionality. 

5. REPRESENTATIONS AND WARRANTIES; DISCLAIMER.
   1. Mutual Representations and Warranties.  Each party represents, warrants and covenants that: 
	(a) it has the full power and authority to enter into this Agreement and to perform its obligations hereunder, without the need for any consents, approvals or immunities not yet obtained; and 
	(b) its acceptance of and performance under this Agreement shall not breach any oral or written agreement with any third party or any obligation owed by it to any third party to keep any information or materials in confidence or in trust. 
   2. Disclaimer.  EXCEPT FOR THE WARRANTIES SET FORTH IN THIS SECTION 5, THE SOFTWARE AND THE DOCUMENTATION ARE PROVIDED ON AN AS-IS BASIS.  USERâS USE OF THE SOFTWARE AND THE DOCUMENTATION IS AT USERâS OWN RISK.  LICENSOR DOES NOT MAKE, AND HEREBY DISCLAIMS, ANY AND ALL OTHER EXPRESS, STATUTORY AND IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT AND TITLE, QUALITY, SUITABILITY, OPERABILITY, CONDITION, SYSTEM INTEGRATION, NON-INTERFERENCE, WORKMANSHIP, TRUTH, ACCURACY (OF DATA OR ANY OTHER INFORMATION OR CONTENT), ABSENCE OF DEFECTS, WHETHER LATENT OR PATENT, AND ANY WARRANTIES ARISING FROM A COURSE OF DEALING, USAGE, OR TRADE PRACTICE.  LICENSOR ALSO DOES NOT WARRANT THAT THE FUNCTIONS CONTAINED IN, PERFORMED AND/OR PROVIDED BY THE SOFTWARE WILL MEET USERâS REQUIREMENTS, THAT THE OPERATION OF THE SOFTWARE WILL BE UNINTERRUPTED OR ERROR-FREE, THAT THE SOFTWARE WILL BE COMPATIBLE OR WORK WITH ANY THIRD-PARTY SOFTWARE, APPLICATIONS OR DEVICES, OR THAT DEFECTS IN THE SOFTWARE WILL BE CORRECTED.  USER EXPRESSLY ACKNOWLEDGES AND AGREES THAT, TO THE EXTENT PERMITTED BY APPLICABLE LAW, ITS USE OF THE SOFTWARE IS AT ITS SOLE RISK AND THAT THE ENTIRE RISK AS TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY AND EFFORT IS WITH USER. USER FURTHER ACKNOWLEDGES AND AGREES THAT THE SOFTWARE IS NOT INTENDED OR SUITABLE FOR USE IN SITUATIONS OR ENVIRONMENTS WHERE THE FAILURE OR TIME DELAYS OF, OR ERRORS OR INACCURACIES IN THE CONTENT, DATA OR INFORMATION PROVIDED BY THE SOFTWARE COULD LEAD TO DEATH, PERSONAL INJURY, OR SEVERE PHYSICAL OR ENVIRONMENTAL DAMAGE. ANY WARRANTIES MADE BY LICENSOR ARE FOR THE BENEFIT OF USER ONLY AND NOT FOR THE BENEFIT OF ANY THIRD PARTY.  THE SOFTWARE AND THE DOCUMENTATION ARE LICENSED AND NOT SOLD. NO AGENT OF LICENSOR IS AUTHORIZED TO ALTER OR EXPAND THE WARRANTIES OF LICENSOR AS SET FORTH HEREIN.  

6. INDEMNIFICATION. User shall defend Licensor and its licensors and their respective officers, directors and employees (âLicensor Indemnified Partiesâ) from and against any and all Third-Party Claims which arise out of or relate to: 
	(a) Userâs use or alleged use of the Software other than as permitted under this Agreement; or 
	(b) arising out of or relating to any violation of Section 2.2, or any violation of applicable laws.  User shall pay all damages, costs and expenses, including attorneysâ fees and costs (whether by settlement or award of by a final judicial judgment) incurred by the Licensor Indemnified Parties from any such Third-Party Claim.  In no event shall Licensor settle any claim without Userâs prior written approval.  Licensor may, at its own expense, engage separate counsel to advise Licensor regarding a Third-Party Claim and to participate in the defense of the claim, subject to Userâs right to control the defense and settlement. If you are a New Jersey resident, this indemnification clause is to be only as broad and inclusive as is permitted by the law of the state of New Jersey.

7. CONFIDENTIALITY. 
   1. Confidential Information. User acknowledges that the Software contains valuable proprietary information and trade secrets and that unauthorized or improper use of the Software will result in irreparable harm to Licensor for which monetary damages would be inadequate and for which Licensor may be entitled to immediate injunctive relief. Accordingly, you will maintain the confidentiality of the proprietary information and not sell, license, publish, display, distribute, disclose or otherwise make available such proprietary information to any third party, nor use such information except as authorized by this Agreement.
   2. Injunctive Relief.  User agrees that any unauthorized disclosure of confidential information may cause immediate and irreparable injury to Licensor and that, in the event of such breach, Licensor will be entitled, in addition to any other available remedies, to seek immediate injunctive and other equitable relief, without bond and without the necessity of showing actual monetary damages.

8. PROPRIETARY RIGHTS. 
   1. Licensor.  As between Licensor and User, all right, title and interest in the Software, the Documentation, and any other Licensor materials furnished or made available hereunder, and all modifications and enhancements thereof, and all suggestions, ideas and feedback proposed by User regarding the Software, including all copyright rights, patent rights and other Intellectual Property Rights in each of the foregoing, belong to and are retained solely by Licensor or Licensorâs licensors and providers, as applicable.  User hereby does and will irrevocably assign to Licensor all evaluations, ideas, feedback and suggestions made by User to Licensor regarding the Software or the Documentation (collectively, âFeedbackâ) and all Intellectual Property Rights in the Feedback.  

9. LIMITATION OF LIABILITY.
   1. No Consequential Damages.  NEITHER LICENSOR NOR ITS LICENSORS SHALL BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR PUNITIVE DAMAGES, OR ANY DAMAGES FOR LOST DATA, BUSINESS INTERRUPTION, LOST PROFITS, LOST REVENUE OR LOST BUSINESS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT, EVEN IF LICENSOR OR ITS LICENSORS OR USER HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES, INCLUDING WITHOUT LIMITATION, ANY SUCH DAMAGES ARISING OUT OF THE LICENSING, PROVISION OR USE OF THE SOFTWARE OR THE RESULTS OF THE USE OF THE SOFTWARE.  LICENSOR WILL NOT BE LIABLE FOR THE COST OF PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES.
   2. LIMITS ON LIABILITY.  NEITHER LICENSOR NOR ITS LICENSORS SHALL BE LIABLE FOR CUMULATIVE, AGGREGATE DAMAGES GREATER THAN FIVE HUNDRED DOLLARS (US $500).  
   3. ESSENTIAL PURPOSE.  USER ACKNOWLEDGES THAT THE TERMS IN THIS SECTION 9 (LIMITATION OF LIABILITY) SHALL APPLY TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW AND SHALL APPLY EVEN IF AN EXCLUSIVE OR LIMITED REMEDY STATED HEREIN FAILS OF ITS ESSENTIAL PURPOSE. SOME JURISDICTIONS DO NOT ALLOW CERTAIN LIMITATIONS OF LIABILITY, SO SOME OR ALL OF THE ABOVE LIMITATIONS OF LIABILITY MAY NOT APPLY TO YOU.  IF YOU ARE A NEW JERSEY RESIDENT, THIS LIMITATION OF LIABILITY SECTION IS TO BE ONLY AS BROAD AND INCLUSIVE AS IS PERMITTED BY THE LAW OF THE STATE OF NEW JERSEY.

10. TERM AND TERMINATION.  
   1. Term.  This Agreement and Userâs right to use the Software commences on earlier of the date that User: 
	(a) installs the Software, 
	(b) begins using the Software or 
	(c) otherwise demonstrates assent to this Agreement.  
	Userâs right to use the Software shall continue until such time as this Agreement is terminated (the âTermâ).  
   2. Termination for Cause.  A party may terminate this Agreement, upon written notice to the other party in the event the other party files a petition for bankruptcy or has a petition for bankruptcy filed against it that is not dismissed within sixty (60) calendar days after filing  or admits its inability to pay its debts as they mature, makes an assignment for the benefit of its creditors or ceases to function as a going concern or to conduct its operations in the normal course of business and such termination shall occur immediately upon notice.  Licensor may terminate this Agreement at any time without notice if it ceases to support the Software, which Licensor may do in its sole discretion. In addition, this Agreement will terminate immediately and automatically without any notice if User breaches any of its terms and conditions.
   3. Termination for Convenience.  Either party may terminate this Agreement for convenience on at least thirty (30) calendar days prior written notice to the other party.  User may also terminate this Agreement by ceasing all use of the Software.
   4. Effects of Termination.  Upon expiration or termination of this Agreement, Userâs shall cease all use of the Software and the Documentation and shall destroy all copies of the Software in Userâs possession or control.
   5. Survival.  This Section and Sections 1, 2.2 (Restrictions), 2.3 (Reservation of Rights), 2.4 (Open Source Software), 5.2 (Disclaimer), 6 (Indemnification), 7 (Confidentiality), 8 (Proprietary Rights), 9 (Limitation of Liability), 10.4 (Effects of Termination) and 11 (Miscellaneous) shall survive any termination or expiration of this Agreement. 

11. MISCELLANEOUS.
   1. Notices.  Licensor may give notice to User by means of a general notice through electronic mail to Userâs e-mail address, or by written communication sent by first class postage prepaid mail or nationally recognized overnight delivery service to Userâs address on record with Licensor. User may give notice to Licensor by written communication sent by first class postage prepaid mail or nationally recognized overnight delivery service addressed to Licensor, Lightbend Inc., 580 California, #1231, San Francisco, CA 94104, Attention: User Support.  Notice shall be deemed to have been given upon receipt or, if earlier, two (2) business days after mailing, as applicable. All communications and notices to be made or given pursuant to this Agreement shall be in the English language.  
   2. Governing Law.  This Agreement and the rights and obligations of the parties to and under this agreement shall be governed by and construed under the laws of the United States and the State of California as applied to agreements entered into and to be performed in such State without giving effect to conflicts of laws rules or principles.  The parties agree that the United Nations Convention on Contracts for the International Sale of Goods is specifically excluded from application to this Agreement and that the application of the Uniform Computer Information Transactions Act (UCITA) is specifically disclaimed.  Any dispute arising out of or in connection with this Agreement, including but not limited to any question regarding its existence, interpretation, validity, performance, or termination, or any dispute between the parties arising from the parties' relationship created by this Agreement, shall be referred to and finally resolved by arbitration administered by the American Arbitration Association under its rules.  The number of arbitrators shall be one (1).  The parties shall endeavor to agree upon the sole arbitrator and jointly nominate the arbitrator.  If the parties cannot agree upon the sole arbitrator within a time prescribed by AAA, the parties shall request the AAA to propose five (5) arbitrators and each party shall rank the proposed arbitrators.  The AAA shall appoint an arbitrator from the list of five (5), based upon the parties' rankings.  
   3. U.S. Government Users.  If User is a Federal Government entity, Licensor provides the Software and the Documentation, including related software and technology, for ultimate Federal Government end use solely in accordance with the following:  Government technical data rights include only those rights customarily provided to the public with a commercial item or process and Government software rights related to the Software and the Documentation include only those rights customarily provided to the public, as defined in this Agreement.  The technical data rights and customary commercial software license is provided in accordance with FAR 12.211 (Technical Data) and FAR 12.212 (Software) and, for Department of Defense transactions, DFAR 252.227-7015 (Technical Data â Commercial Items) and DFAR 227.7202-3 (Rights in Commercial Computer Software or Computer Software Documentation).  If greater rights are needed, a mutually acceptable written addendum specifically conveying such rights must be included in this Agreement. 
   4. Export.  The Software utilizes software and technology that may be subject to United States and foreign export controls. User acknowledges and agrees that the Software shall not be used, and none of the underlying information, software, or technology may be transferred or otherwise exported or re-exported to countries as to which the United States maintains an embargo (collectively, âEmbargoed Countriesâ), or to or by a national or resident thereof, or any person or entity on the U.S. Department of Treasuryâs List of Specially Designated Nationals or the U.S. Department of Commerceâs Table of Denial Orders (collectively, âDesignated Nationalsâ). The lists of Embargoed Countries and Designated Nationals are subject to change without notice. By using the Software, User represents and warrants that it is not located in, under the control of, or a national or resident of an Embargoed Country or Designated National. The Software may use encryption technology that is subject to licensing requirements under the U.S. Export Administration Regulations, 15 C.F.R. Parts 730-774 and Council Regulation (EC) No. 1334/2000. User agrees to comply strictly with all applicable export laws and assume sole responsibility for obtaining licenses to export or re-export as may be required. Licensor and its licensors make no representation that the Software is appropriate or available for use in other locations. By using the Software, User represents and warrants that it is not located in any such country or on any such list.
   5. General.  User shall not assign its rights hereunder, or delegate the performance of any of its duties or obligations hereunder, whether by merger, acquisition, sale of assets, operation of law, or otherwise, without the prior written consent of Licensor.  Any purported assignment in violation of the preceding sentence is null and void.  Subject to the foregoing, this Agreement shall be binding upon, and inure to the benefit of, the successors and assigns of the parties thereto.  Except as otherwise specified in this Agreement, this Agreement may be amended or supplemented only by a writing that refers explicitly to this Agreement and that is signed on behalf of both parties.  No waiver will be implied from conduct or failure to enforce rights.  No waiver will be effective unless in a writing signed on behalf of the party against whom the waiver is asserted.  If any of this Agreement is found invalid or unenforceable that term will be enforced to the maximum extent permitted by law and the remainder of this Agreement will remain in full force.  
Nothing contained herein shall be construed as creating an agency, partnership, or other form of joint enterprise between the parties.  
This Agreement constitutes the entire agreement between the parties relating to this subject matter and supersedes all prior or simultaneous understandings, representations, discussions, negotiations, and agreements, whether written or oral.  
Neither party shall be liable to the other party or any third party for failure or delay in performing its obligations under this Agreement when such failure or delay is due to any cause beyond the control of the party concerned, including, without limitation, force majeure, governmental orders or restrictions, fire, or flood, provided that upon cessation of such events such party shall thereupon promptly perform or complete the performance of its obligations hereunder.


Akka Committer License Agreement


All committers have signed this 
CLA
. It can be 
signed online
.


Licenses for Dependency Libraries


Each dependency and its license can be seen in the project build file (the comment on the side of each dependency): 
AkkaBuild.scala
 














 
Issue Tracking






Frequently Asked Questions 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/persistence-query.html
Persistence Query • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query




Dependency


Introduction


Design overview


Read Journals


Performance and denormalization


Query plugins


Scaling out


Example project




Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query




Dependency


Introduction


Design overview


Read Journals


Performance and denormalization


Query plugins


Scaling out


Example project




Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence Query


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Persistence Query, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-query" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-query_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-query_${versions.ScalaBinary}"
}


This will also add dependency on the 
Akka Persistence
 module.


Introduction


Akka persistence query complements 
Event Sourcing
 by providing a universal asynchronous stream based query interface that various journal plugins can implement in order to expose their query capabilities.


The most typical use case of persistence query is implementing the so-called query side (also known as “read side”) in the popular CQRS architecture pattern - in which the writing side of the application (e.g. implemented using Akka persistence) is completely separated from the “query side”. Akka Persistence Query itself is 
not
 directly the query side of an application, however it can help to migrate data from the write side to the query side database. In very simple scenarios Persistence Query may be powerful enough to fulfill the query needs of your app, however we highly recommend (in the spirit of CQRS) of splitting up the write/read sides into separate datastores as the need arises.


For a similar implementation of query interface to 
Durable State Behaviors
 please refer to 
Persistence Query using Durable State
.


The 
Microservices with Akka tutorial
 explains how to implement an Event Sourced CQRS application with Akka Persistence and Akka Projections.


Design overview


Akka persistence query is purposely designed to be a very loosely specified API. This is in order to keep the provided APIs general enough for each journal implementation to be able to expose its best features, e.g. a SQL journal can use complex SQL queries or if a journal is able to subscribe to a live event stream this should also be possible to expose the same API - a typed stream of events.


Each read journal must explicitly document which types of queries it supports.
 Refer to your journal’s plugins documentation for details on which queries and semantics it supports.


While Akka Persistence Query does not provide actual implementations of ReadJournals, it defines a number of pre-defined query types for the most common query scenarios, that most journals are likely to implement (however they are not required to).


Read Journals


In order to issue queries one has to first obtain an instance of a 
ReadJournal
ReadJournal
. Read journals are implemented as 
Community plugins
, each targeting a specific datastore (for example Cassandra or JDBC databases). For example, given a library that provides a 
akka.persistence.query.my-read-journal
 obtaining the related journal is as simple as:




Scala




copy
source
// obtain read journal by plugin id
val readJournal =
  PersistenceQuery(system).readJournalFor[MyScaladslReadJournal]("akka.persistence.query.my-read-journal")

// issue query to journal
val source: Source[EventEnvelope, NotUsed] =
  readJournal.eventsByPersistenceId("user-1337", 0, Long.MaxValue)

// materialize stream, consuming events
source.runForeach { event =>
  println("Event: " + event)
}


Java




copy
source
// obtain read journal by plugin id
final MyJavadslReadJournal readJournal =
    PersistenceQuery.get(system)
        .getReadJournalFor(
            MyJavadslReadJournal.class, "akka.persistence.query.my-read-journal");

// issue query to journal
Source<EventEnvelope, NotUsed> source =
    readJournal.eventsByPersistenceId("user-1337", 0, Long.MAX_VALUE);

// materialize stream, consuming events
source.runForeach(event -> System.out.println("Event: " + event), system);




Journal implementers are encouraged to put this identifier in a variable known to the user, such that one can access it via 
readJournalFor[NoopJournal](NoopJournal.identifier)
getJournalFor(NoopJournal.class, NoopJournal.identifier)
, however this is not enforced.


Read journal implementations are available as 
Community plugins
.


Predefined queries


Akka persistence query comes with a number of query interfaces built in and suggests Journal implementors to implement them according to the semantics described below. It is important to notice that while these query types are very common a journal is not obliged to implement all of them - for example because in a given journal such query would be significantly inefficient.
Note


Refer to the documentation of the 
ReadJournal
ReadJournal
 plugin you are using for a specific list of supported query types. For example, Journal plugins should document their stream completion strategies.


The predefined queries are:


PersistenceIdsQuery and CurrentPersistenceIdsQuery


persistenceIds
persistenceIds
 which is designed to allow users to subscribe to a stream of all persistent ids in the system. By default this stream should be assumed to be a “live” stream, which means that the journal should keep emitting new persistence ids as they come into the system:




Scala




copy
source
readJournal.persistenceIds()


Java




copy
source
readJournal.persistenceIds();




If your usage does not require a live stream, you can use the 
currentPersistenceIds
currentPersistenceIds
 query:




Scala




copy
source
readJournal.currentPersistenceIds()


Java




copy
source
readJournal.currentPersistenceIds();




EventsByPersistenceIdQuery and CurrentEventsByPersistenceIdQuery


eventsByPersistenceId
eventsByPersistenceId
 is a query equivalent to replaying an 
event sourced actor
, however, since it is a stream it is possible to keep it alive and watch for additional incoming events persisted by the persistent actor identified by the given 
persistenceId
.




Scala




copy
source
readJournal.eventsByPersistenceId("user-us-1337", fromSequenceNr = 0L, toSequenceNr = Long.MaxValue)



Java




copy
source
readJournal.eventsByPersistenceId("user-us-1337", 0L, Long.MAX_VALUE);




Most journals will have to revert to polling in order to achieve this, which can typically be configured with a 
refresh-interval
 configuration property.


If your usage does not require a live stream, you can use the 
currentEventsByPersistenceId
currentEventsByPersistenceId
 query.


EventsByTag and CurrentEventsByTag


eventsByTag
eventsByTag
 allows querying events regardless of which 
persistenceId
 they are associated with. This query is hard to implement in some journals or may need some additional preparation of the used data store to be executed efficiently. The goal of this query is to allow querying for all events which are “tagged” with a specific tag. That includes the use case to query all domain events of an Aggregate Root type. Please refer to your read journal plugin’s documentation to find out if and how it is supported.


Some journals may support 
tagging of events
 or 
Event Adapters
 that wraps the events in a 
akka.persistence.journal.Tagged
akka.persistence.journal.Tagged
 with the given 
tags
. The journal may support other ways of doing tagging - again, how exactly this is implemented depends on the used journal. Here is an example of such a tagging with an 
EventSourcedBehavior
EventSourcedBehavior
:




Scala




copy
source
val NumberOfEntityGroups = 10

def tagEvent(entityId: String, event: Event): Set[String] = {
  val entityGroup = s"group-${math.abs(entityId.hashCode % NumberOfEntityGroups)}"
  event match {
    case _: OrderCompleted => Set(entityGroup, "order-completed")
    case _                 => Set(entityGroup)
  }
}

def apply(entityId: String): Behavior[Command] = {
  EventSourcedBehavior[Command, Event, State](
    persistenceId = PersistenceId("ShoppingCart", entityId),
    emptyState = State(),
    commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
    eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
    .withTagger(event => tagEvent(entityId, event))
}


Java




copy
source
private final String entityId;

public static final int NUMBER_OF_ENTITY_GROUPS = 10;

@Override
public Set<String> tagsFor(Event event) {
  String entityGroup = "group-" + Math.abs(entityId.hashCode() % NUMBER_OF_ENTITY_GROUPS);
  Set<String> tags = new HashSet<>();
  tags.add(entityGroup);
  if (event instanceof OrderCompleted) tags.add("order-completed");
  return tags;
}


Note


A very important thing to keep in mind when using queries spanning multiple persistenceIds, such as 
EventsByTag
EventsByTag
 is that the order of events at which the events appear in the stream rarely is guaranteed (or stable between materializations).


Journals 
may
 choose to opt for strict ordering of the events, and should then document explicitly what kind of ordering guarantee they provide - for example “
ordered by timestamp ascending, independently of persistenceId
” is easy to achieve on relational databases, yet may be hard to implement efficiently on plain key-value datastores.


In the example below we query all events which have been tagged (we assume this was performed by the write-side using 
tagging of events
 or 
Event Adapters
, or that the journal is smart enough that it can figure out what we mean by this tag - for example if the journal stored the events as json it may try to find those with the field 
tag
 set to this value etc.).




Scala




copy
source
// assuming journal is able to work with numeric offsets we can:

val completedOrders: Source[EventEnvelope, NotUsed] =
  readJournal.eventsByTag("order-completed", Offset.noOffset)

// find first 10 completed orders:
val firstCompleted: Future[Vector[OrderCompleted]] =
  completedOrders
    .map(_.event)
    .collectType[OrderCompleted]
    .take(10) // cancels the query stream after pulling 10 elements
    .runFold(Vector.empty[OrderCompleted])(_ :+ _)

// start another query, from the known offset
val furtherOrders = readJournal.eventsByTag("order-completed", offset = Sequence(10))


Java




copy
source
// assuming journal is able to work with numeric offsets we can:
final Source<EventEnvelope, NotUsed> completedOrders =
    readJournal.eventsByTag("order-completed", new Sequence(0L));

// find first 10 completed orders:
final CompletionStage<List<OrderCompleted>> firstCompleted =
    completedOrders
        .map(EventEnvelope::event)
        .collectType(OrderCompleted.class)
        .take(10) // cancels the query stream after pulling 10 elements
        .runFold(
            new ArrayList<>(10),
            (acc, e) -> {
              acc.add(e);
              return acc;
            },
            system);

// start another query, from the known offset
Source<EventEnvelope, NotUsed> furtherOrders =
    readJournal.eventsByTag("order-completed", new Sequence(10));




As you can see, we can use all the usual stream operators available from 
Streams
 on the resulting query stream, including for example taking the first 10 and cancelling the stream. It is worth pointing out that the built-in 
EventsByTag
 query has an optionally supported offset parameter (of type 
Long
) which the journals can use to implement resumable-streams. For example a journal may be able to use a WHERE clause to begin the read starting from a specific row, or in a datastore that is able to order events by insertion time it could treat the Long as a timestamp and select only older events.


If your usage does not require a live stream, you can use the 
currentEventsByTag
currentEventsByTag
 query.


EventsBySlice and CurrentEventsBySlice


Query events for given entity type and slices. A slice is deterministically defined based on the persistence id. The purpose is to evenly distribute all persistence ids over the slices.


See 
EventsBySliceQuery
EventsBySliceQuery
 and 
CurrentEventsBySliceQuery
CurrentEventsBySliceQuery
.


A variation of these are 
EventsBySliceStartingFromSnapshotsQuery
EventsBySliceStartingFromSnapshotsQuery
 and 
CurrentEventsBySliceStartingFromSnapshotsQuery
CurrentEventsBySliceStartingFromSnapshotsQuery
.


EventsBySliceFirehoseQuery
EventsBySliceFirehoseQuery
 can give better scalability when many consumers retrieve the same events, for example many Projections of the same entity type. The purpose is to share the stream of events from the database and fan out to connected consumer streams. Thereby fewer queries and loading of events from the database. It is typically used together with 
Sharded Daemon Process with colocated processes
.


Materialized values of queries


Journals are able to provide additional information related to a query by exposing 
Materialized values
, which are a feature of 
Streams
 that allows to expose additional values at stream materialization time.


More advanced query journals may use this technique to expose information about the character of the materialized stream, for example if it’s finite or infinite, strictly ordered or not ordered at all. The materialized value type is defined as the second type parameter of the returned 
Source
Source
, which allows journals to provide users with their specialised query object, as demonstrated in the sample below:




Scala




copy
source
final case class RichEvent(tags: Set[String], payload: Any)

// a plugin can provide:
case class QueryMetadata(deterministicOrder: Boolean, infinite: Boolean)


Java




copy
source
static class RichEvent {
  public final Set<String> tags;
  public final Object payload;

  public RichEvent(Set<String> tags, Object payload) {
    this.tags = tags;
    this.payload = payload;
  }
}
// a plugin can provide:
static final class QueryMetadata {
  public final boolean deterministicOrder;
  public final boolean infinite;

  public QueryMetadata(boolean deterministicOrder, boolean infinite) {
    this.deterministicOrder = deterministicOrder;
    this.infinite = infinite;
  }
}






Scala




copy
source
def byTagsWithMeta(tags: Set[String]): Source[RichEvent, QueryMetadata] = {


Java




copy
source
public Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) {






Scala




copy
source
val query: Source[RichEvent, QueryMetadata] =
  readJournal.byTagsWithMeta(Set("red", "blue"))

query
  .mapMaterializedValue { meta =>
    println(
      s"The query is: " +
      s"ordered deterministically: ${meta.deterministicOrder}, " +
      s"infinite: ${meta.infinite}")
  }
  .map { event =>
    println(s"Event payload: ${event.payload}")
  }
  .runWith(Sink.ignore)



Java




copy
source
Set<String> tags = new HashSet<String>();
tags.add("red");
tags.add("blue");
final Source<RichEvent, QueryMetadata> events =
    readJournal
        .byTagsWithMeta(tags)
        .mapMaterializedValue(
            meta -> {
              System.out.println(
                  "The query is: "
                      + "ordered deterministically: "
                      + meta.deterministicOrder
                      + " "
                      + "infinite: "
                      + meta.infinite);
              return meta;
            });

events
    .map(
        event -> {
          System.out.println("Event payload: " + event.payload);
          return event.payload;
        })
    .runWith(Sink.ignore(), system);





Performance and denormalization


When building systems using 
Event Sourcing
 and CQRS (
Command & Query Responsibility Segregation
) techniques it is tremendously important to realise that the write-side has completely different needs from the read-side, and separating those concerns into datastores that are optimised for either side makes it possible to offer the best experience for the write and read sides independently.


For example, in a bidding system it is important to “take the write” and respond to the bidder that we have accepted the bid as soon as possible, which means that write-throughput is of highest importance for the write-side â often this means that data stores which are able to scale to accommodate these requirements have a less expressive query side.


On the other hand the same application may have some complex statistics view or we may have analysts working with the data to figure out best bidding strategies and trends â this often requires some kind of expressive query capabilities like for example SQL or writing Spark jobs to analyse the data. Therefore the data stored in the write-side needs to be projected into the other read-optimised datastore.
Note


When referring to 
Materialized Views
 in Akka Persistence think of it as “some persistent storage of the result of a Query”. In other words, it means that the view is created once, in order to be afterwards queried multiple times, as in this format it may be more efficient or interesting to query it (instead of the source events directly).


Materialize view to Reactive Streams compatible datastore


If the read datastore exposes a 
Reactive Streams
 interface then implementing a simple projection is as simple as, using the read-journal and feeding it into the databases driver interface, for example like so:




Scala




copy
source
implicit val system: ActorSystem = ActorSystem()

val readJournal =
  PersistenceQuery(system).readJournalFor[MyScaladslReadJournal](JournalId)
val dbBatchWriter: Subscriber[immutable.Seq[Any]] =
  ReactiveStreamsCompatibleDBDriver.batchWriter

// Using an example (Reactive Streams) Database driver
readJournal
  .eventsByPersistenceId("user-1337", fromSequenceNr = 0L, toSequenceNr = Long.MaxValue)
  .map(envelope => envelope.event)
  .map(convertToReadSideTypes) // convert to datatype
  .grouped(20) // batch inserts into groups of 20
  .runWith(Sink.fromSubscriber(dbBatchWriter)) // write batches to read-side database


Java




copy
source
final ReactiveStreamsCompatibleDBDriver driver = new ReactiveStreamsCompatibleDBDriver();
final Subscriber<List<Object>> dbBatchWriter = driver.batchWriter();

// Using an example (Reactive Streams) Database driver
readJournal
    .eventsByPersistenceId("user-1337", 0L, Long.MAX_VALUE)
    .map(envelope -> envelope.event())
    .grouped(20) // batch inserts into groups of 20
    .runWith(Sink.fromSubscriber(dbBatchWriter), system); // write batches to read-side database




Materialize view using mapAsync


If the target database does not provide a reactive streams 
Subscriber
 that can perform writes, you may have to implement the write logic using plain functions or Actors instead.


In case your write logic is state-less and you need to convert the events from one data type to another before writing into the alternative datastore, then the projection will look like this:




Scala




copy
source
trait ExampleStore {
  def save(event: Any): Future[Unit]
}


Java




copy
source
static class ExampleStore {
  CompletionStage<Void> save(Object any) {
    // ...
  }
}






Scala




copy
source
val store: ExampleStore = ???

readJournal
  .eventsByTag("bid", NoOffset)
  .mapAsync(1) { e =>
    store.save(e)
  }
  .runWith(Sink.ignore)


Java




copy
source
final ExampleStore store = new ExampleStore();

readJournal
    .eventsByTag("bid", new Sequence(0L))
    .mapAsync(1, store::save)
    .runWith(Sink.ignore(), system);




Resumable projections


Sometimes you may need to use “resumable” projections, which will not start from the beginning of time each time when run. In such case, the sequence number (or 
offset
) of the processed event will be stored and used the next time this projection is started. This pattern is implemented in the 
Akka Projections
 module.




Query plugins


Query plugins are various (mostly community driven) 
ReadJournal
ReadJournal
 implementations for all kinds of available datastores. The complete list of available plugins is maintained on the Akka Persistence Query 
Community Plugins
 page.


This section aims to provide tips and guide plugin developers through implementing a custom query plugin. Most users will not need to implement journals themselves, except if targeting a not yet supported datastore.
Note


Since different data stores provide different query capabilities journal plugins 
must extensively document
 their exposed semantics as well as handled query scenarios.


ReadJournal plugin API


A read journal plugin must implement 
akka.persistence.query.ReadJournalProvider
akka.persistence.query.ReadJournalProvider
 which creates instances of 
akka.persistence.query.scaladsl.ReadJournal
 and 
akka.persistence.query.javadsl.ReadJournal
. The plugin must implement both the 
scaladsl
 and the 
javadsl
 
traits
interfaces
 because the 
akka.stream.scaladsl.Source
 and 
akka.stream.javadsl.Source
 are different types and even though those types can be converted to each other it is most convenient for the end user to get access to the Java or Scala 
Source
 directly. As illustrated below one of the implementations can delegate to the other. 


Below is a simple journal implementation:




Scala




copy
source
class MyReadJournalProvider(system: ExtendedActorSystem, config: Config) extends ReadJournalProvider {

  private val readJournal: MyScaladslReadJournal =
    new MyScaladslReadJournal(system, config)

  override def scaladslReadJournal(): MyScaladslReadJournal =
    readJournal

  override def javadslReadJournal(): MyJavadslReadJournal =
    new MyJavadslReadJournal(readJournal)
}

class MyScaladslReadJournal(system: ExtendedActorSystem, config: Config)
    extends akka.persistence.query.scaladsl.ReadJournal
    with akka.persistence.query.scaladsl.EventsByTagQuery
    with akka.persistence.query.scaladsl.EventsByPersistenceIdQuery
    with akka.persistence.query.scaladsl.PersistenceIdsQuery
    with akka.persistence.query.scaladsl.CurrentPersistenceIdsQuery {

  private val refreshInterval: FiniteDuration =
    config.getDuration("refresh-interval", MILLISECONDS).millis

  /**
   * You can use `NoOffset` to retrieve all events with a given tag or retrieve a subset of all
   * events by specifying a `Sequence` `offset`. The `offset` corresponds to an ordered sequence number for
   * the specific tag. Note that the corresponding offset of each event is provided in the
   * [[akka.persistence.query.EventEnvelope]], which makes it possible to resume the
   * stream at a later point from a given offset.
   *
   * The `offset` is exclusive, i.e. the event with the exact same sequence number will not be included
   * in the returned stream. This means that you can use the offset that is returned in `EventEnvelope`
   * as the `offset` parameter in a subsequent query.
   */
  override def eventsByTag(tag: String, offset: Offset): Source[EventEnvelope, NotUsed] = offset match {
    case Sequence(offsetValue) =>
      Source.fromGraph(new MyEventsByTagSource(tag, offsetValue, refreshInterval))
    case NoOffset => eventsByTag(tag, Sequence(0L)) //recursive
    case _ =>
      throw new IllegalArgumentException("MyJournal does not support " + offset.getClass.getName + " offsets")
  }

  override def eventsByPersistenceId(
      persistenceId: String,
      fromSequenceNr: Long,
      toSequenceNr: Long): Source[EventEnvelope, NotUsed] = {
    // implement in a similar way as eventsByTag
    ???
  }

  override def persistenceIds(): Source[String, NotUsed] = {
    // implement in a similar way as eventsByTag
    ???
  }

  override def currentPersistenceIds(): Source[String, NotUsed] = {
    // implement in a similar way as eventsByTag
    ???
  }

  // possibility to add more plugin specific queries

  def byTagsWithMeta(tags: Set[String]): Source[RichEvent, QueryMetadata] = {
    // implement in a similar way as eventsByTag
    ???
  }

}

class MyJavadslReadJournal(scaladslReadJournal: MyScaladslReadJournal)
    extends akka.persistence.query.javadsl.ReadJournal
    with akka.persistence.query.javadsl.EventsByTagQuery
    with akka.persistence.query.javadsl.EventsByPersistenceIdQuery
    with akka.persistence.query.javadsl.PersistenceIdsQuery
    with akka.persistence.query.javadsl.CurrentPersistenceIdsQuery {

  override def eventsByTag(tag: String, offset: Offset = Sequence(0L)): javadsl.Source[EventEnvelope, NotUsed] =
    scaladslReadJournal.eventsByTag(tag, offset).asJava

  override def eventsByPersistenceId(
      persistenceId: String,
      fromSequenceNr: Long = 0L,
      toSequenceNr: Long = Long.MaxValue): javadsl.Source[EventEnvelope, NotUsed] =
    scaladslReadJournal.eventsByPersistenceId(persistenceId, fromSequenceNr, toSequenceNr).asJava

  override def persistenceIds(): javadsl.Source[String, NotUsed] =
    scaladslReadJournal.persistenceIds().asJava

  override def currentPersistenceIds(): javadsl.Source[String, NotUsed] =
    scaladslReadJournal.currentPersistenceIds().asJava

  // possibility to add more plugin specific queries

  def byTagsWithMeta(tags: java.util.Set[String]): javadsl.Source[RichEvent, QueryMetadata] = {
    import scala.jdk.CollectionConverters._
    scaladslReadJournal.byTagsWithMeta(tags.asScala.toSet).asJava
  }
}



Java




copy
source
static class MyReadJournalProvider implements ReadJournalProvider {
  private final MyJavadslReadJournal javadslReadJournal;

  public MyReadJournalProvider(ExtendedActorSystem system, Config config) {
    this.javadslReadJournal = new MyJavadslReadJournal(system, config);
  }

  @Override
  public MyScaladslReadJournal scaladslReadJournal() {
    return new MyScaladslReadJournal(javadslReadJournal);
  }

  @Override
  public MyJavadslReadJournal javadslReadJournal() {
    return this.javadslReadJournal;
  }
}
static class MyJavadslReadJournal
    implements akka.persistence.query.javadsl.ReadJournal,
        akka.persistence.query.javadsl.EventsByTagQuery,
        akka.persistence.query.javadsl.EventsByPersistenceIdQuery,
        akka.persistence.query.javadsl.PersistenceIdsQuery,
        akka.persistence.query.javadsl.CurrentPersistenceIdsQuery {

  private final Duration refreshInterval;
  private Connection conn;

  public MyJavadslReadJournal(ExtendedActorSystem system, Config config) {
    refreshInterval = config.getDuration("refresh-interval");
  }

  /**
   * You can use `NoOffset` to retrieve all events with a given tag or retrieve a subset of all
   * events by specifying a `Sequence` `offset`. The `offset` corresponds to an ordered sequence
   * number for the specific tag. Note that the corresponding offset of each event is provided in
   * the [[akka.persistence.query.EventEnvelope]], which makes it possible to resume the stream at
   * a later point from a given offset.
   *
   * <p>The `offset` is exclusive, i.e. the event with the exact same sequence number will not be
   * included in the returned stream. This means that you can use the offset that is returned in
   * `EventEnvelope` as the `offset` parameter in a subsequent query.
   */
  @Override
  public Source<EventEnvelope, NotUsed> eventsByTag(String tag, Offset offset) {
    if (offset instanceof Sequence) {
      Sequence sequenceOffset = (Sequence) offset;
      return Source.fromGraph(
          new MyEventsByTagSource(conn, tag, sequenceOffset.value(), refreshInterval));
    } else if (offset == NoOffset.getInstance())
      return eventsByTag(tag, Offset.sequence(0L)); // recursive
    else
      throw new IllegalArgumentException(
          "MyJavadslReadJournal does not support " + offset.getClass().getName() + " offsets");
  }

  @Override
  public Source<EventEnvelope, NotUsed> eventsByPersistenceId(
      String persistenceId, long fromSequenceNr, long toSequenceNr) {
    // implement in a similar way as eventsByTag
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  public Source<String, NotUsed> persistenceIds() {
    // implement in a similar way as eventsByTag
    throw new UnsupportedOperationException("Not implemented yet");
  }

  @Override
  public Source<String, NotUsed> currentPersistenceIds() {
    // implement in a similar way as eventsByTag
    throw new UnsupportedOperationException("Not implemented yet");
  }

  // possibility to add more plugin specific queries

  public Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) {
    // implement in a similar way as eventsByTag
    throw new UnsupportedOperationException("Not implemented yet");
  }
}
static class MyScaladslReadJournal
    implements akka.persistence.query.scaladsl.ReadJournal,
        akka.persistence.query.scaladsl.EventsByTagQuery,
        akka.persistence.query.scaladsl.EventsByPersistenceIdQuery,
        akka.persistence.query.scaladsl.PersistenceIdsQuery,
        akka.persistence.query.scaladsl.CurrentPersistenceIdsQuery {

  private final MyJavadslReadJournal javadslReadJournal;

  public MyScaladslReadJournal(MyJavadslReadJournal javadslReadJournal) {
    this.javadslReadJournal = javadslReadJournal;
  }

  @Override
  public akka.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByTag(
      String tag, akka.persistence.query.Offset offset) {
    return javadslReadJournal.eventsByTag(tag, offset).asScala();
  }

  @Override
  public akka.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByPersistenceId(
      String persistenceId, long fromSequenceNr, long toSequenceNr) {
    return javadslReadJournal
        .eventsByPersistenceId(persistenceId, fromSequenceNr, toSequenceNr)
        .asScala();
  }

  @Override
  public akka.stream.scaladsl.Source<String, NotUsed> persistenceIds() {
    return javadslReadJournal.persistenceIds().asScala();
  }

  @Override
  public akka.stream.scaladsl.Source<String, NotUsed> currentPersistenceIds() {
    return javadslReadJournal.currentPersistenceIds().asScala();
  }

  // possibility to add more plugin specific queries

  public akka.stream.scaladsl.Source<RichEvent, QueryMetadata> byTagsWithMeta(
      scala.collection.Set<String> tags) {
    Set<String> jTags = scala.jdk.javaapi.CollectionConverters.asJava(tags);
    return javadslReadJournal.byTagsWithMeta(jTags).asScala();
  }
}




And the 
eventsByTag
eventsByTag
 could be backed by a GraphStage for example:




Scala




copy
source
class MyEventsByTagSource(tag: String, offset: Long, refreshInterval: FiniteDuration)
    extends GraphStage[SourceShape[EventEnvelope]] {

  private case object Continue
  val out: Outlet[EventEnvelope] = Outlet("MyEventByTagSource.out")
  override def shape: SourceShape[EventEnvelope] = SourceShape(out)

  override protected def initialAttributes: Attributes = Attributes(ActorAttributes.IODispatcher)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new TimerGraphStageLogic(shape) with OutHandler {
      lazy val system = materializer.system
      private val Limit = 1000
      private val connection: java.sql.Connection = ???
      private var currentOffset = offset
      private var buf = Vector.empty[EventEnvelope]
      private val serialization = SerializationExtension(system)

      override def preStart(): Unit = {
        scheduleWithFixedDelay(Continue, refreshInterval, refreshInterval)
      }

      override def onPull(): Unit = {
        query()
        tryPush()
      }

      override def onDownstreamFinish(cause: Throwable): Unit = {
        // close connection if responsible for doing so
      }

      private def query(): Unit = {
        if (buf.isEmpty) {
          try {
            buf = Select.run(tag, currentOffset, Limit)
          } catch {
            case NonFatal(e) =>
              failStage(e)
          }
        }
      }

      private def tryPush(): Unit = {
        if (buf.nonEmpty && isAvailable(out)) {
          push(out, buf.head)
          buf = buf.tail
        }
      }

      override protected def onTimer(timerKey: Any): Unit = timerKey match {
        case Continue =>
          query()
          tryPush()
      }

      object Select {
        private def statement() =
          connection.prepareStatement("""
            SELECT id, persistence_id, seq_nr, serializer_id, serializer_manifest, payload 
            FROM journal WHERE tag = ? AND id > ? 
            ORDER BY id LIMIT ?
      """)

        def run(tag: String, from: Long, limit: Int): Vector[EventEnvelope] = {
          val s = statement()
          try {
            s.setString(1, tag)
            s.setLong(2, from)
            s.setLong(3, limit)
            val rs = s.executeQuery()

            val b = Vector.newBuilder[EventEnvelope]
            while (rs.next()) {
              val deserialized = serialization
                .deserialize(rs.getBytes("payload"), rs.getInt("serializer_id"), rs.getString("serializer_manifest"))
                .get
              currentOffset = rs.getLong("id")
              b += EventEnvelope(
                Offset.sequence(currentOffset),
                rs.getString("persistence_id"),
                rs.getLong("seq_nr"),
                deserialized,
                System.currentTimeMillis())
            }
            b.result()
          } finally s.close()
        }
      }
    }

}


Java




copy
source
public class MyEventsByTagSource extends GraphStage<SourceShape<EventEnvelope>> {
  public Outlet<EventEnvelope> out = Outlet.create("MyEventByTagSource.out");
  private static final String QUERY =
      "SELECT id, persistence_id, seq_nr, serializer_id, serializer_manifest, payload "
          + "FROM journal WHERE tag = ? AND id > ? "
          + "ORDER BY id LIMIT ?";

  enum Continue {
    INSTANCE;
  }

  private static final int LIMIT = 1000;
  private final Connection connection;
  private final String tag;
  private final long initialOffset;
  private final Duration refreshInterval;

  // assumes a shared connection, could also be a factory for creating connections/pool
  public MyEventsByTagSource(
      Connection connection, String tag, long initialOffset, Duration refreshInterval) {
    this.connection = connection;
    this.tag = tag;
    this.initialOffset = initialOffset;
    this.refreshInterval = refreshInterval;
  }

  @Override
  public Attributes initialAttributes() {
    return Attributes.apply(ActorAttributes.IODispatcher());
  }

  @Override
  public SourceShape<EventEnvelope> shape() {
    return SourceShape.of(out);
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new TimerGraphStageLogic(shape()) {
      private ActorSystem system = materializer().system();
      private long currentOffset = initialOffset;
      private List<EventEnvelope> buf = new LinkedList<>();
      private final Serialization serialization = SerializationExtension.get(system);

      @Override
      public void preStart() {
        scheduleWithFixedDelay(Continue.INSTANCE, refreshInterval, refreshInterval);
      }

      @Override
      public void onTimer(Object timerKey) {
        query();
        deliver();
      }

      private void deliver() {
        if (isAvailable(out) && !buf.isEmpty()) {
          push(out, buf.remove(0));
        }
      }

      private void query() {
        if (buf.isEmpty()) {

          try (PreparedStatement s = connection.prepareStatement(QUERY)) {
            s.setString(1, tag);
            s.setLong(2, currentOffset);
            s.setLong(3, LIMIT);
            try (ResultSet rs = s.executeQuery()) {
              final List<EventEnvelope> res = new ArrayList<>(LIMIT);
              while (rs.next()) {
                Object deserialized =
                    serialization
                        .deserialize(
                            rs.getBytes("payload"),
                            rs.getInt("serializer_id"),
                            rs.getString("serializer_manifest"))
                        .get();
                currentOffset = rs.getLong("id");
                res.add(
                    new EventEnvelope(
                        Offset.sequence(currentOffset),
                        rs.getString("persistence_id"),
                        rs.getLong("seq_nr"),
                        deserialized,
                        System.currentTimeMillis()));
              }
              buf = res;
            }
          } catch (Exception e) {
            failStage(e);
          }
        }
      }

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() {
                query();
                deliver();
              }
            });
      }
    };
  }
}




The 
ReadJournalProvider
ReadJournalProvider
 class must have a constructor with one of these signatures:




constructor with a 
ExtendedActorSystem
ExtendedActorSystem
 parameter, a 
com.typesafe.config.Config
 parameter, and a 
String
 parameter for the config path


constructor with a 
ExtendedActorSystem
 parameter, and a 
com.typesafe.config.Config
 parameter


constructor with one 
ExtendedActorSystem
 parameter


constructor without parameters




The plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the 
String
 parameter.


If the underlying datastore only supports queries that are completed when they reach the end of the “result set”, the journal has to submit new queries after a while in order to support “infinite” event streams that include events stored after the initial query has completed. It is recommended that the plugin use a configuration property named 
refresh-interval
 for defining such a refresh interval. 


Scaling out


In a use case where the number of events are very high, the work needed for each event is high or where resilience is important so that if a node crashes the persistent queries are quickly started on a new node and can resume operations 
Cluster Sharding
 together with event tagging is an excellent fit to shard events over a cluster.


Example project


The 
Microservices with Akka tutorial
 explains how to use Event Sourcing and Projections together. The events are tagged to be consumed by even processors to build other representations from the events, or publish the events to other services.














 
Schema Evolution for Event Sourced Actors






Persistence Query for LevelDB 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/persistence-testing.html
Testing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing




Module info


Unit testing with the BehaviorTestKit


Unit testing with the the ActorTestKit and EventSourcedBehaviorTestKit


Persistence TestKit


Integration testing




Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing




Module info


Unit testing with the BehaviorTestKit


Unit testing with the the ActorTestKit and EventSourcedBehaviorTestKit


Persistence TestKit


Integration testing




Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Testing


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Persistence TestKit, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}




Project Info: Akka Persistence Testkit


Artifact
com.typesafe.akka


akka-persistence-testkit


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.persistence.testkit


License
BUSL-1.1




Readiness level
Incubating


Since 2.6.5, 2020-04-30




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Unit testing with the BehaviorTestKit


Note!
 The 
UnpersistentBehavior
 is a new feature: the API may have changes breaking source compatibility in future versions.


Unit testing of 
EventSourcedBehavior
 can be performed by converting it into an 
UnpersistentBehavior
UnpersistentBehavior
. Instead of persisting events and snapshots, the 
UnpersistentBehavior
 exposes 
PersistenceProbe
PersistenceProbe
s for events and snapshots which can be asserted on.




Scala




copy
source
private def onAnEmptyAccount
    : UnpersistentBehavior.EventSourced[AccountEntity.Command, AccountEntity.Event, AccountEntity.Account] =
  UnpersistentBehavior.fromEventSourced(AccountEntity("1", PersistenceId("Account", "1")))


Java




copy
source
UnpersistentBehavior.fromEventSourced(
    AccountEntity.create("1", PersistenceId.of("Account", "1")),
    null, // use the initial state
    0 // initial sequence number
    );




The 
UnpersistentBehavior
 can be initialized with arbitrary states:




Scala




copy
source
private def onAnOpenedAccount
    : UnpersistentBehavior.EventSourced[AccountEntity.Command, AccountEntity.Event, AccountEntity.Account] =
  UnpersistentBehavior.fromEventSourced(
    AccountEntity("1", PersistenceId("Account", "1")),
    Some(
      AccountEntity.EmptyAccount.applyEvent(AccountEntity.AccountCreated) -> // reuse the event handler
      1L // assume that CreateAccount was the first command
    ))


Java




copy
source
UnpersistentBehavior.fromEventSourced(
    AccountEntity.create("1", PersistenceId.of("Account", "1")),
    new AccountEntity.EmptyAccount()
        .openedAccount(), // duplicate the event handler for AccountCreated on an EmptyAccount
    1 // assume that CreateAccount was the first command
    );




The 
UnpersistentBehavior
 is especially well-suited to the synchronous 
BehaviorTestKit
: the 
UnpersistentBehavior
 can directly construct a 
BehaviorTestKit
 wrapping the behavior. When commands are run by 
BehaviorTestKit
, they are processed in the calling thread (viz. the test suite), so when the run returns, the suite can be sure that the message has been fully processed. The internal state of the 
EventSourcedBehavior
 is not exposed to the suite except to the extent that it affects how the behavior responds to commands or the events it persists (in addition, any snapshots made by the behavior are available through a 
PersistenceProbe
).


A full test for the 
AccountEntity
, which is shown in the 
Persistence Style Guide
 might look like:




Scala




copy
source
import akka.persistence.testkit.scaladsl.UnpersistentBehavior
import akka.persistence.typed.PersistenceId

class AccountExampleUnpersistentDocSpec
    extends AnyWordSpecLike
    {
  "Account" must {
    "be created with zero balance" in {
      onAnEmptyAccount { (testkit, eventProbe, snapshotProbe) =>
        testkit.runAskWithStatus[Done](AccountEntity.CreateAccount(_)).expectDone()

        eventProbe.expectPersisted(AccountEntity.AccountCreated)

        // internal state is only exposed by the behavior via responses to messages or if it happens
        //  to snapshot.  This particular behavior never snapshots, so we query within the actor's
        //  protocol
        snapshotProbe.hasEffects shouldBe false

        testkit.runAsk[AccountEntity.CurrentBalance](AccountEntity.GetBalance(_)).receiveReply().balance shouldBe 0
      }
    }

    "handle Deposit and Withdraw" in {
      onAnOpenedAccount { (testkit, eventProbe, _) =>
        testkit.runAskWithStatus[Done](AccountEntity.Deposit(100, _)).expectDone()

        eventProbe.expectPersisted(AccountEntity.Deposited(100))

        testkit.runAskWithStatus[Done](AccountEntity.Withdraw(10, _)).expectDone()

        eventProbe.expectPersisted(AccountEntity.Withdrawn(10))

        testkit.runAsk[AccountEntity.CurrentBalance](AccountEntity.GetBalance(_)).receiveReply().balance shouldBe 90
      }
    }

    "reject Withdraw overdraft" in {
      onAnAccountWithBalance(100) { (testkit, eventProbe, _) =>
        testkit.runAskWithStatus(AccountEntity.Withdraw(110, _)).receiveStatusReply().isError shouldBe true

        eventProbe.hasEffects shouldBe false
      }
    }
  }
}


Java




copy
source
import java.math.BigDecimal;
import akka.actor.testkit.typed.javadsl.BehaviorTestKit;
import akka.actor.testkit.typed.javadsl.ReplyInbox;
import akka.actor.testkit.typed.javadsl.StatusReplyInbox;
import akka.actor.testkit.typed.javadsl.TestInbox;
import akka.persistence.testkit.javadsl.UnpersistentBehavior;
import akka.persistence.testkit.javadsl.PersistenceEffect;
import akka.persistence.typed.PersistenceId;

import org.junit.Test;

public class AccountExampleUnpersistentDocTest
{
  @Test
  public void createWithEmptyBalance() {
    UnpersistentBehavior<AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>
        unpersistent = emptyAccount();

    BehaviorTestKit<AccountEntity.Command> testkit = unpersistent.getBehaviorTestKit();

    StatusReplyInbox<Done> ackInbox = testkit.runAskWithStatus(AccountEntity.CreateAccount::new);

    ackInbox.expectValue(Done.getInstance());
    unpersistent.getEventProbe().expectPersisted(AccountEntity.AccountCreated.INSTANCE);

    // internal state is only exposed by the behavior via responses to messages or if it happens
    //  to snapshot.  This particular behavior never snapshots, so we query within the actor's
    //  protocol
    assertFalse(unpersistent.getSnapshotProbe().hasEffects());

    ReplyInbox<AccountEntity.CurrentBalance> currentBalanceInbox =
        testkit.runAsk(AccountEntity.GetBalance::new);

    assertEquals(BigDecimal.ZERO, currentBalanceInbox.receiveReply().balance);
  }

  @Test
  public void handleDepositAndWithdraw() {
    UnpersistentBehavior<AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>
        unpersistent = openedAccount();

    BehaviorTestKit<AccountEntity.Command> testkit = unpersistent.getBehaviorTestKit();
    BigDecimal currentBalance;

    testkit
        .runAskWithStatus(
            Done.class, replyTo -> new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo))
        .expectValue(Done.getInstance());

    assertEquals(
        BigDecimal.valueOf(100),
        unpersistent
            .getEventProbe()
            .expectPersistedClass(AccountEntity.Deposited.class)
            .persistedObject()
            .amount);

    currentBalance =
        testkit
            .runAsk(AccountEntity.CurrentBalance.class, AccountEntity.GetBalance::new)
            .receiveReply()
            .balance;

    assertEquals(BigDecimal.valueOf(100), currentBalance);

    testkit
        .runAskWithStatus(
            Done.class, replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(10), replyTo))
        .expectValue(Done.getInstance());

    // can save the persistence effect for in-depth inspection
    PersistenceEffect<AccountEntity.Withdrawn> withdrawEffect =
        unpersistent.getEventProbe().expectPersistedClass(AccountEntity.Withdrawn.class);
    assertEquals(BigDecimal.valueOf(10), withdrawEffect.persistedObject().amount);
    assertEquals(3L, withdrawEffect.sequenceNr());
    assertTrue(withdrawEffect.tags().isEmpty());

    currentBalance =
        testkit
            .runAsk(AccountEntity.CurrentBalance.class, AccountEntity.GetBalance::new)
            .receiveReply()
            .balance;

    assertEquals(BigDecimal.valueOf(90), currentBalance);
  }

  @Test
  public void rejectWithdrawOverdraft() {
    UnpersistentBehavior<AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>
        unpersistent = accountWithBalance(BigDecimal.valueOf(100));

    BehaviorTestKit<AccountEntity.Command> testkit = unpersistent.getBehaviorTestKit();

    testkit
        .runAskWithStatus(
            Done.class, replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(110), replyTo))
        .expectErrorMessage("not enough funds to withdraw 110");

    assertFalse(unpersistent.getEventProbe().hasEffects());
  }

}




UnpersistentBehavior
 does not require any configuration. It therefore does not verify the serialization of commands, events, or state. If using this style, it is advised to independently test serialization for those classes.


Unit testing with the the ActorTestKit and EventSourcedBehaviorTestKit


Note!
 The 
EventSourcedBehaviorTestKit
 is a new feature: the API may have changes breaking source compatibility in future versions.


Unit testing of 
EventSourcedBehavior
 can be done with the 
EventSourcedBehaviorTestKit
EventSourcedBehaviorTestKit
. It supports running one command at a time and you can assert that the synchronously returned result is as expected. The result contains the events emitted by the command and the new state after applying the events. It also has support for verifying the reply to a command.


You need to configure the 
ActorSystem
 with the 
EventSourcedBehaviorTestKit.config
. The configuration enables the in-memory journal and snapshot storage.




Scala




copy
source
class AccountExampleDocSpec
    extends ScalaTestWithActorTestKit(EventSourcedBehaviorTestKit.config)


Java




copy
source
@ClassRule
public static final TestKitJunitResource testKit =
    new TestKitJunitResource(EventSourcedBehaviorTestKit.config());

private EventSourcedBehaviorTestKit<
        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>
    eventSourcedTestKit =
        EventSourcedBehaviorTestKit.create(
            testKit.system(), AccountEntity.create("1", PersistenceId.of("Account", "1")));




A full test for the 
AccountEntity
, which is shown in the 
Persistence Style Guide
, may look like this:




Scala




copy
source
import akka.Done
import akka.persistence.testkit.scaladsl.EventSourcedBehaviorTestKit
import akka.persistence.typed.PersistenceId
import akka.actor.testkit.typed.scaladsl.LogCapturing
import akka.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit
import akka.pattern.StatusReply
import org.scalatest.BeforeAndAfterEach
import org.scalatest.wordspec.AnyWordSpecLike

class AccountExampleDocSpec
    extends ScalaTestWithActorTestKit(EventSourcedBehaviorTestKit.config)
    with AnyWordSpecLike
    with BeforeAndAfterEach
    with LogCapturing {

  private val eventSourcedTestKit =
    EventSourcedBehaviorTestKit[AccountEntity.Command, AccountEntity.Event, AccountEntity.Account](
      system,
      AccountEntity("1", PersistenceId("Account", "1")))

  override protected def beforeEach(): Unit = {
    super.beforeEach()
    eventSourcedTestKit.clear()
  }

  "Account" must {

    "be created with zero balance" in {
      val result = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))
      result.reply shouldBe StatusReply.Ack
      result.event shouldBe AccountEntity.AccountCreated
      result.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 0
    }

    "handle Withdraw" in {
      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))

      val result1 = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))
      result1.reply shouldBe StatusReply.Ack
      result1.event shouldBe AccountEntity.Deposited(100)
      result1.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 100

      val result2 = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Withdraw(10, _))
      result2.reply shouldBe StatusReply.Ack
      result2.event shouldBe AccountEntity.Withdrawn(10)
      result2.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 90
    }

    "reject Withdraw overdraft" in {
      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))
      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))

      val result = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Withdraw(110, _))
      result.reply.isError shouldBe true
      result.hasNoEvents shouldBe true
    }

    "handle GetBalance" in {
      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))
      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))

      val result = eventSourcedTestKit.runCommand[AccountEntity.CurrentBalance](AccountEntity.GetBalance(_))
      result.reply.balance shouldBe 100
      result.hasNoEvents shouldBe true
    }
  }
}


Java




copy
source
import java.math.BigDecimal;
import akka.actor.testkit.typed.javadsl.LogCapturing;
import akka.actor.testkit.typed.javadsl.TestKitJunitResource;
import akka.actor.typed.ActorRef;
import akka.persistence.testkit.javadsl.EventSourcedBehaviorTestKit;
import akka.persistence.testkit.javadsl.EventSourcedBehaviorTestKit.CommandResultWithReply;
import akka.persistence.typed.PersistenceId;

import org.junit.Before;
import org.junit.ClassRule;
import org.junit.Rule;
import org.junit.Test;

public class AccountExampleDocTest
{

  @ClassRule
  public static final TestKitJunitResource testKit =
      new TestKitJunitResource(EventSourcedBehaviorTestKit.config());

  private EventSourcedBehaviorTestKit<
          AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>
      eventSourcedTestKit =
          EventSourcedBehaviorTestKit.create(
              testKit.system(), AccountEntity.create("1", PersistenceId.of("Account", "1")));

  @Rule public final LogCapturing logCapturing = new LogCapturing();

  @Before
  public void beforeEach() {
    eventSourcedTestKit.clear();
  }

  @Test
  public void createWithEmptyBalance() {
    CommandResultWithReply<
            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>
        result = eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);
    assertEquals(StatusReply.ack(), result.reply());
    assertEquals(AccountEntity.AccountCreated.INSTANCE, result.event());
    assertEquals(BigDecimal.ZERO, result.stateOfType(AccountEntity.OpenedAccount.class).balance);
  }

  @Test
  public void createWithUnHandle() {
    CommandResultWithReply<
            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>
        result = eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);
    assertFalse(result.hasNoReply());
  }

  @Test
  public void handleWithdraw() {
    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);

    CommandResultWithReply<
            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>
        result1 =
            eventSourcedTestKit.runCommand(
                replyTo -> new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));
    assertEquals(StatusReply.ack(), result1.reply());
    assertEquals(
        BigDecimal.valueOf(100), result1.eventOfType(AccountEntity.Deposited.class).amount);
    assertEquals(
        BigDecimal.valueOf(100), result1.stateOfType(AccountEntity.OpenedAccount.class).balance);

    CommandResultWithReply<
            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>
        result2 =
            eventSourcedTestKit.runCommand(
                replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(10), replyTo));
    assertEquals(StatusReply.ack(), result2.reply());
    assertEquals(BigDecimal.valueOf(10), result2.eventOfType(AccountEntity.Withdrawn.class).amount);
    assertEquals(
        BigDecimal.valueOf(90), result2.stateOfType(AccountEntity.OpenedAccount.class).balance);
  }

  @Test
  public void rejectWithdrawOverdraft() {
    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);
    eventSourcedTestKit.runCommand(
        (ActorRef<StatusReply<Done>> replyTo) ->
            new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));

    CommandResultWithReply<
            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>
        result =
            eventSourcedTestKit.runCommand(
                replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(110), replyTo));
    assertTrue(result.reply().isError());
    assertTrue(result.hasNoEvents());
  }

  @Test
  public void handleGetBalance() {
    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);
    eventSourcedTestKit.runCommand(
        (ActorRef<StatusReply<Done>> replyTo) ->
            new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));

    CommandResultWithReply<
            AccountEntity.Command,
            AccountEntity.Event,
            AccountEntity.Account,
            AccountEntity.CurrentBalance>
        result = eventSourcedTestKit.runCommand(AccountEntity.GetBalance::new);
    assertEquals(BigDecimal.valueOf(100), result.reply().balance);
  }
}




Serialization of commands, events and state are verified automatically. The serialization checks can be customized with the 
SerializationSettings
 when creating the 
EventSourcedBehaviorTestKit
. By default, the serialization roundtrip is checked but the equality of the result of the serialization is not checked. 
equals
 must be implemented 
(or using 
case class
)
 in the commands, events and state if 
verifyEquality
 is enabled.


To test recovery the 
restart
 method of the 
EventSourcedBehaviorTestKit
 can be used. It will restart the behavior, which will then recover from stored snapshot and events from previous commands. It’s also possible to populate the storage with events or simulate failures by using the underlying 
PersistenceTestKit
PersistenceTestKit
.


Persistence TestKit


Note!
 The 
PersistenceTestKit
 is a new feature: the API may have changes breaking source compatibility in future versions.


Persistence testkit allows to check events saved in a storage, emulate storage operations and exceptions. To use the testkit you need to add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}


There are two testkit classes which have similar api:




PersistenceTestKit
PersistenceTestKit
 class is for events


SnapshotTestKit
SnapshotTestKit
 class is for snapshots




The testkit classes have two corresponding plugins which emulate the behavior of the storages: 




PersistenceTestKitPlugin
PersistenceTestKitPlugin
 class emulates a events storage


PersistenceTestKitSnapshotPlugin
PersistenceTestKitSnapshotPlugin
 class emulates a snapshots storage




Note!
 The corresponding plugins 
must
 be configured in the actor system which is used to initialize the particular testkit class:




Scala




copy
source
val yourConfiguration = ConfigFactory.defaultApplication()

val system =
  ActorSystem(??? /*some behavior*/, "test-system", PersistenceTestKitPlugin.config.withFallback(yourConfiguration))

val testKit = PersistenceTestKit(system)



Java




copy
source
public class PersistenceTestKitConfig {

  Config conf =
      PersistenceTestKitPlugin.getInstance()
          .config()
          .withFallback(ConfigFactory.defaultApplication());

  ActorSystem<Command> system = ActorSystem.create(new SomeBehavior(), "example", conf);

  PersistenceTestKit testKit = PersistenceTestKit.create(system);
}




and




Scala




copy
source
val yourConfiguration = ConfigFactory.defaultApplication()

val system = ActorSystem(
  ??? /*some behavior*/,
  "test-system",
  PersistenceTestKitSnapshotPlugin.config.withFallback(yourConfiguration))

val testKit = SnapshotTestKit(system)



Java




copy
source
public class SnapshotTestKitConfig {

  Config conf =
      PersistenceTestKitSnapshotPlugin.getInstance()
          .config()
          .withFallback(ConfigFactory.defaultApplication());

  ActorSystem<Command> system = ActorSystem.create(new SomeBehavior(), "example", conf);

  SnapshotTestKit testKit = SnapshotTestKit.create(system);
}




A typical scenario is to create a persistent actor, send commands to it and check that it persists events as it is expected:




Scala




copy
source
import akka.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit
import akka.persistence.testkit.PersistenceTestKitPlugin
import akka.persistence.testkit.scaladsl.PersistenceTestKit

class PersistenceTestKitSampleSpec
    extends ScalaTestWithActorTestKit(PersistenceTestKitPlugin.config.withFallback(ConfigFactory.defaultApplication()))
    with AnyWordSpecLike
    with BeforeAndAfterEach {

  val persistenceTestKit = PersistenceTestKit(system)

  override def beforeEach(): Unit = {
    persistenceTestKit.clearAll()
  }

  "Persistent actor" should {

    "persist all events" in {

      val persistenceId = PersistenceId.ofUniqueId("your-persistence-id")
      val persistentActor = spawn(
        EventSourcedBehavior[Cmd, Evt, State](
          persistenceId,
          emptyState = State.empty,
          commandHandler = (_, cmd) => Effect.persist(Evt(cmd.data)),
          eventHandler = (state, evt) => state.updated(evt)))
      val cmd = Cmd("data")

      persistentActor ! cmd

      val expectedPersistedEvent = Evt(cmd.data)
      persistenceTestKit.expectNextPersisted(persistenceId.id, expectedPersistedEvent)
    }

  }
}


Java




copy
source
public class PersistenceTestKitSampleTest extends AbstractJavaTest {

  @ClassRule
  public static final TestKitJunitResource testKit =
      new TestKitJunitResource(
          PersistenceTestKitPlugin.getInstance()
              .config()
              .withFallback(ConfigFactory.defaultApplication()));

  PersistenceTestKit persistenceTestKit = PersistenceTestKit.create(testKit.system());

  @Before
  public void beforeEach() {
    persistenceTestKit.clearAll();
  }

  @Test
  public void test() {
    PersistenceId persistenceId = PersistenceId.ofUniqueId("some-id");
    ActorRef<YourPersistentBehavior.Cmd> ref =
        testKit.spawn(YourPersistentBehavior.create(persistenceId));

    YourPersistentBehavior.Cmd cmd = new YourPersistentBehavior.Cmd("data");
    ref.tell(cmd);
    YourPersistentBehavior.Evt expectedEventPersisted = new YourPersistentBehavior.Evt(cmd.data);

    persistenceTestKit.expectNextPersisted(persistenceId.id(), expectedEventPersisted);
  }
}

class YourPersistentBehavior
    extends EventSourcedBehavior<
        YourPersistentBehavior.Cmd, YourPersistentBehavior.Evt, YourPersistentBehavior.State> {

  static final class Cmd implements CborSerializable {

    public final String data;

    @JsonCreator
    public Cmd(String data) {
      this.data = data;
    }
  }

  static final class Evt implements CborSerializable {

    public final String data;

    @JsonCreator
    public Evt(String data) {
      this.data = data;
    }

    @Override
    public boolean equals(Object o) {
      if (this == o) return true;
      if (o == null || getClass() != o.getClass()) return false;

      Evt evt = (Evt) o;

      return data.equals(evt.data);
    }

    @Override
    public int hashCode() {
      return data.hashCode();
    }
  }

  static final class State implements CborSerializable {}

  static Behavior<Cmd> create(PersistenceId persistenceId) {
    return Behaviors.setup(context -> new YourPersistentBehavior(persistenceId));
  }

  private YourPersistentBehavior(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    // some state
    return new State();
  }

  @Override
  public CommandHandler<Cmd, Evt, State> commandHandler() {
    return newCommandHandlerBuilder()
        .forAnyState()
        .onCommand(Cmd.class, command -> Effect().persist(new Evt(command.data)))
        .build();
  }

  @Override
  public EventHandler<State, Evt> eventHandler() {
    // TODO handle events
    return newEventHandlerBuilder().forAnyState().onEvent(Evt.class, (state, evt) -> state).build();
  }
}




You can safely use persistence testkit in combination with main akka testkit.


The main methods of the api allow to (see 
PersistenceTestKit
PersistenceTestKit
 and 
SnapshotTestKit
SnapshotTestKit
 for more details):




check if the given event/snapshot object is the next persisted in the storage.


read a sequence of persisted events/snapshots.


check that no events/snapshots have been persisted in the storage.


throw the default exception from the storage on attempt to persist, read or delete the following event/snapshot.


clear the events/snapshots persisted in the storage.


reject the events, but not snapshots (rejections are not supported for snapshots in the original api).


set your own 
policy
 which emulates the work of the storage. Policy determines what to do when persistence needs to execute some operation on the storage (i.e. read, delete, etc.).


get all the events/snapshots persisted in the storage


put the events/snapshots in the storage to test recovery




Setting your own policy for the storage


You can implement and set your own policy for the storage to control its actions on particular operations, for example you can fail or reject events on your own conditions. Implement the 
ProcessingPolicy[EventStorage.JournalOperation]
ProcessingPolicy<EventStorage.JournalOperation>
 
trait
interface
 for event storage or 
ProcessingPolicy[SnapshotStorage.SnapshotOperation]
ProcessingPolicy<SnapshotStorage.SnapshotOperation>
 
trait
interface
 for snapshot storage, and set it with 
withPolicy()
 method.




Scala




copy
source
class PersistenceTestKitSampleSpecWithPolicy
    extends ScalaTestWithActorTestKit(PersistenceTestKitPlugin.config.withFallback(ConfigFactory.defaultApplication()))
    with AnyWordSpecLike
    with BeforeAndAfterEach {

  val persistenceTestKit = PersistenceTestKit(system)

  override def beforeEach(): Unit = {
    persistenceTestKit.clearAll()
    persistenceTestKit.resetPolicy()
  }

  "Testkit policy" should {

    "fail all operations with custom exception" in {
      val policy = new EventStorage.JournalPolicies.PolicyType {

        class CustomFailure extends RuntimeException

        override def tryProcess(persistenceId: String, processingUnit: JournalOperation): ProcessingResult =
          processingUnit match {
            case WriteEvents(_) => StorageFailure(new CustomFailure)
            case _              => ProcessingSuccess
          }
      }
      persistenceTestKit.withPolicy(policy)

      val persistenceId = PersistenceId.ofUniqueId("your-persistence-id")
      val persistentActor = spawn(
        EventSourcedBehavior[Cmd, Evt, State](
          persistenceId,
          emptyState = State.empty,
          commandHandler = (_, cmd) => Effect.persist(Evt(cmd.data)),
          eventHandler = (state, evt) => state.updated(evt)))

      persistentActor ! Cmd("data")
      persistenceTestKit.expectNothingPersisted(persistenceId.id)

    }
  }
}


Java




copy
source
public class PersistenceTestKitPolicySampleTest extends AbstractJavaTest {

  @ClassRule
  public static final TestKitJunitResource testKit =
      new TestKitJunitResource(
          PersistenceTestKitPlugin.getInstance()
              .config()
              .withFallback(ConfigFactory.defaultApplication()));

  PersistenceTestKit persistenceTestKit = PersistenceTestKit.create(testKit.system());

  @Before
  public void beforeEach() {
    persistenceTestKit.clearAll();
    persistenceTestKit.resetPolicy();
  }

  @Test
  public void test() {
    SampleEventStoragePolicy policy = new SampleEventStoragePolicy();
    persistenceTestKit.withPolicy(policy);

    PersistenceId persistenceId = PersistenceId.ofUniqueId("some-id");
    ActorRef<YourPersistentBehavior.Cmd> ref =
        testKit.spawn(YourPersistentBehavior.create(persistenceId));

    YourPersistentBehavior.Cmd cmd = new YourPersistentBehavior.Cmd("data");
    ref.tell(cmd);

    persistenceTestKit.expectNothingPersisted(persistenceId.id());
  }

  static class SampleEventStoragePolicy implements ProcessingPolicy<JournalOperation> {
    @Override
    public ProcessingResult tryProcess(String processId, JournalOperation processingUnit) {
      if (processingUnit instanceof WriteEvents) {
        return StorageFailure.create();
      } else {
        return ProcessingSuccess.getInstance();
      }
    }
  }
}




tryProcess()
 method of the 
ProcessingPolicy
ProcessingPolicy
 has two arguments: persistence id and the storage operation. 


Event storage has the following operations:




ReadEvents
ReadEvents
 Read the events from the storage.


WriteEvents
WriteEvents
 Write the events to the storage.


DeleteEvents
DeleteEvents
 Delete the events from the storage.


ReadSeqNum
ReadSeqNum
 Read the highest sequence number for particular persistence id.




Snapshot storage has the following operations:




ReadSnapshot
ReadSnapshot
 Read the snapshot from the storage.


WriteSnapshot
WriteSnapshot
 Writhe the snapshot to the storage.


DeleteSnapshotsByCriteria
DeleteSnapshotsByCriteria
 Delete snapshots in the storage by criteria.


DeleteSnapshotByMeta
DeleteSnapshotByMeta
 Delete particular snapshot from the storage by its metadata.




The 
tryProcess()
 method must return one of the processing results:




ProcessingSuccess
ProcessingSuccess
 Successful completion of the operation. All the events will be saved/read/deleted.


StorageFailure
StorageFailure
 Emulates exception from the storage.


Reject
Reject
 Emulates rejection from the storage.




Note
 that snapshot storage does not have rejections. If you return 
Reject
 in the 
tryProcess()
 of the snapshot storage policy, it will have the same effect as the 
StorageFailure
.


Here is an example of the policy for an event storage:




Scala




copy
source
import akka.persistence.testkit._

class SampleEventStoragePolicy extends EventStorage.JournalPolicies.PolicyType {

  //you can use internal state, it does not need to be thread safe
  var count = 1

  override def tryProcess(persistenceId: String, processingUnit: JournalOperation): ProcessingResult =
    if (count < 10) {
      count += 1
      //check the type of operation and react with success or with reject or with failure.
      //if you return ProcessingSuccess the operation will be performed, otherwise not.
      processingUnit match {
        case ReadEvents(batch) if batch.nonEmpty => ProcessingSuccess
        case WriteEvents(batch) if batch.size > 1 =>
          ProcessingSuccess
        case ReadSeqNum      => StorageFailure()
        case DeleteEvents(_) => Reject()
        case _               => StorageFailure()
      }
    } else {
      ProcessingSuccess
    }

}


Java




copy
source
class SampleEventStoragePolicy implements ProcessingPolicy<JournalOperation> {

  // you can use internal state, it does not need to be thread safe
  int count = 1;

  @Override
  public ProcessingResult tryProcess(String processId, JournalOperation processingUnit) {
    // check the type of operation and react with success or with reject or with failure.
    // if you return ProcessingSuccess the operation will be performed, otherwise not.
    if (count < 10) {
      count += 1;
      if (processingUnit instanceof ReadEvents) {
        ReadEvents read = (ReadEvents) processingUnit;
        if (read.batch().nonEmpty()) {
          ProcessingSuccess.getInstance();
        } else {
          return StorageFailure.create();
        }
      } else if (processingUnit instanceof WriteEvents) {
        return ProcessingSuccess.getInstance();
      } else if (processingUnit instanceof DeleteEvents) {
        return ProcessingSuccess.getInstance();
      } else if (processingUnit.equals(ReadSeqNum.getInstance())) {
        return Reject.create();
      }
      // you can set your own exception
      return StorageFailure.create(new RuntimeException("your exception"));
    } else {
      return ProcessingSuccess.getInstance();
    }
  }
}




Here is an example of the policy for a snapshot storage:




Scala




copy
source
class SampleSnapshotStoragePolicy extends SnapshotStorage.SnapshotPolicies.PolicyType {

  //you can use internal state, it does not need to be thread safe
  var count = 1

  override def tryProcess(persistenceId: String, processingUnit: SnapshotOperation): ProcessingResult =
    if (count < 10) {
      count += 1
      //check the type of operation and react with success or with reject or with failure.
      //if you return ProcessingSuccess the operation will be performed, otherwise not.
      processingUnit match {
        case ReadSnapshot(_, payload) if payload.nonEmpty =>
          ProcessingSuccess
        case WriteSnapshot(meta, payload) if meta.sequenceNr > 10 =>
          ProcessingSuccess
        case DeleteSnapshotsByCriteria(_) => StorageFailure()
        case DeleteSnapshotByMeta(meta) if meta.sequenceNr < 10 =>
          ProcessingSuccess
        case _ => StorageFailure()
      }
    } else {
      ProcessingSuccess
    }
}


Java




copy
source
class SnapshotStoragePolicy implements ProcessingPolicy<SnapshotOperation> {

  // you can use internal state, it doesn't need to be thread safe
  int count = 1;

  @Override
  public ProcessingResult tryProcess(String processId, SnapshotOperation processingUnit) {
    // check the type of operation and react with success or with failure.
    // if you return ProcessingSuccess the operation will be performed, otherwise not.
    if (count < 10) {
      count += 1;
      if (processingUnit instanceof ReadSnapshot) {
        ReadSnapshot read = (ReadSnapshot) processingUnit;
        if (read.getSnapshot().isPresent()) {
          ProcessingSuccess.getInstance();
        } else {
          return StorageFailure.create();
        }
      } else if (processingUnit instanceof WriteSnapshot) {
        return ProcessingSuccess.getInstance();
      } else if (processingUnit instanceof DeleteSnapshotsByCriteria) {
        return ProcessingSuccess.getInstance();
      } else if (processingUnit instanceof DeleteSnapshotByMeta) {
        return ProcessingSuccess.getInstance();
      }
      // you can set your own exception
      return StorageFailure.create(new RuntimeException("your exception"));
    } else {
      return ProcessingSuccess.getInstance();
    }
  }
}




Configuration of Persistence TestKit


There are several configuration properties for persistence testkit, please refer to the 
reference configuration


Integration testing


EventSourcedBehavior
 actors can be tested with the 
ActorTestKit
 together with other actors. The in-memory journal and snapshot storage from the 
Persistence TestKit
 can be used also for integration style testing of a single 
ActorSystem
, for example when using Cluster Sharding with a single Cluster node.


For tests that involve more than one Cluster node you have to use another journal and snapshot store. While it’s possible to use the 
Persistence Plugin Proxy
 it’s often better and more realistic to use a real database.


The 
Microservices with Akka tutorial
 includes tests that are using a real database.


Plugin initialization


Some Persistence plugins create tables automatically, but has the limitation that it can’t be done concurrently from several ActorSystems. That can be a problem if the test creates a Cluster and all nodes tries to initialize the plugins at the same time. To coordinate initialization you can use the 
PersistenceInit
 utility.


PersistenceInit
 is part of 
akka-persistence-testkit
 and you need to add the dependency to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}




Scala




copy
source
import akka.persistence.testkit.scaladsl.PersistenceInit

import scala.concurrent.Await
import scala.concurrent.Future
import scala.concurrent.duration._

val timeout = 5.seconds
val done: Future[Done] = PersistenceInit.initializeDefaultPlugins(system, timeout)
Await.result(done, timeout)


Java




copy
source
import akka.persistence.testkit.javadsl.PersistenceInit;
import akka.Done;

import java.time.Duration;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.TimeUnit;

Duration timeout = Duration.ofSeconds(5);
CompletionStage<Done> done =
    PersistenceInit.initializeDefaultPlugins(testKit.system(), timeout);
done.toCompletableFuture().get(timeout.getSeconds(), TimeUnit.SECONDS);
















 
Snapshotting






Schema Evolution for Event Sourced Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-grpc/current/
Akka gRPC







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka gRPC





Version 2.5.3





Java
Scala


sbt
Gradle
Maven










Overview


Why gRPC?


Getting started


Protobuf Service Descriptors


Providing Services (Server)


Consuming Services (Client)


Build Tool Support


Binary compatibility


API Design


Deployment


Mutual authentication with TLS


Troubleshooting


















Akka gRPC





Version 2.5.3





Java
Scala


sbt
Gradle
Maven












Overview


Why gRPC?


Getting started


Protobuf Service Descriptors


Providing Services (Server)


Consuming Services (Client)


Build Tool Support


Binary compatibility


API Design


Deployment


Mutual authentication with TLS


Troubleshooting




















Akka gRPC






Overview




gRPC


Akka gRPC


Project Information


Project Status




Why gRPC?




gRPC vs REST


gRPC vs SOAP


gRPC vs Message Bus


gRPC vs Akka Remoting




Getting started




Akka gRPC Quickstart


Video Introduction




Protobuf Service Descriptors




Messages


Services


Code generation options




Providing Services (Server)




Walkthrough




Setting up




Dependencies




Writing a service definition


Generating interfaces and stubs


Implementing the service


Serving the service with Akka HTTP


Serving multiple services


Running the server


Stateful services




gRPC-Web


Server Reflection




Providing


Consuming




Akka HTTP interop




Example: authentication/authorization




Akka HTTP authentication route


Akka gRPC route


Securing the Akka gRPC route


Tying it all together




Example: logging, error handling, and passing request context




Implementation


Method to log, handle, and recover each RPC


Custom error mapping


Tying it all together


Results




Future work




Details




Accessing request metadata


Status codes


Rich error model




Kubernetes




LoadBalancer Service


NGINX Ingress


GCE Ingress


Google Cloud Endpoints






Consuming Services (Client)




Walkthrough




Setting up




Dependencies




Generating Service Stubs


Writing a Client Program




Configuration




By Code


By Configuration


Using Akka Discovery for Endpoint Discovery


Debug logging




Details




Client Lifecycle


Shared Channels


Channel laziness


Load balancing


Request Metadata


Rich error model






Build Tool Support




sbt




Configuring what to generate




Configurations


Generating server “power APIs”




Passing parameters to the generators




ScalaPB settings




Using a local 
protoc
 command




sbt-protoc
 settings




Loading proto files from artifacts


Starting your Akka gRPC server from sbt




Gradle




Configuring plugin




Installation


Available plugin options


Generating server “power APIs”




Protoc version


Proto source directory


Loading proto files from artifacts


Starting your Akka gRPC server from gradle




Maven




Configuring what to generate




Generating server “power APIs”




Proto source directory


Loading proto files from artifacts


Starting your Akka gRPC server from Maven






Binary compatibility




Limitations




New features


Deprecations


Internal and ApiMayChange API’s




Upstream libraries




API Design




Methods without request or response


Declare and enforce constraints for your request and response payloads with 
protoc-gen-validate




Java support


Scala support






Deployment




Serve gRPC over HTTPS


Building Native Images




Mutual authentication with TLS




Setting the server up


Setting the client up


Further limiting of access using client certificate identities




Troubleshooting




Client


Server
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka gRPC is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-testkit.html
Testing streams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams




Dependency


Introduction


Built-in sources, sinks and operators


TestKit


Streams TestKit


Fuzzing Mode




Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams




Dependency


Introduction


Built-in sources, sinks and operators


TestKit


Streams TestKit


Fuzzing Mode




Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Testing streams


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Stream TestKit, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream-testkit" % AkkaVersion % Test
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  testImplementation "com.typesafe.akka:akka-stream-testkit_${versions.ScalaBinary}"
}


Introduction


Verifying behavior of Akka Stream sources, flows and sinks can be done using various code patterns and libraries. Here we will discuss testing these elements using:




simple sources, sinks and flows;


sources and sinks in combination with 
TestProbe
TestProbe
 from the 
akka-testkit
 module;


sources and sinks specifically crafted for writing tests from the 
akka-stream-testkit
 module.




It is important to keep your data processing pipeline as separate sources, flows and sinks. This makes them testable by wiring them up to other sources or sinks, or some test harnesses that 
akka-testkit
 or 
akka-stream-testkit
 provide.


Built-in sources, sinks and operators


Testing a custom sink can be as simple as attaching a source that emits elements from a predefined collection, running a constructed test flow and asserting on the results that sink produced. Here is an example of a test for a sink:




Scala




copy
source
val sinkUnderTest =
  Flow[Int].map(_ * 2).toMat(Sink.fold(0)(_ + _))(Keep.right)

val future = Source(1 to 4).runWith(sinkUnderTest)
val result = Await.result(future, 3.seconds)
assert(result == 20)


Java




copy
source
final Sink<Integer, CompletionStage<Integer>> sinkUnderTest =
    Flow.of(Integer.class)
        .map(i -> i * 2)
        .toMat(Sink.fold(0, (agg, next) -> agg + next), Keep.right());

final CompletionStage<Integer> future =
    Source.from(Arrays.asList(1, 2, 3, 4)).runWith(sinkUnderTest, system);
final Integer result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);
assertEquals(20, result.intValue());




The same strategy can be applied for sources as well. In the next example we have a source that produces an infinite stream of elements. Such source can be tested by asserting that first arbitrary number of elements hold some condition. Here the 
take
take
 operator and 
Sink.seq
Sink.seq
 are very useful.




Scala




copy
source
val sourceUnderTest = Source.repeat(1).map(_ * 2)

val future = sourceUnderTest.take(10).runWith(Sink.seq)
val result = Await.result(future, 3.seconds)
assert(result == Seq.fill(10)(2))


Java




copy
source
final Source<Integer, NotUsed> sourceUnderTest = Source.repeat(1).map(i -> i * 2);

final CompletionStage<List<Integer>> future =
    sourceUnderTest.take(10).runWith(Sink.seq(), system);
final List<Integer> result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);
assertEquals(Collections.nCopies(10, 2), result);




When testing a flow we need to attach a source and a sink. As both stream ends are under our control, we can choose sources that tests various edge cases of the flow and sinks that ease assertions.




Scala




copy
source
val flowUnderTest = Flow[Int].takeWhile(_ < 5)

val future = Source(1 to 10).via(flowUnderTest).runWith(Sink.fold(Seq.empty[Int])(_ :+ _))
val result = Await.result(future, 3.seconds)
assert(result == (1 to 4))


Java




copy
source
final Flow<Integer, Integer, NotUsed> flowUnderTest =
    Flow.of(Integer.class).takeWhile(i -> i < 5);

final CompletionStage<Integer> future =
    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6))
        .via(flowUnderTest)
        .runWith(Sink.fold(0, (agg, next) -> agg + next), system);
final Integer result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);
assertEquals(10, result.intValue());




TestKit


Akka Stream offers integration with Actors out of the box. This support can be used for writing stream tests that use familiar 
TestProbe
TestProbe
 from the 
akka-testkit
 API.


One of the more straightforward tests would be to materialize stream to a 
Future
CompletionStage
 and then use 
pipe
Patterns.pipe
 pattern to pipe the result of that future to the probe.




Scala




copy
source
import system.dispatcher
import akka.pattern.pipe

val sourceUnderTest = Source(1 to 4).grouped(2)

val probe = TestProbe()
sourceUnderTest.runWith(Sink.seq).pipeTo(probe.ref)
probe.expectMsg(3.seconds, Seq(Seq(1, 2), Seq(3, 4)))


Java




copy
source
final Source<List<Integer>, NotUsed> sourceUnderTest =
    Source.from(Arrays.asList(1, 2, 3, 4)).grouped(2);

final TestKit probe = new TestKit(system);
final CompletionStage<List<List<Integer>>> future =
    sourceUnderTest.grouped(2).runWith(Sink.head(), system);
akka.pattern.Patterns.pipe(future, system.dispatcher()).to(probe.getRef());
probe.expectMsg(Duration.ofSeconds(3), Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4)));




Instead of materializing to a future, we can use a 
Sink.actorRef
Sink.actorRef
 that sends all incoming elements to the given 
ActorRef
ActorRef
. Now we can use assertion methods on 
TestProbe
TestProbe
 and expect elements one by one as they arrive. We can also assert stream completion by expecting for 
onCompleteMessage
 which was given to 
Sink.actorRef
.




Scala




copy
source
case object Tick
val sourceUnderTest = Source.tick(0.seconds, 200.millis, Tick)

val probe = TestProbe()
val cancellable = sourceUnderTest
  .to(Sink.actorRef(probe.ref, onCompleteMessage = "completed", onFailureMessage = _ => "failed"))
  .run()

probe.expectMsg(1.second, Tick)
probe.expectNoMessage(100.millis)
probe.expectMsg(3.seconds, Tick)
cancellable.cancel()
probe.expectMsg(3.seconds, "completed")


Java




copy
source
final Source<Tick, Cancellable> sourceUnderTest =
    Source.tick(Duration.ZERO, Duration.ofMillis(200), Tick.TOCK);

final TestKit probe = new TestKit(system);
final Cancellable cancellable =
    sourceUnderTest.to(Sink.actorRef(probe.getRef(), Tick.COMPLETED)).run(system);
probe.expectMsg(Duration.ofSeconds(3), Tick.TOCK);
probe.expectNoMessage(Duration.ofMillis(100));
probe.expectMsg(Duration.ofSeconds(3), Tick.TOCK);
cancellable.cancel();
probe.expectMsg(Duration.ofSeconds(3), Tick.COMPLETED);




Similarly to 
Sink.actorRef
 that provides control over received elements, we can use 
Source.actorRef
Source.actorRef
 and have full control over elements to be sent.




Scala




copy
source
val sinkUnderTest = Flow[Int].map(_.toString).toMat(Sink.fold("")(_ + _))(Keep.right)

val (ref, future) = Source
  .actorRef(
    completionMatcher = {
      case Done =>
        CompletionStrategy.draining
    },
    // Never fail the stream because of a message:
    failureMatcher = PartialFunction.empty,
    bufferSize = 8,
    overflowStrategy = OverflowStrategy.fail)
  .toMat(sinkUnderTest)(Keep.both)
  .run()

ref ! 1
ref ! 2
ref ! 3
ref ! Done

val result = Await.result(future, 3.seconds)
assert(result == "123")


Java




copy
source
final Sink<Integer, CompletionStage<String>> sinkUnderTest =
    Flow.of(Integer.class)
        .map(i -> i.toString())
        .toMat(Sink.fold("", (agg, next) -> agg + next), Keep.right());

final Pair<ActorRef, CompletionStage<String>> refAndCompletionStage =
    Source.<Integer>actorRef(
            elem -> {
              // complete stream immediately if we send it Done
              if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());
              else return Optional.empty();
            },
            // never fail the stream because of a message
            elem -> Optional.empty(),
            8,
            OverflowStrategy.fail())
        .toMat(sinkUnderTest, Keep.both())
        .run(system);
final ActorRef ref = refAndCompletionStage.first();
final CompletionStage<String> future = refAndCompletionStage.second();

ref.tell(1, ActorRef.noSender());
ref.tell(2, ActorRef.noSender());
ref.tell(3, ActorRef.noSender());
ref.tell(Done.getInstance(), ActorRef.noSender());

final String result = future.toCompletableFuture().get(1, TimeUnit.SECONDS);
assertEquals("123", result);




Streams TestKit


You may have noticed various code patterns that emerge when testing stream pipelines. Akka Stream has a separate 
akka-stream-testkit
 module that provides tools specifically for writing stream tests. This module comes with two main components that are 
TestSource
TestSource
 and 
TestSink
TestSink
 which provide sources and sinks that materialize to probes that allow fluent API.


Using the TestKit


A sink returned by 
TestSink.probe
TestSink.probe
 allows manual control over demand and assertions over elements coming downstream.




Scala




copy
source
val sourceUnderTest = Source(1 to 4).filter(_ % 2 == 0).map(_ * 2)

sourceUnderTest.runWith(TestSink[Int]()).request(2).expectNext(4, 8).expectComplete()


Java




copy
source
final Source<Integer, NotUsed> sourceUnderTest =
    Source.from(Arrays.asList(1, 2, 3, 4)).filter(elem -> elem % 2 == 0).map(elem -> elem * 2);

sourceUnderTest
    .runWith(TestSink.probe(system), system)
    .request(2)
    .expectNext(4, 8)
    .expectComplete();




A source returned by 
TestSource.probe
TestSource.probe
 can be used for asserting demand or controlling when stream is completed or ended with an error.




Scala




copy
source
val sinkUnderTest = Sink.cancelled

TestSource[Int]().toMat(sinkUnderTest)(Keep.left).run().expectCancellation()


Java




copy
source
final Sink<Integer, NotUsed> sinkUnderTest = Sink.cancelled();

TestSource.<Integer>probe(system)
    .toMat(sinkUnderTest, Keep.left())
    .run(system)
    .expectCancellation();




You can also inject exceptions and test sink behavior on error conditions.




Scala




copy
source
val sinkUnderTest = Sink.head[Int]

val (probe, future) = TestSource[Int]().toMat(sinkUnderTest)(Keep.both).run()
probe.sendError(new Exception("boom"))

assert(future.failed.futureValue.getMessage == "boom")


Java




copy
source
final Sink<Integer, CompletionStage<Integer>> sinkUnderTest = Sink.head();

final Pair<TestPublisher.Probe<Integer>, CompletionStage<Integer>> probeAndCompletionStage =
    TestSource.<Integer>probe(system).toMat(sinkUnderTest, Keep.both()).run(system);
final TestPublisher.Probe<Integer> probe = probeAndCompletionStage.first();
final CompletionStage<Integer> future = probeAndCompletionStage.second();
probe.sendError(new Exception("boom"));

ExecutionException exception =
    Assert.assertThrows(
        ExecutionException.class, () -> future.toCompletableFuture().get(3, TimeUnit.SECONDS));
assertEquals("boom", exception.getCause().getMessage());




Test source and sink can be used together in combination when testing flows.




Scala




copy
source
val flowUnderTest = Flow[Int].mapAsyncUnordered(2) { sleep =>
  pattern.after(10.millis * sleep, using = system.scheduler)(Future.successful(sleep))
}

val (pub, sub) = TestSource[Int]().via(flowUnderTest).toMat(TestSink[Int]())(Keep.both).run()

sub.request(n = 3)
pub.sendNext(3)
pub.sendNext(2)
pub.sendNext(1)
sub.expectNextUnordered(1, 2, 3)

pub.sendError(new Exception("Power surge in the linear subroutine C-47!"))
val ex = sub.expectError()
assert(ex.getMessage.contains("C-47"))


Java




copy
source
final Flow<Integer, Integer, NotUsed> flowUnderTest =
    Flow.of(Integer.class)
        .mapAsyncUnordered(
            2,
            sleep ->
                akka.pattern.Patterns.after(
                    Duration.ofMillis(10),
                    system.scheduler(),
                    system.dispatcher(),
                    () -> CompletableFuture.completedFuture(sleep)));

final Pair<TestPublisher.Probe<Integer>, TestSubscriber.Probe<Integer>> pubAndSub =
    TestSource.<Integer>probe(system)
        .via(flowUnderTest)
        .toMat(TestSink.<Integer>probe(system), Keep.both())
        .run(system);
final TestPublisher.Probe<Integer> pub = pubAndSub.first();
final TestSubscriber.Probe<Integer> sub = pubAndSub.second();

sub.request(3);
pub.sendNext(3);
pub.sendNext(2);
pub.sendNext(1);
sub.expectNextUnordered(1, 2, 3);

pub.sendError(new Exception("Power surge in the linear subroutine C-47!"));
final Throwable ex = sub.expectError();
assertTrue(ex.getMessage().contains("C-47"));




Fuzzing Mode


For testing, it is possible to enable a special stream execution mode that exercises concurrent execution paths more aggressively (at the cost of reduced performance) and therefore helps exposing race conditions in tests. To enable this setting add the following line to your configuration:


akka.stream.materializer.debug.fuzzing-mode = on

Warning


Never use this setting in production or benchmarks. This is a testing tool to provide more coverage of your code during tests, but it reduces the throughput of streams. A warning message will be logged if you have this setting enabled.














 
Pipelining and Parallelism






Substreams 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-membership.html
Cluster Membership Service • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service




Introduction


Member States


Member Events


Membership Lifecycle


Leader


WeaklyUp Members


Full cluster shutdown


State Diagrams




Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service




Introduction


Member States


Member Events


Membership Lifecycle


Leader


WeaklyUp Members


Full cluster shutdown


State Diagrams




Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Membership Service


The core of Akka Cluster is the cluster membership, to keep track of what nodes are part of the cluster and their health. Cluster membership is communicated using 
gossip
 and 
failure detection
.


There are several 
Higher level Cluster tools
 that are built on top of the cluster membership service.


Introduction


A cluster is made up of a set of member nodes. The identifier for each node is a 
hostname:port:uid
 tuple. An Akka application can be distributed over a cluster with each node hosting some part of the application. Cluster membership and the actors running on that node of the application are decoupled. A node could be a member of a cluster without hosting any actors. Joining a cluster is initiated by issuing a 
Join
 command to one of the nodes in the cluster to join.


The node identifier internally also contains a UID that uniquely identifies this actor system instance at that 
hostname:port
. Akka uses the UID to be able to reliably trigger remote death watch. This means that the same actor system can never join a cluster again once it’s been removed from that cluster. To re-join an actor system with the same 
hostname:port
 to a cluster you have to stop the actor system and start a new one with the same 
hostname:port
 which will then receive a different UID.


Member States


The cluster membership state is a specialized 
CRDT
, which means that it has a monotonic merge function. When concurrent changes occur on different nodes the updates can always be merged and converge to the same end result.






joining
 - transient state when joining a cluster




weakly up
 - transient state while network split (only if 
akka.cluster.allow-weakly-up-members=on
)




up
 - normal operating state




preparing for shutdown
 / 
ready for shutdown
 - an optional state that can be moved to before doing a full cluster shut down




leaving
 / 
exiting
 - states during graceful removal




down
 - marked as down (no longer part of cluster decisions)




removed
 - tombstone state (no longer a member)




Member Events


The events to track the life-cycle of members are:




ClusterEvent.MemberJoined
 - A new member has joined the cluster and its status has been changed to 
Joining


ClusterEvent.MemberUp
 - A new member has joined the cluster and its status has been changed to 
Up


ClusterEvent.MemberExited
 - A member is leaving the cluster and its status has been changed to 
Exiting
 Note that the node might already have been shutdown when this event is published on another node.


ClusterEvent.MemberRemoved
 - Member completely removed from the cluster.


ClusterEvent.UnreachableMember
 - A member is considered as unreachable, detected by the failure detector of at least one other node.


ClusterEvent.ReachableMember
 - A member is considered as reachable again, after having been unreachable. All nodes that previously detected it as unreachable has detected it as reachable again.


ClusterEvent.MemberPreparingForShutdown
 - A member is preparing for a full cluster shutdown


ClusterEvent.MemberReadyForShutdown
 - A member is ready for a full cluster shutdown




Membership Lifecycle


A node is introduced to the cluster by invoking the 
join
 action which puts the node in the 
joining
 state. Once all nodes have seen that the new node is joining (through 
gossip convergence
) the 
leader
 will set the member state to 
up
.


If a node is leaving the cluster in a safe, expected manner, for example through 
coordinated shutdown
, it invokes the 
leave
 action which switches it to the 
leaving
 state. Once the leader sees the convergence on the node in the 
leaving
 state, the leader will then move it to 
exiting
. Once all nodes have seen the exiting state (convergence) the 
leader
 will remove the node from the cluster, marking it as 
removed
.


If a node is 
unreachable
 then gossip convergence is not possible and therefore most 
leader
 actions are impossible (for instance, allowing a node to become a part of the cluster). To be able to move forward, the node must become 
reachable
 again or the node must be explicitly “downed”. This is required because the state of an unreachable node is unknown and the cluster cannot know if the node has crashed or is only temporarily unreachable because of network issues or GC pauses. See the section about 
User Actions
 below for ways a node can be downed.


The actor system on a node that exited or was downed cannot join the cluster again. In particular, a node that was downed while being unreachable and then regains connectivity cannot rejoin the cluster. Instead, the process has to be restarted on the node, creating a new actor system that can go through the joining process again.


A special case is a node that was restarted without going through the leaving or downing process e.g. because the machine hosting the node was unexpectedly restarted. When the new instance of the node tries to rejoin the cluster, the cluster might still track the old instance as unreachable. In this case, however, it is clear that the old node is gone because the new instance will have the same address (host and port) as its old instance. In this case, the previous instance will be automatically marked as 
down
 and the new instance can rejoin the cluster without manual intervention.


Leader


The purpose of the 
leader
 is to confirm state changes when convergence is reached. The 
leader
 can be determined by each node unambiguously after gossip convergence. Any node might be required to take the role of the 
leader
 depending on the current cluster composition.


Without convergence, different nodes might have different views about which node is the leader. Therefore, most leader actions are only allowed if there is convergence to ensure that all nodes agree about the current state of the cluster and state changes are originated from a single node. Most regular state changes like changing a node from 
joining
 to 
up
 are of that kind.


Other situations require that an action is taken even if convergence cannot be reached currently. Notably, convergence cannot be reached if one or more nodes in the cluster are currently unreachable as determined by the 
failure detector
. In such a case, the cluster might be partitioned (a split brain scenario) and each partition might have its own view about which nodes are reachable and which are not. In this case, a node on each side of the partition might view itself as the leader of the reachable nodes. Any action that the leader performs in such a case must be designed in a way that all concurrent leaders would come to the same conclusion (which might be impossible in general and only feasible under additional constraints). The most important case of that kind is a split brain scenario where nodes need to be downed, either manually or automatically, to bring the cluster back to convergence.


The 
Split Brain Resolver
 is the built-in implementation of that.


Another transition that is possible without convergence is marking members as 
WeaklyUp
 as described in the next section.




WeaklyUp Members


If a node is 
unreachable
 then gossip convergence is not possible and therefore most 
leader
 actions are impossible. By enabling 
akka.cluster.allow-weakly-up-members
 (which is enabled by default), joining nodes can be promoted to 
WeaklyUp
 even while convergence is not yet reached. Once gossip convergence can be established again, the leader will move 
WeaklyUp
 members to 
Up
.


You can subscribe to the 
WeaklyUp
 membership event to make use of the members that are in this state, but you should be aware of that members on the other side of a network partition have no knowledge about the existence of the new members. You should for example not count 
WeaklyUp
 members in quorum decisions.


Full cluster shutdown


In some rare cases it may be desirable to do a full cluster shutdown rather than a rolling deploy. For example, a protocol change where it is simpler to restart the cluster than to make the protocol change backward compatible.


As of Akka 
2.6.13
 it can be signalled that a full cluster shutdown is about to happen and any expensive actions such as:




Cluster sharding rebalances


Moving of Cluster singletons




Won’t happen. That way the shutdown will be as quick as possible and a new version can be started up without delay.


If a cluster isn’t to be restarted right away then there is no need to prepare it for shutdown.


To use this feature use 
Cluster(system).prepareForFullClusterShutdown()
 in classic or 
PrepareForFullClusterShutdown
PrepareForFullClusterShutdown
 in typed.


Wait for all 
Up
 members to become 
ReadyForShutdown
 and then all nodes can be shutdown and restarted. Members that aren’t 
Up
 yet will remain in the 
Joining
 or 
WeaklyUp
 states. Any node that is already leaving the cluster i.e. in the 
Leaving
 or 
Exiting
 states will continue to leave the cluster via the normal path.


State Diagrams


State Diagram for the Member States




User Actions






join
 - join a single node to a cluster - can be explicit or automatic on startup if a node to join have been specified in the configuration




leave
 - tell a node to leave the cluster gracefully, normally triggered by ActorSystem or JVM shutdown through 
coordinated shutdown




down
 - mark a node as down. This action is required to remove crashed nodes (that did not ‘leave’) from the cluster. It can be triggered manually, through 
Cluster HTTP Management
, or automatically by a 
downing provider
 like 
Split Brain Resolver




Leader Actions


The 
leader
 has the duty of confirming user actions to shift members in and out of the cluster:




joining â­¢ up


joining â­¢ weakly up 
(no convergence is needed for this leader action to be performed which works even if there are unreachable nodes)


weakly up â­¢ up 
(after full convergence is reached again)


leaving â­¢ exiting


exiting â­¢ removed


down â­¢ removed




Failure Detection and Unreachability


Being unreachable is not a separate member state but rather a flag in addition to the state. A failure detector on each node that monitors another node can mark the monitored node as unreachable independent of its state. Afterwards the failure detector continues monitoring the node until it detects it as reachable again and removes the flag. A node is considered reachable again only after all monitoring nodes see it as reachable again.














 
Cluster Specification






Phi Accrual Failure Detector 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-context.html
Context Propagation • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation




Restrictions


Creation


Composition




Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation




Restrictions


Creation


Composition




Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Context Propagation


It can be convenient to attach metadata to each element in the stream.


For example, when reading from an external data source it can be useful to keep track of the read offset, so it can be marked as processed when the element reaches the 
Sink
Sink
.


For this use case we provide the 
SourceWithContext
SourceWithContext
 and 
FlowWithContext
FlowWithContext
 variations on 
Source
Source
 and 
Flow
Flow
.


Essentially, a 
FlowWithContext
FlowWithContext
 is just a 
Flow
Flow
 that contains 
tuples
pairs
 of element and context, but the advantage is in the operators: most operators on 
FlowWithContext
FlowWithContext
 will work on the element rather than on the 
tuple
pair
, allowing you to focus on your application logic rather without worrying about the context.


Restrictions


Not all operations that are available on 
Flow
Flow
 are also available on 
FlowWithContext
FlowWithContext
. This is intentional: in the use case of keeping track of a read offset, if the 
FlowWithContext
FlowWithContext
 was allowed to arbitrarily filter and reorder the stream, the 
Sink
Sink
 would have no way to determine whether an element was skipped or merely reordered and still in flight.


For this reason, 
FlowWithContext
FlowWithContext
 allows filtering operations (such as 
filter
, 
filterNot
, 
collect
, etc.) and grouping operations (such as 
grouped
, 
sliding
, etc.) but not reordering operations (such as 
mapAsyncUnordered
 and 
statefulMapConcat
). Finally, also ‘one-to-n’ operations such as 
mapConcat
 are allowed.


Filtering operations will drop the context along with dropped elements, while grouping operations will keep all contexts from the elements in the group. Streaming one-to-many operations such as 
mapConcat
 associate the original context with each of the produced elements.


As an escape hatch, there is a 
via
 operator that allows you to insert an arbitrary 
Flow
Flow
 that can process the 
tuples
pairs
 of elements and context in any way desired. When using this operator, it is the responsibility of the implementor to make sure this 
Flow
Flow
 does not perform any operations (such as reordering) that might break assumptions made by the 
Sink
Sink
 consuming the context elements.


Creation


The simplest way to create a 
SourceWithContext
SourceWithContext
 is to first create a regular 
Source
Source
 with elements from which the context can be extracted, and then use 
Source.asSourceWithContext
.


Composition


When you have a 
SourceWithContext
SourceWithContext
 
source
 that produces elements of type 
Foo
 with a context of type 
Ctx
, and a 
Flow
Flow
 
flow
 from 
Foo
 to 
Bar
, you cannot simply 
source.via(flow)
 to arrive at a 
SourceWithContext
SourceWithContext
 that produces elements of type 
Bar
 with contexts of type 
Ctx
. The reason for this is that 
flow
 might reorder the elements flowing through it, making 
via
 challenging to implement.


Due to this there is a 
unsafeDataVia
 that can be used instead however no protection is offered to prevent reordering or dropping/duplicating elements from stream so use this operation with great care.


There is also a 
Flow.asFlowWithContext
 which can be used when the types used in the inner 
Flow
Flow
 have room to hold the context. If this is not the case, a better solution is usually to build the flow from the ground up as a 
FlowWithContext
FlowWithContext
, instead of first building a 
Flow
Flow
 and trying to convert it to 
FlowWithContext
FlowWithContext
 after-the-fact.














 
Buffers and working with rate






Dynamic stream handling 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/migration-guides.html
Migration Guides • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides




Migration Guide 2.9.x to 2.10.x


Migration Guide 2.8.x to 2.9.x


Migration Guide 2.7.x to 2.8.x


Migration Guide 2.6.x to 2.7.x


Migration Guide 2.5.x to 2.6.x


Older Migration Guides




Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides




Migration Guide 2.9.x to 2.10.x


Migration Guide 2.8.x to 2.9.x


Migration Guide 2.7.x to 2.8.x


Migration Guide 2.6.x to 2.7.x


Migration Guide 2.5.x to 2.6.x


Older Migration Guides




Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Migration Guides






Migration Guide 2.9.x to 2.10.x


Migration Guide 2.8.x to 2.9.x


Migration Guide 2.7.x to 2.8.x


Migration Guide 2.6.x to 2.7.x


Migration Guide 2.5.x to 2.6.x


Older Migration Guides


















 
Immutability using Lombok






Migration Guide 2.9.x to 2.10.x 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/actors.html
Introduction to Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors




Module info


Akka Actors


First example


A More Complex Example




Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors




Module info


Akka Actors


First example


A More Complex Example




Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Introduction to Actors


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Actors
.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actors, add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-actor-testkit-typed" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-testkit-typed_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-actor-testkit-typed_${versions.ScalaBinary}"
}


Both the Java and Scala DSLs of Akka modules are bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting 
javadsl
 imports when working in Scala, or viceversa. See 
IDE Tips
. 




Project Info: Akka Actors (typed)


Artifact
com.typesafe.akka


akka-actor-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.actor.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Akka Actors


The 
Actor Model
 provides a higher level of abstraction for writing concurrent and distributed systems. It alleviates the developer from having to deal with explicit locking and thread management, making it easier to write correct concurrent and parallel systems. Actors were defined in the 1973 paper by Carl Hewitt but have been popularized by the Erlang language, and used for example at Ericsson with great success to build highly concurrent and reliable telecom systems. The API of Akkaâs Actors has borrowed some of its syntax from Erlang.


Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.


First example


If you are new to Akka we recommend watching the short 
introduction video to Akka actors
.


This sample can be downloaded and includes 
Maven
sbt
 project with the needed dependencies:




Scala 
akka-quickstart-scala.zip


Java 
akka-quickstart-java.zip




It is helpful to become familiar with the foundational, external and internal ecosystem of your Actors, to see what you can leverage and customize as needed, see 
Actor Systems
 and 
Actor References, Paths and Addresses
.


As discussed in 
Actor Systems
 Actors are about sending messages between independent units of computation, but what does that look like?


In all of the following these imports are assumed:




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.ActorSystem
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;





With these in place we can define our first Actor, and it will say hello!






Scala




copy
source
object HelloWorld {
  final case class Greet(whom: String, replyTo: ActorRef[Greeted])
  final case class Greeted(whom: String, from: ActorRef[Greet])

  def apply(): Behavior[Greet] = Behaviors.receive { (context, message) =>
    context.log.info("Hello {}!", message.whom)
    message.replyTo ! Greeted(message.whom, context.self)
    Behaviors.same
  }
}


Java




copy
source
public class HelloWorld extends AbstractBehavior<HelloWorld.Greet> {

  public static record Greet(String whom, ActorRef<Greeted> replyTo) {}
  public static record Greeted(String whom, ActorRef<Greet> from) {}

  public static Behavior<Greet> create() {
    return Behaviors.setup(HelloWorld::new);
  }

  private HelloWorld(ActorContext<Greet> context) {
    super(context);
  }

  @Override
  public Receive<Greet> createReceive() {
    return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build();
  }

  private Behavior<Greet> onGreet(Greet command) {
    getContext().getLog().info("Hello {}!", command.whom);
    command.replyTo.tell(new Greeted(command.whom, getContext().getSelf()));
    return this;
  }
}




This small piece of code defines two message types, one for commanding the Actor to greet someone and one that the Actor will use to confirm that it has done so. The 
Greet
 type contains not only the information of whom to greet, it also holds an 
ActorRef
ActorRef
 that the sender of the message supplies so that the 
HelloWorld
 Actor can send back the confirmation message.


The behavior of the Actor is defined as the 
Greeter
 with the help of the 
receive
receive
 behavior factory. Processing the next message then results in a new behavior that can potentially be different from this one. State is updated by returning a new behavior that holds the new immutable state. In this case we don’t need to update any state, so we return 
same
same
, which means the next behavior is “the same as the current one”.


The type of the messages handled by this behavior is declared to be of class 
Greet
.
, meaning that 
message
 argument is also typed as such. This is why we can access the 
whom
 and 
replyTo
 members without needing to use a pattern match.
 Typically, an actor handles more than one specific message type where all of them directly or indirectly 
extend
implement
 a common 
trait
interface
.


On the last line we see the 
HelloWorld
 Actor send a message to another Actor, which is done using the 
!
 operator (pronounced âbangâ or âtellâ)
tell
 method
. It is an asynchronous operation that doesn’t block the caller’s thread.


Since the 
replyTo
 address is declared to be of type 
ActorRef[Greeted]
ActorRef<Greeted>
, the compiler will only permit us to send messages of this type, other usage will be a compiler error.


The accepted message types of an Actor together with all reply types defines the protocol spoken by this Actor; in this case it is a simple requestâreply protocol but Actors can model arbitrarily complex protocols when needed. The protocol is bundled together with the behavior that implements it in a nicely wrapped scopeâthe 
HelloWorld
 
object
class
.


As Carl Hewitt said, one Actor is no Actor â it would be quite lonely with nobody to talk to. We need another Actor that interacts with the 
Greeter
. Let’s make a 
HelloWorldBot
 that receives the reply from the 
Greeter
 and sends a number of additional greeting messages and collect the replies until a given max number of messages have been reached.






Scala




copy
source
object HelloWorldBot {

  def apply(max: Int): Behavior[HelloWorld.Greeted] = {
    bot(0, max)
  }

  private def bot(greetingCounter: Int, max: Int): Behavior[HelloWorld.Greeted] =
    Behaviors.receive { (context, message) =>
      val n = greetingCounter + 1
      context.log.info("Greeting {} for {}", n, message.whom)
      if (n == max) {
        Behaviors.stopped
      } else {
        message.from ! HelloWorld.Greet(message.whom, context.self)
        bot(n, max)
      }
    }
}


Java




copy
source
public class HelloWorldBot extends AbstractBehavior<HelloWorld.Greeted> {

  public static Behavior<HelloWorld.Greeted> create(int max) {
    return Behaviors.setup(context -> new HelloWorldBot(context, max));
  }

  private final int max;
  private int greetingCounter;

  private HelloWorldBot(ActorContext<HelloWorld.Greeted> context, int max) {
    super(context);
    this.max = max;
  }

  @Override
  public Receive<HelloWorld.Greeted> createReceive() {
    return newReceiveBuilder().onMessage(HelloWorld.Greeted.class, this::onGreeted).build();
  }

  private Behavior<HelloWorld.Greeted> onGreeted(HelloWorld.Greeted message) {
    greetingCounter++;
    getContext().getLog().info("Greeting {} for {}", greetingCounter, message.from());
    if (greetingCounter == max) {
      return Behaviors.stopped();
    } else {
      message.from().tell(new HelloWorld.Greet(message.whom(), getContext().getSelf()));
      return this;
    }
  }
}




Note how this Actor manages the counter by changing the behavior for each 
Greeted
 reply rather than using any variables.
Note how this Actor manages the counter with an instance variable.
 No concurrency guards such as 
synchronized
 or 
AtomicInteger
 are needed since an actor instance processes one message at a time.


A third actor spawns the 
Greeter
 and the 
HelloWorldBot
 and starts the interaction between those.




Scala




copy
source
object HelloWorldMain {

  final case class SayHello(name: String)

  def apply(): Behavior[SayHello] =
    Behaviors.setup { context =>
      val greeter = context.spawn(HelloWorld(), "greeter")

      Behaviors.receiveMessage { message =>
        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)
        greeter ! HelloWorld.Greet(message.name, replyTo)
        Behaviors.same
      }
    }

}


Java




copy
source
public class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {

  public static record SayHello(String name) {}

  public static Behavior<SayHello> create() {
    return Behaviors.setup(HelloWorldMain::new);
  }

  private final ActorRef<HelloWorld.Greet> greeter;

  private HelloWorldMain(ActorContext<SayHello> context) {
    super(context);
    greeter = context.spawn(HelloWorld.create(), "greeter");
  }

  @Override
  public Receive<SayHello> createReceive() {
    return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build();
  }

  private Behavior<SayHello> onSayHello(SayHello command) {
    ActorRef<HelloWorld.Greeted> replyTo =
        getContext().spawn(HelloWorldBot.create(3), command.name);
    greeter.tell(new HelloWorld.Greet(command.name, replyTo));
    return this;
  }
}




Now we want to try out this Actor, so we must start an ActorSystem to host it:




Scala




copy
source
val system: ActorSystem[HelloWorldMain.SayHello] =
  ActorSystem(HelloWorldMain(), "hello")

system ! HelloWorldMain.SayHello("World")
system ! HelloWorldMain.SayHello("Akka")


Java




copy
source
final ActorSystem<SayHello> system =
    ActorSystem.create(HelloWorldMain.create(), "hello");

system.tell(new HelloWorldMain.SayHello("World"));
system.tell(new HelloWorldMain.SayHello("Akka"));




We start an Actor system from the defined 
HelloWorldMain
 behavior and send two 
SayHello
 messages that will kick-off the interaction between two separate 
HelloWorldBot
 actors and the single 
Greeter
 actor.


An application normally consists of a single 
ActorSystem
ActorSystem
, running many actors, per JVM. 


The console output may look like this:


[INFO] [03/13/2018 15:50:05.814] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/greeter] Hello World!
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/greeter] Hello Akka!
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-2] [akka://hello/user/World] Greeting 1 for World
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/Akka] Greeting 1 for Akka
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-5] [akka://hello/user/greeter] Hello World!
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-5] [akka://hello/user/greeter] Hello Akka!
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/World] Greeting 2 for World
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-5] [akka://hello/user/greeter] Hello World!
[INFO] [03/13/2018 15:50:05.815] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/Akka] Greeting 2 for Akka
[INFO] [03/13/2018 15:50:05.816] [hello-akka.actor.default-dispatcher-5] [akka://hello/user/greeter] Hello Akka!
[INFO] [03/13/2018 15:50:05.816] [hello-akka.actor.default-dispatcher-4] [akka://hello/user/World] Greeting 3 for World
[INFO] [03/13/2018 15:50:05.816] [hello-akka.actor.default-dispatcher-6] [akka://hello/user/Akka] Greeting 3 for Akka



You will also need to add a 
logging dependency
 to see that output when running.
Note


ð For a deeper introduction to actors, consider the free online courses 
Akka Basics for Java
Akka Basics for Scala
) in Akkademy.


A More Complex Example


The next example is more realistic and demonstrates some important patterns:




Using 
a sealed trait and case class/objects
an interface and classes implementing that interface
 to represent multiple messages an actor can receive


Handle sessions by using child actors


Handling state by changing behavior


Using multiple actors to represent different parts of a protocol in a type safe way






Functional Style


First we will show this example in a functional style, and then the same example is shown with an 
Object-oriented style
. Which style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. Considerations for the choice is provided in the 
Style Guide
.


Consider an Actor that runs a chat room: client Actors may connect by sending a message that contains their screen name and then they can post messages. The chat room Actor will disseminate all posted messages to all currently connected client Actors. The protocol definition could look like the following:




Scala




copy
source
object ChatRoom {
  sealed trait RoomCommand
  final case class GetSession(screenName: String, replyTo: ActorRef[SessionEvent]) extends RoomCommand

  sealed trait SessionEvent
  final case class SessionGranted(handle: ActorRef[PostMessage]) extends SessionEvent
  final case class SessionDenied(reason: String) extends SessionEvent
  final case class MessagePosted(screenName: String, message: String) extends SessionEvent

  sealed trait SessionCommand
  final case class PostMessage(message: String) extends SessionCommand
  private final case class NotifyClient(message: MessagePosted) extends SessionCommand
}


Java




copy
source
static interface RoomCommand {}

public static final class GetSession implements RoomCommand {
  public final String screenName;
  public final ActorRef<SessionEvent> replyTo;

  public GetSession(String screenName, ActorRef<SessionEvent> replyTo) {
    this.screenName = screenName;
    this.replyTo = replyTo;
  }
}

interface SessionEvent {}

public static final class SessionGranted implements SessionEvent {
  public final ActorRef<PostMessage> handle;

  public SessionGranted(ActorRef<PostMessage> handle) {
    this.handle = handle;
  }
}

public static final class SessionDenied implements SessionEvent {
  public final String reason;

  public SessionDenied(String reason) {
    this.reason = reason;
  }
}

public static final class MessagePosted implements SessionEvent {
  public final String screenName;
  public final String message;

  public MessagePosted(String screenName, String message) {
    this.screenName = screenName;
    this.message = message;
  }
}

interface SessionCommand {}

public static final class PostMessage implements SessionCommand {
  public final String message;

  public PostMessage(String message) {
    this.message = message;
  }
}

private static final class NotifyClient implements SessionCommand {
  final MessagePosted message;

  NotifyClient(MessagePosted message) {
    this.message = message;
  }
}




Initially the client Actors only get access to an 
ActorRef[GetSession]
ActorRef<GetSession>
 which allows them to make the first step. Once a clientâs session has been established it gets a 
SessionGranted
 message that contains a 
handle
 to unlock the next protocol step, posting messages. The 
PostMessage
 command will need to be sent to this particular address that represents the session that has been added to the chat room. The other aspect of a session is that the client has revealed its own address, via the 
replyTo
 argument, so that subsequent 
MessagePosted
 events can be sent to it.


This illustrates how Actors can express more than just the equivalent of method calls on Java objects. The declared message types and their contents describe a full protocol that can involve multiple Actors and that can evolve over multiple steps. Here’s the implementation of the chat room protocol:




Scala




copy
source
object ChatRoom {
  private final case class PublishSessionMessage(screenName: String, message: String) extends RoomCommand

  def apply(): Behavior[RoomCommand] =
    chatRoom(List.empty)

  private def chatRoom(sessions: List[ActorRef[SessionCommand]]): Behavior[RoomCommand] =
    Behaviors.receive { (context, message) =>
      message match {
        case GetSession(screenName, client) =>
          // create a child actor for further interaction with the client
          val ses = context.spawn(
            session(context.self, screenName, client),
            name = URLEncoder.encode(screenName, StandardCharsets.UTF_8.name))
          client ! SessionGranted(ses)
          chatRoom(ses :: sessions)
        case PublishSessionMessage(screenName, message) =>
          val notification = NotifyClient(MessagePosted(screenName, message))
          sessions.foreach(_ ! notification)
          Behaviors.same
      }
    }

  private def session(
      room: ActorRef[PublishSessionMessage],
      screenName: String,
      client: ActorRef[SessionEvent]): Behavior[SessionCommand] =
    Behaviors.receiveMessage {
      case PostMessage(message) =>
        // from client, publish to others via the room
        room ! PublishSessionMessage(screenName, message)
        Behaviors.same
      case NotifyClient(message) =>
        // published from the room
        client ! message
        Behaviors.same
    }
}


Java




copy
source
public class ChatRoom {
  private static final class PublishSessionMessage implements RoomCommand {
    public final String screenName;
    public final String message;

    public PublishSessionMessage(String screenName, String message) {
      this.screenName = screenName;
      this.message = message;
    }
  }

  public static Behavior<RoomCommand> create() {
    return Behaviors.setup(
        ctx -> new ChatRoom(ctx).chatRoom(new ArrayList<ActorRef<SessionCommand>>()));
  }

  private final ActorContext<RoomCommand> context;

  private ChatRoom(ActorContext<RoomCommand> context) {
    this.context = context;
  }

  private Behavior<RoomCommand> chatRoom(List<ActorRef<SessionCommand>> sessions) {
    return Behaviors.receive(RoomCommand.class)
        .onMessage(GetSession.class, getSession -> onGetSession(sessions, getSession))
        .onMessage(PublishSessionMessage.class, pub -> onPublishSessionMessage(sessions, pub))
        .build();
  }

  private Behavior<RoomCommand> onGetSession(
      List<ActorRef<SessionCommand>> sessions, GetSession getSession)
      throws UnsupportedEncodingException {
    ActorRef<SessionEvent> client = getSession.replyTo;
    ActorRef<SessionCommand> ses =
        context.spawn(
            Session.create(context.getSelf(), getSession.screenName, client),
            URLEncoder.encode(getSession.screenName, StandardCharsets.UTF_8.name()));
    // narrow to only expose PostMessage
    client.tell(new SessionGranted(ses.narrow()));
    List<ActorRef<SessionCommand>> newSessions = new ArrayList<>(sessions);
    newSessions.add(ses);
    return chatRoom(newSessions);
  }

  private Behavior<RoomCommand> onPublishSessionMessage(
      List<ActorRef<SessionCommand>> sessions, PublishSessionMessage pub) {
    NotifyClient notification =
        new NotifyClient((new MessagePosted(pub.screenName, pub.message)));
    sessions.forEach(s -> s.tell(notification));
    return Behaviors.same();
  }

  static class Session {
    static Behavior<ChatRoom.SessionCommand> create(
        ActorRef<RoomCommand> room, String screenName, ActorRef<SessionEvent> client) {
      return Behaviors.receive(ChatRoom.SessionCommand.class)
          .onMessage(PostMessage.class, post -> onPostMessage(room, screenName, post))
          .onMessage(NotifyClient.class, notification -> onNotifyClient(client, notification))
          .build();
    }

    private static Behavior<SessionCommand> onPostMessage(
        ActorRef<RoomCommand> room, String screenName, PostMessage post) {
      // from client, publish to others via the room
      room.tell(new PublishSessionMessage(screenName, post.message));
      return Behaviors.same();
    }

    private static Behavior<SessionCommand> onNotifyClient(
        ActorRef<SessionEvent> client, NotifyClient notification) {
      // published from the room
      client.tell(notification.message);
      return Behaviors.same();
    }
  }
}




The state is managed by changing behavior rather than using any variables.


When a new 
GetSession
 command comes in we add that client to the list that is in the returned behavior. Then we also need to create the sessionâs 
ActorRef
ActorRef
 that will be used to post messages. In this case we want to create a very simple Actor that repackages the 
PostMessage
 command into a 
PublishSessionMessage
 command which also includes the screen name.


The behavior that we declare here can handle both subtypes of 
RoomCommand
. 
GetSession
 has been explained already and the 
PublishSessionMessage
 commands coming from the session Actors will trigger the dissemination of the contained chat room message to all connected clients. But we do not want to give the ability to send 
PublishSessionMessage
 commands to arbitrary clients, we reserve that right to the internal session actors we createâotherwise clients could pose as completely different screen names (imagine the 
GetSession
 protocol to include authentication information to further secure this). Therefore 
PublishSessionMessage
 has 
private
 visibility and can’t be created outside the 
ChatRoom
 
object
class
.


If we did not care about securing the correspondence between a session and a screen name then we could change the protocol such that 
PostMessage
 is removed and all clients just get an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 to send to. In this case no session actor would be needed and we could use 
context.self
context.getSelf()
. The type-checks work out in that case because 
ActorRef[-T]
ActorRef<-T>
ActorRef<T>
ActorRef<T>
 is contravariant in its type parameter, meaning that we can use a 
ActorRef[RoomCommand]
ActorRef<RoomCommand>
 wherever an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 is neededâthis makes sense because the former simply speaks more languages than the latter. The opposite would be problematic, so passing an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 where 
ActorRef[RoomCommand]
ActorRef<RoomCommand>
 is required will lead to a type error.


Trying it out


In order to see this chat room in action we need to write a client Actor that can use it:




Scala




copy
source
object Gabbler {
  import ChatRoom._

  def apply(): Behavior[SessionEvent] =
    Behaviors.setup { context =>
      Behaviors.receiveMessage {
        case SessionGranted(handle) =>
          handle ! PostMessage("Hello World!")
          Behaviors.same
        case MessagePosted(screenName, message) =>
          context.log.info("message has been posted by '{}': {}", screenName, message)
          Behaviors.stopped
      }
    }
}


Java




copy
source
public class Gabbler {
  public static Behavior<ChatRoom.SessionEvent> create() {
    return Behaviors.setup(ctx -> new Gabbler(ctx).behavior());
  }

  private final ActorContext<ChatRoom.SessionEvent> context;

  private Gabbler(ActorContext<ChatRoom.SessionEvent> context) {
    this.context = context;
  }

  private Behavior<ChatRoom.SessionEvent> behavior() {
    return Behaviors.receive(ChatRoom.SessionEvent.class)
        .onMessage(ChatRoom.SessionDenied.class, this::onSessionDenied)
        .onMessage(ChatRoom.SessionGranted.class, this::onSessionGranted)
        .onMessage(ChatRoom.MessagePosted.class, this::onMessagePosted)
        .build();
  }

  private Behavior<ChatRoom.SessionEvent> onSessionDenied(ChatRoom.SessionDenied message) {
    context.getLog().info("cannot start chat room session: {}", message.reason);
    return Behaviors.stopped();
  }

  private Behavior<ChatRoom.SessionEvent> onSessionGranted(ChatRoom.SessionGranted message) {
    message.handle.tell(new ChatRoom.PostMessage("Hello World!"));
    return Behaviors.same();
  }

  private Behavior<ChatRoom.SessionEvent> onMessagePosted(ChatRoom.MessagePosted message) {
    context
        .getLog()
        .info("message has been posted by '{}': {}", message.screenName, message.message);
    return Behaviors.stopped();
  }
}




From this behavior we can create an Actor that will accept a chat room session, post a message, wait to see it published, and then terminate. The last step requires the ability to change behavior, we need to transition from the normal running behavior into the terminated state. This is why here we do not return 
same
same
, as above, but another special value 
stopped
stopped
.


Since 
SessionEvent
 is a sealed trait the Scala compiler will warn us if we forget to handle one of the subtypes; in this case it reminded us that alternatively to 
SessionGranted
 we may also receive a 
SessionDenied
 event.


Now to try things out we must start both a chat room and a gabbler and of course we do this inside an Actor system. Since there can be only one user guardian we could either start the chat room from the gabbler (which we donât wantâit complicates its logic) or the gabbler from the chat room (which is nonsensical) or we start both of them from a third Actorâour only sensible choice:




Scala




copy
source
object Main {
  def apply(): Behavior[NotUsed] =
    Behaviors.setup { context =>
      val chatRoom = context.spawn(ChatRoom(), "chatroom")
      val gabblerRef = context.spawn(Gabbler(), "gabbler")
      context.watch(gabblerRef)
      chatRoom ! ChatRoom.GetSession("olâ Gabbler", gabblerRef)

      Behaviors.receiveSignal {
        case (_, Terminated(_)) =>
          Behaviors.stopped
      }
    }

  def main(args: Array[String]): Unit = {
    ActorSystem(Main(), "ChatRoomDemo")
  }

}


Java




copy
source
public class Main {
  public static Behavior<Void> create() {
    return Behaviors.setup(
        context -> {
          ActorRef<ChatRoom.RoomCommand> chatRoom = context.spawn(ChatRoom.create(), "chatRoom");
          ActorRef<ChatRoom.SessionEvent> gabbler = context.spawn(Gabbler.create(), "gabbler");
          context.watch(gabbler);
          chatRoom.tell(new ChatRoom.GetSession("olâ Gabbler", gabbler));

          return Behaviors.receive(Void.class)
              .onSignal(Terminated.class, sig -> Behaviors.stopped())
              .build();
        });
  }

  public static void main(String[] args) {
    ActorSystem.create(Main.create(), "ChatRoomDemo");
  }
}




In good tradition we call the 
Main
 Actor what it is, it directly corresponds to the 
main
 method in a traditional Java application. This Actor will perform its job on its own accord, we do not need to send messages from the outside, so we declare it to be of type 
NotUsed
Void
. Actors receive not only external messages, they also are notified of certain system events, so-called Signals. In order to get access to those we choose to implement this particular one using the 
receive
receive
 behavior decorator. The provided 
onSignal
 function will be invoked for signals (subclasses of 
Signal
Signal
) or the 
onMessage
 function for user messages.


This particular 
Main
 Actor is created using 
Behaviors.setup
Behaviors.setup
, which is like a factory for a behavior. Creation of the behavior instance is deferred until the actor is started, as opposed to 
Behaviors.receive
Behaviors.receive
 that creates the behavior instance immediately before the actor is running. The factory function in 
setup
 is passed the 
ActorContext
ActorContext
 as parameter and that can for example be used for spawning child actors. This 
Main
 Actor creates the chat room and the gabbler and the session between them is initiated, and when the gabbler is finished we will receive the 
Terminated
Terminated
 event due to having called 
context.watch
context.watch
 for it. This allows us to shut down the Actor system: when the 
Main
 Actor terminates there is nothing more to do.


Therefore after creating the Actor system with the 
Main
 Actorâs 
Behavior
Behavior
 we can let the 
main
 method return, the 
ActorSystem
ActorSystem
 will continue running and the JVM alive until the root actor stops.


Object-oriented style


The above sample used the functional programming style where you pass a function to a factory which then constructs a behavior, for stateful actors this means passing immutable state around as parameters and switching to a new behavior whenever you need to act on a changed state. An alternative way to express the same is a more object oriented style where a concrete class for the actor behavior is defined and mutable state is kept inside of it as fields.


Which style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. Considerations for the choice is provided in the 
Style Guide
.


AbstractBehavior API


Defining a class based actor behavior starts with extending 
AbstractBehavior
AbstractBehavior
<T>
[T]
 where 
T
 is the type of messages the behavior will accept.


Let’s repeat the chat room sample from 
A more complex example above
 but implemented using 
AbstractBehavior
. The protocol for interacting with the actor looks the same:




Scala




copy
source
object ChatRoom {
  sealed trait RoomCommand
  final case class GetSession(screenName: String, replyTo: ActorRef[SessionEvent]) extends RoomCommand

  sealed trait SessionEvent
  final case class SessionGranted(handle: ActorRef[PostMessage]) extends SessionEvent
  final case class SessionDenied(reason: String) extends SessionEvent
  final case class MessagePosted(screenName: String, message: String) extends SessionEvent

  sealed trait SessionCommand
  final case class PostMessage(message: String) extends SessionCommand
  private final case class NotifyClient(message: MessagePosted) extends SessionCommand
}


Java




copy
source
static interface RoomCommand {}

public static final class GetSession implements RoomCommand {
  public final String screenName;
  public final ActorRef<SessionEvent> replyTo;

  public GetSession(String screenName, ActorRef<SessionEvent> replyTo) {
    this.screenName = screenName;
    this.replyTo = replyTo;
  }
}

static interface SessionEvent {}

public static final class SessionGranted implements SessionEvent {
  public final ActorRef<PostMessage> handle;

  public SessionGranted(ActorRef<PostMessage> handle) {
    this.handle = handle;
  }
}

public static final class SessionDenied implements SessionEvent {
  public final String reason;

  public SessionDenied(String reason) {
    this.reason = reason;
  }
}

public static final class MessagePosted implements SessionEvent {
  public final String screenName;
  public final String message;

  public MessagePosted(String screenName, String message) {
    this.screenName = screenName;
    this.message = message;
  }
}

static interface SessionCommand {}

public static final class PostMessage implements SessionCommand {
  public final String message;

  public PostMessage(String message) {
    this.message = message;
  }
}

private static final class NotifyClient implements SessionCommand {
  final MessagePosted message;

  NotifyClient(MessagePosted message) {
    this.message = message;
  }
}




Initially the client Actors only get access to an 
ActorRef[GetSession]
ActorRef<GetSession>
 which allows them to make the first step. Once a clientâs session has been established it gets a 
SessionGranted
 message that contains a 
handle
 to unlock the next protocol step, posting messages. The 
PostMessage
 command will need to be sent to this particular address that represents the session that has been added to the chat room. The other aspect of a session is that the client has revealed its own address, via the 
replyTo
 argument, so that subsequent 
MessagePosted
 events can be sent to it.


This illustrates how Actors can express more than just the equivalent of method calls on Java objects. The declared message types and their contents describe a full protocol that can involve multiple Actors and that can evolve over multiple steps. Here’s the 
AbstractBehavior
 implementation of the chat room protocol:




Scala




copy
source
object ChatRoom {
  private final case class PublishSessionMessage(screenName: String, message: String) extends RoomCommand

  def apply(): Behavior[RoomCommand] =
    Behaviors.setup(context => new ChatRoomBehavior(context))

  class ChatRoomBehavior(context: ActorContext[RoomCommand]) extends AbstractBehavior[RoomCommand](context) {
    private var sessions: List[ActorRef[SessionCommand]] = List.empty

    override def onMessage(message: RoomCommand): Behavior[RoomCommand] = {
      message match {
        case GetSession(screenName, client) =>
          // create a child actor for further interaction with the client
          val ses = context.spawn(
            SessionBehavior(context.self, screenName, client),
            name = URLEncoder.encode(screenName, StandardCharsets.UTF_8.name))
          client ! SessionGranted(ses)
          sessions = ses :: sessions
          this
        case PublishSessionMessage(screenName, message) =>
          val notification = NotifyClient(MessagePosted(screenName, message))
          sessions.foreach(_ ! notification)
          this
      }
    }
  }

  private object SessionBehavior {
    def apply(
        room: ActorRef[PublishSessionMessage],
        screenName: String,
        client: ActorRef[SessionEvent]): Behavior[SessionCommand] =
      Behaviors.setup(ctx => new SessionBehavior(ctx, room, screenName, client))
  }

  private class SessionBehavior(
      context: ActorContext[SessionCommand],
      room: ActorRef[PublishSessionMessage],
      screenName: String,
      client: ActorRef[SessionEvent])
      extends AbstractBehavior[SessionCommand](context) {

    override def onMessage(msg: SessionCommand): Behavior[SessionCommand] =
      msg match {
        case PostMessage(message) =>
          // from client, publish to others via the room
          room ! PublishSessionMessage(screenName, message)
          Behaviors.same
        case NotifyClient(message) =>
          // published from the room
          client ! message
          Behaviors.same
      }
  }
}


Java




copy
source
public class ChatRoom {
  private static final class PublishSessionMessage implements RoomCommand {
    public final String screenName;
    public final String message;

    public PublishSessionMessage(String screenName, String message) {
      this.screenName = screenName;
      this.message = message;
    }
  }

  public static Behavior<RoomCommand> create() {
    return Behaviors.setup(ChatRoomBehavior::new);
  }

  public static class ChatRoomBehavior extends AbstractBehavior<RoomCommand> {
    final List<ActorRef<SessionCommand>> sessions = new ArrayList<>();

    private ChatRoomBehavior(ActorContext<RoomCommand> context) {
      super(context);
    }

    @Override
    public Receive<RoomCommand> createReceive() {
      ReceiveBuilder<RoomCommand> builder = newReceiveBuilder();

      builder.onMessage(GetSession.class, this::onGetSession);
      builder.onMessage(PublishSessionMessage.class, this::onPublishSessionMessage);

      return builder.build();
    }

    private Behavior<RoomCommand> onGetSession(GetSession getSession)
        throws UnsupportedEncodingException {
      ActorRef<SessionEvent> client = getSession.replyTo;
      ActorRef<SessionCommand> ses =
          getContext()
              .spawn(
                  SessionBehavior.create(getContext().getSelf(), getSession.screenName, client),
                  URLEncoder.encode(getSession.screenName, StandardCharsets.UTF_8.name()));
      // narrow to only expose PostMessage
      client.tell(new SessionGranted(ses.narrow()));
      sessions.add(ses);
      return this;
    }

    private Behavior<RoomCommand> onPublishSessionMessage(PublishSessionMessage pub) {
      NotifyClient notification =
          new NotifyClient((new MessagePosted(pub.screenName, pub.message)));
      sessions.forEach(s -> s.tell(notification));
      return this;
    }
  }

  static class SessionBehavior extends AbstractBehavior<ChatRoom.SessionCommand> {
    private final ActorRef<RoomCommand> room;
    private final String screenName;
    private final ActorRef<SessionEvent> client;

    public static Behavior<ChatRoom.SessionCommand> create(
        ActorRef<RoomCommand> room, String screenName, ActorRef<SessionEvent> client) {
      return Behaviors.setup(context -> new SessionBehavior(context, room, screenName, client));
    }

    private SessionBehavior(
        ActorContext<ChatRoom.SessionCommand> context,
        ActorRef<RoomCommand> room,
        String screenName,
        ActorRef<SessionEvent> client) {
      super(context);
      this.room = room;
      this.screenName = screenName;
      this.client = client;
    }

    @Override
    public Receive<SessionCommand> createReceive() {
      return newReceiveBuilder()
          .onMessage(PostMessage.class, this::onPostMessage)
          .onMessage(NotifyClient.class, this::onNotifyClient)
          .build();
    }

    private Behavior<SessionCommand> onPostMessage(PostMessage post) {
      // from client, publish to others via the room
      room.tell(new PublishSessionMessage(screenName, post.message));
      return Behaviors.same();
    }

    private Behavior<SessionCommand> onNotifyClient(NotifyClient notification) {
      // published from the room
      client.tell(notification.message);
      return Behaviors.same();
    }
  }
}




The state is managed through fields in the class, just like with a regular object oriented class. As the state is mutable, we never return a different behavior from the message logic, but can return the 
AbstractBehavior
 instance itself (
this
) as a behavior to use for processing the next message coming in. We could also return 
Behaviors.same
Behaviors.same
 to achieve the same.


In this sample we make separate statements for creating the behavior builder, but it also returns the builder itself from each step so a more fluent behavior definition style is also possible. What you should prefer depends on how big the set of messages the actor accepts is.


It is also possible to return a new different 
AbstractBehavior
, for example to represent a different state in a finite state machine (FSM), or use one of the functional behavior factories to combine the object oriented with the functional style for different parts of the lifecycle of the same Actor behavior.


When a new 
GetSession
 command comes in we add that client to the list of current sessions. Then we also need to create the sessionâs 
ActorRef
ActorRef
 that will be used to post messages. In this case we want to create a very simple Actor that repackages the 
PostMessage
 command into a 
PublishSessionMessage
 command which also includes the screen name.


To implement the logic where we spawn a child for the session we need access to the 
ActorContext
ActorContext
. This is injected as a constructor parameter upon creation of the behavior, note how we combine the 
AbstractBehavior
AbstractBehavior
 with 
Behaviors.setup
Behaviors.setup
 to do this in the 
apply
create
 factory method.


The behavior that we declare here can handle both subtypes of 
RoomCommand
. 
GetSession
 has been explained already and the 
PublishSessionMessage
 commands coming from the session Actors will trigger the dissemination of the contained chat room message to all connected clients. But we do not want to give the ability to send 
PublishSessionMessage
 commands to arbitrary clients, we reserve that right to the internal session actors we createâotherwise clients could pose as completely different screen names (imagine the 
GetSession
 protocol to include authentication information to further secure this). Therefore 
PublishSessionMessage
 has 
private
 visibility and can’t be created outside the 
ChatRoom
 
object
class
.


If we did not care about securing the correspondence between a session and a screen name then we could change the protocol such that 
PostMessage
 is removed and all clients just get an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 to send to. In this case no session actor would be needed and we could use 
context.self
context.getSelf()
. The type-checks work out in that case because 
ActorRef[-T]
ActorRef<T>
 is contravariant in its type parameter, meaning that we can use a 
ActorRef[RoomCommand]
ActorRef<RoomCommand>
 wherever an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 is neededâthis makes sense because the former simply speaks more languages than the latter. The opposite would be problematic, so passing an 
ActorRef[PublishSessionMessage]
ActorRef<PublishSessionMessage>
 where 
ActorRef[RoomCommand]
ActorRef<RoomCommand>
 is required will lead to a type error.


AbstractOnMessageBehavior API


The 
AbstractBehavior
 API makes use of a builder on receipt of the first message by the actor. The 
Receive
 built by this builder performs 
instanceof
 checks and casts “behind the scenes”. Pattern-matching features introduced in Java 17 and refined in subsequent versions improve the ergonomics of expressing this logic directly in code. Users of other JVM languages (such as Kotlin) may also prefer to not use a builder while using the Java DSL (note that the Scala DSL’s 
AbstractBehavior
 does not make use of builders).


To support this “direct” style, an alternative API for defining behavior in an object-oriented style is available by extending 
AbstractOnMessageBehavior
 and implementing the 
onMessage
 method.


Here’s the 
AbstractOnMessageBehavior
-based implementation of the chat room protocol using records for messages and Java 21 switch pattern matching:




Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.ActorSystem;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractOnMessageBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;

import java.nio.charset.StandardCharsets;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.List;

public class ChatRoom {
  public sealed interface RoomCommand {}
  public record GetSession(String screenName, ActorRef<SessionEvent> replyTo) implements RoomCommand {}
  private record PublishSessionMessage(String screenName, String message) implements RoomCommand {}

  sealed interface SessionEvent {}
  public record SessionGranted(ActorRef<PostMessage> handle) implements SessionEvent {}
  public record SessionDenied(String reason) implements SessionEvent {}
  public record MessagePosted(String screenName, String message) implements SessionEvent {}

  sealed interface SessionCommand {}
  public record PostMessage(String message) implements SessionCommand {}
  private record NotifyClient(MessagePosted message) implements SessionCommand {}

  public static Behavior<RoomCommand> create() {
    return Behaviors.setup(ChatRoomBehavior::new);
  }

  private static class ChatRoomBehavior extends AbstractOnMessageBehavior<RoomCommand> {

    private final List<ActorRef<SessionCommand>> sessions = new ArrayList<>();

    private ChatRoomBehavior(ActorContext<RoomCommand> context) {
      super(context);
    }

    @Override
    public Behavior<RoomCommand> onMessage(RoomCommand msg) {
      return switch(msg) {
        case GetSession gs -> onGetSession(gs);
        case PublishSessionMessage psm -> onPublishSessionMessage(psm);
      };
    }

    private Behavior<RoomCommand> onGetSession(GetSession gs) {
      ActorRef<SessionEvent> client = gs.replyTo;
      ActorRef<SessionCommand> ses =
          getContext()
              .spawn(
                  SessionBehavior.create(getContext().getSelf(), gs.screenName, client),
                  URLEncoder.encode(gs.screenName, StandardCharsets.UTF_8));

      // narrow to only expose PostMessage
      client.tell(new SessionGranted(ses.narrow()));
      sessions.add(ses);

      return this;
    }

    private Behavior<RoomCommand> onPublishSessionMessage(PublishSessionMessage pub) {
      NotifyClient notification =
          new NotifyClient(new MessagePosted(pub.screenName, pub.message));

      sessions.forEach(s -> s.tell(notification));
      return this;
    }
  }

}




Try it out


In order to see this chat room in action we need to write a client Actor that can use it 
, for this stateless actor it doesn’t make much sense to use the 
AbstractBehavior
 so let’s just reuse the functional style gabbler from the sample above
:




Scala




copy
source
object Gabbler {
  import ChatRoom._

  def apply(): Behavior[SessionEvent] =
    Behaviors.setup { context =>
      Behaviors.receiveMessage {
        case SessionDenied(reason) =>
          context.log.info("cannot start chat room session: {}", reason)
          Behaviors.stopped
        case SessionGranted(handle) =>
          handle ! PostMessage("Hello World!")
          Behaviors.same
        case MessagePosted(screenName, message) =>
          context.log.info("message has been posted by '{}': {}", screenName, message)
          Behaviors.stopped
      }
    }


Java




copy
source
public class Gabbler extends AbstractBehavior<ChatRoom.SessionEvent> {
  public static Behavior<ChatRoom.SessionEvent> create() {
    return Behaviors.setup(Gabbler::new);
  }

  private Gabbler(ActorContext<ChatRoom.SessionEvent> context) {
    super(context);
  }

  @Override
  public Receive<ChatRoom.SessionEvent> createReceive() {
    ReceiveBuilder<ChatRoom.SessionEvent> builder = newReceiveBuilder();
    return builder
        .onMessage(ChatRoom.SessionDenied.class, this::onSessionDenied)
        .onMessage(ChatRoom.SessionGranted.class, this::onSessionGranted)
        .onMessage(ChatRoom.MessagePosted.class, this::onMessagePosted)
        .build();
  }

  private Behavior<ChatRoom.SessionEvent> onSessionDenied(ChatRoom.SessionDenied message) {
    getContext().getLog().info("cannot start chat room session: {}", message.reason);
    return Behaviors.stopped();
  }

  private Behavior<ChatRoom.SessionEvent> onSessionGranted(ChatRoom.SessionGranted message) {
    message.handle.tell(new ChatRoom.PostMessage("Hello World!"));
    return Behaviors.same();
  }

  private Behavior<ChatRoom.SessionEvent> onMessagePosted(ChatRoom.MessagePosted message) {
    getContext()
        .getLog()
        .info("message has been posted by '{}': {}", message.screenName, message.message);
    return Behaviors.stopped();
  }
}




Now to try things out we must start both a chat room and a gabbler and of course we do this inside an Actor system. Since there can be only one user guardian we could either start the chat room from the gabbler (which we donât wantâit complicates its logic) or the gabbler from the chat room (which is nonsensical) or we start both of them from a third Actorâour only sensible choice:




Scala




copy
source
object Main {
  def apply(): Behavior[NotUsed] =
    Behaviors.setup { context =>
      val chatRoom = context.spawn(ChatRoom(), "chatroom")
      val gabblerRef = context.spawn(Gabbler(), "gabbler")
      context.watch(gabblerRef)
      chatRoom ! ChatRoom.GetSession("olâ Gabbler", gabblerRef)

      Behaviors.receiveSignal {
        case (_, Terminated(_)) =>
          Behaviors.stopped
      }
    }

  def main(args: Array[String]): Unit = {
    ActorSystem(Main(), "ChatRoomDemo")
  }

}


Java




copy
source
public class Main {
  public static Behavior<Void> create() {
    return Behaviors.setup(
        context -> {
          ActorRef<ChatRoom.RoomCommand> chatRoom = context.spawn(ChatRoom.create(), "chatRoom");
          ActorRef<ChatRoom.SessionEvent> gabbler = context.spawn(Gabbler.create(), "gabbler");
          context.watch(gabbler);
          chatRoom.tell(new ChatRoom.GetSession("olâ Gabbler", gabbler));

          return Behaviors.receive(Void.class)
              .onSignal(Terminated.class, sig -> Behaviors.stopped())
              .build();
        });
  }

  public static void main(String[] args) {
    ActorSystem.create(Main.create(), "ChatRoomDemo");
  }
}




In good tradition we call the 
Main
 Actor what it is, it directly corresponds to the 
main
 method in a traditional Java application. This Actor will perform its job on its own accord, we do not need to send messages from the outside, so we declare it to be of type 
NotUsed
Void
. Actors receive not only external messages, they also are notified of certain system events, so-called Signals. In order to get access to those we choose to implement this particular one using the 
receive
receive
 behavior decorator. The provided 
onSignal
 function will be invoked for signals (subclasses of 
Signal
Signal
) or the 
onMessage
 function for user messages.


This particular 
Main
 Actor is created using 
Behaviors.setup
Behaviors.setup
, which is like a factory for a behavior. Creation of the behavior instance is deferred until the actor is started, as opposed to 
Behaviors.receive
Behaviors.receive
 that creates the behavior instance immediately before the actor is running. The factory function in 
setup
 is passed the 
ActorContext
ActorContext
 as parameter and that can for example be used for spawning child actors. This 
Main
 Actor creates the chat room and the gabbler and the session between them is initiated, and when the gabbler is finished we will receive the 
Terminated
Terminated
 event due to having called 
context.watch
context.watch
 for it. This allows us to shut down the Actor system: when the 
Main
 Actor terminates there is nothing more to do.


Therefore after creating the Actor system with the 
Main
 Actorâs 
Behavior
Behavior
 we can let the 
main
 method return, the 
ActorSystem
ActorSystem
 will continue running and the JVM alive until the root actor stops.
Note


ð For a deeper introduction to actors, consider the free online courses 
Akka Basics for Java
Akka Basics for Scala
 in Akkademy.














 
Actors






Actor lifecycle 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/fsm.html
Behaviors as finite state machines • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines




Example project




Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines




Example project




Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Behaviors as finite state machines


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic FSM
.


An actor can be used to model a Finite State Machine (FSM).


To demonstrate this, consider an actor which shall receive and queue messages while they arrive in a burst and send them on after the burst ended or a flush request is received.


This example demonstrates how to:




Model states using different behaviors


Model storing data at each state by representing the behavior as a method


Implement state timeouts




The events the FSM can receive become the type of message the Actor can receive:




Scala




copy
source
object Buncher {

  // FSM event becomes the type of the message Actor supports
  sealed trait Event
  final case class SetTarget(ref: ActorRef[Batch]) extends Event
  final case class Queue(obj: Any) extends Event
  case object Flush extends Event
  private case object Timeout extends Event
}


Java




copy
source
public abstract class Buncher {

  public interface Event {}

  public static final class SetTarget implements Event {
    public final ActorRef<Batch> ref;

    public SetTarget(ActorRef<Batch> ref) {
      this.ref = ref;
    }
  }

  private enum Timeout implements Event {
    INSTANCE
  }

  public enum Flush implements Event {
    INSTANCE
  }

  public static final class Queue implements Event {
    public final Object obj;

    public Queue(Object obj) {
      this.obj = obj;
    }
  }
}




SetTarget
 is needed for starting it up, setting the destination for the 
Batches
 to be passed on; 
Queue
 will add to the internal queue while 
Flush
 will mark the end of a burst.




Scala




copy
source
sealed trait Data
case object Uninitialized extends Data
final case class Todo(target: ActorRef[Batch], queue: immutable.Seq[Any]) extends Data

final case class Batch(obj: immutable.Seq[Any])


Java




copy
source
interface Data {}

public static final class Todo implements Data {
  public final ActorRef<Batch> target;
  public final List<Object> queue;

  public Todo(ActorRef<Batch> target, List<Object> queue) {
    this.target = target;
    this.queue = queue;
  }

}

public static final class Batch {
  public final List<Object> list;

  public Batch(List<Object> list) {
    this.list = list;
  }

}




Each state becomes a distinct behavior and after processing a message the next state in the form of a 
Behavior
Behavior
 is returned.




Scala




copy
source
object Buncher {
  // states of the FSM represented as behaviors

  // initial state
  def apply(): Behavior[Event] = idle(Uninitialized)

  private def idle(data: Data): Behavior[Event] = Behaviors.receiveMessage[Event] { message =>
    (message, data) match {
      case (SetTarget(ref), Uninitialized) =>
        idle(Todo(ref, Vector.empty))
      case (Queue(obj), t @ Todo(_, v)) =>
        active(t.copy(queue = v :+ obj))
      case _ =>
        Behaviors.unhandled
    }
  }

  private def active(data: Todo): Behavior[Event] =
    Behaviors.withTimers[Event] { timers =>
      // instead of FSM state timeout
      timers.startSingleTimer(Timeout, 1.second)
      Behaviors.receiveMessagePartial {
        case Flush | Timeout =>
          data.target ! Batch(data.queue)
          idle(data.copy(queue = Vector.empty))
        case Queue(obj) =>
          active(data.copy(queue = data.queue :+ obj))
      }
    }

}


Java




copy
source
public abstract class Buncher {
  // FSM states represented as behaviors

  // initial state
  public static Behavior<Event> create() {
    return uninitialized();
  }

  private static Behavior<Event> uninitialized() {
    return Behaviors.receive(Event.class)
        .onMessage(
            SetTarget.class, message -> idle(new Todo(message.ref, Collections.emptyList())))
        .build();
  }

  private static Behavior<Event> idle(Todo data) {
    return Behaviors.receive(Event.class)
        .onMessage(Queue.class, message -> active(data.addElement(message)))
        .build();
  }

  private static Behavior<Event> active(Todo data) {
    return Behaviors.withTimers(
        timers -> {
          // State timeouts done with withTimers
          timers.startSingleTimer("Timeout", Timeout.INSTANCE, Duration.ofSeconds(1));
          return Behaviors.receive(Event.class)
              .onMessage(Queue.class, message -> active(data.addElement(message)))
              .onMessage(Flush.class, message -> activeOnFlushOrTimeout(data))
              .onMessage(Timeout.class, message -> activeOnFlushOrTimeout(data))
              .build();
        });
  }

  private static Behavior<Event> activeOnFlushOrTimeout(Todo data) {
    data.target.tell(new Batch(data.queue));
    return idle(data.copy(new ArrayList<>()));
  }

}




The method 
idle
 above makes use of 
Behaviors.unhandled
Behaviors.unhandled
 which advises the system to reuse the previous behavior, including the hint that the message has not been handled. There are two related behaviors:




return 
Behaviors.empty
Behaviors.empty
 as next behavior in case you reached a state where you don’t expect messages any more.  For instance if an actor only waits until all spawned child actors stopped.  Unhandled messages are still logged with this behavior.


return 
Behaviors.ignore
Behaviors.ignore
 as next behavior in case you don’t care about unhandled messages.  All messages sent to an actor with such a behavior are simply dropped and ignored (without logging)




To set state timeouts use 
Behaviors.withTimers
Behaviors.withTimers
 along with a 
startSingleTimer
startSingleTimer
.


Example project


FSM example project
 
FSM example project
 is an example project that can be downloaded, and with instructions of how to run.


This project contains a Dining Hakkers sample illustrating how to model a Finite State Machine (FSM) with actors.














 
Stash






Coordinated Shutdown 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/terminology.html
Terminology, Concepts • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts




Concurrency vs. Parallelism


Asynchronous vs. Synchronous


Non-blocking vs. Blocking


Deadlock vs. Starvation vs. Live-lock


Race Condition


Non-blocking Guarantees (Progress Conditions)


Recommended literature




Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts




Concurrency vs. Parallelism


Asynchronous vs. Synchronous


Non-blocking vs. Blocking


Deadlock vs. Starvation vs. Live-lock


Race Condition


Non-blocking Guarantees (Progress Conditions)


Recommended literature




Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Terminology, Concepts


In this chapter we attempt to establish a common terminology to define a solid ground for communicating about concurrent, distributed systems which Akka targets. Please note that, for many of these terms, there is no single agreed definition. We seek to give working definitions that will be used in the scope of the Akka documentation.


Concurrency vs. Parallelism


Concurrency and parallelism are related concepts, but there are small differences. 
Concurrency
 means that two or more tasks are making progress even though they might not be executing simultaneously. This can for example be realized with time slicing where parts of tasks are executed sequentially and mixed with parts of other tasks. 
Parallelism
 on the other hand arise when the execution can be truly simultaneous.


Asynchronous vs. Synchronous


A method call is considered 
synchronous
 if the caller cannot make progress until the method returns a value or throws an exception. On the other hand, an 
asynchronous
 call allows the caller to progress after a finite number of steps, and the completion of the method may be signalled via some additional mechanism (it might be a registered callback, a Future, or a message).


A synchronous API may use blocking to implement synchrony, but this is not a necessity. A very CPU intensive task might give a similar behavior as blocking. In general, it is preferred to use asynchronous APIs, as they guarantee that the system is able to progress. Actors are asynchronous by nature: an actor can progress after a message send without waiting for the actual delivery to happen.


Non-blocking vs. Blocking


We talk about 
blocking
 if the delay of one thread can indefinitely delay some of the other threads. A good example is a resource which can be used exclusively by one thread using mutual exclusion. If a thread holds on to the resource indefinitely (for example accidentally running an infinite loop) other threads waiting on the resource can not progress. In contrast, 
non-blocking
 means that no thread is able to indefinitely delay others.


Non-blocking operations are preferred to blocking ones, as the overall progress of the system is not trivially guaranteed when it contains blocking operations.


It’s not always possible to avoid using blocking APIs, please see 
Blocking Needs Careful Management
 to use those safely within the actor code.


Deadlock vs. Starvation vs. Live-lock


Deadlock
 arises when several participants are waiting on each other to reach a specific state to be able to progress. As none of them can progress without some other participant to reach a certain state (a “Catch-22” problem) all affected subsystems stall. Deadlock is closely related to 
blocking
, as it is necessary that a participant thread be able to delay the progression of other threads indefinitely.


In the case of 
deadlock
, no participants can make progress, while in contrast 
Starvation
 happens, when there are participants that can make progress, but there might be one or more that cannot. Typical scenario is the case of a naive scheduling algorithm that always selects high-priority tasks over low-priority ones. If the number of incoming high-priority tasks is constantly high enough, no low-priority ones will be ever finished.


Livelock
 is similar to 
deadlock
 as none of the participants make progress. The difference though is that instead of being frozen in a state of waiting for others to progress, the participants continuously change their state. An example scenario when two participants have two identical resources available. They each try to get the resource, but they also check if the other needs the resource, too. If the resource is requested by the other participant, they try to get the other instance of the resource. In the unfortunate case it might happen that the two participants “bounce” between the two resources, never acquiring it, but always yielding to the other.


Race Condition


We call it a 
Race condition
 when an assumption about the ordering of a set of events might be violated by external non-deterministic effects. Race conditions often arise when multiple threads have a shared mutable state, and the operations of thread on the state might be interleaved causing unexpected behavior. While this is a common case, shared state is not necessary to have race conditions. One example could be a client sending unordered packets (e.g. UDP datagrams) 
P1
, 
P2
 to a server. As the packets might potentially travel via different network routes, it is possible that the server receives 
P2
 first and 
P1
 afterwards. If the messages contain no information about their sending order it is impossible to determine by the server that they were sent in a different order. Depending on the meaning of the packets this can cause race conditions.
Note


The only guarantee that Akka provides about messages sent between a given pair of actors is that their order is always preserved. see 
Message Delivery Reliability


Non-blocking Guarantees (Progress Conditions)


As discussed in the previous sections blocking is undesirable for several reasons, including the dangers of deadlocks and reduced throughput in the system. In the following sections we discuss various non-blocking properties with different strength.


Wait-freedom


A method is 
wait-free
 if every call is guaranteed to finish in a finite number of steps. If a method is 
bounded wait-free
 then the number of steps has a finite upper bound.


From this definition it follows that wait-free methods are never blocking, therefore deadlock can not happen. Additionally, as each participant can progress after a finite number of steps (when the call finishes), wait-free methods are free of starvation.


Lock-freedom


Lock-freedom
 is a weaker property than 
wait-freedom
. In the case of lock-free calls, infinitely often some method finishes in a finite number of steps. This definition implies that no deadlock is possible for lock-free calls. On the other hand, the guarantee that 
some call finishes
 in a finite number of steps is not enough to guarantee that 
all of them eventually finish
. In other words, lock-freedom is not enough to guarantee the lack of starvation.


Obstruction-freedom


Obstruction-freedom
 is the weakest non-blocking guarantee discussed here. A method is called 
obstruction-free
 if there is a point in time after which it executes in isolation (other threads make no steps, e.g.: become suspended), it finishes in a bounded number of steps. All lock-free objects are obstruction-free, but the opposite is generally not true.


Optimistic concurrency control
 (OCC) methods are usually obstruction-free. The OCC approach is that every participant tries to execute its operation on the shared object, but if a participant detects conflicts from others, it rolls back the modifications, and tries again according to some schedule. If there is a point in time, where one of the participants is the only one trying, the operation will succeed.


Recommended literature




The Art of Multiprocessor Programming, M. Herlihy and N Shavit, 2008. ISBN 978-0123705914


Java Concurrency in Practice, B. Goetz, T. Peierls, J. Bloch, J. Bowbeer, D. Holmes and D. Lea, 2006. ISBN 978-0321349606
















 
General Concepts






Actor Systems 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/circuitbreaker.html
Circuit Breaker • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker




Why are they used?


What do they do?


Examples




Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker




Why are they used?


What do they do?


Examples




Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Circuit Breaker


Why are they used?


A circuit breaker is used to provide stability and prevent cascading failures in distributed systems. These should be used in conjunction with judicious timeouts at the interfaces between remote systems to prevent the failure of a single component from bringing down all components.


As an example, we have a web application interacting with a remote third party web service.
Let’s say the third party has oversold their capacity and their database melts down under load.
Assume that the database fails in such a way that it takes a very long time to hand back an error to the third party web service. This in turn makes calls fail after a long period of time. Back to our web application, the users have noticed that their form submissions take much longer seeming to hang. Well the users do what they know to do which is use the refresh button, adding more requests to their already running requests. This eventually causes the failure of the web application due to resource exhaustion. This will affect all users, even those who are not using functionality dependent on this third party web service.


Introducing circuit breakers on the web service call would cause the requests to begin to fail-fast, letting the user know that something is wrong and that they need not refresh their request. This also confines the failure behavior to only those users that are using functionality dependent on the third party, other users are no longer affected as there is no resource exhaustion. Circuit breakers can also allow savvy developers to mark portions of the site that use the functionality unavailable, or perhaps show some cached content as appropriate while the breaker is open.


The Akka library provides an implementation of a circuit breaker called 
CircuitBreaker
CircuitBreaker
 which has the behavior described below.


What do they do?






During normal operation, a circuit breaker is in the 
Closed
 state:




Exceptions or calls exceeding the configured 
callTimeout
 increment a failure counter


Successes reset the failure count to zero


When the failure counter reaches a 
maxFailures
 count, the breaker is tripped into 
Open
 state








While in 
Open
 state:




All calls fail-fast with a 
CircuitBreakerOpenException
CircuitBreakerOpenException


After the configured 
resetTimeout
, the circuit breaker enters a 
Half-Open
 state








In 
Half-Open
 state:




The first call attempted is allowed through without failing fast


All other calls fail-fast with an exception just as in 
Open
 state


If the first call succeeds, the breaker is reset back to 
Closed
 state and the 
resetTimeout
 is reset


If the first call fails, the breaker is tripped again into the 
Open
 state (as for exponential backoff circuit breaker, the 
resetTimeout
 is multiplied by the exponential backoff factor)








State transition listeners:




Callbacks can be provided for every state entry via 
onOpen
onOpen
, 
onClose
onClose
, and 
onHalfOpen
onHalfOpen


These are executed in the 
ExecutionContext
 provided.








Calls result listeners:




Callbacks can be used eg. to collect statistics about all invocations or to react on specific call results like success, failures or timeouts.


Supported callbacks are: 
onCallSuccess
onCallSuccess
, 
onCallFailure
onCallFailure
, 
onCallTimeout
onCallTimeout
, 
onCallBreakerOpen
onCallBreakerOpen
.


These are executed in the 
ExecutionContext
 provided.










Examples


Initialization


Here’s how a named 
CircuitBreaker
CircuitBreaker
 is configured with the name 
data-access
:




5 maximum failures


a call timeout of 10 seconds


a reset timeout of 1 minute




copy
source
akka.circuit-breaker.data-access {
  max-failures = 5
  call-timeout = 10s
  reset-timeout = 1m
}


The circuit breaker is created on first access with the same name, subsequent lookups will return the same circuit breaker instance. Looking up the circuit breaker and using it looks like this:




Scala




copy
source
val circuitBreaker = CircuitBreaker("data-access")(context.system)


Java




copy
source
CircuitBreaker circuitBreaker =
    CircuitBreaker.lookup("data-access", context.getSystem());




Future & Synchronous based API


Once a circuit breaker actor has been initialized, interacting with that actor is done by either using the Future based API or the synchronous API. Both of these APIs are considered 
Call Protection
 because whether synchronously or asynchronously, the purpose of the circuit breaker is to protect your system from cascading failures while making a call to another service. 


In the future based API, we use the 
withCircuitBreaker
callWithCircuitBreakerCS
 which takes an asynchronous method (some method wrapped in a 
Future
CompletionState
), for instance a call to retrieve data from a service, and we pipe the result back to the sender. If for some reason the service in this example isn’t responding, or there is another issue, the circuit breaker will open and stop trying to hit the service again and again until the timeout is reached.




Scala




copy
source
class DataAccess(
    context: ActorContext[DataAccess.Command],
    id: String,
    service: ThirdPartyWebService,
    circuitBreaker: CircuitBreaker) {
  import DataAccess._

  private def active(): Behavior[Command] = {
    Behaviors.receiveMessagePartial {
      case Handle(value, replyTo) =>
        val futureResult: Future[Done] = circuitBreaker.withCircuitBreaker {
          service.call(id, value)
        }
        context.pipeToSelf(futureResult) {
          case Success(_)         => HandleSuceeded(replyTo)
          case Failure(exception) => HandleFailed(replyTo, exception)
        }
        Behaviors.same
      case HandleSuceeded(replyTo) =>
        replyTo ! StatusReply.Ack
        Behaviors.same
      case HandleFailed(replyTo, exception) =>
        context.log.warn("Failed to call web service", exception)
        replyTo ! StatusReply.error("Dependency service not available")
        Behaviors.same
    }

  }
}


Java




copy
source
class DataAccess extends AbstractBehavior<DataAccess.Command> {

  public interface Command {}

  public static class Handle implements Command {
    final String value;
    final ActorRef<StatusReply<Done>> replyTo;

    public Handle(String value, ActorRef<StatusReply<Done>> replyTo) {
      this.value = value;
      this.replyTo = replyTo;
    }
  }

  private final class HandleFailed implements Command {
    final Throwable failure;
    final ActorRef<StatusReply<Done>> replyTo;

    public HandleFailed(Throwable failure, ActorRef<StatusReply<Done>> replyTo) {
      this.failure = failure;
      this.replyTo = replyTo;
    }
  }

  private final class HandleSuceeded implements Command {
    final ActorRef<StatusReply<Done>> replyTo;

    public HandleSuceeded(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  private final class CircuitBreakerStateChange implements Command {
    final String newState;

    public CircuitBreakerStateChange(String newState) {
      this.newState = newState;
    }
  }

  public static Behavior<Command> create(String id, ThirdPartyWebService service) {
    return Behaviors.setup(
        context -> {
          CircuitBreaker circuitBreaker =
              CircuitBreaker.lookup("data-access", context.getSystem());
          return new DataAccess(context, id, service, circuitBreaker);
        });
  }

  private final String id;
  private final ThirdPartyWebService service;
  private final CircuitBreaker circuitBreaker;

  public DataAccess(
      ActorContext<Command> context,
      String id,
      ThirdPartyWebService service,
      CircuitBreaker circuitBreaker) {
    super(context);
    this.id = id;
    this.service = service;
    this.circuitBreaker = circuitBreaker;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Handle.class, this::onHandle)
        .onMessage(HandleSuceeded.class, this::onHandleSucceeded)
        .onMessage(HandleFailed.class, this::onHandleFailed)
        .build();
  }

  private Behavior<Command> onHandle(Handle handle) {
    CompletionStage<Done> futureResult =
        circuitBreaker.callWithCircuitBreakerCS(() -> service.call(id, handle.value));
    getContext()
        .pipeToSelf(
            futureResult,
            (done, throwable) -> {
              if (throwable != null) {
                return new HandleFailed(throwable, handle.replyTo);
              } else {
                return new HandleSuceeded(handle.replyTo);
              }
            });
    return this;
  }

  private Behavior<Command> onHandleSucceeded(HandleSuceeded handleSuceeded) {
    handleSuceeded.replyTo.tell(StatusReply.ack());
    return this;
  }

  private Behavior<Command> onHandleFailed(HandleFailed handleFailed) {
    getContext().getLog().warn("Failed to call web service", handleFailed.failure);
    handleFailed.replyTo.tell(StatusReply.error("Dependency service not available"));
    return this;
  }

}




The Synchronous API would also wrap your call with the circuit breaker logic, however, it uses the 
withSyncCircuitBreaker
callWithSyncCircuitBreaker
 and receives a method that is not wrapped in a 
Future
CompletionState
.


The 
CircuitBreaker
 will execute all callbacks on the default system dispatcher.


Control failure count explicitly


By default, the circuit breaker treats 
Exception
 as failure in synchronized API, or failed 
Future
CompletionState
 as failure in future based API. On failure, the failure count will increment. If the failure count reaches the 
maxFailures
, the circuit breaker will be opened. However, some applications may require certain exceptions to not increase the failure count. In other cases one may want to increase the failure count even if the call succeeded. Akka circuit breaker provides a way to achieve such use cases: 
withCircuitBreaker
 and 
withSyncCircuitBreaker
callWithCircuitBreaker
, 
callWithSyncCircuitBreaker
 and 
callWithCircuitBreakerCS
.


All methods above accept an argument 
defineFailureFn


Type of 
defineFailureFn
: 
Try[T]
 => 
Boolean
BiFunction
[
Optional[T]
, 
Optional
[
Throwable
], 
Boolean
]


This is a function which takes in a 
Try[T]
 and returns a 
Boolean
. The 
Try[T]
 correspond to the 
Future[T]
 of the protected call.
 
The response of a protected call is modelled using 
Optional[T]
 for a successful return value and 
Optional
[
Throwable
] for exceptions.
 This function should return 
true
 if the result of a call should increase the failure count, or 
false
 to not affect the count.




Scala




copy
source
val evenNumberAsFailure: Try[Int] => Boolean = {
  case Success(n) => n % 2 == 0
  case Failure(_) => true
}

val breaker = CircuitBreaker("dangerous-breaker")

// this call will return 8888 and increase failure count at the same time
breaker.withCircuitBreaker(Future(8888), evenNumberAsFailure)


Java




copy
source
BiFunction<Optional<Integer>, Optional<Throwable>, Boolean> evenNoAsFailure =
    (result, err) -> (result.isPresent() && result.get() % 2 == 0);

// this will return 8888 and increase failure count at the same time
return circuitBreaker.callWithSyncCircuitBreaker(() -> 8888, evenNoAsFailure);




Low level API


Instead of looking up a configured circuit breaker by name, it is also possible to construct it in the source code:




Scala




copy
source
import akka.actor.typed.scaladsl.adapter._
val breaker =
  new CircuitBreaker(
    context.system.scheduler.toClassic,
    maxFailures = 5,
    callTimeout = 10.seconds,
    resetTimeout = 1.minute).onOpen(context.self ! BreakerOpen)


Java




copy
source
breaker =
    CircuitBreaker.create(
            getContext().getSystem().classicSystem().getScheduler(),
            // maxFailures
            5,
            // callTimeout
            Duration.ofSeconds(10),
            // resetTimeout
            Duration.ofMinutes(1))
        .addOnOpenListener(() -> context.getSelf().tell(new BreakerOpen()));




This also allows for creating the circuit breaker with a specific execution context to run its callbacks on.


The low-level API allows you to describe the behavior of the 
CircuitBreaker
CircuitBreaker
 in detail, including deciding what to return to the calling 
Actor
Actor
 in case of success or failure. This is especially useful when expecting the remote call to send a reply. 
CircuitBreaker
CircuitBreaker
 doesn’t support 
Tell Protection
 (protecting against calls that expect a reply) natively at the moment. Thus, you need to use the low-level power-user APIs, 
succeed
succeed
 and 
fail
fail
 methods, as well as 
isClosed
isClosed
, 
isOpen
isOpen
, 
isHalfOpen
isHalfOpen
 to implement it.


As can be seen in the examples below, a 
Tell Protection
 pattern could be implemented by using the 
succeed
succeed
 and 
fail
fail
 methods, which would count towards the 
CircuitBreaker
CircuitBreaker
 counts. In the example, a call is made to the remote service if the breaker is closed or half open. Once a response is received, the 
succeed
succeed
 method is invoked, which tells the 
CircuitBreaker
CircuitBreaker
 to keep the breaker closed. On the other hand, if an error or timeout is received we trigger a 
fail
fail
, and the breaker accrues this failure towards its count for opening the breaker.




Scala




copy
source
object CircuitBreakingIntermediateActor {
  sealed trait Command
  case class Call(payload: String, replyTo: ActorRef[StatusReply[Done]]) extends Command
  private case class OtherActorReply(reply: Try[Done], originalReplyTo: ActorRef[StatusReply[Done]]) extends Command
  private case object BreakerOpen extends Command

  def apply(recipient: ActorRef[OtherActor.Command]): Behavior[Command] =
    Behaviors.setup { context =>
      implicit val askTimeout: Timeout = 11.seconds
      import context.executionContext
      import akka.actor.typed.scaladsl.adapter._
      val breaker =
        new CircuitBreaker(
          context.system.scheduler.toClassic,
          maxFailures = 5,
          callTimeout = 10.seconds,
          resetTimeout = 1.minute).onOpen(context.self ! BreakerOpen)

      Behaviors.receiveMessage {
        case Call(payload, replyTo) =>
          if (breaker.isClosed || breaker.isHalfOpen) {
            context.askWithStatus(recipient, OtherActor.Call(payload, _))(OtherActorReply(_, replyTo))
          } else {
            replyTo ! StatusReply.error("Service unavailable")
          }
          Behaviors.same
        case OtherActorReply(reply, originalReplyTo) =>
          if (reply.isSuccess) breaker.succeed()
          else breaker.fail()
          originalReplyTo ! StatusReply.fromTry(reply)
          Behaviors.same
        case BreakerOpen =>
          context.log.warn("Circuit breaker open")
          Behaviors.same
      }
    }
}


Java




copy
source
static class CircuitBreakingIntermediateActor
    extends AbstractBehavior<CircuitBreakingIntermediateActor.Command> {

  public interface Command {}

  public static class Call implements Command {
    final String payload;
    final ActorRef<StatusReply<Done>> replyTo;

    public Call(String payload, ActorRef<StatusReply<Done>> replyTo) {
      this.payload = payload;
      this.replyTo = replyTo;
    }
  }

  private class OtherActorReply implements Command {
    final Optional<Throwable> failure;
    final ActorRef<StatusReply<Done>> originalReplyTo;

    public OtherActorReply(
        Optional<Throwable> failure, ActorRef<StatusReply<Done>> originalReplyTo) {
      this.failure = failure;
      this.originalReplyTo = originalReplyTo;
    }
  }

  private class BreakerOpen implements Command {}

  private final ActorRef<OtherActor.Command> target;
  private final CircuitBreaker breaker;

  public CircuitBreakingIntermediateActor(
      ActorContext<Command> context, ActorRef<OtherActor.Command> targetActor) {
    super(context);
    this.target = targetActor;
    breaker =
        CircuitBreaker.create(
                getContext().getSystem().classicSystem().getScheduler(),
                // maxFailures
                5,
                // callTimeout
                Duration.ofSeconds(10),
                // resetTimeout
                Duration.ofMinutes(1))
            .addOnOpenListener(() -> context.getSelf().tell(new BreakerOpen()));
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Call.class, this::onCall)
        .onMessage(OtherActorReply.class, this::onOtherActorReply)
        .onMessage(BreakerOpen.class, this::breakerOpened)
        .build();
  }

  private Behavior<Command> onCall(Call call) {
    if (breaker.isClosed() || breaker.isHalfOpen()) {
      getContext()
          .askWithStatus(
              Done.class,
              target,
              Duration.ofSeconds(11),
              (replyTo) -> new OtherActor.Call(call.payload, replyTo),
              (done, failure) -> new OtherActorReply(Optional.ofNullable(failure), call.replyTo));
    } else {
      call.replyTo.tell(StatusReply.error("Service unavailable"));
    }
    return this;
  }

  private Behavior<Command> onOtherActorReply(OtherActorReply otherActorReply) {
    if (otherActorReply.failure.isPresent()) {
      breaker.fail();
      getContext().getLog().warn("Service failure", otherActorReply.failure.get());
      otherActorReply.originalReplyTo.tell(StatusReply.error("Service unavailable"));
    } else {
      breaker.succeed();
      otherActorReply.originalReplyTo.tell(StatusReply.ack());
    }
    return this;
  }

  private Behavior<Command> breakerOpened(BreakerOpen breakerOpen) {
    getContext().getLog().warn("Circuit breaker open");
    return this;
  }
}


Note


This example always makes remote calls when the state is 
HalfOpen
. Using the power-user APIs, it is your responsibility to judge when to make remote calls in 
HalfOpen
.














 
Logging






Futures patterns 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/links.html
Project • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Commercial Support


Sponsors


Akka Discuss Forums


Source Code


Releases Repository


Snapshots Repository






Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Commercial Support


Sponsors


Akka Discuss Forums


Source Code


Releases Repository


Snapshots Repository






Akka Classic




















Project


Commercial Support


Commercial support is provided by 
Akka
.


Sponsors


Akka
 is the company behind Akka product, Scala Programming Language, Play Web Framework, Lagom, sbt and many other open source and source available projects. Learn more at 
akka.io
.


Akka Discuss Forums


Akka Discuss Forums


Source Code


Akka uses Git and is hosted at 
Github akka/akka
.


Releases Repository


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Snapshots Repository


Snapshot builds are available from the repository at 
https://repo.akka.io/snapshots
. All Akka modules that belong to the same build have the same version.
Warning


The use of Akka SNAPSHOTs, nightlies and milestone releases is discouraged unless you know what you are doing.


sbt definition of snapshot repository


Make sure that you add the repository to the sbt resolvers:


resolvers += "Akka library snapshot repository".at("https://repo.akka.io/snapshots")



Define the library dependencies with the complete version. For example:


libraryDependencies += "com.typesafe.akka" % "akka-cluster_2.13" % "2.9.0+72-53943d99-SNAPSHOT"



Maven definition of snapshot repository


Make sure that you add the repository to the Maven repositories in pom.xml:


<repositories>
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library snapshot repository</name>
      <url>https://repo.akka.io/snapshots</url>
    </repository>
  </repositories>
</repositories>



Define the library dependencies with the complete version. For example:


<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster_2.13</artifactId>
    <version>2.9.0+72-53943d99-SNAPSHOT</version>
  </dependency>
</dependencies>















 
Example projects






Akka Classic 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/index.html
Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actors






Introduction to Actors




Module info


Akka Actors


First example


A More Complex Example




Actor lifecycle




Dependency


Introduction


Creating Actors


Stopping Actors


Watching Actors




Interaction Patterns




Dependency


Introduction


Fire and Forget


Request-Response


Adapted Response


Request-Response with ask between two actors


Request-Response with ask from outside an Actor


Generic response wrapper


Ignoring replies


Send Future result to self


Per session child Actor


General purpose response aggregator


Latency tail chopping


Scheduling messages to self


Responding to a sharded actor




Fault Tolerance




Supervision


Child actors are stopped when parent is restarting


The PreRestart signal


Bubble failures up through the hierarchy




Actor discovery




Dependency


Obtaining Actor references


Receptionist


Cluster Receptionist


Receptionist Scalability




Routers




Dependency


Introduction


Pool Router


Group Router


Routing strategies


Routers and performance




Stash




Dependency


Introduction




Behaviors as finite state machines




Example project




Coordinated Shutdown


Dispatchers




Dependency


Introduction


Default dispatcher


Internal dispatcher


Looking up a Dispatcher


Selecting a dispatcher


Types of dispatchers


Dispatcher aliases


Blocking Needs Careful Management


More dispatcher configuration examples




Mailboxes




Dependency


Introduction


Selecting what mailbox is used


Mailbox Implementations


Custom Mailbox type




Testing




Module info


Introduction


Asynchronous testing


Synchronous behavior testing




Coexistence




Dependency


Introduction


Classic to typed


Typed to classic


Supervision




Style guide




Functional versus object-oriented style


Passing around too many parameters


Behavior factory method


Where to define messages


Public versus private messages


Lambdas versus method references


Partial versus total Function


How to compose Partial Functions


ask versus ?


ReceiveBuilder


Nesting setup


Additional naming conventions




Learning Akka Typed from Classic




Dependencies


Package names


Actor definition


actorOf and Props


ActorRef


ActorSystem


become


sender


parent


Supervision


Lifecycle hooks


watch


Stopping


ActorSelection


ask


pipeTo


ActorContext


ActorContext.children


Remote deployment


Routers


FSM


Timers


Stash


PersistentActor


Asynchronous Testing


Synchronous Testing




















 
Default configuration






Introduction to Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/binary-compatibility-rules.html
Binary Compatibility Rules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules




Binary compatibility rules explained


Change in versioning scheme, stronger compatibility since 2.4


Mixed versioning is not allowed


The meaning of “may change”


API stability annotations and comments


Binary Compatibility Checking Toolchain


Serialization compatibility across Scala versions




Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules




Binary compatibility rules explained


Change in versioning scheme, stronger compatibility since 2.4


Mixed versioning is not allowed


The meaning of “may change”


API stability annotations and comments


Binary Compatibility Checking Toolchain


Serialization compatibility across Scala versions




Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Binary Compatibility Rules


Akka maintains and verifies 
backwards binary compatibility
 across versions of modules.


In the rest of this document whenever 
binary compatibility
 is mentioned “
backwards binary compatibility
” is meant (as opposed to forward compatibility).


This means that the new JARs are a drop-in replacement for the old one (but not the other way around) as long as your build does not enable the inliner (Scala-only restriction).


Because of this approach applications can upgrade to the latest version of Akka even when 
intermediate satellite projects are not yet upgraded


Binary compatibility rules explained


Binary compatibility is maintained between:




minor
 and 
patch
 versions - please note that the meaning of “minor” has shifted to be more restrictive with Akka 
2.4.0
, read 
Change in versioning scheme
 for details.




Binary compatibility is 
NOT
 maintained between:




major
 versions


any versions of 
may change
 modules â read 
Modules marked “May Change”
 for details


a few notable exclusions explained below




Specific examples (please read 
Change in versioning scheme
 to understand the difference in “before 2.4 era” and “after 2.4 era”):


# [epoch.major.minor] era
OK:  2.2.0 --> 2.2.1 --> ... --> 2.2.x
NO:  2.2.y --x 2.3.y
OK:  2.3.0 --> 2.3.1 --> ... --> 2.3.x
OK:  2.3.x --> 2.4.x (special case, migration to new versioning scheme)
# [major.minor.patch] era
OK:  2.4.0 --> 2.5.x
OK:  2.5.0 --> 2.6.x
NO:  2.x.y --x 3.x.y
OK:  3.0.0 --> 3.0.1 --> ... --> 3.0.n
OK:  3.0.n --> 3.1.0 --> ... --> 3.1.n
OK:  3.1.n --> 3.2.0 ...
     ...



Cases where binary compatibility is not retained


If a security vulnerability is reported in Akka or a transient dependency of Akka and it cannot be solved without breaking binary compatibility then fixing the security issue is more important. In such cases binary compatibility might not be retained when releasing a minor version. Such exception is always noted in the release announcement.


We do not guarantee binary compatibility with versions that are EOL, though in practice this does not make a big difference: only in rare cases would a change be binary compatible with recent previous releases but not with older ones.


Some modules are excluded from the binary compatibility guarantees, such as:




*-testkit
 modules - since these are to be used only in tests, which usually are re-compiled and run on demand


*-tck
 modules - since they may want to add new tests (or force configuring something), in order to discover possible failures in an existing implementation that the TCK is supposed to be testing. Compatibility here is not 
guaranteed
, however it is attempted to make the upgrade process as smooth as possible.


all 
may change
 modules - which by definition are subject to rapid iteration and change. Read more about that in 
Modules marked “May Change”




When will a deprecated method be removed entirely


Once a method has been deprecated then the guideline* is that it will be kept, at minimum, for one 
full
 minor version release. For example, if it is deprecated in version 2.5.2 then it will remain through the rest of 2.5, as well as the entirety of 2.6. 


*This is a guideline because in 
rare
 instances, after careful consideration, an exception may be made and the method removed earlier.




Change in versioning scheme, stronger compatibility since 2.4


Since the release of Akka 
2.4.0
 a new versioning scheme is in effect.


Historically, Akka has been following the Java or Scala style of versioning in which the first number would mean “
epoch
”, the second one would mean 
major
, and third be the 
minor
, thus: 
epoch.major.minor
 (versioning scheme followed until and during 
2.3.x
).


Currently
, since Akka 
2.4.0
, the new versioning applies which is closer to semantic versioning many have come to expect, in which the version number is deciphered as 
major.minor.patch
. This also means that Akka 
2.5.x
 is binary compatible with the 
2.4
 series releases (with the exception of “may change” APIs).


In addition to that, Akka 
2.4.x
 has been made binary compatible with the 
2.3.x
 series, so there is no reason to remain on Akka 2.3.x, since upgrading is completely compatible (and many issues have been fixed ever since).


Mixed versioning is not allowed


Modules that are released together under the Akka project are intended to be upgraded together. For example, it is not legal to mix Akka Actor 
2.6.2
 with Akka Cluster 
2.6.5
 even though “Akka 
2.6.2
” and “Akka 
2.6.5
” 
are
 binary compatible. 


This is because modules may assume internals changes across module boundaries, for example some feature in Clustering may have required an internals change in Actor, however it is not public API, thus such change is considered safe.


If you accidentally mix Akka versions, for example through transitive dependencies, you might get a warning at run time such as:


You are using version 2.6.6 of Akka, but it appears you (perhaps indirectly) also depend on older versions 
of related artifacts. You can solve this by adding an explicit dependency on version 2.6.6 of the 
[akka-persistence-query] artifacts to your project. Here's a complete collection of detected 
artifacts: (2.5.3, [akka-persistence-query]), (2.6.6, [akka-actor, akka-cluster]).
See also: https://doc.akka.io/libraries/akka-core/current/common/binary-compatibility-rules.html#mixed-versioning-is-not-allowed



The fix is typically to pick the highest Akka version, and add explicit dependencies to your project as needed. For example, in the example above you might want to add 
akka-persistence-query
 dependency for 2.6.6.
Note


We recommend keeping an 
akkaVersion
 variable in your build file, and re-use it for all included modules, so when you upgrade you can simply change it in this one place.


The warning includes a full list of Akka runtime dependencies in the classpath, and the version detected. You can use that information to include an explicit list of Akka artifacts you depend on into your build. If you use Maven or Gradle, you can include the 
Akka Maven BOM
 (bill of materials) to help you keep all the versions of your Akka dependencies in sync. 


The meaning of “may change”


May change
 is used in module descriptions and docs in order to signify that the API that they contain is subject to change without any prior warning and is not covered by the binary compatibility promise. Read more in 
Modules marked “May Change”
.


API stability annotations and comments


Akka gives a very strong binary compatibility promise to end-users. However some parts of Akka are excluded from these rules, for example internal or known evolving APIs may be marked as such and shipped as part of an overall stable module. As general rule any breakage is avoided and handled via deprecation and method addition, however certain APIs which are known to not yet be fully frozen (or are fully internal) are marked as such and subject to change at any time (even if best-effort is taken to keep them compatible).


The INTERNAL API and 
@InternalAPI
 marker


When browsing the source code and/or looking for methods available to be called, especially from Java which does not have as rich of an access protection system as Scala has, you may sometimes find methods or classes annotated with the 
/** INTERNAL API */
 comment or the 
@InternalApi
 annotation. 


No compatibility guarantees are given about these classes. They may change or even disappear in minor versions, and user code is not supposed to call them.


Side-note on JVM representation details of the Scala 
private[akka]
 pattern that Akka is using extensively in its internals: Such methods or classes, which act as “accessible only from the given package” in Scala, are compiled down to 
public
 (!) in raw Java bytecode. The access restriction, that Scala understands is carried along as metadata stored in the classfile. Thus, such methods are safely guarded from being accessed from Scala, however Java users will not be warned about this fact by the 
javac
 compiler. Please be aware of this and do not call into Internal APIs, as they are subject to change without any warning.


The 
@DoNotInherit
 and 
@ApiMayChange
 markers


In addition to the special internal API marker two annotations exist in Akka and specifically address the following use cases:




@ApiMayChange
 â which marks APIs which are known to be not fully stable yet. Read more in 
Modules marked “May Change”


@DoNotInherit
 â which marks APIs that are designed under a closed-world assumption, and thus must not be extended outside Akka itself (or such code will risk facing binary incompatibilities). E.g. an interface may be marked using this annotation, and while the type is public, it is not meant for extension by user-code. This allows adding new methods to these interfaces without risking to break client code. Examples of such API are the 
FlowOps
 trait or the Akka HTTP domain model.




Please note that a best-effort approach is always taken when having to change APIs and breakage is avoided as much as possible, however these markers allow to experiment, gather feedback and stabilize the best possible APIs we could build.


Binary Compatibility Checking Toolchain


Akka uses the Lightbend maintained 
MiMa
, for enforcing binary compatibility is kept where it was promised.


All Pull Requests must pass MiMa validation (which happens automatically), and if failures are detected, manual exception overrides may be put in place if the change happened to be in an Internal API for example.


Serialization compatibility across Scala versions


Scala does not maintain serialization compatibility across major versions. This means that if Java serialization is used there is no guarantee objects can be cleanly deserialized if serialized with a different version of Scala.














 
Project Information






Downstream upgrade strategy 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://github.com/akka/akka/tree/main/akka-docs/src/main/paradox/index.md
akka/akka-docs/src/main/paradox/index.md at main · akka/akka · GitHub




























































































Skip to content




























Navigation Menu




Toggle navigation










 



























            Sign in
          

















        Product
        




























GitHub Copilot

        Write better code with AI
      
















Security

        Find and fix vulnerabilities
      
















Actions

        Automate any workflow
      
















Codespaces

        Instant dev environments
      
















Issues

        Plan and track work
      
















Code Review

        Manage code changes
      
















Discussions

        Collaborate outside of code
      
















Code Search

        Find more, search less
      














Explore







      All features

    






      Documentation

    











      GitHub Skills

    











      Blog

    





















        Solutions
        














By company size







      Enterprises

    






      Small and medium teams

    






      Startups

    






      Nonprofits

    









By use case







      DevSecOps

    






      DevOps

    






      CI/CD

    






      View all use cases

    













By industry







      Healthcare

    






      Financial services

    






      Manufacturing

    






      Government

    






      View all industries

    












              View all solutions
              






 









        Resources
        














Topics







      AI

    






      DevOps

    






      Security

    






      Software Development

    






      View all

    













Explore







      Learning Pathways

    











      White papers, Ebooks, Webinars

    











      Customer Stories

    






      Partners

    











      Executive Insights

    
















        Open Source
        






















GitHub Sponsors

        Fund open source developers
      


















The ReadME Project

        GitHub community articles
      










Repositories







      Topics

    






      Trending

    






      Collections

    
















        Enterprise
        




























Enterprise platform

        AI-powered developer platform
      










Available add-ons
















Advanced Security

        Enterprise-grade security features
      
















GitHub Copilot

        Enterprise-grade AI features
      
















Premium Support

        Enterprise-grade 24/7 support
      
















Pricing


























Search or jump to...
















Search code, repositories, users, issues, pull requests...




 









        Search
      




























Clear


 


































































































 








Search syntax tips
 





























        Provide feedback
      




















 


We read every piece of feedback, and take your input very seriously.






Include my email address so I can be contacted






 
    Cancel



    Submit feedback




















        Saved searches
      


Use saved searches to filter your results more quickly




















 












Name














Query







            To see all available qualifiers, see our 
documentation
.
          


 












 
    Cancel



    Create saved search
















                Sign in
              





                Sign up
              


Reseting focus




















You signed in with another tab or window. 
Reload
 to refresh your session.


You signed out in another tab or window. 
Reload
 to refresh your session.


You switched accounts on another tab or window. 
Reload
 to refresh your session.


 






Dismiss alert





































        akka

 


/




akka




Public












 




Notifications

 
You must be signed in to change notification settings






 




Fork
    
3.6k










 





          Star

 
13.1k






























Code
















Issues


858














Pull requests


31














Actions
















Security
















Insights








 




 






Additional navigation options






 





















          Code






















          Issues






















          Pull requests






















          Actions






















          Security






















          Insights













 
















 
 
 
Files
 
main
Breadcrumbs
akka
/
akka-docs
/
src
/
main
/
paradox
/
index.md
Copy path
 
Blame
 
 
Blame
 
 
 
 
 
 
 
 
Latest commit
 
History
History
22 lines (18 loc) · 640 Bytes
 
main
Breadcrumbs
akka
/
akka-docs
/
src
/
main
/
paradox
/
index.md
Top
File metadata and controls
Preview
Code
Blame
22 lines (18 loc) · 640 Bytes
Raw
Akka Documentation


@@toc { depth=2 }


@@@ index




security/index


guide/index


general/index


index-actors


index-cluster


index-persistence


index-persistence-durable-state


stream/index


discovery


index-utilities


common/other-modules


additional/deploy


project/index


classic




@@@


 
 
 
 
 
 


















Footer

















        © 2025 GitHub, Inc.
      






Footer navigation






Terms






Privacy






Security






Status






Docs






Contact









      Manage cookies
    











      Do not share my personal information
    

































    You can’t perform that action at this time.

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cqrs.html
CQRS • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















CQRS


EventSourcedBehavior
s along with 
Akka Projections
 can be used to implement Command Query Responsibility Segregation (CQRS). The 
Microservices with Akka tutorial
 explains how to use Event Sourcing and Projections together. For implementing CQRS using 
DurableStateBehavior
, please take a look at the corresponding 
CQRS
 documentation.














 
Replicated Event Sourcing






Style Guide 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/durable-state/persistence-style.html
Style Guide • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide




Command handlers in the state


Optional initial state


Leveraging Java 21 features




CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide




Command handlers in the state


Optional initial state


Leveraging Java 21 features




CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Style Guide


Command handlers in the state


We can take the previous bank account example one step further by handling the commands within the state as well.




Scala




copy
source
object AccountEntity {
  // Command
  sealed trait Command extends CborSerializable
  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command
  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command

  // Reply
  final case class CurrentBalance(balance: BigDecimal)

  val Zero = BigDecimal(0)

  // type alias to reduce boilerplate
  type ReplyEffect = akka.persistence.typed.state.scaladsl.ReplyEffect[Account]

  // State
  sealed trait Account extends CborSerializable {
    def applyCommand(cmd: Command): ReplyEffect
  }
  case object EmptyAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case CreateAccount(replyTo) =>
          Effect.persist(OpenedAccount(Zero)).thenReply(replyTo)(_ => StatusReply.Ack)
        case _ =>
          // CreateAccount before handling any other commands
          Effect.unhandled.thenNoReply()
      }
  }
  case class OpenedAccount(balance: BigDecimal) extends Account {
    require(balance >= Zero, "Account balance can't be negative")

    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case cmd @ Deposit(_, _) => deposit(cmd)

        case cmd @ Withdraw(_, _) => withdraw(cmd)

        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(balance))

        case CloseAccount(replyTo) =>
          if (balance == Zero)
            Effect.persist(ClosedAccount).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error("Can't close account with non-zero balance"))

        case CreateAccount(replyTo) =>
          Effect.reply(replyTo)(StatusReply.Error("Account is already created"))

      }

    private def canWithdraw(amount: BigDecimal): Boolean = {
      balance - amount >= Zero
    }

    private def deposit(cmd: Deposit) = {
      Effect.persist(copy(balance = balance + cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
    }

    private def withdraw(cmd: Withdraw) = {
      if (canWithdraw(cmd.amount))
        Effect.persist(copy(balance = balance - cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
      else
        Effect.reply(cmd.replyTo)(
          StatusReply.Error(s"Insufficient balance ${balance} to be able to withdraw ${cmd.amount}"))
    }

  }
  case object ClosedAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case c: Deposit =>
          replyClosed(c.replyTo)
        case c: Withdraw =>
          replyClosed(c.replyTo)
        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(Zero))
        case CloseAccount(replyTo) =>
          replyClosed(replyTo)
        case CreateAccount(replyTo) =>
          replyClosed(replyTo)
      }

    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =
      Effect.reply(replyTo)(StatusReply.Error(s"Account is closed"))
  }

  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:
  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("Account")

  def apply(persistenceId: PersistenceId): Behavior[Command] = {
    DurableStateBehavior
      .withEnforcedReplies[Command, Account](persistenceId, EmptyAccount, (state, cmd) => state.applyCommand(cmd))
  }
}




Take note of how the command handler is delegating to 
applyCommand
 in the 
Account
 (state), which is implemented in the concrete 
EmptyAccount
, 
OpenedAccount
, and 
ClosedAccount
.


Optional initial state


Sometimes it’s not desirable to use a separate state class for the empty initial state, but rather act as if there is no state yet. 
You can use 
null
 as the 
emptyState
, but be aware of that the 
state
 parameter will be 
null
 until the first non-null state has been persisted It’s possible to use 
Optional
 instead of 
null
, but that requires extra boilerplate to unwrap the 
Optional
 state parameter. Therefore use of 
null
 is simpler. The following example illustrates using 
null
 as the 
emptyState
.
 
Option[State]
 can be used as the state type and 
None
 as the 
emptyState
. Then pattern matching is used in command handlers at the outer layer before delegating to the state or other methods.




Scala




copy
source
object AccountEntity {
  // Command
  sealed trait Command extends CborSerializable
  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command
  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command

  // Reply
  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable

  val Zero = BigDecimal(0)

  // type alias to reduce boilerplate
  type ReplyEffect = akka.persistence.typed.state.scaladsl.ReplyEffect[Option[Account]]

  // State
  sealed trait Account extends CborSerializable {
    def applyCommand(cmd: Command): ReplyEffect
  }
  case class OpenedAccount(balance: BigDecimal) extends Account {
    require(balance >= Zero, "Account balance can't be negative")

    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case Deposit(amount, replyTo) =>
          Effect.persist(Some(copy(balance = balance + amount))).thenReply(replyTo)(_ => StatusReply.Ack)

        case Withdraw(amount, replyTo) =>
          if (canWithdraw(amount))
            Effect.persist(Some(copy(balance = balance - amount))).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error(s"Insufficient balance $balance to be able to withdraw $amount"))

        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(balance))

        case CloseAccount(replyTo) =>
          if (balance == Zero)
            Effect.persist(Some(ClosedAccount)).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error("Can't close account with non-zero balance"))

        case CreateAccount(replyTo) =>
          Effect.reply(replyTo)(StatusReply.Error("Account is already created"))

      }

    def canWithdraw(amount: BigDecimal): Boolean = {
      balance - amount >= Zero
    }
  }

  case object ClosedAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case c: Deposit =>
          replyClosed(c.replyTo)
        case c: Withdraw =>
          replyClosed(c.replyTo)
        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(Zero))
        case CloseAccount(replyTo) =>
          replyClosed(replyTo)
        case CreateAccount(replyTo) =>
          replyClosed(replyTo)
      }

    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =
      Effect.reply(replyTo)(StatusReply.Error(s"Account is closed"))
  }

  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:
  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("Account")

  def apply(persistenceId: PersistenceId): Behavior[Command] = {
    DurableStateBehavior.withEnforcedReplies[Command, Option[Account]](
      persistenceId,
      None,
      (state, cmd) =>
        state match {
          case None          => onFirstCommand(cmd)
          case Some(account) => account.applyCommand(cmd)
        })
  }

  def onFirstCommand(cmd: Command): ReplyEffect = {
    cmd match {
      case CreateAccount(replyTo) =>
        Effect.persist(Some(OpenedAccount(Zero))).thenReply(replyTo)(_ => StatusReply.Ack)
      case _ =>
        // CreateAccount before handling any other commands
        Effect.unhandled.thenNoReply()
    }
  }
}


Java




copy
source
public class AccountEntity
    extends DurableStateBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Account> {

  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =
      EntityTypeKey.create(Command.class, "Account");

  // Command
  interface Command extends CborSerializable {}

  public static class CreateAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Deposit implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
      this.amount = amount;
    }
  }

  public static class Withdraw implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.amount = amount;
      this.replyTo = replyTo;
    }
  }

  public static class GetBalance implements Command {
    public final ActorRef<CurrentBalance> replyTo;

    @JsonCreator
    public GetBalance(ActorRef<CurrentBalance> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class CloseAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  // Reply
  public static class CurrentBalance implements CborSerializable {
    public final BigDecimal balance;

    @JsonCreator
    public CurrentBalance(BigDecimal balance) {
      this.balance = balance;
    }
  }

  // State
  interface Account extends CborSerializable {}

  public static class OpenedAccount implements Account {
    public final BigDecimal balance;

    public OpenedAccount() {
      this.balance = BigDecimal.ZERO;
    }

    @JsonCreator
    public OpenedAccount(BigDecimal balance) {
      this.balance = balance;
    }

    OpenedAccount makeDeposit(BigDecimal amount) {
      return new OpenedAccount(balance.add(amount));
    }

    boolean canWithdraw(BigDecimal amount) {
      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);
    }

    OpenedAccount makeWithdraw(BigDecimal amount) {
      if (!canWithdraw(amount))
        throw new IllegalStateException("Account balance can't be negative");
      return new OpenedAccount(balance.subtract(amount));
    }

    ClosedAccount closedAccount() {
      return new ClosedAccount();
    }
  }

  public static class ClosedAccount implements Account {}

  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {
    return new AccountEntity(accountNumber, persistenceId);
  }

  private final String accountNumber;

  private AccountEntity(String accountNumber, PersistenceId persistenceId) {
    super(persistenceId);
    this.accountNumber = accountNumber;
  }

  @Override
  public Account emptyState() {
    return null;
  }

  @Override
  public CommandHandlerWithReply<Command, Account> commandHandler() {
    CommandHandlerWithReplyBuilder<Command, Account> builder =
        newCommandHandlerWithReplyBuilder();

    builder.forNullState().onCommand(CreateAccount.class, this::createAccount);

    builder
        .forStateType(OpenedAccount.class)
        .onCommand(Deposit.class, this::deposit)
        .onCommand(Withdraw.class, this::withdraw)
        .onCommand(GetBalance.class, this::getBalance)
        .onCommand(CloseAccount.class, this::closeAccount);

    builder
        .forStateType(ClosedAccount.class)
        .onAnyCommand(() -> Effect().unhandled().thenNoReply());

    return builder.build();
  }

  private ReplyEffect<Account> createAccount(CreateAccount command) {
    return Effect()
        .persist(new OpenedAccount())
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Account> deposit(OpenedAccount account, Deposit command) {
    return Effect()
        .persist(account.makeDeposit(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Account> withdraw(OpenedAccount account, Withdraw command) {
    if (!account.canWithdraw(command.amount)) {
      return Effect()
          .reply(
              command.replyTo,
              StatusReply.error("not enough funds to withdraw " + command.amount));
    } else {
      return Effect()
          .persist(account.makeWithdraw(command.amount))
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    }
  }

  private ReplyEffect<Account> getBalance(OpenedAccount account, GetBalance command) {
    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));
  }

  private ReplyEffect<Account> closeAccount(OpenedAccount account, CloseAccount command) {
    if (account.balance.equals(BigDecimal.ZERO)) {
      return Effect()
          .persist(account.closedAccount())
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    } else {
      return Effect()
          .reply(command.replyTo, StatusReply.error("balance must be zero for closing account"));
    }
  }
}





Leveraging Java 21 features


When building durable state entities in a project using Java 21 or newer, the 
DurableStateOnCommandBehavior
 base class provides an API that let you leverage the switch pattern match feature. When combined with 
sealed
 command and event top types this gives you a more direct handling of commands and events as well as a compile time completeness check that you have handled all kinds of commands and events in your event sourced behavior handler methods:




Java




copy
source
public class BlogPostEntityDurableState
    extends DurableStateOnCommandBehavior<
        BlogPostEntityDurableState.Command, BlogPostEntityDurableState.State> {

  public sealed interface Command {}

  public record AddPost(PostContent content, ActorRef<AddPostDone> replyTo) implements Command {}
  public record GetPost(ActorRef<PostContent> replyTo) implements Command {}
  public record ChangeBody(String newBody, ActorRef<Done> replyTo) implements Command {}
  public record Publish(ActorRef<Done> replyTo) implements Command {}
  public record PostContent(String postId, String title, String body) implements Command {}
  // reply
  public record AddPostDone(String postId) {}


  public sealed interface State {}

  public record BlankState() implements State {}

  public record DraftState(PostContent content) implements State {
    DraftState withContent(PostContent newContent) {
      return new DraftState(newContent);
    }
    DraftState withBody(String newBody) {
      return withContent(new PostContent(postId(), content.title, newBody));
    }
    String postId() {
      return content.postId;
    }
  }

  public record PublishedState(PostContent content) implements State {
    PublishedState withContent(PostContent newContent) {
      return new PublishedState(newContent);
    }
    PublishedState withBody(String newBody) {
      return withContent(new PostContent(postId(), content.title, newBody));
    }
    String postId() {
      return content.postId;
    }
  }

  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {
    return Behaviors.setup(
        context -> {
          context.getLog().info("Starting BlogPostEntityDurableState {}", entityId);
          return new BlogPostEntityDurableState(persistenceId);
        });
  }

  private BlogPostEntityDurableState(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return new BlankState();
  }

  @Override
  public Effect<State> onCommand(State state, Command command) {
    return switch(state) {
      case BlankState ignored ->
        switch(command) {
          case AddPost addPost -> onAddPost(addPost);
          default -> Effect().unhandled();
        };
      case DraftState draft ->
        switch(command) {
          case ChangeBody changeBody -> onChangeBody(draft, changeBody);
          case Publish publish -> onPublish(draft, publish);
          case GetPost getPost -> onGetPost(draft, getPost);
          default -> Effect().unhandled();
        };
      case PublishedState published ->
        switch(command) {
          case ChangeBody changeBody -> onChangeBody(published, changeBody);
          case GetPost getPost -> onGetPost(published, getPost);
          default -> Effect().unhandled();
        };
    };
  }

  private Effect<State> onAddPost(AddPost cmd) {
    return Effect()
        .persist(new DraftState(cmd.content))
        .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));
  }

  private Effect<State> onChangeBody(DraftState state, ChangeBody cmd) {
    return Effect()
        .persist(state.withBody(cmd.newBody))
        .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
  }

  private Effect<State> onChangeBody(PublishedState state, ChangeBody cmd) {
    return Effect()
        .persist(state.withBody(cmd.newBody))
        .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
  }

  private Effect<State> onPublish(DraftState state, Publish cmd) {
    return Effect()
        .persist(new PublishedState(state.content))
        .thenRun(
            () -> {
              System.out.println("Blog post published: " + state.postId());
              cmd.replyTo.tell(Done.getInstance());
            });
  }

  private Effect<State> onGetPost(DraftState state, GetPost cmd) {
    cmd.replyTo.tell(state.content);
    return Effect().none();
  }

  private Effect<State> onGetPost(PublishedState state, GetPost cmd) {
    cmd.replyTo.tell(state.content);
    return Effect().none();
  }
}
















 
Durable State






CQRS 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/durable-state/cqrs.html
CQRS • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















CQRS


DurableStateBehavior
s along with 
Akka Projections
 can be used to implement Command Query Responsibility Segregation (CQRS). For implementing CQRS using 
EventSourcedBehavior
, please take a look at the corresponding 
CQRS
 documentation.














 
Style Guide






Persistence Query 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/certified-reactive-architect
Certified Reactive Architect




























































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation




























































Certification







          Get Certified.
 Validate Your Skills.
        






Want to stand out from the crowd? An Akka certification will validate your understanding of Reactive Architectures and deliver proof of your skills.






































What To Know About The Certification


This certification is based on the 
Reactive Architecture
 series of training courses. You will have the highest chance of success if you are intimately familiar with these topics or have taken these courses.


This certification covers detailed knowledge and understanding of Reactive Systems architecture. It’s language agnostic, and ensures the certified individual is ready to architect a fully Reactive system. What you’ll be asked about includes practical design principles and architectural patterns:




Domain-Driven Design


Distributed Systems Design


Event-Sourcing, CQRS


Scalability, Resilience, Consistency models


Delivery guarantees


Microservice systems


The Actor model


CAP theorem (and more)


SOLID design principles, hexagonal and onion architecture


CRDTs, the Saga pattern


Asynchronous, non-blocking designs




About The Exam




Exam length is 90 minutes and “closed-book”


Score of 80% or higher required in order to pass


Exams are online at Mettl.com, proctored by humans, and available in any timezone


Exam fee of $250 includes two (2) additional retries


Certifications are valid for three (3) years to ensure relevance




 


REGISTER FOR THE EXAM






































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#discovery-method-configuration
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/stash.html
Stash • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash




Dependency


Introduction




Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash




Dependency


Introduction




Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Stash


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Actors
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


Stashing enables an actor to temporarily buffer all or some messages that cannot or should not be handled using the actor’s current behavior.


A typical example when this is useful is if the actor has to load some initial state or initialize some resources before it can accept the first real message. Another example is when the actor is waiting for something to complete before processing the next message.


Let’s illustrate these two with an example. The 
DataAccess
 actor below is used like a single access point to a value stored in a database. When it’s started it loads current state from the database, and while waiting for that initial value all incoming messages are stashed.


When a new state is saved in the database it also stashes incoming messages to make the processing sequential, one after the other without multiple pending writes.




Scala




copy
source
import scala.concurrent.Future
import scala.util.Failure
import scala.util.Success

import akka.Done
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors
import akka.actor.typed.scaladsl.StashBuffer

trait DB {
  def save(id: String, value: String): Future[Done]
  def load(id: String): Future[String]
}

object DataAccess {
  sealed trait Command
  final case class Save(value: String, replyTo: ActorRef[Done]) extends Command
  final case class Get(replyTo: ActorRef[String]) extends Command
  private final case class InitialState(value: String) extends Command
  private case object SaveSuccess extends Command
  private final case class DBError(cause: Throwable) extends Command

  def apply(id: String, db: DB): Behavior[Command] = {
    Behaviors.withStash(100) { buffer =>
      Behaviors.setup[Command] { context =>
        new DataAccess(context, buffer, id, db).start()
      }
    }
  }
}

class DataAccess(
    context: ActorContext[DataAccess.Command],
    buffer: StashBuffer[DataAccess.Command],
    id: String,
    db: DB) {
  import DataAccess._

  private def start(): Behavior[Command] = {
    context.pipeToSelf(db.load(id)) {
      case Success(value) => InitialState(value)
      case Failure(cause) => DBError(cause)
    }

    Behaviors.receiveMessage {
      case InitialState(value) =>
        // now we are ready to handle stashed messages if any
        buffer.unstashAll(active(value))
      case DBError(cause) =>
        throw cause
      case other =>
        // stash all other messages for later processing
        buffer.stash(other)
        Behaviors.same
    }
  }

  private def active(state: String): Behavior[Command] = {
    Behaviors.receiveMessagePartial {
      case Get(replyTo) =>
        replyTo ! state
        Behaviors.same
      case Save(value, replyTo) =>
        context.pipeToSelf(db.save(id, value)) {
          case Success(_)     => SaveSuccess
          case Failure(cause) => DBError(cause)
        }
        saving(value, replyTo)
    }
  }

  private def saving(state: String, replyTo: ActorRef[Done]): Behavior[Command] = {
    Behaviors.receiveMessage {
      case SaveSuccess =>
        replyTo ! Done
        buffer.unstashAll(active(state))
      case DBError(cause) =>
        throw cause
      case other =>
        buffer.stash(other)
        Behaviors.same
    }
  }

}


Java




copy
source
import akka.Done;
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.StashBuffer;
import java.util.concurrent.CompletionStage;


interface DB {
  CompletionStage<Done> save(String id, String value);

  CompletionStage<String> load(String id);
}

public class DataAccess {

  public interface Command {}

  public static class Save implements Command {
    public final String payload;
    public final ActorRef<Done> replyTo;

    public Save(String payload, ActorRef<Done> replyTo) {
      this.payload = payload;
      this.replyTo = replyTo;
    }
  }

  public static class Get implements Command {
    public final ActorRef<String> replyTo;

    public Get(ActorRef<String> replyTo) {
      this.replyTo = replyTo;
    }
  }

  private static class InitialState implements Command {
    public final String value;

    InitialState(String value) {
      this.value = value;
    }
  }

  private enum SaveSuccess implements Command {
    INSTANCE
  }

  private static class DBError implements Command {
    public final RuntimeException cause;

    DBError(RuntimeException cause) {
      this.cause = cause;
    }
  }

  private final ActorContext<Command> context;
  private final StashBuffer<Command> buffer;
  private final String id;
  private final DB db;

  private DataAccess(
      ActorContext<Command> context, StashBuffer<Command> buffer, String id, DB db) {
    this.context = context;
    this.buffer = buffer;
    this.id = id;
    this.db = db;
  }

  public static Behavior<Command> create(String id, DB db) {
    return Behaviors.withStash(
        100,
        stash ->
            Behaviors.setup(
                ctx -> {
                  ctx.pipeToSelf(
                      db.load(id),
                      (value, cause) -> {
                        if (cause == null) return new InitialState(value);
                        else return new DBError(asRuntimeException(cause));
                      });

                  return new DataAccess(ctx, stash, id, db).start();
                }));
  }

  private Behavior<Command> start() {
    return Behaviors.receive(Command.class)
        .onMessage(InitialState.class, this::onInitialState)
        .onMessage(DBError.class, this::onDBError)
        .onMessage(Command.class, this::stashOtherCommand)
        .build();
  }

  private Behavior<Command> onInitialState(InitialState message) {
    // now we are ready to handle stashed messages if any
    return buffer.unstashAll(active(message.value));
  }

  private Behavior<Command> onDBError(DBError message) {
    throw message.cause;
  }

  private Behavior<Command> stashOtherCommand(Command message) {
    // stash all other messages for later processing
    buffer.stash(message);
    return Behaviors.same();
  }

  private Behavior<Command> active(String state) {
    return Behaviors.receive(Command.class)
        .onMessage(Get.class, message -> onGet(state, message))
        .onMessage(Save.class, this::onSave)
        .build();
  }

  private Behavior<Command> onGet(String state, Get message) {
    message.replyTo.tell(state);
    return Behaviors.same();
  }

  private Behavior<Command> onSave(Save message) {
    context.pipeToSelf(
        db.save(id, message.payload),
        (value, cause) -> {
          if (cause == null) return SaveSuccess.INSTANCE;
          else return new DBError(asRuntimeException(cause));
        });
    return saving(message.payload, message.replyTo);
  }

  private Behavior<Command> saving(String state, ActorRef<Done> replyTo) {
    return Behaviors.receive(Command.class)
        .onMessage(SaveSuccess.class, message -> onSaveSuccess(state, replyTo))
        .onMessage(DBError.class, this::onDBError)
        .onMessage(Command.class, this::stashOtherCommand)
        .build();
  }

  private Behavior<Command> onSaveSuccess(String state, ActorRef<Done> replyTo) {
    replyTo.tell(Done.getInstance());
    return buffer.unstashAll(active(state));
  }

  private static RuntimeException asRuntimeException(Throwable t) {
    // can't throw Throwable in lambdas
    if (t instanceof RuntimeException) {
      return (RuntimeException) t;
    } else {
      return new RuntimeException(t);
    }
  }
}





One important thing to be aware of is that the 
StashBuffer
StashBuffer
 is a buffer and stashed messages will be kept in memory until they are unstashed (or the actor is stopped and garbage collected). It’s recommended to avoid stashing too many messages to avoid too much memory usage and even risking 
OutOfMemoryError
 if many actors are stashing many messages. Therefore the 
StashBuffer
StashBuffer
 is bounded and the 
capacity
 of how many messages it can hold must be specified when it’s created.


If you try to stash more messages than the 
capacity
 a 
StashOverflowException
StashOverflowException
 will be thrown. You can use 
StashBuffer.isFull
StashBuffer.isFull
 before stashing a message to avoid that and take other actions, such as dropping the message.


When unstashing the buffered messages by calling 
unstashAll
unstashAll
 the messages will be processed sequentially in the order they were added and all are processed unless an exception is thrown. The actor is unresponsive to other new messages until 
unstashAll
unstashAll
 is completed. That is another reason for keeping the number of stashed messages low. Actors that hog the message processing thread for too long can result in starvation of other actors.


That can be mitigated by using the 
StashBuffer.unstash
StashBuffer.unstash
 with 
numberOfMessages
 parameter and then send a message to 
context.self
context.getSelf
 before continuing unstashing more. That means that other new messages may arrive in-between and those must be stashed to keep the original order of messages. It becomes more complicated, so better keep the number of stashed messages low.














 
Routers






Behaviors as finite state machines 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-http/current/
Akka HTTP









































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka HTTP





Version 10.7.0





Scala
Java
















Security Announcements


1. Introduction


2. Usage


3. Data Types & Abstractions


4. Server API


5. Client API


6. Extensions


7. Supported Technologies


8. Tips And Tricks


9. Building Native Images


10. Contributing


11. Reference


12. Release Notes


















Akka HTTP





Version 10.7.0





Scala
Java


















Security Announcements


1. Introduction


2. Usage


3. Data Types & Abstractions


4. Server API


5. Client API


6. Extensions


7. Supported Technologies


8. Tips And Tricks


9. Building Native Images


10. Contributing


11. Reference


12. Release Notes




















Akka HTTP






Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Fixed Security Vulnerabilities


2021


2018


2017


2016




1. Introduction




Philosophy


Using Akka HTTP


Routing DSL for HTTP servers


Marshalling


Streaming


Low-level HTTP server APIs


HTTP Client API


The modules that make up Akka HTTP




2. Usage




Configuration


Migration Guides


Compatibility Guidelines




3. Data Types & Abstractions




HTTP Model


The URI model


Marshalling


Unmarshalling


Encoding / Decoding


JSON Support


XML Support


Server-Sent Events Support


Timeouts


Caching




4. Server API




Routing DSL


Core Server API


Server WebSocket Support


Server HTTPS Support


Graceful termination


Server-Side HTTP/2




5. Client API




Configuration


HttpRequest and HttpResponse


Request-Level Client-Side API


Host-Level Client-Side API


Connection-Level Client-Side API


Pool overflow and the max-open-requests setting


Client-Side HTTPS Support


Pluggable Client Transports / HTTP(S) proxy Support


Client-Side WebSocket Support


Client-Side HTTP/2 (Preview)




6. Extensions


7. Supported Technologies




HTTP


HTTPS


WebSocket


HTTP/2


DNS


Multipart


Server-sent Events (SSE)


JSON


XML


Gzip and Deflate Content-Encoding




8. Tips And Tricks




Troubleshooting


Handling blocking operations in Akka HTTP


Implications of the streaming nature of Request/Response Entities




9. Building Native Images




Unsupported features


Features requiring additional metadata


Marshalling


Custom parsing error handler




10. Contributing




Welcome!


Snapshots




11. Reference




API Documentation


Directives


Books




12. Release Notes




10.7.x release notes


10.6.x release notes


10.5.x release notes


10.4.x release notes


10.2.x Release Notes


10.1.x Release Notes


10.0.x Release Notes
























Security Announcements 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka HTTP is available under the 
Business Source License 1.1
.



© 2011-2024 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Listing
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/2018-08-29-aes-rng.html
Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16 • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




CVE ID


Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements






Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




CVE ID


Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements






Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


CVE ID


CVE-2018-16115


Date


29 August 2018


Description of Vulnerability


A random number generator is used in Akka Remoting for TLS (both classic and Artery Remoting). Akka allows to configure custom random number generators. For historical reasons, Akka included the 
AES128CounterSecureRNG
 and 
AES256CounterSecureRNG
 random number generators. The implementations had a bug that caused the generated numbers to be repeated after only a few bytes.


The custom RNG implementations were not configured by default but examples in the documentation showed (and therefore implicitly recommended) using the custom ones.


This can be used by an attacker to compromise the communication if these random number generators are enabled in configuration. It would be possible to eavesdrop, replay or modify the messages sent with Akka Remoting/Cluster.


To protect against such attacks the system should be updated to Akka 
2.5.16
 or later, or the default configuration of the TLS random number generator should be used:


# Set `SecureRandom` RNG explicitly (but it is also the default)
akka.remote.classic.netty.ssl.random-number-generator = "SecureRandom"
akka.remote.artery.ssl.config-ssl-engine.random-number-generator = "SecureRandom"



Please subscribe to the 
akka-security
 mailing list to be notified promptly about future security issues.


Severity


The 
CVSS
 score of this vulnerability is 5.9 (Medium), based on vector 
AV:A/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:N/E:U/RL:O/RC:C
.


Rationale for the score:




AV:A - Best practice is that Akka remoting nodes should only be accessible from the adjacent network, so in  good setups, this will be adjacent.


AC:H - Any one in the adjacent network can launch the attack with non-special access privileges,  but man-in-the-middle attacks are not trivial.


C:H, I:H - Confidentiality and Integrity are only partially affected because only the networking component  is affected and not the whole Akka cluster. Assessed to be High anyway because access to actor system data would  probably be possible by injecting messages into the remoting communication.




Affected Versions




Akka 
2.5.0 - 2.5.15
 with any of the following configuration properties defined:




akka.remote.netty.ssl.random-number-generator = "AES128CounterSecureRNG"
akka.remote.netty.ssl.random-number-generator = "AES256CounterSecureRNG"
akka.remote.artery.ssl.config-ssl-engine.random-number-generator = "AES128CounterSecureRNG"
akka.remote.artery.ssl.config-ssl-engine.random-number-generator = "AES256CounterSecureRNG"



Akka 
2.4.x
 versions are not affected by this particular bug. It has reached end-of-life since start of 2018. If you still run on Akka 2.4, we still recommend to use the default 
SecureRandom
 implementation for the reasons given below. Please check your configuration files not to configure the custom RNGs.


Fixed Versions


We have prepared patches for the affected versions, and have released the following version which resolve the issue:




Akka 
2.5.16
 (Scala 2.11, 2.12)




Binary and source compatibility has been maintained for the patched releases so the upgrade procedure is as simple as changing the library dependency.


The exact historical reasons to include custom RNG implementations could not be reconstructed but it was likely because RNGs provided by previous versions of the JDK were deemed too slow.


Including custom cryptographic components in your library (or application) should not be done lightly. We acknowledge that we cannot prove that the custom RNGs that Akka provides or has been providing are generally correct or just correct enough for the purposes in Akka.


The reporter of this vulnerability, RafaÅ SumisÅawski, kindly provided us with fixes for the custom RNGs in Akka. However, as we cannot thoroughly verify the correctness of the algorithm we decided to remove custom RNGs from Akka.


If the “AES128CounterSecureRNG” and “AES256CounterSecureRNG” configuration values are still used with Akka 2.5.16 they will be ignored and the default 
SecureRandom
 is used and a warning is logged. This is to avoid accidental use of these unverified and possibly insecure implementations. The deprecated implementations are not recommended, but they can be enabled by using configuration values “DeprecatedAES128CounterSecureRNG” or “DeprecatedAES256CounterSecureRNG” during the transition period until they have been removed.


Edit
: 
DeprecatedAES128CounterSecureRNG
 and 
DeprecatedAES256CounterSecureRNG
 have been removed since Akka 2.5.19.


Acknowledgements


We would like to thank RafaÅ SumisÅawski at NetworkedAssets for bringing this issue to our attention and providing a patch.














 
Camel Dependency, Fixed in Akka 2.5.4






Getting Started Guide 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/remoting-artery.html
Remoting • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting




Dependency


Configuration


Introduction


Selecting a transport


Migrating from classic remoting


Canonical address


Acquiring references to remote actors


Remote Security


Quarantine


Serialization


Routers with Remote Destinations


What is new in Artery


Performance tuning


Remote Configuration


Creating Actors Remotely




Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting




Dependency


Configuration


Introduction


Selecting a transport


Migrating from classic remoting


Canonical address


Acquiring references to remote actors


Remote Security


Quarantine


Serialization


Routers with Remote Destinations


What is new in Artery


Performance tuning


Remote Configuration


Creating Actors Remotely




Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Remoting
Note


Remoting is the mechanism by which Actors on different nodes talk to each other internally.


When building an Akka application, you would usually not use the Remoting concepts directly, but instead use the more high-level 
Akka Cluster
 utilities or technology-agnostic protocols such as 
HTTP
, 
gRPC
 etc.


If migrating from classic remoting see 
what’s new in Artery


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Remoting, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-remote" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-remote_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-remote_${versions.ScalaBinary}"
}


One option is to use Artery with Aeron, see 
Selecting a transport
. The Aeron dependency needs to be explicitly added if using the 
aeron-udp
 transport:
sbt
libraryDependencies ++= Seq(
  "io.aeron" % "aeron-driver" % "1.44.6",
  "io.aeron" % "aeron-client" % "1.44.6"
)
Maven
<dependencies>
  <dependency>
    <groupId>io.aeron</groupId>
    <artifactId>aeron-driver</artifactId>
    <version>1.44.6</version>
  </dependency>
  <dependency>
    <groupId>io.aeron</groupId>
    <artifactId>aeron-client</artifactId>
    <version>1.44.6</version>
  </dependency>
</dependencies>
Gradle
dependencies {
  implementation "io.aeron:aeron-driver:1.44.6"
  implementation "io.aeron:aeron-client:1.44.6"
}
Java 17


When using Aeron with Java 17 you have to add JVM flag 
--add-opens=java.base/sun.nio.ch=ALL-UNNAMED
.


Configuration


To enable remote capabilities in your Akka project you should, at a minimum, add the following changes to your 
application.conf
 file:


akka {
  actor {
    # provider=remote is possible, but prefer cluster
    provider = cluster 
  }
  remote {
    artery {
      transport = tcp # See Selecting a transport below
      canonical.hostname = "127.0.0.1"
      canonical.port = 25520
    }
  }
}



As you can see in the example above there are four things you need to add to get started:




Change provider from 
local
. We recommend using 
Akka Cluster
 over using remoting directly.


Enable Artery to use it as the remoting implementation


Add host name - the machine you want to run the actor system on; this host name is exactly what is passed to remote systems in order to identify this system and consequently used for connecting back to this system if need be, hence set it to a reachable IP address or resolvable name in case you want to communicate across the network.


Add port number - the port the actor system should listen on, set to 0 to have it chosen automatically


Note


The port number needs to be unique for each actor system on the same machine even if the actor systems have different names. This is because each actor system has its own networking subsystem listening for connections and handling messages as not to interfere with other actor systems.


The example above only illustrates the bare minimum of properties you have to add to enable remoting. All settings are described in 
Remote Configuration
.


Introduction


We recommend 
Akka Cluster
 over using remoting directly. As remoting is the underlying module that allows for Cluster, it is still useful to understand details about it though.


Remoting enables Actor systems on different hosts or JVMs to communicate with each other. By enabling remoting the system will start listening on a provided network address and also gains the ability to connect to other systems through the network. From the application’s perspective there is no API difference between local or remote systems, 
ActorRef
ActorRef
 instances that point to remote systems look exactly the same as local ones: they can be sent messages to, watched, etc. Every 
ActorRef
 contains hostname and port information and can be passed around even on the network. This means that on a network every 
ActorRef
 is a unique identifier of an actor on that network.


You need to enable 
serialization
 for your actor messages. 
Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference.


Remoting is not a server-client technology. All systems using remoting can contact any other system on the network if they possess an 
ActorRef
 pointing to those system. This means that every system that is remoting enabled acts as a “server” to which arbitrary systems on the same network can connect to.


Selecting a transport


There are three alternatives of which underlying transport to use. It is configured by property 
akka.remote.artery.transport
 with the possible values:




tcp
 - Based on 
Akka Streams TCP
 (default if other not configured)


tls-tcp
 - Same as 
tcp
 with encryption using 
Akka Streams TLS


aeron-udp
 - Based on 
Aeron (UDP)




If you are uncertain of what to select a good choice is to use the default, which is 
tcp
.


The Aeron (UDP) transport is a high performance transport and should be used for systems that require high throughput and low latency. It uses more CPU than TCP when the system is idle or at low message rates. There is no encryption for Aeron.


The TCP and TLS transport is implemented using Akka Streams TCP/TLS. This is the choice when encryption is needed, but it can also be used with plain TCP without TLS. It’s also the obvious choice when UDP can’t be used. It has very good performance (high throughput and low latency) but latency at high throughput might not be as good as the Aeron transport. It has less operational complexity than the Aeron transport and less risk of trouble in container environments.


Aeron requires 64bit JVM to work reliably and is only officially supported on Linux, Mac and Windows. It may work on other Unixes e.g. Solaris but insufficient testing has taken place for it to be officially supported. If you’re on a Big Endian processor, such as Sparc, it is recommended to use TCP.
Note


Rolling update
 is not supported when changing from one transport to another.


Migrating from classic remoting


See 
migrating from classic remoting


Canonical address


In order for remoting to work properly, where each system can send messages to any other system on the same network (for example a system forwards a message to a third system, and the third replies directly to the sender system) it is essential for every system to have a 
unique, globally reachable
 address and port. This address is part of the unique name of the system and will be used by other systems to open a connection to it and send messages. This means that if a host has multiple names (different DNS records pointing to the same IP address) then only one of these can be 
canonical
. If a message arrives to a system but it contains a different hostname than the expected canonical name then the message will be dropped. If multiple names for a system would be allowed, then equality checks among 
ActorRef
ActorRef
 instances would no longer to be trusted and this would violate the fundamental assumption that an actor has a globally unique reference on a given network. As a consequence, this also means that localhost addresses (e.g. 
127.0.0.1
) cannot be used in general (apart from local development) since they are not unique addresses in a real network.


In cases, where Network Address Translation (NAT) is used or other network bridging is involved, it is important to configure the system so that it understands that there is a difference between his externally visible, canonical address and between the host-port pair that is used to listen for connections. See 
Akka behind NAT or in a Docker container
 for details.


Acquiring references to remote actors


In order to communicate with an actor, it is necessary to have its 
ActorRef
ActorRef
. In the local case it is usually the creator of the actor (the caller of 
actorOf()
) is who gets the 
ActorRef
 for an actor that it can then send to other actors. In other words:




An Actor can get a remote Actor’s reference by receiving a message from it (as it’s available as 
sender()
getSender()
 then), or inside of a remote message (e.g. 
PleaseReply(message: String, remoteActorRef: ActorRef)
)




Alternatively, an actor can look up another located at a known path using 
ActorSelection
ActorSelection
. These methods are available even in remoting enabled systems:




Remote Lookup : used to look up an actor on a remote node with 
actorSelection(path)
actorSelection(path)


Remote Creation : used to create an actor on a remote node with 
actorOf(Props(...), actorName)
actorOf(Props(...), actorName)




In the next sections the two alternatives are described in detail.


Looking up Remote Actors


actorSelection(path)
actorSelection(path)
 will obtain an 
ActorSelection
ActorSelection
 to an Actor on a remote node, e.g.:




Scala




val selection =
  context.actorSelection("akka://
[email protected]
:25520/user/actorName")



Java




ActorSelection selection =
  context.actorSelection("akka://
[email protected]
:25520/user/actorName");





As you can see from the example above the following pattern is used to find an actor on a remote node:


akka://<actor system>@<hostname>:<port>/<actor path>

Note


Unlike with earlier remoting, the protocol field is always 
akka
 as pluggable transports are no longer supported.


Once you obtained a selection to the actor you can interact with it in the same way you would with a local actor, e.g.:




Scala




selection ! "Pretty awesome feature"



Java




selection.tell("Pretty awesome feature", getSelf());





To acquire an 
ActorRef
ActorRef
 for an 
ActorSelection
ActorSelection
 you need to send a message to the selection and use the 
sender()
getSender()
 reference of the reply from the actor. There is a built-in 
Identify
Identify
 message that all Actors will understand and automatically reply to with a 
ActorIdentity
ActorIdentity
 message containing the 
ActorRef
. This can also be done with the 
resolveOne
resolveOne
 method of the 
ActorSelection
, which returns a 
Future
CompletionStage
 of the matching 
ActorRef
.


For more details on how actor addresses and paths are formed and used, please refer to 
Actor References, Paths and Addresses
.
Note


Message sends to actors that are actually in the sending actor system do not get delivered via the remote actor ref provider. They’re delivered directly, by the local actor ref provider.


Aside from providing better performance, this also means that if the hostname you configure remoting to listen as cannot actually be resolved from within the very same actor system, such messages will (perhaps counterintuitively) be delivered just fine.




Remote Security


An 
ActorSystem
ActorSystem
 should not be exposed via Akka Remote (Artery) over plain Aeron/UDP or TCP to an untrusted network (e.g. Internet). It should be protected by network security, such as a firewall. If that is not considered as enough protection TLS with mutual authentication should be enabled. Read more about 
how to enable remote security
.


Quarantine


Akka remoting is using TCP or Aeron as underlying message transport. Aeron is using UDP and adds among other things reliable delivery and session semantics, very similar to TCP. This means that the order of the messages are preserved, which is needed for the 
Actor message ordering guarantees
. Under normal circumstances all messages will be delivered but there are cases when messages may not be delivered to the destination:




during a network partition when the TCP connection or the Aeron session is broken, this automatically recovered once the partition is over


when sending too many messages without flow control and thereby filling up the outbound send queue (
outbound-message-queue-size
 config)


if serialization or deserialization of a message fails (only that message will be dropped)


if an unexpected exception occurs in the remoting infrastructure




In short, Actor message delivery is âat-most-onceâ as described in 
Message Delivery Reliability


Some messages in Akka are called system messages and those cannot be dropped because that would result in an inconsistent state between the systems. Such messages are used for essentially two features; remote death watch and remote deployment. These messages are delivered by Akka remoting with âexactly-onceâ guarantee by confirming each message and resending unconfirmed messages. If a system message anyway cannot be delivered the association with the destination system is irrecoverable failed, and Terminated is signaled for all watched actors on the remote system. It is placed in a so called quarantined state. Quarantine usually does not happen if remote watch or remote deployment is not used.


Each 
ActorSystem
ActorSystem
 instance has an unique identifier (UID), which is important for differentiating between incarnations of a system when it is restarted with the same hostname and port. It is the specific incarnation (UID) that is quarantined. The only way to recover from this state is to restart one of the actor systems.


Messages that are sent to and received from a quarantined system will be dropped. However, it is possible to send messages with 
actorSelection
 to the address of a quarantined system, which is useful to probe if the system has been restarted.


An association will be quarantined when:




Cluster node is removed from the cluster membership.


Remote failure detector triggers, i.e. remote watch is used. This is different when 
Akka Cluster
 is used. The unreachable observation by the cluster failure detector can go back to reachable if the network partition heals. A cluster member is not quarantined when the failure detector triggers.


Overflow of the system message delivery buffer, e.g. because of too many 
watch
 requests at the same time (
system-message-buffer-size
 config).


Unexpected exception occurs in the control subchannel of the remoting infrastructure.




The UID of the 
ActorSystem
ActorSystem
 is exchanged in a two-way handshake when the first message is sent to a destination. The handshake will be retried until the other system replies and no other messages will pass through until the handshake is completed. If the handshake cannot be established within a timeout (
handshake-timeout
 config) the association is stopped (freeing up resources). Queued messages will be dropped if the handshake cannot be established. It will not be quarantined, because the UID is unknown. New handshake attempt will start when next message is sent to the destination.


Handshake requests are actually also sent periodically to be able to establish a working connection when the destination system has been restarted.


Watching Remote Actors


Watching a remote actor is API wise not different than watching a local actor, as described in 
Lifecycle Monitoring aka DeathWatch
. However, it is important to note, that unlike in the local case, remoting has to handle when a remote actor does not terminate in a graceful way sending a system message to notify the watcher actor about the event, but instead being hosted on a system which stopped abruptly (crashed). These situations are handled by the built-in failure detector.


Failure Detector


Under the hood remote death watch uses heartbeat messages and a failure detector to generate 
Terminated
Terminated
 message from network failures and JVM crashes, in addition to graceful termination of watched actor.


The heartbeat arrival times is interpreted by an implementation of 
The Phi Accrual Failure Detector
.


The suspicion level of failure is given by a value called 
phi
. The basic idea of the phi failure detector is to express the value of 
phi
 on a scale that is dynamically adjusted to reflect current network conditions.


The value of 
phi
 is calculated as:


phi = -log10(1 - F(timeSinceLastHeartbeat))



where F is the cumulative distribution function of a normal distribution with mean and standard deviation estimated from historical heartbeat inter-arrival times.


In the 
Remote Configuration
 you can adjust the 
akka.remote.watch-failure-detector.threshold
 to define when a 
phi
 value is considered to be a failure.


A low 
threshold
 is prone to generate many false positives but ensures a quick detection in the event of a real crash. Conversely, a high 
threshold
 generates fewer mistakes but needs more time to detect actual crashes. The default 
threshold
 is 10 and is appropriate for most situations. However in cloud environments, such as Amazon EC2, the value could be increased to 12 in order to account for network issues that sometimes occur on such platforms.


The following chart illustrates how 
phi
 increase with increasing time since the previous heartbeat.




Phi is calculated from the mean and standard deviation of historical inter arrival times. The previous chart is an example for standard deviation of 200 ms. If the heartbeats arrive with less deviation the curve becomes steeper, i.e. it is possible to determine failure more quickly. The curve looks like this for a standard deviation of 100 ms.




To be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is configured with a margin, 
akka.remote.watch-failure-detector.acceptable-heartbeat-pause
. You may want to adjust the 
Remote Configuration
 of this depending on you environment. This is how the curve looks like for 
acceptable-heartbeat-pause
 configured to 3 seconds.




Serialization


You need to enable 
serialization
 for your actor messages. 
Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference.




ByteBuffer based serialization


Artery introduces a new serialization mechanism which allows the 
ByteBufferSerializer
ByteBufferSerializer
 to directly write into a shared 
java.nio.ByteBuffer
 instead of being forced to allocate and return an 
Array[Byte]
 for each serialized message. For high-throughput messaging this API change can yield significant performance benefits, so we recommend changing your serializers to use this new mechanism.


This new API also plays well with new versions of Google Protocol Buffers and other serialization libraries, which gained the ability to serialize directly into and from ByteBuffers.


As the new feature only changes how bytes are read and written, and the rest of the serialization infrastructure remained the same, we recommend reading the 
Serialization
 documentation first.


Implementing an 
akka.serialization.ByteBufferSerializer
 works the same way as any other serializer,




Scala




copy
source
trait ByteBufferSerializer {

  /**
   * Serializes the given object into the `ByteBuffer`.
   */
  def toBinary(o: AnyRef, buf: ByteBuffer): Unit

  /**
   * Produces an object from a `ByteBuffer`, with an optional type-hint;
   * the class should be loaded using ActorSystem.dynamicAccess.
   */
  @throws(classOf[NotSerializableException])
  def fromBinary(buf: ByteBuffer, manifest: String): AnyRef

}


Java




copy
source
interface ByteBufferSerializer {
  /** Serializes the given object into the `ByteBuffer`. */
  void toBinary(Object o, ByteBuffer buf);

  /**
   * Produces an object from a `ByteBuffer`, with an optional type-hint; the class should be
   * loaded using ActorSystem.dynamicAccess.
   */
  Object fromBinary(ByteBuffer buf, String manifest);
}




Implementing a serializer for Artery is therefore as simple as implementing this interface, and binding the serializer as usual (which is explained in 
Serialization
).


Implementations should typically extend 
SerializerWithStringManifest
SerializerWithStringManifest
 and in addition to the 
ByteBuffer
 based 
toBinary
toBinary
 and 
fromBinary
fromBinary
 methods also implement the array based 
toBinary
toBinary
 and 
fromBinary
fromBinary
 methods. The array based methods will be used when 
ByteBuffer
 is not used, e.g. in Akka Persistence.


Note that the array based methods can be implemented by delegation like this:




Scala




copy
source
import java.nio.ByteBuffer
import akka.serialization.ByteBufferSerializer
import akka.serialization.SerializerWithStringManifest

class ExampleByteBufSerializer extends SerializerWithStringManifest with ByteBufferSerializer {
  override def identifier: Int = 1337
  override def manifest(o: AnyRef): String = "naive-toStringImpl"

  // Implement this method for compatibility with `SerializerWithStringManifest`.
  override def toBinary(o: AnyRef): Array[Byte] = {
    // in production code, acquire this from a BufferPool
    val buf = ByteBuffer.allocate(256)

    toBinary(o, buf)
    buf.flip()
    val bytes = new Array[Byte](buf.remaining)
    buf.get(bytes)
    bytes
  }

  // Implement this method for compatibility with `SerializerWithStringManifest`.
  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =
    fromBinary(ByteBuffer.wrap(bytes), manifest)

  // Actual implementation in the ByteBuffer versions of to/fromBinary:
  override def toBinary(o: AnyRef, buf: ByteBuffer): Unit = ??? // implement actual logic here
  override def fromBinary(buf: ByteBuffer, manifest: String): AnyRef = ??? // implement actual logic here
}


Java




copy
source
import akka.serialization.ByteBufferSerializer;
import akka.serialization.SerializerWithStringManifest;

class ExampleByteBufSerializer extends SerializerWithStringManifest
    implements ByteBufferSerializer {

  @Override
  public int identifier() {
    return 1337;
  }

  @Override
  public String manifest(Object o) {
    return "serialized-" + o.getClass().getSimpleName();
  }

  @Override
  public byte[] toBinary(Object o) {
    // in production code, acquire this from a BufferPool
    final ByteBuffer buf = ByteBuffer.allocate(256);

    toBinary(o, buf);
    buf.flip();
    final byte[] bytes = new byte[buf.remaining()];
    buf.get(bytes);
    return bytes;
  }

  @Override
  public Object fromBinary(byte[] bytes, String manifest) {
    return fromBinary(ByteBuffer.wrap(bytes), manifest);
  }

  @Override
  public void toBinary(Object o, ByteBuffer buf) {
    // Implement actual serialization here
  }

  @Override
  public Object fromBinary(ByteBuffer buf, String manifest) {
    // Implement actual deserialization here
    return null;
  }
}




Routers with Remote Destinations


It is absolutely feasible to combine remoting with 
Routing
.


A pool of remote deployed routees can be configured as:


copy
source
akka.actor.deployment {
  /parent/remotePool {
    router = round-robin-pool
    nr-of-instances = 10
    target.nodes = ["tcp://
[email protected]
:2552", "akka://
[email protected]
:2552"]
  }
}


This configuration setting will clone the actor defined in the 
Props
 of the 
remotePool
 10 times and deploy it evenly distributed across the two given target nodes.


When using a pool of remote deployed routees you must ensure that all parameters of the 
Props
 can be 
serialized
.


A group of remote actors can be configured as:


copy
source
akka.actor.deployment {
  /parent/remoteGroup2 {
    router = round-robin-group
    routees.paths = [
      "akka://
[email protected]
:2552/user/workers/w1",
      "akka://
[email protected]
:2552/user/workers/w1",
      "akka://
[email protected]
:2552/user/workers/w1"]
  }
}


This configuration setting will send messages to the defined remote actor paths. It requires that you create the destination actors on the remote nodes with matching paths. That is not done by the router.


What is new in Artery


Artery is a reimplementation of the old remoting module aimed at improving performance and stability. It is mostly source compatible with the old implementation and it is a drop-in replacement in many cases. Main features of Artery compared to the previous implementation:




Based on Akka Streams TCP/TLS or 
Aeron
 (UDP) instead of Netty TCP


Focused on high-throughput, low-latency communication


Isolation of internal control messages from user messages improving stability and reducing false failure detection in case of heavy traffic by using a dedicated subchannel.


Mostly allocation-free operation


Support for a separate subchannel for large messages to avoid interference with smaller messages


Compression of actor paths on the wire to reduce overhead for smaller messages


Support for faster serialization/deserialization using ByteBuffers directly


Built-in Java Flight Recorder (JFR) to help debugging implementation issues without polluting users logs with implementation specific events


Providing protocol stability across major Akka versions to support rolling updates of large-scale systems




The main incompatible change from the previous implementation is that the protocol field of the string representation of an 
ActorRef
ActorRef
 is always 
akka
 instead of the previously used 
akka.tcp
 or 
akka.ssl.tcp
. Configuration properties are also different.


Performance tuning


Lanes


Message serialization and deserialization can be a bottleneck for remote communication. Therefore there is support for parallel inbound and outbound lanes to perform serialization and other tasks for different destination actors in parallel. Using multiple lanes is of most value for the inbound messages, since all inbound messages from all remote systems share the same inbound stream. For outbound messages there is already one stream per remote destination system, so multiple outbound lanes only add value when sending to different actors in same destination system.


The selection of lane is based on consistent hashing of the recipient ActorRef to preserve message ordering per receiver.


Note that lowest latency can be achieved with 
inbound-lanes=1
 and 
outbound-lanes=1
 because multiple lanes introduce an asynchronous boundary. 


Also note that the total amount of parallel tasks are bound by the 
remote-dispatcher
 and the thread pool size should not exceed the number of CPU cores minus headroom for actually processing the messages in the application, i.e. in practice the the pool size should be less than half of the number of cores.


See 
inbound-lanes
 and 
outbound-lanes
 in the 
reference configuration
 for default values.


Dedicated subchannel for large messages


All the communication between user defined remote actors are isolated from the channel of Akka internal messages so a large user message cannot block an urgent system message. While this provides good isolation for Akka services, all user communications by default happen through a shared network connection. When some actors send large messages this can cause other messages to suffer higher latency as they need to wait until the full message has been transported on the shared channel (and hence, shared bottleneck). In these cases it is usually helpful to separate actors that have different QoS requirements: large messages vs. low latency.


Akka remoting provides a dedicated channel for large messages if configured. Since actor message ordering must not be violated the channel is actually dedicated for 
actors
 instead of messages, to ensure all of the messages arrive in send order. It is possible to assign actors on given paths to use this dedicated channel by using path patterns that have to be specified in the actor system’s configuration on both the sending and the receiving side:


akka.remote.artery.large-message-destinations = [
   "/user/largeMessageActor",
   "/user/largeMessagesGroup/*",
   "/user/anotherGroup/*/largeMesssages",
   "/user/thirdGroup/**",
   "/temp/session-ask-actor*"
]



*NOTE: Support for * inside of an actor path (ie. /temp/session-ask-actor*) is only available in 2.6.18+


This means that all messages sent to the following actors will pass through the dedicated, large messages channel:




/user/largeMessageActor


/user/largeMessageActorGroup/actor1


/user/largeMessageActorGroup/actor2


/user/anotherGroup/actor1/largeMessages


/user/anotherGroup/actor2/largeMessages


/user/thirdGroup/actor3/


/user/thirdGroup/actor4/actor5


/temp/session-ask-actor$abc




Messages destined for actors not matching any of these patterns are sent using the default channel as before.


To notice large messages you can enable logging of message types with payload size in bytes larger than the configured 
log-frame-size-exceeding
.


akka.remote.artery {
  log-frame-size-exceeding = 10000b
}



Example log messages:


[INFO] Payload size for [java.lang.String] is [39068] bytes. Sent to Actor[akka://Sys@localhost:53039/user/destination#-1908386800]
[INFO] New maximum payload size for [java.lang.String] is [44068] bytes. Sent to Actor[akka://Sys@localhost:53039/user/destination#-1908386800].



The large messages channel can still not be used for extremely large messages, a few MB per message at most. An alternative is to use the 
Reliable delivery
 that has support for automatically 
splitting up large messages
 and assemble them again on the receiving side.


External, shared Aeron media driver


The Aeron transport is running in a so called 
media driver
. By default, Akka starts the media driver embedded in the same JVM process as application. This is convenient and simplifies operational concerns by only having one process to start and monitor.


The media driver may use rather much CPU resources. If you run more than one Akka application JVM on the same machine it can therefore be wise to share the media driver by running it as a separate process.


The media driver has also different resource usage characteristics than a normal application and it can therefore be more efficient and stable to run the media driver as a separate process.


Given that Aeron jar files are in the classpath the standalone media driver can be started with:


java io.aeron.driver.MediaDriver



The needed classpath:


Agrona-0.5.4.jar:aeron-driver-1.0.1.jar:aeron-client-1.0.1.jar



You find those jar files on 
Maven Central
, or you can create a package with your preferred build tool.


You can pass 
Aeron properties
 as command line 
-D
 system properties:


-Daeron.dir=/dev/shm/aeron



You can also define Aeron properties in a file:


java io.aeron.driver.MediaDriver config/aeron.properties



An example of such a properties file:


aeron.mtu.length=16384
aeron.socket.so_sndbuf=2097152
aeron.socket.so_rcvbuf=2097152
aeron.rcv.buffer.length=16384
aeron.rcv.initial.window.length=2097152
agrona.disable.bounds.checks=true

aeron.threading.mode=SHARED_NETWORK

# low latency settings
#aeron.threading.mode=DEDICATED
#aeron.sender.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy
#aeron.receiver.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy

# use same director in akka.remote.artery.advanced.aeron-dir config
# of the Akka application
aeron.dir=/dev/shm/aeron



Read more about the media driver in the 
Aeron documentation
.


To use the external media driver from the Akka application you need to define the following two configuration properties:


akka.remote.artery.advanced.aeron {
  embedded-media-driver = off
  aeron-dir = /dev/shm/aeron
}



The 
aeron-dir
 must match the directory you started the media driver with, i.e. the 
aeron.dir
 property.


Several Akka applications can then be configured to use the same media driver by pointing to the same directory.


Note that if the media driver process is stopped the Akka applications that are using it will also be stopped.


Aeron Tuning


See Aeron documentation about 
Performance Testing
.


Fine-tuning CPU usage latency tradeoff


Artery has been designed for low latency and as a result it can be CPU hungry when the system is mostly idle. This is not always desirable. When using the Aeron transport it is possible to tune the tradeoff between CPU usage and latency with the following configuration:


# Values can be from 1 to 10, where 10 strongly prefers low latency
# and 1 strongly prefers less CPU usage
akka.remote.artery.advanced.aeron.idle-cpu-level = 1



By setting this value to a lower number, it tells Akka to do longer “sleeping” periods on its thread dedicated for 
spin-waiting
 and hence reducing CPU load when there is no immediate task to execute at the cost of a longer reaction time to an event when it actually happens. It is worth to be noted though that during a continuously high-throughput period this setting makes not much difference as the thread mostly has tasks to execute. This also means that under high throughput (but below maximum capacity) the system might have less latency than at low message rates.




Remote Configuration


There are lots of configuration properties that are related to remoting in Akka. We refer to the 
reference configuration
 for more information.
Note


Setting properties like the listening IP and port number programmatically is best done by using something like the following:


copy
source
ConfigFactory.parseString("akka.remote.artery.canonical.hostname=\"1.2.3.4\"")
    .withFallback(ConfigFactory.load());




Akka behind NAT or in a Docker container


In setups involving Network Address Translation (NAT), Load Balancers or Docker containers the hostname and port pair that Akka binds to will be different than the “logical” host name and port pair that is used to connect to the system from the outside. This requires special configuration that sets both the logical and the bind pairs for remoting.


akka {
  remote {
    artery {
      canonical.hostname = my.domain.com      # external (logical) hostname
      canonical.port = 8000                   # external (logical) port

      bind.hostname = local.address # internal (bind) hostname
      bind.port = 25520              # internal (bind) port
    }
 }
}



Running in Docker/Kubernetes


When using 
aeron-udp
 in a containerized environment special care must be taken that the media driver runs on a ram disk. This by default is located in 
/dev/shm
 which on most physical Linux machines will be mounted as half the size of the system memory.


Docker and Kubernetes mount a 64Mb ram disk. This is unlikely to be large enough. For docker this can be overridden with 
--shm-size="512mb"
.


In Kubernetes there is no direct support (yet) for setting 
shm
 size. Instead mount an 
EmptyDir
 with type 
Memory
 to 
/dev/shm
 for example in a deployment.yml:


spec:
  containers:
  - name: artery-udp-cluster
    // rest of container spec...
    volumeMounts:
    - mountPath: /dev/shm
      name: media-driver
  volumes:
  - name: media-driver
    emptyDir:
      medium: Memory
      name: media-driver



There is currently no way to limit the size of a memory empty dir but there is a 
pull request
 for adding it.


Any space used in the mount will count towards your container’s memory usage.


Flight Recorder


When running on JDK 11 Artery specific flight recording is available through the 
Java Flight Recorder (JFR)
. The flight recorder is automatically enabled by detecting JDK 11 but can be disabled if needed by setting 
akka.java-flight-recorder.enabled = false
.


Low overhead Artery specific events are emitted by default when JFR is enabled, higher overhead events needs a custom settings template and are not enabled automatically with the 
profiling
 JFR template. To enable those create a copy of the 
profiling
 template and enable all 
Akka
 sub category events, for example through the JMC GUI. 


Creating Actors Remotely
Warning


We recommend against Creating Actors Remotely, also known as remote deployment, but it is documented here for completeness.


If you want to use the creation functionality in Akka remoting you have to further amend the 
application.conf
 file in the following way (only showing deployment section):


akka {
  actor {
    deployment {
      /sampleActor {
        remote = "akka.tcp://
[email protected]
:2553"
      }
    }
  }
}



The configuration above instructs Akka to react when an actor with path 
/sampleActor
 is created, i.e. using 
system.actorOf(Props(...), "sampleActor")
system.actorOf(new Props(...), "sampleActor")
. This specific actor will not be directly instantiated, but instead the remote daemon of the remote system will be asked to create the actor, which in this sample corresponds to 
[email protected]
:2553
.


Once you have configured the properties above you would do the following in code:




Scala




copy
source
val actor = system.actorOf(Props[SampleActor](), "sampleActor")
actor ! "Pretty slick"


Java




copy
source
ActorRef actor = system.actorOf(Props.create(SampleActor.class), "sampleActor");
actor.tell("Pretty slick", ActorRef.noSender());




The actor class 
SampleActor
 has to be available to the runtimes using it, i.e. the classloader of the actor systems has to have a JAR containing the class.


When using remote deployment of actors you must ensure that all parameters of the 
Props
 can be 
serialized
.
Note


In order to ensure serializability of 
Props
 when passing constructor arguments to the actor being created, do not make the factory 
an
a non-static
 inner class: this will inherently capture a reference to its enclosing object, which in most cases is not serializable. It is best to 
create a factory method in the companion object of the actorâs class
make a static inner class which implements 
Creator<T extends Actor>
.


Serializability of all Props can be tested by setting the configuration item 
akka.actor.serialize-creators=on
. Only Props whose 
deploy
 has 
LocalScope
 are exempt from this check.
Note


You can use asterisks as wildcard matches for the actor path sections, so you could specify: 
/*/sampleActor
 and that would match all 
sampleActor
 on that level in the hierarchy. You can also use wildcard in the last position to match all actors at a certain level: 
/someParent/*
. Non-wildcard matches always have higher priority to match than wildcards, so: 
/foo/bar
 is considered 
more specific
 than 
/foo/*
 and only the highest priority match is used. Please note that it 
cannot
 be used to partially match section, like this: 
/foo*/bar
, 
/f*o/bar
 etc.


Programmatic Remote Deployment
Warning


We recommend against Creating Actors Remotely, also known as remote deployment, but it is documented here for completeness. This is only available for the classic Actor API.


To allow dynamically deployed systems, it is also possible to include deployment configuration in the 
Props
 which are used to create an actor: this information is the equivalent of a deployment section from the configuration file, and if both are given, the external configuration takes precedence.


With these imports:




Scala




copy
source
import akka.actor.{ Address, AddressFromURIString, Deploy, Props }
import akka.remote.RemoteScope


Java




copy
source
import akka.actor.ActorRef;
import akka.actor.Address;
import akka.actor.AddressFromURIString;
import akka.actor.Deploy;
import akka.actor.Props;
import akka.actor.ActorSystem;
import akka.remote.RemoteScope;




and a remote address like this:




Scala




copy
source
val one = AddressFromURIString("akka://sys@host:1234")
val two = Address("akka", "sys", "host", 1234) // this gives the same


Java




copy
source
Address addr = new Address("akka", "sys", "host", 1234);
addr = AddressFromURIString.parse("akka://sys@host:1234"); // the same




you can advise the system to create a child on that remote node like so:




Scala




copy
source
val ref = system.actorOf(Props[SampleActor]().withDeploy(Deploy(scope = RemoteScope(address))))


Java




copy
source
Props props = Props.create(SampleActor.class).withDeploy(new Deploy(new RemoteScope(addr)));
ActorRef ref = system.actorOf(props);




Remote deployment allow list


As remote deployment can potentially be abused by both users and even attackers an allow list feature is available to guard the ActorSystem from deploying unexpected actors. Please note that remote deployment is 
not
 remote code loading, the Actors class to be deployed onto a remote system needs to be present on that remote system. This still however may pose a security risk, and one may want to restrict remote deployment to only a specific set of known actors by enabling the allow list feature.


To enable remote deployment allow list set the 
akka.remote.deployment.enable-allow-list
 value to 
on
. The list of allowed classes has to be configured on the “remote” system, in other words on the system onto which others will be attempting to remote deploy Actors. That system, locally, knows best which Actors it should or should not allow others to remote deploy onto it. The full settings section may for example look like this:


copy
source
akka.remote.deployment {
  enable-allow-list = on
  
  allowed-actor-classes = [
    "akka.remote.artery.RemoteDeploymentSpec.Echo1"
  ]
}


Actor classes not included in the allow list will not be allowed to be remote deployed onto this system.














 
Multi Node Testing






Remote Security 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/configuration.html
Configuration • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration




Where configuration is read from


License key


When using JarJar, OneJar, Assembly or any jar-bundler


Custom application.conf


Including files


Logging of Configuration


A Word About ClassLoaders


Application specific settings


Configuring multiple ActorSystem


Reading configuration from a custom location


Listing of the Reference Configuration




Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration




Where configuration is read from


License key


When using JarJar, OneJar, Assembly or any jar-bundler


Custom application.conf


Including files


Logging of Configuration


A Word About ClassLoaders


Application specific settings


Configuring multiple ActorSystem


Reading configuration from a custom location


Listing of the Reference Configuration




Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Configuration


You can start using Akka without defining any configuration, since sensible default values are provided. Later on you might need to amend the settings to change the default behavior or adapt for specific runtime environments. Typical examples of settings that you might amend:




log level and logger backend


enable Cluster


message serializers


tuning of dispatchers




Akka uses the 
Typesafe Config Library
, which might also be a good choice for the configuration of your own application or library built with or without Akka. This library is implemented in Java with no external dependencies; This is only a summary of the most important parts for more details see 
the config library docs
.


Where configuration is read from


All configuration for Akka is held within instances of 
ActorSystem
ActorSystem
, or put differently, as viewed from the outside, 
ActorSystem
ActorSystem
 is the only consumer of configuration information. While constructing an actor system, you can either pass in a 
Config
 object or not, where the second case is equivalent to passing 
ConfigFactory.load()
 (with the right class loader). This means roughly that the default is to parse all 
application.conf
, 
application.json
 and 
application.properties
 found at the root of the class pathâplease refer to the aforementioned documentation for details. The actor system then merges in all 
reference.conf
 resources found at the root of the class path to form the fallback configuration, i.e. it internally uses


appConfig.withFallback(ConfigFactory.defaultReference(classLoader))



The philosophy is that code never contains default values, but instead relies upon their presence in the 
reference.conf
 supplied with the library in question.


Highest precedence is given to overrides given as system properties, see 
the HOCON specification
 (near the bottom). Also noteworthy is that the application configurationâwhich defaults to 
application
âmay be overridden using the 
config.resource
 property (there are more, please refer to the 
Config docs
).
Note


If you are writing an Akka application, keep your configuration in 
application.conf
 at the root of the class path. If you are writing an Akka-based library, keep its configuration in 
reference.conf
 at the root of the JAR file. It’s not supported to override a config property owned by one library in a 
reference.conf
 of another library.


License key


Akka requires a license key for use in production. Free keys can be obtained at 
https://akka.io/key
. Add the key to the configuration property 
akka.license-key
.


For local development, Akka can be used without a key, but be aware that the 
ActorSystem
 will terminate after a while when a key isn’t configured.


If the license key has expired when the 
ActorSystem
 is started the system will terminate after a while. The expiry date is exposed by 
ActorSystem
ActorSystem
 so that you can write a test to remind yourself that it is time for renewal before it has expired.


When using JarJar, OneJar, Assembly or any jar-bundler
Warning


Akka’s configuration approach relies heavily on the notion of every module/jar having its own 
reference.conf
 file. All of these will be discovered by the configuration and loaded. Unfortunately this also means that if you put/merge multiple jars into the same jar, you need to merge all the 
reference.conf
 files as well: otherwise all defaults will be lost.


See the 
deployment documentation
 for information on how to merge the 
reference.conf
 resources while bundling.


Custom application.conf


A custom 
application.conf
 might look like this:


# In this file you can override any option defined in the reference files.
# Copy in parts of the reference files and modify as you please.

akka {

  # Logger config for Akka internals and classic actors, the new API relies
  # directly on SLF4J and your config for the logger backend.

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    provider = "cluster"

    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
  }

  remote.artery {
    # The port clients should connect to.
    canonical.port = 4711
  }
}



Including files


Sometimes it can be useful to include another configuration file, for example if you have one 
application.conf
 with all environment independent settings and then override some settings for specific environments.


Specifying system property with 
-Dconfig.resource=/dev.conf
 will load the 
dev.conf
 file, which includes the 
application.conf


dev.conf


include "application"

akka {
  loglevel = "DEBUG"
}



More advanced include and substitution mechanisms are explained in the 
HOCON
 specification.




Logging of Configuration


If the system or config property 
akka.log-config-on-start
 is set to 
on
, then the complete configuration is logged at INFO level when the actor system is started. This is useful when you are uncertain of what configuration is used.


If in doubt, you can inspect your configuration objects before or after using them to construct an actor system:


Welcome to Scala 2.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0).
Type in expressions to have them evaluated.
Type :help for more information.

scala> import com.typesafe.config._
import com.typesafe.config._

scala> ConfigFactory.parseString("a.b=12")
res0: com.typesafe.config.Config = Config(SimpleConfigObject({"a" : {"b" : 12}}))

scala> res0.root.render
res1: java.lang.String =
{
    # String: 1
    "a" : {
        # String: 1
        "b" : 12
    }
}



The comments preceding every item give detailed information about the origin of the setting (file & line number) plus possible comments which were present, e.g. in the reference configuration. The settings as merged with the reference and parsed by the actor system can be displayed like this:




Scala




copy
source
val system = ActorSystem(rootBehavior, "MySystem")
system.logConfiguration()


Java




copy
source
ActorSystem<Void> system = ActorSystem.create(rootBehavior, "MySystem");
system.logConfiguration();




A Word About ClassLoaders


In several places of the configuration file it is possible to specify the fully-qualified class name of something to be instantiated by Akka. This is done using Java reflection, which in turn uses a 
ClassLoader
. Getting the right one in challenging environments like application containers or OSGi bundles is not always trivial, the current approach of Akka is that each 
ActorSystem
ActorSystem
 implementation stores the current threadâs context class loader (if available, otherwise just its own loader as in 
this.getClass.getClassLoader
) and uses that for all reflective accesses. This implies that putting Akka on the boot class path will yield 
NullPointerException
 from strange places: this is not supported.


Application specific settings


The configuration can also be used for application specific settings. A good practice is to place those settings in an 
Extension
. 


Configuring multiple ActorSystem


If you have more than one 
ActorSystem
ActorSystem
 (or you’re writing a library and have an 
ActorSystem
ActorSystem
 that may be separate from the application’s) you may want to separate the configuration for each system.


Given that 
ConfigFactory.load()
 merges all resources with matching name from the whole class path, it is easiest to utilize that functionality and differentiate actor systems within the hierarchy of the configuration:


myapp1 {
  akka.loglevel = "WARNING"
  my.own.setting = 43
}
myapp2 {
  akka.loglevel = "ERROR"
  app2.setting = "appname"
}
my.own.setting = 42
my.other.setting = "hello"





Scala




copy
source
val config = ConfigFactory.load()
val app1 = ActorSystem(rootBehavior, "MyApp1", config.getConfig("myapp1").withFallback(config))
val app2 = ActorSystem(rootBehavior, "MyApp2", config.getConfig("myapp2").withOnlyPath("akka").withFallback(config))


Java




copy
source
Config config = ConfigFactory.load();
ActorSystem<Void> app1 =
    ActorSystem.create(rootBehavior, "MyApp1", config.getConfig("myapp1").withFallback(config));
ActorSystem<Void> app2 =
    ActorSystem.create(
        rootBehavior,
        "MyApp2",
        config.getConfig("myapp2").withOnlyPath("akka").withFallback(config));




These two samples demonstrate different variations of the âlift-a-subtreeâ trick: in the first case, the configuration accessible from within the actor system is this


akka.loglevel = "WARNING"
my.own.setting = 43
my.other.setting = "hello"
// plus myapp1 and myapp2 subtrees



while in the second one, only the âakkaâ subtree is lifted, with the following result


akka.loglevel = "ERROR"
my.own.setting = 42
my.other.setting = "hello"
// plus myapp1 and myapp2 subtrees

Note


The configuration library is really powerful, explaining all features exceeds the scope affordable here. In particular not covered are how to include other configuration files within other files (see a small example at 
Including files
) and copying parts of the configuration tree by way of path substitutions.


You may also specify and parse the configuration programmatically in other ways when instantiating the 
ActorSystem
ActorSystem
.




Scala




copy
source
import akka.actor.typed.ActorSystem
import com.typesafe.config.ConfigFactory
val customConf = ConfigFactory.parseString("""
  akka.log-config-on-start = on
""")
// ConfigFactory.load sandwiches customConfig between default reference
// config and default overrides, and then resolves it.
val system = ActorSystem(rootBehavior, "MySystem", ConfigFactory.load(customConf))


Java




copy
source
import akka.actor.typed.ActorSystem;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.Behaviors;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;

Config customConf = ConfigFactory.parseString("akka.log-config-on-start = on");
// ConfigFactory.load sandwiches customConfig between default reference
// config and default overrides, and then resolves it.
ActorSystem<Void> system =
    ActorSystem.create(rootBehavior, "MySystem", ConfigFactory.load(customConf));




Reading configuration from a custom location


You can replace or supplement 
application.conf
 either in code or using system properties.


If you’re using 
ConfigFactory.load()
 (which Akka does by default) you can replace 
application.conf
 by defining 
-Dconfig.resource=whatever
, 
-Dconfig.file=whatever
, or 
-Dconfig.url=whatever
.


From inside your replacement file specified with 
-Dconfig.resource
 and friends, you can 
include
"application"
 if you still want to use 
application.{conf,json,properties}
 as well. Settings specified before 
include "application"
 would be overridden by the included file, while those after would override the included file.


In code, there are many customization options.


There are several overloads of 
ConfigFactory.load()
; these allow you to specify something to be sandwiched between system properties (which override) and the defaults (from 
reference.conf
), replacing the usual 
application.{conf,json,properties}
 and replacing 
-Dconfig.file
 and friends.


The simplest variant of 
ConfigFactory.load()
 takes a resource basename (instead of 
application
); 
myname.conf
, 
myname.json
, and 
myname.properties
 would then be used instead of 
application.{conf,json,properties}
.


The most flexible variant takes a 
Config
 object, which you can load using any method in 
ConfigFactory
. For example you could put a config string in code using 
ConfigFactory.parseString()
 or you could make a map and 
ConfigFactory.parseMap()
, or you could load a file.


You can also combine your custom config with the usual config, that might look like:




Scala




copy
source
// make a Config with just your special setting
val myConfig = ConfigFactory.parseString("something=somethingElse");
// load the normal config stack (system props,
// then application.conf, then reference.conf)
val regularConfig = ConfigFactory.load();
// override regular stack with myConfig
val combined = myConfig.withFallback(regularConfig);
// put the result in between the overrides
// (system props) and defaults again
val complete = ConfigFactory.load(combined);
// create ActorSystem
val system = ActorSystem(rootBehavior, "myname", complete);


Java




copy
source
// make a Config with just your special setting
Config myConfig = ConfigFactory.parseString("something=somethingElse");
// load the normal config stack (system props,
// then application.conf, then reference.conf)
Config regularConfig = ConfigFactory.load();
// override regular stack with myConfig
Config combined = myConfig.withFallback(regularConfig);
// put the result in between the overrides
// (system props) and defaults again
Config complete = ConfigFactory.load(combined);
// create ActorSystem
ActorSystem system = ActorSystem.create(rootBehavior, "myname", complete);




When working with 
Config
 objects, keep in mind that there are three “layers” in the cake:




ConfigFactory.defaultOverrides()
 (system properties)


the app’s settings


ConfigFactory.defaultReference()
 (reference.conf)




The normal goal is to customize the middle layer while leaving the other two alone.




ConfigFactory.load()
 loads the whole stack


the overloads of 
ConfigFactory.load()
 let you specify a different middle layer


the 
ConfigFactory.parse
 variations load single files or resources




To stack two layers, use 
override.withFallback(fallback)
; try to keep system props (
defaultOverrides()
) on top and 
reference.conf
 (
defaultReference()
) on the bottom.


Do keep in mind, you can often just add another 
include
 statement in 
application.conf
 rather than writing code. Includes at the top of 
application.conf
 will be overridden by the rest of 
application.conf
, while those at the bottom will override the earlier stuff.


Listing of the Reference Configuration


Each Akka module has a reference configuration file with the default values. Those 
reference.conf
 files are listed in 
Default configuration














 
Message Delivery Reliability






Default configuration 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/replicated-eventsourcing-examples.html
Replicated Event Sourcing Examples • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Additional samples


Auction example


Shopping cart example






Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Additional samples


Auction example


Shopping cart example






Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Replicated Event Sourcing Examples


The following are more realistic examples of building systems with Replicated Event Sourcing.






Additional samples


Auction example


Shopping cart example






Additional samples


See also the 
Akka Projection gRPC documentation
 with details on setting up the gRPC replication transport.


Additionally, complete samples using the recommended gRPC transport set up can be found in the 
Akka Distributed Cluster Guide
.














 
Replicated Event Sourcing replication via direct access to replica databases






Auction example 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/bsl/license
BSL License 


























































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



































































          Business Source License 1.1
        






































Parameters








Licensor:


Lightbend, Inc.






Licensed Work:


Akka
The Licensed Work is (c) 2022 Lightbend Inc.






Additional Use Grant:


If you develop an application using a version of Play Framework that utilizes binary versions of akka-streams and its dependencies, you may use such binary versions of akka-streams and its dependencies in the development of your application only as they are incorporated into Play Framework and solely to implement the functionality provided by Play Framework; provided that, they are only used in the following way: Connecting to a Play Framework websocket and/or Play Framework request/response bodies for server and play-ws client.






Change Date:


3 years after release






Change License:


Apache License, Version 2.0








For information about alternative licensing arrangements for the Software, please visit: https://www.akka.io/




Business Source License 1.1


License text copyright (c) 2017 MariaDB Corporation Ab, All Rights Reserved. “Business Source License” is a trademark of MariaDB Corporation Ab.


Terms


The Licensor hereby grants you the right to copy, modify, create derivative works, redistribute, and make non-production use of the Licensed Work. The Licensor may make an Additional Use Grant, above, permitting limited production use.


Effective on the Change Date, or the fourth anniversary of the first publicly available distribution of a specific version of the Licensed Work under this License, whichever comes first, the Licensor hereby grants you rights under the terms of the Change License, and the rights granted in the paragraph above terminate.


If your use of the Licensed Work does not comply with the requirements currently in effect as described in this License, you must purchase a commercial license from the Licensor, its affiliated entities, or authorized resellers, or you must refrain from using the Licensed Work.


All copies of the original and modified Licensed Work, and derivative works of the Licensed Work, are subject to this License. This License applies separately for each version of the Licensed Work and the Change Date may vary for each version of the Licensed Work released by Licensor.


You must conspicuously display this License on each original or modified copy of the Licensed Work. If you receive the Licensed Work in original or modified form from a third party, the terms and conditions set forth in this License apply to your use of that work.


Any use of the Licensed Work in violation of this License will automatically terminate your rights under this License for the current and all other versions of the Licensed Work.


This License does not grant you any right in any trademark or logo of Licensor or its affiliates (provided that you may use a trademark or logo of Licensor as expressly required by this License).


TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE LICENSED WORK IS PROVIDED ON AN “AS IS” BASIS. LICENSOR HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS OR IMPLIED, INCLUDING (WITHOUT LIMITATION) WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, AND TITLE.


MariaDB hereby grants you permission to use this License’s text to license your works, and to refer to it using the trademark “Business Source License”, as long as you comply with the Covenants of Licensor below.


Covenants of Licensor


In consideration of the right to use this License’s text and the “Business Source License” name and trademark, Licensor covenants to MariaDB, and to all other recipients of the licensed work to be provided by Licensor:




To specify as the Change License the GPL Version 2.0 or any later version, or a license that is compatible with GPL Version 2.0 or a later version, where “compatible” means that software provided under the Change License can be included in a program with software provided under GPL Version 2.0 or a later version. Licensor may specify additional Change Licenses without limitation.


To either: (a) specify an additional grant of rights to use that does not impose any additional restriction on the right granted in this License, as the Additional Use Grant; or (b) insert the text “None”.


To specify a Change Date.


Not to modify this License in any other way.
















 Legal










Privacy Policy 


Terms of Use 


 Business Source License 1.1 










































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka-persistence-r2dbc/current/
Akka Persistence R2DBC Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Persistence R2DBC Documentation





Version 1.3.3





Java
Scala


Postgres
Yugabyte
H2
SQLServer










Overview


Getting Started


Journal plugin


Snapshot store plugin


Durable state store plugin


Query Plugin


PostgreSQL JSON


Projection


Configuration


Database sharding


Database Cleanup


Migration tool


Migration Guide


Building Native Images


Troubleshooting


Contributing


















Akka Persistence R2DBC Documentation





Version 1.3.3





Java
Scala


Postgres
Yugabyte
H2
SQLServer












Overview


Getting Started


Journal plugin


Snapshot store plugin


Durable state store plugin


Query Plugin


PostgreSQL JSON


Projection


Configuration


Database sharding


Database Cleanup


Migration tool


Migration Guide


Building Native Images


Troubleshooting


Contributing




















Akka Persistence R2DBC Documentation


The Akka Persistence R2DBC plugin allows for using SQL database with R2DBC as a backend for Akka Persistence.






Overview




Project Info


Dependencies




Getting Started




Dependencies


Enabling


Local testing with docker




Journal plugin




Schema


Configuration


Event serialization


Deletes




Snapshot store plugin




Schema


Configuration


Usage


Snapshot serialization


Retention




Durable state store plugin




Schema


Configuration


State serialization


Deletes


Storing query representation




Query Plugin




Event sourced queries


Durable state queries


Configuration




PostgreSQL JSON


Projection


Configuration




Connection configuration


Journal configuration


Snapshot configuration


Durable state configuration


Query configuration


Multiple plugins




Database sharding




Example


Configuration


Schema


Reducing number of database connections


Changing data partitions




Database Cleanup




Event Sourced cleanup tool


Durable State cleanup tool




Migration tool




Dependencies


Progress table


Running


Configuration




Migration Guide




1.2.x to 1.3.0


1.1.x to 1.2.0




Building Native Images


Troubleshooting




Failing to use the configured plugin




Contributing






















Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka Persistence plugin for R2DBC is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#module-info
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-core/current/typed/index-cluster.html
Cluster • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster






Cluster Usage




Module info


Cluster API Extension


Cluster Membership API


Node Roles


Failure Detector


How to test


Configuration


Higher level Cluster tools


Example project




Cluster Specification




Introduction


Terms




Cluster Membership Service




Introduction


Member States


Member Events


Membership Lifecycle


Leader


WeaklyUp Members


Full cluster shutdown


State Diagrams




Phi Accrual Failure Detector




Introduction


Failure Detector Heartbeats


Logging


Failure Detector Threshold




Distributed Data




Module info


Introduction


Using the Replicator


Replicated data types


Durable Storage


Limitations


Learn More about CRDTs


Configuration


Example project




Cluster Singleton




Module info


Introduction


Potential problems to be aware of


Example


Supervision


Application specific stop message


Lease


Accessing singleton of another data centre


Configuration




Cluster Sharding




Module info


Introduction


Basic example


Persistence example


Shard allocation


How it works


Passivation


Automatic Passivation


Sharding State


Remembering Entities


Startup after minimum number of members


Health check


Inspecting cluster sharding state


Lease


Removal of internal Cluster Sharding data


Configuration


Example project




Cluster Sharding concepts




Scenarios


Shard location


Shard rebalancing


ShardCoordinator state


Message ordering


Reliable delivery


Overhead




Sharded Daemon Process




Module info


Introduction


Basic example


Addressing the actors


Dynamic scaling of number of workers


Scalability


Configuration




Distributed Publish Subscribe in Cluster




Module info


The Topic Registry


The Topic Actor


Pub Sub Scalability


Delivery Guarantee




Reliable delivery




Module info


Introduction


Point-to-point


Work pulling


Sharding


Durable producer


Ask from the producer


Only flow control


Chunk large messages


Configuration




Serialization




Dependency


Introduction


Usage


Customization


Serialization of Akka’s messages


Java serialization


Rolling updates




Serialization with Jackson




Dependency


Introduction


Usage


Security


Annotations


Schema Evolution


Rolling updates


Jackson Modules


Using Akka Serialization for embedded types


Additional configuration


Additional features




Multi JVM Testing




Setup


Running tests


Creating application tests


Changing Defaults


Configuration of the JVM instances


ScalaTest


Multi Node Additions


Example project




Multi Node Testing




Module info


Multi Node Testing Concepts


The Test Conductor


The Multi Node Spec


The SbtMultiJvm Plugin


A Multi Node Testing Example


Things to Keep in Mind


Configuration




Remoting




Dependency


Configuration


Introduction


Selecting a transport


Migrating from classic remoting


Canonical address


Acquiring references to remote actors


Remote Security


Quarantine


Serialization


Routers with Remote Destinations


What is new in Artery


Performance tuning


Remote Configuration


Creating Actors Remotely




Remote Security




Configuring SSL/TLS for Akka Remoting


mTLS with rotated certificates in Kubernetes


Untrusted Mode




Split Brain Resolver




Module info


Enable the Split Brain Resolver


The Problem


Strategies


Indirectly connected nodes


Down all when unstable


Cluster Singleton and Cluster Sharding




Coordination




Module info


Lease


Using a lease


Usages in other Akka modules


Lease implementations


Implementing a lease




Choosing Akka Cluster




Microservices


Traditional distributed application


Distributed monolith




















 
Learning Akka Typed from Classic






Cluster Usage 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/multi-node-testing.html
Multi Node Testing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing




Module info


Multi Node Testing Concepts


The Test Conductor


The Multi Node Spec


The SbtMultiJvm Plugin


A Multi Node Testing Example


Things to Keep in Mind


Configuration




Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing




Module info


Multi Node Testing Concepts


The Test Conductor


The Multi Node Spec


The SbtMultiJvm Plugin


A Multi Node Testing Example


Things to Keep in Mind


Configuration




Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Multi Node Testing


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Multi Node Testing, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-multi-node-testkit" % AkkaVersion % Test
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-multi-node-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  testImplementation "com.typesafe.akka:akka-multi-node-testkit_${versions.ScalaBinary}"
}




Project Info: Akka Multi-node Testkit


Artifact
com.typesafe.akka


akka-multi-node-testkit


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.remote.testkit


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since before 2.5.0, 2017-04-13




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Multi Node Testing Concepts


When we talk about multi node testing in Akka we mean the process of running coordinated tests on multiple actor systems in different JVMs. The multi node testing kit consist of three main parts.




The Test Conductor
. that coordinates and controls the nodes under test.


The Multi Node Spec
. that is a convenience wrapper for starting the 
TestConductor
TestConductor
 and letting all nodes connect to it.


The SbtMultiJvm Plugin
. that starts tests in multiple JVMs possibly on multiple machines.




The Test Conductor


The basis for the multi node testing is the 
TestConductor
TestConductor
. It is an Akka Extension that plugs in to the network stack and it is used to coordinate the nodes participating in the test and provides several features including:




Node Address Lookup: Finding out the full path to another test node (No need to share configuration between test nodes)


Node Barrier Coordination: Waiting for other nodes at named barriers.


Network Failure Injection: Throttling traffic, dropping packets, unplugging and plugging nodes back in.




This is a schematic overview of the test conductor.




The test conductor server is responsible for coordinating barriers and sending commands to the test conductor clients that act upon them, e.g. throttling network traffic to/from another client. More information on the possible operations is available in the 
akka.remote.testconductor.Conductor
akka.remote.testconductor.Conductor
 API documentation.


The Multi Node Spec


The Multi Node Spec consists of two parts. The 
MultiNodeConfig
MultiNodeConfig
 that is responsible for common configuration and enumerating and naming the nodes under test. The 
MultiNodeSpec
 that contains a number of convenience functions for making the test nodes interact with each other. More information on the possible operations is available in the 
akka.remote.testkit.MultiNodeSpec
akka.remote.testkit.MultiNodeSpec
 API documentation.


The setup of the 
MultiNodeSpec
 is configured through java system properties that you set on all JVMs that’s going to run a node under test. These can be set on the JVM command line with 
-Dproperty=value
.




These are the available properties








 
multinode.max-nodes
  The maximum number of nodes that a test can have.


 
multinode.host
  The host name or IP for this node. Must be resolvable using InetAddress.getByName.


 
multinode.port
  The port number for this node. Defaults to 0 which will use a random port.


 
multinode.server-host
  The host name or IP for the server node. Must be resolvable using InetAddress.getByName.


 
multinode.server-port
  The port number for the server node. Defaults to 4711.


 
multinode.index
  The index of this node in the sequence of roles defined for the test. The index 0 is special and that machine will be the server. All failure injection and throttling must be done from this node.




The SbtMultiJvm Plugin


The 
SbtMultiJvm Plugin
 has been updated to be able to run multi node tests, by automatically generating the relevant 
multinode.*
 properties. This means that you can run multi node tests on a single machine without any special configuration by running them as normal multi-jvm tests. These tests can then be run distributed over multiple machines without any changes by using the multi-node additions to the plugin.


Multi Node Specific Additions


The plugin also has a number of new 
multi-node-*
 sbt tasks and settings to support running tests on multiple machines. The necessary test classes and dependencies are packaged for distribution to other machines with 
SbtAssembly
 into a jar file with a name on the format 
<projectName>_<scalaVersion>-<projectVersion>-multi-jvm-assembly.jar
Note


To be able to distribute and kick off the tests on multiple machines, it is assumed that both host and target systems are POSIX like systems with 
ssh
 and 
rsync
 available.




These are the available sbt multi-node settings
  








 
multiNodeHosts
  A sequence of hosts to use for running the test, on the form 
user@host:java
 where host is the only required part. Will override settings from file.


 
multiNodeHostsFileName
  A file to use for reading in the hosts to use for running the test. One per line on the same format as above. Defaults to 
multi-node-test.hosts
 in the base project directory.


 
multiNodeTargetDirName
  A name for the directory on the target machine, where to copy the jar file. Defaults to 
multi-node-test
 in the base directory of the ssh user used to rsync the jar file.


 
multiNodeJavaName
  The name of the default Java executable on the target machines. Defaults to 
java
.






Here are some examples of how you define hosts








 
localhost
  The current user on localhost using the default java.


 
user1@host1
  User 
user1
 on host 
host1
 with the default java.


 
user2@host2:/usr/lib/jvm/java-7-openjdk-amd64/bin/java
  User 
user2
 on host 
host2
 using java 7.


 
host3:/usr/lib/jvm/java-6-openjdk-amd64/bin/java
  The current user on host 
host3
 using java 6.




Running the Multi Node Tests


To run all the multi node test in multi-node mode (i.e. distributing the jar files and kicking off the tests remotely) from inside sbt, use the 
multiNodeTest
 task:


multiNodeTest



To run all of them in multi-jvm mode (i.e. all JVMs on the local machine) do:


multi-jvm:test



To run individual tests use the 
multiNodeTestOnly
 task:


multiNodeTestOnly your.MultiNodeTest



To run individual tests in the multi-jvm mode do:


multi-jvm:testOnly your.MultiNodeTest



More than one test name can be listed to run multiple specific tests. Tab completion in sbt makes it easy to complete the test names.


A Multi Node Testing Example


First we need some scaffolding to hook up the 
MultiNodeSpec
MultiNodeSpec
 with your favorite test framework. Lets define a trait 
STMultiNodeSpec
 that uses ScalaTest to start and stop 
MultiNodeSpec
.


copy
source
package akka.remote.testkit

import scala.language.implicitConversions

import org.scalatest.BeforeAndAfterAll
import org.scalatest.matchers.should.Matchers
import org.scalatest.wordspec.AnyWordSpecLike

/**
 * Hooks up MultiNodeSpec with ScalaTest
 */
trait STMultiNodeSpec extends MultiNodeSpecCallbacks with AnyWordSpecLike with Matchers with BeforeAndAfterAll {
  self: MultiNodeSpec =>

  override def beforeAll() = multiNodeSpecBeforeAll()

  override def afterAll() = multiNodeSpecAfterAll()

  // Might not be needed anymore if we find a nice way to tag all logging from a node
  override implicit def convertToWordSpecStringWrapper(s: String): WordSpecStringWrapper =
    new WordSpecStringWrapper(s"$s (on node '${self.myself.name}', $getClass)")
}


Then we need to define a configuration. Lets use two nodes 
"node1
 and 
"node2"
 and call it 
MultiNodeSampleConfig
.


copy
source
package akka.remote.sample

import akka.remote.testkit.{ MultiNodeConfig, STMultiNodeSpec }

object MultiNodeSampleConfig extends MultiNodeConfig {
  val node1 = role("node1")
  val node2 = role("node2")
}


And then finally to the node test code. That starts the two nodes, and demonstrates a barrier, and a remote actor message send/receive.


copy
source
package akka.remote.sample

import akka.actor.{ Actor, Props }
import akka.remote.testkit.MultiNodeSpec
import akka.testkit.ImplicitSender

class MultiNodeSampleSpecMultiJvmNode1 extends MultiNodeSample
class MultiNodeSampleSpecMultiJvmNode2 extends MultiNodeSample

object MultiNodeSample {
  class Ponger extends Actor {
    def receive = {
      case "ping" => sender() ! "pong"
    }
  }
}

class MultiNodeSample extends MultiNodeSpec(MultiNodeSampleConfig) with STMultiNodeSpec with ImplicitSender {

  import MultiNodeSample._
  import MultiNodeSampleConfig._

  def initialParticipants = roles.size

  "A MultiNodeSample" must {

    "wait for all nodes to enter a barrier" in {
      enterBarrier("startup")
    }

    "send to and receive from a remote node" in {
      runOn(node1) {
        enterBarrier("deployed")
        val ponger = system.actorSelection(node(node2) / "user" / "ponger")
        ponger ! "ping"
        import scala.concurrent.duration._
        expectMsg(10.seconds, "pong")
      }

      runOn(node2) {
        system.actorOf(Props[Ponger](), "ponger")
        enterBarrier("deployed")
      }

      enterBarrier("finished")
    }
  }
}


Things to Keep in Mind


There are a couple of things to keep in mind when writing multi node tests or else your tests might behave in surprising ways.




Don’t issue a shutdown of the first node. The first node is the controller and if it shuts down your test will break.


To be able to use 
blackhole
, 
passThrough
, and 
throttle
 you must activate the failure injector and throttler transport adapters by specifying 
testTransport(on = true)
testTransport(true)
 in your 
MultiNodeConfig
.


Throttling, shutdown and other failure injections can only be done from the first node, which again is the controller.


Don’t ask for the address of a node using 
node(address)
 after the node has been shut down. Grab the address before shutting down the node.


Don’t use MultiNodeSpec methods like address lookup, barrier entry et.c. from other threads than the main test thread. This also means that you shouldn’t use them from inside an actor, a future, or a scheduled task.




Configuration


There are several configuration properties for the Multi-Node Testing module, please refer to the 
reference configuration
.














 
Multi JVM Testing






Remoting 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/reference/release-notes.html
Release notes for Akka :: Akka Documentation




























 














 


























Developers








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












SDK 








SDK


Key Value Entities


Event Sourced Entities


Views






 


Timers


Workflows










 


Contact Us


Sign In




Get Started











                    Try Akka for free
                    


Deploy and scale a multi-region app
No credit card required







                    Develop your own Akka app
                    


Akka account not required
Free and open SDK for offline dev







                    Request demo
                    


Personalized demo
Akka app design consultation












 


































Akka














Understanding






Architecture model






Deployment model






Development process






Declarative effects






Entity state models






Multi-region operations






Saga patterns






















Developing






Author your first service










Components






Event Sourced Entities






Key Value Entities






HTTP Endpoints






Views






Workflows






Timers






Consumers














Integrations






Component and service calls






Configure message brokers






Streaming














Setup and configuration






Setup and dependency injection






Serialization






Errors and failures






Access Control Lists (ACLs)






JSON Web Tokens (JWT)






Run a service locally










Developer best practices










Samples






Shopping cart quickstart


























Operating










Organizations






Manage users






Regions






Billing














Projects






Create






Manage users










Configure a container registry






Configure an external container registry














Configure message brokers






Aiven for Kafka






AWS MSK Kafka






Confluent Cloud






Google Pub/Sub










Manage secrets














Services






Deploy and manage services






Invoking Akka services






Viewing data






Data migration










Regions










Observability and monitoring






View logs






View metrics






View traces






Exporting metrics, logs, and traces














Integrating with CI/CD tools






CI/CD with GitHub Actions










Operator best practices






















Securing






Access Control Lists (ACLs)






TLS certificates






JSON Web Tokens (JWTs)






















Support






Community Forum






Email






Frequently Asked Questions






Paid Plans






Status






Request a Demo






Troubleshooting






















Reference






Service descriptor






Route descriptor






Observability descriptor






Glossary of terms






Security announcements






Release notes






Migration guide






API documentation






View query language










CLI






Install the Akka CLI






Using the Akka CLI






Enable CLI command completion










CLI command reference






akka






akka auth






akka auth container-registry






akka auth container-registry clear-cached-token






akka auth container-registry configure






akka auth container-registry credentials






akka auth container-registry install-helper






akka auth current-login






akka auth login






akka auth logout






akka auth signup






akka auth tokens






akka auth tokens create






akka auth tokens list






akka auth tokens revoke






akka auth use-token






akka completion






akka config






akka config clear-cache






akka config clear






akka config current-context






akka config delete-context






akka config get-organization






akka config get-project






akka config get






akka config list-contexts






akka config list






akka config rename-context






akka config set






akka config use-context






akka container-registry






akka container-registry delete-image






akka container-registry list-images






akka container-registry list-tags






akka container-registry list






akka container-registry print






akka container-registry push






akka docker






akka docker add-credentials






akka docker list-credentials






akka docker remove-credentials






akka docs






akka local






akka local console






akka local services






akka local services components






akka local services components get-state






akka local services components get-workflow






akka local services components list-events






akka local services components list-ids






akka local services components list-timers






akka local services components list






akka local services connectivity






akka local services list






akka local services views






akka local services views describe






akka local services views drop






akka local services views list






akka logs






akka organizations






akka organizations auth






akka organizations auth add






akka organizations auth add openid






akka organizations auth list






akka organizations auth remove






akka organizations auth show






akka organizations auth update






akka organizations auth update openid






akka organizations get






akka organizations invitations






akka organizations invitations cancel






akka organizations invitations create






akka organizations invitations list






akka organizations list






akka organizations users






akka organizations users add-binding






akka organizations users delete-binding






akka organizations users list-bindings






akka projects






akka projects config






akka projects config get






akka projects config get broker






akka projects config set






akka projects config set broker






akka projects config unset






akka projects config unset broker






akka projects delete






akka projects get






akka projects hostnames






akka projects hostnames add






akka projects hostnames list






akka projects hostnames remove






akka projects list






akka projects new






akka projects observability






akka projects observability apply






akka projects observability config






akka projects observability config traces






akka projects observability edit






akka projects observability export






akka projects observability get






akka projects observability set






akka projects observability set default






akka projects observability set default akka-console






akka projects observability set default google-cloud






akka projects observability set default otlp






akka projects observability set default splunk-hec






akka projects observability set logs






akka projects observability set logs google-cloud






akka projects observability set logs otlp






akka projects observability set logs splunk-hec






akka projects observability set metrics






akka projects observability set metrics google-cloud






akka projects observability set metrics otlp






akka projects observability set metrics prometheus






akka projects observability set metrics splunk-hec






akka projects observability set traces






akka projects observability set traces google-cloud






akka projects observability set traces otlp






akka projects observability unset






akka projects observability unset default






akka projects observability unset logs






akka projects observability unset metrics






akka projects observability unset traces






akka projects open






akka projects regions






akka projects regions add






akka projects regions list






akka projects regions set-primary






akka projects tokens






akka projects tokens create






akka projects tokens list






akka projects tokens revoke






akka projects update






akka quickstart






akka quickstart download






akka quickstart list






akka regions






akka regions list






akka roles






akka roles add-binding






akka roles delete-binding






akka roles invitations






akka roles invitations delete






akka roles invitations invite-user






akka roles invitations list






akka roles list-bindings






akka roles list






akka routes






akka routes create






akka routes delete






akka routes edit






akka routes export






akka routes get






akka routes list






akka routes update






akka secrets






akka secrets create






akka secrets create asymmetric






akka secrets create generic






akka secrets create symmetric






akka secrets create tls-ca






akka secrets create tls






akka secrets delete






akka secrets get






akka secrets list






akka services






akka services apply






akka services components






akka services components get-state






akka services components get-workflow






akka services components list-events






akka services components list-ids






akka services components list-timers






akka services components list






akka services connectivity






akka services data






akka services data cancel-task






akka services data export






akka services data get-task






akka services data import






akka services data list-tasks






akka services data watch-task






akka services delete






akka services deploy






akka services edit






akka services export






akka services expose






akka services get






akka services jwts






akka services jwts add






akka services jwts generate






akka services jwts list-algorithms






akka services jwts list






akka services jwts remove






akka services jwts update






akka services list






akka services logging






akka services logging list






akka services logging set-level






akka services logging unset-level






akka services pause






akka services proxy






akka services restart






akka services restore






akka services resume






akka services unexpose






akka services views






akka services views describe






akka services views drop






akka services views list






akka version


























Akka Libraries


















Akka


default








Akka
























Akka


Reference


Release notes












Release notes for Akka












Akka constantly gets updates and improvements enabling new features and expanding on existing. This page lists all releases of Akka components including the Akka libraries.






Current versions










Akka SDK 3.1.0






Akka CLI 3.0.10






A glance of all Akka libraries and their current versions is presented at 
Akka library versions
.














January 2025










Akka Projections 1.6.8






Akka Persistence R2DBC 1.3.2






Akka core 2.10.1






Akka SDK 3.1.0








Internal refactoring of SPI between SDK and runtime






Akka runtime 1.3.0












Akka CLI 3.0.9








Fixes listing of user role bindings












Platform update 2025-01-13








updates to internal libraries for security fixes






switch of internal framework to apply environment configuration






minor updates to the Console




















December 2024










Akka CLI 3.0.8








Updates to configure SSO integrations












Akka SDK 3.0.2








Integration Tests are now bound to 
mvn verify
 and not a specific profile












Platform update 2024-12-10








New internal structure to capture usage data






Updated email server for signup emails






Updated JVM memory settings for services






Akka Runtime 1.2.5






Better gRPC support for the CLI






Console updates








Empty projects can now be deleted from the Console












GCP: Updates of GKE node versions












Akka Runtime 1.2.5








Improves handling of 
count(*)
 in the view query language












Akka CLI 3.0.7








Improvements to the Local Console












Akka SDK 3.0.1








Minor improvements




















November 2024










Akka Projections 1.6.5






Akka Projections 1.6.4






Akka Projections 1.6.3






Akka DynamoDB 2.0.3






Akka DynamoDB 2.0.2






Akka CLI 3.0.6








Automatically retry calls






Improved help texts












Akka Projections 1.6.2






Akka DynamoDB 2.0.1






Akka Runtime 1.2.2








Disable projection scaling until issue has been investigated and fixed






fix problem with read only commands in workflows












Akka SDK 3.0.0








Runtime 1.2.1






Accept old type url for components that can consume pre-existing events












Akka Runtime 1.2.1








Remove logback startup warnings






Don’t log TImeoutException at error level






Allow root route for both sdks












Akka CLI 3.0.4








Changed Docker credentials commands






Improved logging commands






New commands for dynamic logging levels (
akka service logging
)












Akka SDK 3.0.0-RC4








Fix dependency excludes












Akka SDK 3.0.0-RC1








Json type url cleanup






Allow more customization of brokers in dev mode






Akka dependencies






Smaller improvements












Akka Runtime 1.2.0








Fix configuration for tracing






Json type url cleanup






Allow more customization of brokers in dev mode






Akka dependencies






Smaller improvements












Akka Projections 1.6.1








Configurable parallelism in initial offset store query for AWS DynamoDB












Akka Runtime 1.1.53








Several smaller bug fixes and improvements












Akka Runtime 1.1.52








Several smaller bug fixes and improvements




















October 2024










Akka Runtime 1.1.51








Several smaller bug fixes and improvements












Akka CLI 3.0.3








Improved support for pushing Service images to multiple Akka Container Registries












Akka libraries 24.10 releases








overview in 
Akka 24.10 Released












Akka Runtime 1.1.50








Several smaller bug fixes and improvements












Akka Runtime 1.1.49








JWT support for HTTP Endpoints






Several smaller bug fixes and improvements












Akka CLI 3.0.2








Added region synchronisation status for the following commands:








akka service get






akka service list






akka routes get






akka routes list






akka project observability get












Region management






Data export and import management












Akka Runtime 1.1.46








View indexing improvements for some join conditions






Other smaller improvements












Akka Projection 1.5.9








improvement of projection scaling












Akka Persistence R2DBC 1.2.6








improvement of latency for eventsBySlices after idle












Akka Projection 1.5.8








fix protobuf serialization in Replicated Event Sourcing












Akka core 2.9.7








event interceptor in Replicated Event Sourcing






expose license key expiry












Akka Projection 1.5.7








dependency updates












Akka gRPC 2.4.4








Allow rotation of client certs






updates for 
CVE-2024-7254












Akka core 2.9.6








updates for 
CVE-2024-7254






Akka license key






license key validation












Akka core 2.8.7








Akka license key












Akka core 2.7.1








Akka license key




















May 2024










Akka libraries 24.05 releases








overview in 
Akka 24.05 released




















October 2023










Akka libraries 23.10 releases








overview in 
Akka 23.10 Released




















May 2023










Akka libraries 23.05 releases








overview in 
Akka 23.05 Released




















October 2022










Akka libraries 22.10 releases








overview in 
Akka 22.10 Released






















Security announcements


Migration guide












































© 2011 - 
, Lightbend, Inc. All rights reserved. | 
Licenses
 | 
Terms
 | 
Privacy Policy
 | 
Cookie Listing
 | 
Cookie Settings
 | 
RSS

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-flows-and-basics.html
Basics and working with Flows • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows




Dependency


Introduction


Core concepts


Defining and running streams


Back-pressure explained


Stream Materialization


Stream ordering


Actor Materializer Lifecycle




Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows




Dependency


Introduction


Core concepts


Defining and running streams


Back-pressure explained


Stream Materialization


Stream ordering


Actor Materializer Lifecycle




Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Basics and working with Flows


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


Core concepts


Akka Streams is a library to process and transfer a sequence of elements using bounded buffer space. This latter property is what we refer to as 
boundedness
, and it is the defining feature of Akka Streams. Translated to everyday terms, it is possible to express a chain (or as we see later, graphs) of processing entities. Each of these entities executes independently (and possibly concurrently) from the others while only buffering a limited number of elements at any given time. This property of bounded buffers is one of the differences from the actor model, where each actor usually has an unbounded, or a bounded, but dropping mailbox. Akka Stream processing entities have bounded “mailboxes” that do not drop.


Before we move on, let’s define some basic terminology which will be used throughout the entire documentation:




Stream


An active process that involves moving and transforming data.


Element


An element is the processing unit of streams. All operations transform and transfer elements from upstream to downstream. Buffer sizes are always expressed as number of elements independently from the actual size of the elements.


Back-pressure
  


A means of flow-control, a way for consumers of data to notify a producer about their current availability, effectively slowing down the upstream producer to match their consumption speeds. In the context of Akka Streams back-pressure is always understood as 
non-blocking
 and 
asynchronous
.


Non-Blocking
  


Means that a certain operation does not hinder the progress of the calling thread, even if it takes a long time to finish the requested operation.


Graph


A description of a stream processing topology, defining the pathways through which elements shall flow when the stream is running.


Operator


The common name for all building blocks that build up a Graph. Examples of operators are 
map()
, 
filter()
, custom ones extending 
GraphStage
s
 and graph junctions like 
Merge
 or 
Broadcast
. For the full list of built-in operators see the 
operator index




When we talk about 
asynchronous, non-blocking backpressure
, we mean that the operators available in Akka Streams will not use blocking calls but asynchronous message passing to exchange messages between each other. This way they can slow down a fast producer without blocking its thread. This is a thread-pool friendly design, since entities that need to wait (a fast producer waiting on a slow consumer) will not block the thread but can hand it back for further use to an underlying thread-pool.


Defining and running streams


Linear processing pipelines can be expressed in Akka Streams using the following core abstractions:




Source


An operator with 
exactly one output
, emitting data elements whenever downstream operators are ready to receive them.


Sink


An operator with 
exactly one input
, requesting and accepting data elements, possibly slowing down the upstream producer of elements.


Flow


An operator which has 
exactly one input and output
, which connects its upstream and downstream by transforming the data elements flowing through it.


RunnableGraph


A Flow that has both ends “attached” to a Source and Sink respectively, and is ready to be 
run()
.




It is possible to attach a 
Flow
Flow
 to a 
Source
Source
 resulting in a composite source, and it is also possible to prepend a 
Flow
 to a 
Sink
Sink
 to get a new sink. After a stream is properly constructed by having both a source and a sink, it will be represented by the 
RunnableGraph
RunnableGraph
 type, indicating that it is ready to be executed.


It is important to remember that even after constructing the 
RunnableGraph
 by connecting all the source, sink and different operators, no data will flow through it until it is materialized. Materialization is the process of allocating all resources needed to run the computation described by a Graph (in Akka Streams this will often involve starting up Actors). Thanks to Flows being a description of the processing pipeline they are 
immutable, thread-safe, and freely shareable
, which means that it is for example safe to share and send them between actors, to have one actor prepare the work, and then have it be materialized at some completely different place in the code.




Scala




copy
source
val source = Source(1 to 10)
val sink = Sink.fold[Int, Int](0)(_ + _)

// connect the Source to the Sink, obtaining a RunnableGraph
val runnable: RunnableGraph[Future[Int]] = source.toMat(sink)(Keep.right)

// materialize the flow and get the value of the sink
val sum: Future[Int] = runnable.run()



Java




copy
source
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));
// note that the Future is scala.concurrent.Future
final Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);

// connect the Source to the Sink, obtaining a RunnableFlow
final RunnableGraph<CompletionStage<Integer>> runnable = source.toMat(sink, Keep.right());

// materialize the flow
final CompletionStage<Integer> sum = runnable.run(system);




After running (materializing) the 
RunnableGraph[T]
 we get back the materialized value of type T. Every stream operator can produce a materialized value, and it is the responsibility of the user to combine them to a new type. In the above example, we used 
toMat
 to indicate that we want to transform the materialized value of the source and sink, and we used the convenience function 
Keep.right
 to say that we are only interested in the materialized value of the sink.


In our example, the 
Sink.fold
 materializes a value of type 
Future
 which will represent the result of the folding process over the stream. In general, a stream can expose multiple materialized values, but it is quite common to be interested in only the value of the Source or the Sink in the stream. For this reason there is a convenience method called 
runWith()
 available for 
Sink
, 
Source
 or 
Flow
 requiring, respectively, a supplied 
Source
 (in order to run a 
Sink
), a 
Sink
 (in order to run a 
Source
) or both a 
Source
 and a 
Sink
 (in order to run a 
Flow
, since it has neither attached yet).


After running (materializing) the 
RunnableGraph
 we get a special container object, the 
MaterializedMap
. Both sources and sinks are able to put specific objects into this map. Whether they put something in or not is implementation dependent. 


For example, a 
Sink.fold
 will make a 
CompletionStage
 available in this map which will represent the result of the folding process over the stream. In general, a stream can expose multiple materialized values, but it is quite common to be interested in only the value of the Source or the Sink in the stream. For this reason there is a convenience method called 
runWith()
 available for 
Sink
, 
Source
 or 
Flow
 requiring, respectively, a supplied 
Source
 (in order to run a 
Sink
), a 
Sink
 (in order to run a 
Source
) or both a 
Source
 and a 
Sink
 (in order to run a 
Flow
, since it has neither attached yet).




Scala




copy
source
val source = Source(1 to 10)
val sink = Sink.fold[Int, Int](0)(_ + _)

// materialize the flow, getting the Sink's materialized value
val sum: Future[Int] = source.runWith(sink)


Java




copy
source
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));
final Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);

// materialize the flow, getting the Sink's materialized value
final CompletionStage<Integer> sum = source.runWith(sink, system);




It is worth pointing out that since operators are 
immutable
, connecting them returns a new operator, instead of modifying the existing instance, so while constructing long flows, remember to assign the new value to a variable or run it:




Scala




copy
source
val source = Source(1 to 10)
source.map(_ => 0) // has no effect on source, since it's immutable
source.runWith(Sink.fold(0)(_ + _)) // 55

val zeroes = source.map(_ => 0) // returns new Source[Int], with `map()` appended
zeroes.runWith(Sink.fold(0)(_ + _)) // 0


Java




copy
source
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));
source.map(x -> 0); // has no effect on source, since it's immutable
source.runWith(Sink.fold(0, Integer::sum), system); // 55

// returns new Source<Integer>, with `map()` appended
final Source<Integer, NotUsed> zeroes = source.map(x -> 0);
final Sink<Integer, CompletionStage<Integer>> fold = Sink.fold(0, Integer::sum);
zeroes.runWith(fold, system); // 0


Note


By default, Akka Streams elements support 
exactly one
 downstream operator. Making fan-out (supporting multiple downstream operators) an explicit opt-in feature allows default stream elements to be less complex and more efficient. Also, it allows for greater flexibility on 
how exactly
 to handle the multicast scenarios, by providing named fan-out elements such as broadcast (signals all down-stream elements) or balance (signals one of available down-stream elements).


In the above example we used the 
runWith
 method, which both materializes the stream and returns the materialized value of the given sink or source.


Since a stream can be materialized multiple times, the 
materialized value will also be calculated anew
 
MaterializedMap
 returned is different
 for each such materialization, usually leading to different values being returned each time. In the example below, we create two running materialized instances of the stream that we described in the 
runnable
 variable. Both materializations give us a different 
Future
CompletionStage
 from the map even though we used the same 
sink
 to refer to the future:




Scala




copy
source
// connect the Source to the Sink, obtaining a RunnableGraph
val sink = Sink.fold[Int, Int](0)(_ + _)
val runnable: RunnableGraph[Future[Int]] =
  Source(1 to 10).toMat(sink)(Keep.right)

// get the materialized value of the sink
val sum1: Future[Int] = runnable.run()
val sum2: Future[Int] = runnable.run()

// sum1 and sum2 are different Futures!


Java




copy
source
// connect the Source to the Sink, obtaining a RunnableGraph
final Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);
final RunnableGraph<CompletionStage<Integer>> runnable =
    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).toMat(sink, Keep.right());

// get the materialized value of the FoldSink
final CompletionStage<Integer> sum1 = runnable.run(system);
final CompletionStage<Integer> sum2 = runnable.run(system);

// sum1 and sum2 are different Futures!




Defining sources, sinks and flows


The objects 
Source
Source
 and 
Sink
Sink
 define various ways to create sources and sinks of elements. The following examples show some of the most useful constructs (refer to the API documentation for more details):




Scala




copy
source
// Create a source from an Iterable
Source(List(1, 2, 3))

// Create a source from a Future
Source.future(Future.successful("Hello Streams!"))

// Create a source from a single element
Source.single("only one element")

// an empty source
Source.empty

// Sink that folds over the stream and returns a Future
// of the final result as its materialized value
Sink.fold[Int, Int](0)(_ + _)

// Sink that returns a Future as its materialized value,
// containing the first element of the stream
Sink.head

// A Sink that consumes a stream without doing anything with the elements
Sink.ignore

// A Sink that executes a side-effecting call for every element of the stream
Sink.foreach[String](println(_))


Java




copy
source
// Create a source from an Iterable
List<Integer> list = new LinkedList<>();
list.add(1);
list.add(2);
list.add(3);
Source.from(list);

// Create a source form a Future
Source.future(Futures.successful("Hello Streams!"));

// Create a source from a single element
Source.single("only one element");

// an empty source
Source.empty();

// Sink that folds over the stream and returns a Future
// of the final result in the MaterializedMap
Sink.fold(0, Integer::sum);

// Sink that returns a Future in the MaterializedMap,
// containing the first element of the stream
Sink.head();

// A Sink that consumes a stream without doing anything with the elements
Sink.ignore();

// A Sink that executes a side-effecting call for every element of the stream
Sink.foreach(System.out::println);




There are various ways to wire up different parts of a stream, the following examples show some of the available options:




Scala




copy
source
// Explicitly creating and wiring up a Source, Sink and Flow
Source(1 to 6).via(Flow[Int].map(_ * 2)).to(Sink.foreach(println(_)))

// Starting from a Source
val source = Source(1 to 6).map(_ * 2)
source.to(Sink.foreach(println(_)))

// Starting from a Sink
val sink: Sink[Int, NotUsed] = Flow[Int].map(_ * 2).to(Sink.foreach(println(_)))
Source(1 to 6).to(sink)

// Broadcast to a sink inline
val otherSink: Sink[Int, NotUsed] =
  Flow[Int].alsoTo(Sink.foreach(println(_))).to(Sink.ignore)
Source(1 to 6).to(otherSink)



Java




copy
source
// Explicitly creating and wiring up a Source, Sink and Flow
Source.from(Arrays.asList(1, 2, 3, 4))
    .via(Flow.of(Integer.class).map(elem -> elem * 2))
    .to(Sink.foreach(System.out::println));

// Starting from a Source
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(1, 2, 3, 4)).map(elem -> elem * 2);
source.to(Sink.foreach(System.out::println));

// Starting from a Sink
final Sink<Integer, NotUsed> sink =
    Flow.of(Integer.class).map(elem -> elem * 2).to(Sink.foreach(System.out::println));
Source.from(Arrays.asList(1, 2, 3, 4)).to(sink);




Illegal stream elements


In accordance to the Reactive Streams specification (
Rule 2.13
) Akka Streams do not allow 
null
 to be passed through the stream as an element. In case you want to model the concept of absence of a value we recommend using 
scala.Option
 or 
scala.util.Either
java.util.Optional
 which is available since Java 8
.


Back-pressure explained


Akka Streams implement an asynchronous non-blocking back-pressure protocol standardised by the 
Reactive Streams
 specification, which Akka is a founding member of.


The user of the library does not have to write any explicit back-pressure handling code â it is built in and dealt with automatically by all of the provided Akka Streams operators. It is possible however to add explicit buffer operators with overflow strategies that can influence the behavior of the stream. This is especially important in complex processing graphs which may even contain loops (which 
must
 be treated with very special care, as explained in 
Graph cycles, liveness and deadlocks
).


The back pressure protocol is defined in terms of the number of elements a downstream 
Subscriber
 is able to receive and buffer, referred to as 
demand
. The source of data, referred to as 
Publisher
 in Reactive Streams terminology and implemented as 
Source
Source
 in Akka Streams, guarantees that it will never emit more elements than the received total demand for any given 
Subscriber
.
Note


The Reactive Streams specification defines its protocol in terms of 
Publisher
 and 
Subscriber
. These types are 
not
 meant to be user facing API, instead they serve as the low-level building blocks for different Reactive Streams implementations.


Akka Streams implements these concepts as 
Source
Source
, 
Flow
Flow
 (referred to as 
Processor
 in Reactive Streams) and 
Sink
Sink
 without exposing the Reactive Streams interfaces directly. If you need to integrate with other Reactive Stream libraries, read 
Integrating with Reactive Streams
.


The mode in which Reactive Streams back-pressure works can be colloquially described as “dynamic push / pull mode”, since it will switch between push and pull based back-pressure models depending on the downstream being able to cope with the upstream production rate or not.


To illustrate this further let us consider both problem situations and how the back-pressure protocol handles them:


Slow Publisher, fast Subscriber


This is the happy case â we do not need to slow down the Publisher in this case. However signalling rates are rarely constant and could change at any point in time, suddenly ending up in a situation where the Subscriber is now slower than the Publisher. In order to safeguard from these situations, the back-pressure protocol must still be enabled during such situations, however we do not want to pay a high penalty for this safety net being enabled.


The Reactive Streams protocol solves this by asynchronously signalling from the Subscriber to the Publisher 
Request(n:Int)
 
Request(int n)
 signals. The protocol guarantees that the Publisher will never signal 
more
 elements than the signalled demand. Since the Subscriber however is currently faster, it will be signalling these Request messages at a higher rate (and possibly also batching together the demand - requesting multiple elements in one Request signal). This means that the Publisher should not ever have to wait (be back-pressured) with publishing its incoming elements.


As we can see, in this scenario we effectively operate in so called push-mode since the Publisher can continue producing elements as fast as it can, since the pending demand will be recovered just-in-time while it is emitting elements.


Fast Publisher, slow Subscriber


This is the case when back-pressuring the 
Publisher
 is required, because the 
Subscriber
 is not able to cope with the rate at which its upstream would like to emit data elements.


Since the 
Publisher
 is not allowed to signal more elements than the pending demand signalled by the 
Subscriber
, it will have to abide to this back-pressure by applying one of the below strategies:




not generate elements, if it is able to control their production rate,


try buffering the elements in a 
bounded
 manner until more demand is signalled,


drop elements until more demand is signalled,


tear down the stream if unable to apply any of the above strategies.




As we can see, this scenario effectively means that the 
Subscriber
 will 
pull
 the elements from the Publisher â this mode of operation is referred to as pull-based back-pressure.


Stream Materialization


When constructing flows and graphs in Akka Streams think of them as preparing a blueprint, an execution plan. Stream materialization is the process of taking a stream description (
RunnableGraph
RunnableGraph
) and allocating all the necessary resources it needs in order to run. In the case of Akka Streams this often means starting up Actors which power the processing, but is not restricted to thatâit could also mean opening files or socket connections etc.âdepending on what the stream needs.


Materialization is triggered at so called “terminal operations”. Most notably this includes the various forms of the 
run()
 and 
runWith()
 methods defined on 
Source
Source
 and 
Flow
Flow
 elements as well as a small number of special syntactic sugars for running with well-known sinks, such as 
runForeach(el => ...)
runForeach(el -> ...)
 (being an alias to 
runWith(Sink.foreach(el => ...))
runWith(Sink.foreach(el -> ...))
).


Materialization is performed synchronously on the materializing thread by an 
ActorSystem
ActorSystem
 global 
Materializer
Materializer
. The actual stream processing is handled by actors started up during the streams materialization, which will be running on the thread pools they have been configured to run on. By default, the thread pool used is the dispatcher set on the 
ActorSystem
 config, but it is possible set other thread pools by providing 
Attributes
Attributes
 to




the stream to be materialized


creating a custom instance of 
Materializer
Materializer
 with 
defaultAttributes


Note


Reusing 
instances
 of linear computation operators (Source, Sink, Flow) inside composite Graphs is legal, yet will materialize that operator multiple times.


Operator Fusion


By default, Akka Streams will fuse the stream operators. This means that the processing steps of a flow or stream can be executed within the same Actor and has two consequences:




passing elements from one operator to the next is a lot faster between fused operators due to avoiding the asynchronous messaging overhead


fused stream operators do not run in parallel to each other, meaning that only up to one CPU core is used for each fused part




To allow for parallel processing you will have to insert asynchronous boundaries manually into your flows and operators by way of adding 
Attributes.asyncBoundary
Attributes.asyncBoundary
 using the method 
async
 on 
Source
Source
, 
Sink
Sink
 and 
Flow
Flow
 to operators that shall communicate with the downstream of the graph in an asynchronous fashion.




Scala




copy
source
Source(List(1, 2, 3)).map(_ + 1).async.map(_ * 2).to(Sink.ignore)


Java




copy
source
Source.range(1, 3).map(x -> x + 1).async().map(x -> x * 2).to(Sink.ignore());




In this example we create two regions within the flow which will be executed in one Actor eachâassuming that adding and multiplying integers is an extremely costly operation this will lead to a performance gain since two CPUs can work on the tasks in parallel. It is important to note that asynchronous boundaries are not singular places within a flow where elements are passed asynchronously (as in other streaming libraries), but instead attributes always work by adding information to the flow graph that has been constructed up to this point:




This means that everything that is inside the red bubble will be executed by one actor and everything outside of it by another. This scheme can be applied successively, always having one such boundary enclose the previous ones plus all operators that have been added since then.
Warning


Without fusing (i.e. up to version 2.0-M2) each stream operator had an implicit input buffer that holds a few elements for efficiency reasons. If your flow graphs contain cycles then these buffers may have been crucial in order to avoid deadlocks. With fusing these implicit buffers are no longer there, data elements are passed without buffering between fused operators. In those cases where buffering is needed in order to allow the stream to run at all, you will have to insert explicit buffers with the 
.buffer()
 operatorâtypically a buffer of size 2 is enough to allow a feedback loop to function.




Combining materialized values


Since every operator in Akka Streams can provide a materialized value after being materialized, it is necessary to somehow express how these values should be composed to a final value when we plug these operators together. For this, many operator methods have variants that take an additional argument, a function, that will be used to combine the resulting values. Some examples of using these combiners are illustrated in the example below.




Scala




copy
source
// A source that can be signalled explicitly from the outside
val source: Source[Int, Promise[Option[Int]]] = Source.maybe[Int]

// A flow that internally throttles elements to 1/second, and returns a Cancellable
// which can be used to shut down the stream
val flow: Flow[Int, Int, Cancellable] = throttler

// A sink that returns the first element of a stream in the returned Future
val sink: Sink[Int, Future[Int]] = Sink.head[Int]

// By default, the materialized value of the leftmost stage is preserved
val r1: RunnableGraph[Promise[Option[Int]]] = source.via(flow).to(sink)

// Simple selection of materialized values by using Keep.right
val r2: RunnableGraph[Cancellable] = source.viaMat(flow)(Keep.right).to(sink)
val r3: RunnableGraph[Future[Int]] = source.via(flow).toMat(sink)(Keep.right)

// Using runWith will always give the materialized values of the stages added
// by runWith() itself
val r4: Future[Int] = source.via(flow).runWith(sink)
val r5: Promise[Option[Int]] = flow.to(sink).runWith(source)
val r6: (Promise[Option[Int]], Future[Int]) = flow.runWith(source, sink)

// Using more complex combinations
val r7: RunnableGraph[(Promise[Option[Int]], Cancellable)] =
  source.viaMat(flow)(Keep.both).to(sink)

val r8: RunnableGraph[(Promise[Option[Int]], Future[Int])] =
  source.via(flow).toMat(sink)(Keep.both)

val r9: RunnableGraph[((Promise[Option[Int]], Cancellable), Future[Int])] =
  source.viaMat(flow)(Keep.both).toMat(sink)(Keep.both)

val r10: RunnableGraph[(Cancellable, Future[Int])] =
  source.viaMat(flow)(Keep.right).toMat(sink)(Keep.both)

// It is also possible to map over the materialized values. In r9 we had a
// doubly nested pair, but we want to flatten it out
val r11: RunnableGraph[(Promise[Option[Int]], Cancellable, Future[Int])] =
  r9.mapMaterializedValue {
    case ((promise, cancellable), future) =>
      (promise, cancellable, future)
  }

// Now we can use pattern matching to get the resulting materialized values
val (promise, cancellable, future) = r11.run()

// Type inference works as expected
promise.success(None)
cancellable.cancel()
future.map(_ + 3)

// The result of r11 can be also achieved by using the Graph API
val r12: RunnableGraph[(Promise[Option[Int]], Cancellable, Future[Int])] =
  RunnableGraph.fromGraph(GraphDSL.createGraph(source, flow, sink)((_, _, _)) { implicit builder => (src, f, dst) =>
    import GraphDSL.Implicits._
    src ~> f ~> dst
    ClosedShape
  })



Java




copy
source
// An empty source that can be shut down explicitly from the outside
Source<Integer, CompletableFuture<Optional<Integer>>> source = Source.<Integer>maybe();

// A flow that internally throttles elements to 1/second, and returns a Cancellable
// which can be used to shut down the stream
Flow<Integer, Integer, Cancellable> flow = throttler;

// A sink that returns the first element of a stream in the returned Future
Sink<Integer, CompletionStage<Integer>> sink = Sink.head();

// By default, the materialized value of the leftmost stage is preserved
RunnableGraph<CompletableFuture<Optional<Integer>>> r1 = source.via(flow).to(sink);

// Simple selection of materialized values by using Keep.right
RunnableGraph<Cancellable> r2 = source.viaMat(flow, Keep.right()).to(sink);
RunnableGraph<CompletionStage<Integer>> r3 = source.via(flow).toMat(sink, Keep.right());

// Using runWith will always give the materialized values of the stages added
// by runWith() itself
CompletionStage<Integer> r4 = source.via(flow).runWith(sink, system);
CompletableFuture<Optional<Integer>> r5 = flow.to(sink).runWith(source, system);
Pair<CompletableFuture<Optional<Integer>>, CompletionStage<Integer>> r6 =
    flow.runWith(source, sink, system);

// Using more complex combinations
RunnableGraph<Pair<CompletableFuture<Optional<Integer>>, Cancellable>> r7 =
    source.viaMat(flow, Keep.both()).to(sink);

RunnableGraph<Pair<CompletableFuture<Optional<Integer>>, CompletionStage<Integer>>> r8 =
    source.via(flow).toMat(sink, Keep.both());

RunnableGraph<
        Pair<Pair<CompletableFuture<Optional<Integer>>, Cancellable>, CompletionStage<Integer>>>
    r9 = source.viaMat(flow, Keep.both()).toMat(sink, Keep.both());

RunnableGraph<Pair<Cancellable, CompletionStage<Integer>>> r10 =
    source.viaMat(flow, Keep.right()).toMat(sink, Keep.both());

// It is also possible to map over the materialized values. In r9 we had a
// doubly nested pair, but we want to flatten it out

RunnableGraph<Cancellable> r11 =
    r9.mapMaterializedValue(
        (nestedTuple) -> {
          CompletableFuture<Optional<Integer>> p = nestedTuple.first().first();
          Cancellable c = nestedTuple.first().second();
          CompletionStage<Integer> f = nestedTuple.second();

          // Picking the Cancellable, but we could  also construct a domain class here
          return c;
        });


Note


In Graphs it is possible to access the materialized value from inside the stream. For details see 
Accessing the materialized value inside the Graph
.


Source pre-materialization


There are situations in which you require a 
Source
Source
 materialized value 
before
 the 
Source
 gets hooked up to the rest of the graph. This is particularly useful in the case of “materialized value powered” 
Source
s, like 
Source.queue
, 
Source.actorRef
 or 
Source.maybe
.


By using the 
preMaterialize
preMaterialize
 operator on a 
Source
, you can obtain its materialized value and another 
Source
. The latter can be used to consume messages from the original 
Source
. Note that this can be materialized multiple times.




Scala




copy
source
val completeWithDone: PartialFunction[Any, CompletionStrategy] = { case Done => CompletionStrategy.immediately }
val matValuePoweredSource =
  Source.actorRef[String](
    completionMatcher = completeWithDone,
    failureMatcher = PartialFunction.empty,
    bufferSize = 100,
    overflowStrategy = OverflowStrategy.fail)

val (actorRef, source) = matValuePoweredSource.preMaterialize()

actorRef ! "Hello!"

// pass source around for materialization
source.runWith(Sink.foreach(println))


Java




copy
source
Source<String, ActorRef> matValuePoweredSource =
    Source.actorRef(
        elem -> {
          // complete stream immediately if we send it Done
          if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());
          else return Optional.empty();
        },
        // never fail the stream because of a message
        elem -> Optional.empty(),
        100,
        OverflowStrategy.fail());

Pair<ActorRef, Source<String, NotUsed>> actorRefSourcePair =
    matValuePoweredSource.preMaterialize(system);

actorRefSourcePair.first().tell("Hello!", ActorRef.noSender());

// pass source around for materialization
actorRefSourcePair.second().runWith(Sink.foreach(System.out::println), system);




Stream ordering


In Akka Streams almost all computation operators 
preserve input order
 of elements. This means that if inputs 
{IA1,IA2,...,IAn}
 “cause” outputs 
{OA1,OA2,...,OAk}
 and inputs 
{IB1,IB2,...,IBm}
 “cause” outputs 
{OB1,OB2,...,OBl}
 and all of 
IAi
 happened before all 
IBi
 then 
OAi
 happens before 
OBi
.


This property is even upheld by async operations such as 
mapAsync
mapAsync
, however an unordered version exists called 
mapAsyncUnordered
mapAsyncUnordered
 which does not preserve this ordering.


However, in the case of Junctions which handle multiple input streams (e.g. 
Merge
) the output order is, in general, 
not defined
 for elements arriving on different input ports. That is a merge-like operation may emit 
Ai
 before emitting 
Bi
, and it is up to its internal logic to decide the order of emitted elements. Specialized elements such as 
Zip
 however 
do guarantee
 their outputs order, as each output element depends on all upstream elements having been signalled already â thus the ordering in the case of zipping is defined by this property.


If you find yourself in need of fine grained control over order of emitted elements in fan-in scenarios consider using 
MergePreferred
, 
MergePrioritized
 or 
GraphStage
 â which gives you full control over how the merge is performed.


Actor Materializer Lifecycle


The 
Materializer
Materializer
 is a component that is responsible for turning the stream blueprint into a running stream and emitting the “materialized value”. An 
ActorSystem
ActorSystem
 wide 
Materializer
 is provided by the Akka 
Extension
 
SystemMaterializer
SystemMaterializer
 by 
having an implicit 
ActorSystem
 in scope
passing the 
ActorSystem
 to the various 
run
 methods
 this way there is no need to worry about the 
Materializer
 unless there are special requirements.


One use case that may require a custom instance of 
Materializer
 is when all streams materialized in an actor should be tied to the Actor lifecycle and stop if the Actor stops or crashes. 


An important aspect of working with streams and actors is understanding a 
Materializer
’s life-cycle. The materializer is bound to the lifecycle of the 
ActorRefFactory
ActorRefFactory
 it is created from, which in practice will be either an 
ActorSystem
ActorSystem
 or 
ActorContext
ActorContext
 (when the materializer is created within an 
Actor
Actor
). 


Tying it to the 
ActorSystem
 should be replaced with using the system materializer from Akka 2.6 and on.


When run by the system materializer the streams will run until the 
ActorSystem
 is shut down. When the materializer is shut down 
before
 the streams have run to completion, they will be terminated abruptly. This is a little different than the usual way to terminate streams, which is by cancelling/completing them. The stream lifecycles are bound to the materializer like this to prevent leaks, and in normal operations you should not rely on the mechanism and rather use 
KillSwitch
KillSwitch
 or normal completion signals to manage the lifecycles of your streams. 


If we look at the following example, where we create the 
Materializer
 within an 
Actor
:




Scala




copy
source
final class RunWithMyself extends Actor {
  implicit val mat: Materializer = Materializer(context)

  Source.maybe.runWith(Sink.onComplete {
    case Success(done) => println(s"Completed: $done")
    case Failure(ex)   => println(s"Failed: ${ex.getMessage}")
  })

  def receive = {
    case "boom" =>
      context.stop(self) // will also terminate the stream
  }
}


Java




copy
source
final class RunWithMyself extends AbstractActor {

  Materializer mat = Materializer.createMaterializer(context());

  @Override
  public void preStart() throws Exception {
    Source.repeat("hello")
        .runWith(
            Sink.onComplete(
                tryDone -> {
                  System.out.println("Terminated stream: " + tryDone);
                }),
            mat);
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            String.class,
            p -> {
              // this WILL terminate the above stream as well
              context().stop(self());
            })
        .build();
  }
}




In the above example we used the 
ActorContext
ActorContext
 to create the materializer. This binds its lifecycle to the surrounding 
Actor
Actor
. In other words, while the stream we started there would under normal circumstances run forever, if we stop the Actor it would terminate the stream as well. We have 
bound the stream’s lifecycle to the surrounding actor’s lifecycle
. This is a very useful technique if the stream is closely related to the actor, e.g. when the actor represents a user or other entity, that we continuously query using the created stream – and it would not make sense to keep the stream alive when the actor has terminated already. The streams termination will be signalled by an “Abrupt termination exception” signaled by the stream.


You may also cause a 
Materializer
 to shut down by explicitly calling 
shutdown()
shutdown()
 on it, resulting in abruptly terminating all of the streams it has been running then. 


Sometimes, however, you may want to explicitly create a stream that will out-last the actor’s life. For example, you are using an Akka stream to push some large stream of data to an external service. You may want to eagerly stop the Actor since it has performed all of its duties already:




Scala




copy
source
final class RunForever(implicit val mat: Materializer) extends Actor {

  Source.maybe.runWith(Sink.onComplete {
    case Success(done) => println(s"Completed: $done")
    case Failure(ex)   => println(s"Failed: ${ex.getMessage}")
  })

  def receive = {
    case "boom" =>
      context.stop(self) // will NOT terminate the stream (it's bound to the system!)
  }
}


Java




copy
source
final class RunForever extends AbstractActor {

  private final Materializer materializer;

  public RunForever(Materializer materializer) {
    this.materializer = materializer;
  }

  @Override
  public void preStart() throws Exception {
    Source.repeat("hello")
        .runWith(
            Sink.onComplete(
                tryDone -> {
                  System.out.println("Terminated stream: " + tryDone);
                }),
            materializer);
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            String.class,
            p -> {
              // will NOT terminate the stream (it's bound to the system!)
              context().stop(self());
            })
        .build();
  }




In the above example we pass in a materializer to the Actor, which results in binding its lifecycle to the entire 
ActorSystem
ActorSystem
 rather than the single enclosing actor. This can be useful if you want to share a materializer or group streams into specific materializers, for example because of the materializer’s settings etc.
Warning


Do not create new actor materializers inside actors by passing the 
context.system
context.system()
 to it. This will cause a new 
Materializer
Materializer
 to be created and potentially leaked (unless you shut it down explicitly) for each such actor. It is instead recommended to either pass-in the Materializer or create one using the actor’s 
context
.














 
Design Principles behind Akka Streams






Working with Graphs 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-core/current/typed/
Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actors






Introduction to Actors




Module info


Akka Actors


First example


A More Complex Example




Actor lifecycle




Dependency


Introduction


Creating Actors


Stopping Actors


Watching Actors




Interaction Patterns




Dependency


Introduction


Fire and Forget


Request-Response


Adapted Response


Request-Response with ask between two actors


Request-Response with ask from outside an Actor


Generic response wrapper


Ignoring replies


Send Future result to self


Per session child Actor


General purpose response aggregator


Latency tail chopping


Scheduling messages to self


Responding to a sharded actor




Fault Tolerance




Supervision


Child actors are stopped when parent is restarting


The PreRestart signal


Bubble failures up through the hierarchy




Actor discovery




Dependency


Obtaining Actor references


Receptionist


Cluster Receptionist


Receptionist Scalability




Routers




Dependency


Introduction


Pool Router


Group Router


Routing strategies


Routers and performance




Stash




Dependency


Introduction




Behaviors as finite state machines




Example project




Coordinated Shutdown


Dispatchers




Dependency


Introduction


Default dispatcher


Internal dispatcher


Looking up a Dispatcher


Selecting a dispatcher


Types of dispatchers


Dispatcher aliases


Blocking Needs Careful Management


More dispatcher configuration examples




Mailboxes




Dependency


Introduction


Selecting what mailbox is used


Mailbox Implementations


Custom Mailbox type




Testing




Module info


Introduction


Asynchronous testing


Synchronous behavior testing




Coexistence




Dependency


Introduction


Classic to typed


Typed to classic


Supervision




Style guide




Functional versus object-oriented style


Passing around too many parameters


Behavior factory method


Where to define messages


Public versus private messages


Lambdas versus method references


Partial versus total Function


How to compose Partial Functions


ask versus ?


ReceiveBuilder


Nesting setup


Additional naming conventions




Learning Akka Typed from Classic




Dependencies


Package names


Actor definition


actorOf and Props


ActorRef


ActorSystem


become


sender


parent


Supervision


Lifecycle hooks


watch


Stopping


ActorSelection


ask


pipeTo


ActorContext


ActorContext.children


Remote deployment


Routers


FSM


Timers


Stash


PersistentActor


Asynchronous Testing


Synchronous Testing




















 
Default configuration






Introduction to Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-singleton.html
Cluster Singleton • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton




Module info


Introduction


Potential problems to be aware of


Example


Supervision


Application specific stop message


Lease


Accessing singleton of another data centre


Configuration




Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton




Module info


Introduction


Potential problems to be aware of


Example


Supervision


Application specific stop message


Lease


Accessing singleton of another data centre


Configuration




Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Singleton


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Cluster Singleton
.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Cluster Singleton, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}




Project Info: Akka Cluster (typed)


Artifact
com.typesafe.akka


akka-cluster-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


For some use cases it is convenient and sometimes also mandatory to ensure that you have exactly one actor of a certain type running somewhere in the cluster.


Some examples:




single point of responsibility for certain cluster-wide consistent decisions, or coordination of actions across the cluster system


single entry point to an external system


single master, many workers


centralized naming service, or routing logic




Using a singleton should not be the first design choice. It has several drawbacks, such as single-point of bottleneck. Single-point of failure is also a relevant concern, but for some cases this feature takes care of that by making sure that another singleton instance will eventually be started.
Warning


Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in in 
multiple Singletons
 being started, one in each separate cluster! See 
Downing
.


Singleton manager


The cluster singleton pattern manages one singleton actor instance among all cluster nodes or a group of nodes tagged with a specific role. The singleton manager is an actor that is supposed to be started with 
ClusterSingleton.init
ClusterSingleton.init
 as early as possible on all nodes, or all nodes with specified role, in the cluster. 


The actual singleton actor is




Started on the oldest node by creating a child actor from supplied 
Behavior
Behavior
. It makes sure that at most one singleton instance is running at any point in time.


Always running on the oldest member with specified role.




The oldest member is determined by 
akka.cluster.Member#isOlderThan
akka.cluster.Member#isOlderThan
 This can change when removing that member from the cluster. Be aware that there is a short time period when there is no active singleton during the hand-over process.


When the oldest node is 
Leaving
 the cluster there is an exchange from the oldest and the new oldest before a new singleton is started up.


The cluster 
failure detector
 will notice when oldest node becomes unreachable due to things like JVM crash, hard shut down, or network failure. After 
Downing
 and removing that node the a new oldest node will take over and a new singleton actor is created. For these failure scenarios there will not be a graceful hand-over, but more than one active singletons is prevented by all reasonable means. Some corner cases are eventually resolved by configurable timeouts. Additional safety can be added by using a 
Lease
. 


Singleton proxy


To communicate with a given named singleton in the cluster you can access it though a proxy 
ActorRef
ActorRef
. When calling 
ClusterSingleton.init
ClusterSingleton.init
 for a given 
singletonName
 on a node an 
ActorRef
 is returned. It is to this 
ActorRef
 that you can send messages to the singleton instance, independent of which node the singleton instance is active. 
ClusterSingleton.init
 can be called multiple times, if there already is a singleton manager running on this node, no additional manager is started, and if there is one running an 
ActorRef
 to the proxy is returned.


The proxy will route all messages to the current instance of the singleton, and keep track of the oldest node in the cluster and discover the singleton’s 
ActorRef
. There might be periods of time during which the singleton is unavailable, e.g., when a node leaves the cluster. In these cases, the proxy will buffer the messages sent to the singleton and then deliver them when the singleton is finally available. If the buffer is full the proxy will drop old messages when new messages are sent via the proxy. The size of the buffer is configurable and it can be disabled by using a buffer size of 0.


It’s worth noting that messages can always be lost because of the distributed nature of these actors. As always, additional logic should be implemented in the singleton (acknowledgement) and in the client (retry) actors to ensure at-least-once message delivery.


The singleton instance will not run on members with status 
WeaklyUp
.


Potential problems to be aware of


This pattern may seem to be very tempting to use at first, but it has several drawbacks, some of them are listed below:




The cluster singleton may quickly become a 
performance bottleneck
.


You can not rely on the cluster singleton to be 
non-stop
 available â e.g. when the node on which the singleton  has been running dies, it will take a few seconds for this to be noticed and the singleton be migrated to another node.


If many singletons are used be aware of that all will run on the oldest node (or oldest with configured role).  
Cluster Sharding
 combined with keeping the “singleton” entities alive can be a better  alternative.


Warning


Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in in 
multiple Singletons
 being started, one in each separate cluster! See 
Downing
.


Example


Any 
Behavior
Behavior
 can be run as a singleton. E.g. a basic counter:




Scala




copy
source
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Int]) extends Command
  case object GoodByeCounter extends Command

  def apply(): Behavior[Command] = {
    def updated(value: Int): Behavior[Command] = {
      Behaviors.receiveMessage[Command] {
        case Increment =>
          updated(value + 1)
        case GetValue(replyTo) =>
          replyTo ! value
          Behaviors.same
        case GoodByeCounter =>
          // Possible async action then stop
          Behaviors.stopped
      }
    }

    updated(0)
  }
}


Java




copy
source
public class Counter extends AbstractBehavior<Counter.Command> {

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    private final ActorRef<Integer> replyTo;

    public GetValue(ActorRef<Integer> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public enum GoodByeCounter implements Command {
    INSTANCE
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(Counter::new);
  }

  private int value = 0;

  private Counter(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, msg -> onIncrement())
        .onMessage(GetValue.class, this::onGetValue)
        .onMessage(GoodByeCounter.class, msg -> onGoodByCounter())
        .build();
  }

  private Behavior<Command> onIncrement() {
    value++;
    return this;
  }

  private Behavior<Command> onGetValue(GetValue msg) {
    msg.replyTo.tell(value);
    return this;
  }

  private Behavior<Command> onGoodByCounter() {
    // Possible async action then stop
    return this;
  }
}




Then on every node in the cluster, or every node with a given role, use the 
ClusterSingleton
ClusterSingleton
 extension to spawn the singleton. An instance will per data centre of the cluster:




Scala




copy
source
import akka.cluster.typed.ClusterSingleton
import akka.cluster.typed.SingletonActor

val singletonManager = ClusterSingleton(system)
// Start if needed and provide a proxy to a named singleton
val proxy: ActorRef[Counter.Command] = singletonManager.init(
  SingletonActor(Behaviors.supervise(Counter()).onFailure[Exception](SupervisorStrategy.restart), "GlobalCounter"))

proxy ! Counter.Increment


Java




copy
source
import akka.cluster.typed.ClusterSingleton;
import akka.cluster.typed.ClusterSingletonSettings;
import akka.cluster.typed.SingletonActor;

ClusterSingleton singleton = ClusterSingleton.get(system);
// Start if needed and provide a proxy to a named singleton
ActorRef<Counter.Command> proxy =
    singleton.init(SingletonActor.of(Counter.create(), "GlobalCounter"));

proxy.tell(Counter.Increment.INSTANCE);




Supervision


The default 
supervision strategy
 when an exception is thrown is for an actor to be stopped. The above example overrides this to 
restart
 to ensure it is always running. Another option would be to restart with a backoff: 




Scala




copy
source
val proxyBackOff: ActorRef[Counter.Command] = singletonManager.init(
  SingletonActor(
    Behaviors
      .supervise(Counter())
      .onFailure[Exception](SupervisorStrategy.restartWithBackoff(1.second, 10.seconds, 0.2)),
    "GlobalCounter"))


Java




copy
source
ClusterSingleton singleton = ClusterSingleton.get(system);
ActorRef<Counter.Command> proxy =
    singleton.init(
        SingletonActor.of(
            Behaviors.supervise(Counter.create())
                .onFailure(
                    SupervisorStrategy.restartWithBackoff(
                        Duration.ofSeconds(1), Duration.ofSeconds(10), 0.2)),
            "GlobalCounter"));




Be aware that this means there will be times when the singleton won’t be running as restart is delayed. See 
Fault Tolerance
 for a full list of supervision options.


Application specific stop message


An application specific 
stopMessage
 can be used to close the resources before actually stopping the singleton actor. This 
stopMessage
 is sent to the singleton actor to tell it to finish its work, close resources, and stop. The hand-over to the new oldest node is completed when the singleton actor is terminated. If the shutdown logic does not include any asynchronous actions it can be executed in the 
PostStop
PostStop
 signal handler.




Scala




copy
source
val singletonActor = SingletonActor(Counter(), "GlobalCounter").withStopMessage(Counter.GoodByeCounter)
singletonManager.init(singletonActor)


Java




copy
source
SingletonActor<Counter.Command> counterSingleton =
    SingletonActor.of(Counter.create(), "GlobalCounter")
        .withStopMessage(Counter.GoodByeCounter.INSTANCE);
ActorRef<Counter.Command> proxy = singleton.init(counterSingleton);




Lease


A 
lease
 can be used as an additional safety measure to ensure that two singletons don’t run at the same time. Reasons for how this can happen:




Network partitions without an appropriate downing provider


Mistakes in the deployment process leading to two separate Akka Clusters


Timing issues between removing members from the Cluster on one side of a network partition and shutting them down on the other side




A lease can be a final backup that means that the singleton actor won’t be created unless the lease can be acquired. 


To use a lease for every singleton in an application set 
akka.cluster.singleton.use-lease
 to the configuration location of the lease to use. A lease with the name 
<actor system name>-singleton-<singleton actor path>
 is used and the owner is set to the 
Cluster(system).selfAddress.hostPort
Cluster.get(system).selfAddress().hostPort()
.


Note that the 
akka.cluster.singleton.lease-name
 configuration key is ignored and cannot be used to configure singleton lease names.


It is also possible to configure one individual singleton to use lease by defining a lease config block specifically for it:


copy
source
my.app.my-singleton-lease {
  use-lease = "akka.coordination.lease.kubernetes"
  lease-retry-interval = 5s
  lease-name = "my-pingpong-singleton-lease"
}


And then setting that into 
LeaseUsageSettings
LeaseUsageSettings
 that can be set in the 
ClusterSingletonSettings
ClusterSingletonSettings
:




Scala




copy
source
val settings =
  ClusterSingletonSettings(system).withLeaseSettings(
    LeaseUsageSettings(system.settings.config.getConfig("my.app.my-singleton-lease")))
val singletonActor = SingletonActor(pingPong, "ping-pong").withStopMessage(Perish).withSettings(settings)
  ClusterSingleton(system).init(singletonActor)


Java




copy
source
LeaseUsageSettings leaseSettings =
    LeaseUsageSettings.create(
        system.settings().config().getConfig("my.app.my-singleton-lease"));
ClusterSingletonSettings settings =
    ClusterSingletonSettings.create(system).withLeaseSettings(leaseSettings);
SingletonActor<PingPong.Command> singletonActor =
    SingletonActor.of(PingPong.create(), "ping-pong")
        .withStopMessage(new PingPong.Perish())
        .withSettings(settings);
ClusterSingleton.get(system).init(singletonActor);




Or programmatically specifying the lease settings:




Scala




copy
source
val settings = ClusterSingletonSettings(system).withLeaseSettings(
  LeaseUsageSettings("akka.coordination.lease.kubernetes", 5.seconds, "my-pingpong-singleton-lease"))
val singletonActor = SingletonActor(pingPong, "ping-pong").withStopMessage(Perish).withSettings(settings)


Java




copy
source
ClusterSingletonSettings settings =
    ClusterSingletonSettings.create(system)
        .withLeaseSettings(
            LeaseUsageSettings.create(
                "akka.coordination.lease.kubernetes",
                Duration.ofSeconds(5),
                "my-pingpong-singleton-lease"));
SingletonActor<PingPong.Command> singletonActor =
    SingletonActor.of(PingPong.create(), "ping-pong")
        .withStopMessage(new PingPong.Perish())
        .withSettings(settings);




If the cluster singleton manager can’t acquire the lease it will keep retrying while it is the oldest node in the cluster. If the lease is lost then the singleton actor will be terminated then the lease will be re-tried.


Accessing singleton of another data centre


TODO 
#27705


Configuration


The following configuration properties are read by the 
ClusterSingletonManagerSettings
ClusterSingletonManagerSettings
 when created with a 
ActorSystem
ActorSystem
 parameter. It is also possible to amend the 
ClusterSingletonManagerSettings
 or create it from another config section with the same layout as below. 
ClusterSingletonManagerSettings
 is a parameter to the 
ClusterSingletonManager.props
ClusterSingletonManager.props
 factory method, i.e. each singleton can be configured with different settings if needed.


copy
source
akka.cluster.singleton {
  # The actor name of the child singleton actor.
  singleton-name = "singleton"
  
  # Singleton among the nodes tagged with specified role.
  # If the role is not specified it's a singleton among all nodes in the cluster.
  role = ""
  
  # When a node is becoming oldest it sends hand-over request to previous oldest, 
  # that might be leaving the cluster. This is retried with this interval until 
  # the previous oldest confirms that the hand over has started or the previous 
  # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
  hand-over-retry-interval = 1s
  
  # The number of retries are derived from hand-over-retry-interval and
  # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
  # but it will never be less than this property.
  # After the hand over retries and it's still not able to exchange the hand over messages
  # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,
  # to start from a clean state. After that it will still not start the singleton instance
  # until the previous oldest node has been removed from the cluster.
  # On the other side, on the previous oldest node, the same number of retries - 3 are used
  # and after that the singleton instance is stopped.
  # For large clusters it might be necessary to increase this to avoid too early timeouts while
  # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios
  # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios
  # the recovery might be faster.
  min-number-of-hand-over-retries = 15

  # Config path of the lease to be taken before creating the singleton actor
  # if the lease is lost then the actor is restarted and it will need to re-acquire the lease
  # the default is no lease
  use-lease = ""

  # The interval between retries for acquiring the lease
  lease-retry-interval = 5s

  # Custom lease name. Note that if you have several singletons each one must have a unique
  # lease name, which can be defined with the leaseSettings of ClusterSingletonSettings.
  # If undefined it will be derived from ActorSystem name and singleton actor path,
  # but that may result in too long lease names.
  # Note that for typed cluster it is not possible to change this through configuration
  # any value here is ignored, custom names must be set through programmatic API.
  lease-name = ""
}


The following configuration properties are read by the 
ClusterSingletonSettings
ClusterSingletonSettings
 when created with a 
ActorSystem
ActorSystem
 parameter. 
ClusterSingletonSettings
 is an optional parameter in 
ClusterSingleton.init
ClusterSingleton.init
. It is also possible to amend the 
ClusterSingletonProxySettings
ClusterSingletonProxySettings
 or create it from another config section with the same layout as below.


copy
source
akka.cluster.singleton-proxy {
  # The actor name of the singleton actor that is started by the ClusterSingletonManager
  singleton-name = ${akka.cluster.singleton.singleton-name}
  
  # The role of the cluster nodes where the singleton can be deployed.
  # Corresponding to the role used by the `ClusterSingletonManager`. If the role is not
  # specified it's a singleton among all nodes in the cluster, and the `ClusterSingletonManager`
  # must then also be configured in same way.
  role = ""
  
  # Interval at which the proxy will try to resolve the singleton instance.
  singleton-identification-interval = 1s
  
  # If the location of the singleton is unknown the proxy will buffer this
  # number of messages and deliver them when the singleton is identified. 
  # When the buffer is full old messages will be dropped when new messages are
  # sent via the proxy.
  # Use 0 to disable buffering, i.e. messages will be dropped immediately if
  # the location of the singleton is unknown.
  # Maximum allowed buffer size is 10000.
  buffer-size = 1000 
}














 
Distributed Data






Cluster Sharding 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/durable-state/persistence.html
Durable State • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and DurableStateBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Tagging


Wrapping DurableStateBehavior




Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and DurableStateBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Tagging


Wrapping DurableStateBehavior




Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Durable State


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Persistence, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}


You also have to select durable state store plugin, see 
Persistence Plugins
.




Project Info: Akka Event Sourcing (typed)


Artifact
com.typesafe.akka


akka-persistence-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.persistence.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


This model of Akka Persistence enables a stateful actor / entity to store the full state after processing each command instead of using event sourcing. This reduces the conceptual complexity and can be a handy tool for simple use cases. Very much like a CRUD based operation, the API is conceptually simple - a function from current state and incoming command to the next state which replaces the current state in the database. 


(State, Command) => State



The current state is always stored in the database. Since only the latest state is stored, we don’t have access to any of the history of changes, unlike event sourced storage. Akka Persistence would read that state and store it in memory. After processing of the command is finished, the new state will be stored in the database. The processing of the next command will not start until the state has been successfully stored in the database.


Akka Persistence also supports 
Event Sourcing
 based implementation, where only the 
events
 that are persisted by the actor are stored, but not the actual state of the actor. By storing all events, using this model, a stateful actor can be recovered by replaying the stored events to the actor, which allows it to rebuild its state.


Since each entity lives on one node, consistency is guaranteed and reads can be served directly from memory. For details on how this guarantee is ensured, have a look at the 
Cluster Sharding and DurableStateBehavior
 section below.


Example and core API


Let’s start with a simple example that models a counter using an Akka persistent actor. The minimum required for a 
DurableStateBehavior
DurableStateBehavior
 is:




Scala




copy
source
import akka.persistence.typed.state.scaladsl.DurableStateBehavior
import akka.persistence.typed.PersistenceId

object MyPersistentCounter {
  sealed trait Command[ReplyMessage] extends CborSerializable

  final case class State(value: Int) extends CborSerializable

  def counter(persistenceId: PersistenceId): DurableStateBehavior[Command[_], State] = {
    DurableStateBehavior.apply[Command[_], State](
      persistenceId,
      emptyState = State(0),
      commandHandler =
        (state, command) => throw new NotImplementedError("TODO: process the command & return an Effect"))
  }
}


Java




copy
source
public class MyPersistentCounter
    extends DurableStateBehavior<MyPersistentCounter.Command<?>, MyPersistentCounter.State> {

  interface Command<ReplyMessage> {}

  public static class State {
    private final int value;

    public State(int value) {
      this.value = value;
    }

    public int get() {
      return value;
    }
  }

  public static Behavior<Command<?>> create(PersistenceId persistenceId) {
    return new MyPersistentCounter(persistenceId);
  }

  private MyPersistentCounter(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return new State(0);
  }

  @Override
  public CommandHandler<Command<?>, State> commandHandler() {
    return (state, command) -> {
      throw new RuntimeException("TODO: process the command & return an Effect");
    };
  }
}




The first important thing to notice is the 
Behavior
 of a persistent actor is typed to the type of the 
Command
 because this is the type of message a persistent actor should receive. In Akka this is now enforced by the type system.


The components that make up a 
DurableStateBehavior
 are:




persistenceId
 is the stable unique identifier for the persistent actor.


emptyState
 defines the 
State
 when the entity is first created e.g. a Counter would start with 0 as state.


commandHandler
 defines how to handle commands and map to appropriate effects e.g. persisting state and replying to actors.




Next we’ll discuss each of these in detail.


PersistenceId


The 
PersistenceId
PersistenceId
 is the stable unique identifier for the persistent actor in the backend durabe state store.


Cluster Sharding
 is typically used together with 
DurableStateBehavior
 to ensure that there is only one active entity for each 
PersistenceId
 (
entityId
). There are techniques to ensure this uniqueness, an example of which can be found in the 
Persistence example in the Cluster Sharding documentation
. This illustrates how to construct the 
PersistenceId
 from the 
entityTypeKey
 and 
entityId
 provided by the 
EntityContext
.


The 
entityId
 in Cluster Sharding is the business domain identifier which uniquely identifies the instance of that specific 
EntityType
. This means that across the cluster we have a unique combination of (
EntityType
, 
EntityId
). Hence the 
entityId
 might not be unique enough to be used as the 
PersistenceId
 by itself. For example two different types of entities may have the same 
entityId
. To create a unique 
PersistenceId
 the 
entityId
 should be prefixed with a stable name of the entity type, which typically is the same as the 
EntityTypeKey.name
 that is used in Cluster Sharding. There are 
PersistenceId.apply
PersistenceId.of
 factory methods to help with constructing such 
PersistenceId
 from an 
entityTypeHint
 and 
entityId
.


The default separator when concatenating the 
entityTypeHint
 and 
entityId
 is 
|
, but a custom separator is supported.


A custom identifier can be created with 
PersistenceId.ofUniqueId
. 


Command handler


The command handler is a function with 2 parameters, the current 
State
 and the incoming 
Command
.


A command handler returns an 
Effect
 directive that defines what state, if any, to persist. Effects are created using 
a factory that is returned via the 
Effect()
 method
 
the 
Effect
 factory
.


The two most commonly used effects are: 




persist
 will persist the latest value of the state. No history of state changes will be stored


none
 no state to be persisted, for example a read-only command




The state is typically defined as an immutable class and a new instance of the state is passed to the 
persist
 effect. You may choose to use a mutable class for the state, and then the command handler may update the state instance, but it must still pass the updated state to the 
persist
 effect.


More effects are explained in 
Effects and Side Effects
.


In addition to returning the primary 
Effect
 for the command, 
DurableStateBehavior
s can also chain side effects that are to be performed after successful persist which is achieved with the 
thenRun
 function e.g. 
Effect.persist(..).thenRun
Effect().persist(..).thenRun
.


Completing the example


Let’s fill in the details of the example.


Commands:




Scala




copy
source
sealed trait Command[ReplyMessage] extends CborSerializable
final case object Increment extends Command[Nothing]
final case class IncrementBy(value: Int) extends Command[Nothing]
final case class GetValue(replyTo: ActorRef[State]) extends Command[State]
final case object Delete extends Command[Nothing]


Java




copy
source
interface Command<ReplyMessage> {}

public enum Increment implements Command<Void> {
  INSTANCE
}

public static class IncrementBy implements Command<Void> {
  public final int value;

  public IncrementBy(int value) {
    this.value = value;
  }
}

public static class GetValue implements Command<State> {
  private final ActorRef<Integer> replyTo;

  public GetValue(ActorRef<Integer> replyTo) {
    this.replyTo = replyTo;
  }
}

public enum Delete implements Command<Void> {
  INSTANCE
}




State is a storage for the latest value of the counter.




Scala




copy
source
final case class State(value: Int) extends CborSerializable


Java




copy
source
public static class State {
  private final int value;

  public State(int value) {
    this.value = value;
  }

  public int get() {
    return value;
  }
}




The command handler handles the commands 
Increment
, 
IncrementBy
 and 
GetValue
. 




Increment
 increments the counter by 
1
 and persists the updated value as an effect in the State


IncrementBy
 increments the counter by the value passed to it and persists the updated value as an effect in the State


GetValue
 retrieves the value of the counter from the State and replies with it to the actor passed in






Scala




copy
source
import akka.persistence.typed.state.scaladsl.Effect

val commandHandler: (State, Command[_]) => Effect[State] = (state, command) =>
  command match {
    case Increment         => Effect.persist(state.copy(value = state.value + 1))
    case IncrementBy(by)   => Effect.persist(state.copy(value = state.value + by))
    case GetValue(replyTo) => Effect.reply(replyTo)(state)
    case Delete            => Effect.delete[State]()
  }


Java




copy
source
@Override
public CommandHandler<Command<?>, State> commandHandler() {
  return newCommandHandlerBuilder()
      .forAnyState()
      .onCommand(
          Increment.class, (state, command) -> Effect().persist(new State(state.get() + 1)))
      .onCommand(
          IncrementBy.class,
          (state, command) -> Effect().persist(new State(state.get() + command.value)))
      .onCommand(
          GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))
      .onCommand(
          Delete.class, (state, command) -> Effect().delete())
      .build();
}




These are used to create a 
DurableStateBehavior
:
 
These are defined in an 
DurableStateBehavior
:




Scala




copy
source
import akka.persistence.typed.state.scaladsl.DurableStateBehavior
import akka.persistence.typed.PersistenceId

def counter(id: String): DurableStateBehavior[Command[_], State] = {
  DurableStateBehavior.apply[Command[_], State](
    persistenceId = PersistenceId.ofUniqueId(id),
    emptyState = State(0),
    commandHandler = commandHandler)
}


Java




copy
source
import akka.persistence.typed.state.javadsl.DurableStateBehavior;
import akka.persistence.typed.PersistenceId;

public class MyPersistentCounter
    extends DurableStateBehavior<MyPersistentCounter.Command<?>, MyPersistentCounter.State> {

  // commands, events and state defined here

  public static Behavior<Command<?>> create(PersistenceId persistenceId) {
    return new MyPersistentCounter(persistenceId);
  }

  private MyPersistentCounter(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return new State(0);
  }

  @Override
  public CommandHandler<Command<?>, State> commandHandler() {
    return newCommandHandlerBuilder()
        .forAnyState()
        .onCommand(
            Increment.class, (state, command) -> Effect().persist(new State(state.get() + 1)))
        .onCommand(
            IncrementBy.class,
            (state, command) -> Effect().persist(new State(state.get() + command.value)))
        .onCommand(
            GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))
        .onCommand(
            Delete.class, (state, command) -> Effect().delete())
        .build();
  }
}




Effects and Side Effects


A command handler returns an 
Effect
 directive that defines what state, if any, to persist. Effects are created using 
a factory that is returned via the 
Effect()
 method
 
the 
Effect
 factory
 and can be one of: 




persist
 will persist the latest state. If it’s a new persistence id, the record will be inserted. In case of an existing persistence id, the record will be updated only if the revision number of the incoming record is 1 more than the already existing record. Otherwise 
persist
 will fail.


delete
 will delete the state by setting it to the empty state and the revision number will be incremented by 1.


none
 no state to be persisted, for example a read-only command


unhandled
 the command is unhandled (not supported) in current state


stop
 stop this actor


stash
 the current command is stashed


unstashAll
 process the commands that were stashed with 
Effect.stash
Effect().stash


reply
 send a reply message to the given 
ActorRef




Note that only one of those can be chosen per incoming command. It is not possible to both persist and say none/unhandled.


In addition to returning the primary 
Effect
 for the command 
DurableStateBehavior
s can also chain side effects that are to be performed after successful persist which is achieved with the 
thenRun
 function that runs the callback passed to it e.g. 
Effect.persist(..).thenRun
Effect().persist(..).thenRun
. 


All 
thenRun
 registered callbacks are executed sequentially after successful execution of the persist statement (or immediately, in case of 
none
 and 
unhandled
).


In addition to 
thenRun
 the following actions can also be performed after successful persist:




thenStop
 the actor will be stopped


thenUnstashAll
 process the commands that were stashed with 
Effect.stash
Effect().stash


thenReply
 send a reply message to the given 
ActorRef




In the example below, we use a different constructor of 
DurableStateBehavior.withEnforcedReplies
, which creates a 
Behavior
 for a persistent actor that ensures that every command sends a reply back. Hence it will be a compilation error if the returned effect from a 
CommandHandler
 isn’t a 
ReplyEffect
.


Instead of 
Increment
 we will have a new command 
IncrementWithConfirmation
 that, along with persistence will also send an acknowledgement as a reply to the 
ActorRef
 passed in the command. 


Example of effects and side-effects:




Scala




copy
source
sealed trait Command[ReplyMessage] extends CborSerializable
final case class IncrementWithConfirmation(replyTo: ActorRef[Done]) extends Command[Done]
final case class GetValue(replyTo: ActorRef[State]) extends Command[State]

final case class State(value: Int) extends CborSerializable

def counter(persistenceId: PersistenceId): DurableStateBehavior[Command[_], State] = {
  DurableStateBehavior.withEnforcedReplies[Command[_], State](
    persistenceId,
    emptyState = State(0),
    commandHandler = (state, command) =>
      command match {

        case IncrementWithConfirmation(replyTo) =>
          Effect.persist(state.copy(value = state.value + 1)).thenReply(replyTo)(_ => Done)

        case GetValue(replyTo) =>
          Effect.reply(replyTo)(state)
      })
}


Java




copy
source
import akka.Done;
interface Command<ReplyMessage> {}

public static class IncrementWithConfirmation implements Command<Void> {
  public final ActorRef<Done> replyTo;

  public IncrementWithConfirmation(ActorRef<Done> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class GetValue implements Command<State> {
  private final ActorRef<Integer> replyTo;

  public GetValue(ActorRef<Integer> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class State {
  private final int value;

  public State(int value) {
    this.value = value;
  }

  public int get() {
    return value;
  }
}

public static Behavior<Command<?>> create(PersistenceId persistenceId) {
  return new MyPersistentCounterWithReplies(persistenceId);
}

private MyPersistentCounterWithReplies(PersistenceId persistenceId) {
  super(persistenceId);
}

@Override
public State emptyState() {
  return new State(0);
}

@Override
public CommandHandler<Command<?>, State> commandHandler() {
  return newCommandHandlerBuilder()
      .forAnyState()
      .onCommand(
          IncrementWithConfirmation.class,
          (state, command) ->
              Effect()
                  .persist(new State(state.get() + 1))
                  .thenReply(command.replyTo, (st) -> Done.getInstance()))
      .onCommand(
          GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))
      .build();
}




The most common way to have a side-effect is to use the 
thenRun
 method on 
Effect
. In case you have multiple side-effects that needs to be run for several commands, you can factor them out into functions and reuse for all the commands. For example:




Scala




copy
source
// Example factoring out a chained effect to use in several places with `thenRun`
val commonChainedEffects: Mood => Unit = _ => println("Command processed")
// Then in a command handler:
Effect
  .persist(Remembered("Yep")) // persist event
  .thenRun(commonChainedEffects) // add on common chained effect


Java




copy
source
// Example factoring out a chained effect to use in several places with `thenRun`
static final Procedure<ExampleState> commonChainedEffect =
    state -> System.out.println("Command handled!");

      @Override
      public CommandHandler<MyCommand, MyEvent, ExampleState> commandHandler() {
        return newCommandHandlerBuilder()
            .forStateType(ExampleState.class)
            .onCommand(
                Cmd.class,
                (state, cmd) ->
                    Effect()
                        .persist(new Evt(cmd.data))
                        .thenRun(() -> cmd.replyTo.tell(new Ack()))
                        .thenRun(commonChainedEffect))
            .build();
      }




Side effects ordering and guarantees


Any side effects are executed on an at-most-once basis and will not be executed if the persist fails.


Side effects are not run when the actor is restarted or started again after being stopped.


The side effects are executed sequentially, it is not possible to execute side effects in parallel, unless they call out to something that is running concurrently (for example sending a message to another actor).


It’s possible to execute a side effect before persisting the state, but that can result in that the side effect is performed but that the state is not stored if the persist fails.


Cluster Sharding and DurableStateBehavior


Cluster Sharding
 is an excellent fit to spread persistent actors over a cluster, addressing them by id. It makes it possible to have more persistent actors exist in the cluster than what would fit in the memory of one node. Cluster sharding improves the resilience of the cluster. If a node crashes, the persistent actors are quickly started on a new node and can resume operations.


The 
DurableStateBehavior
 can then be run as any plain actor as described in 
actors documentation
, but since Akka Persistence is based on the single-writer principle, the persistent actors are typically used together with Cluster Sharding. For a particular 
persistenceId
 only one persistent actor instance should be active at one time. Cluster Sharding ensures that there is only one active entity (or actor instance) for each id. 


Accessing the ActorContext


If the 
DurableStateBehavior
DurableStateBehavior
 needs to use the 
ActorContext
ActorContext
, for example to spawn child actors, it can be obtained by wrapping construction with 
Behaviors.setup
:




Scala




copy
source
import akka.persistence.typed.state.scaladsl.Effect
import akka.persistence.typed.state.scaladsl.DurableStateBehavior.CommandHandler

def apply(): Behavior[String] =
  Behaviors.setup { context =>
    DurableStateBehavior[String, State](
      persistenceId = PersistenceId.ofUniqueId("myPersistenceId"),
      emptyState = State(0),
      commandHandler = CommandHandler.command { cmd =>
        context.log.info("Got command {}", cmd)
        Effect.none
      })
  }


Java




copy
source
public class MyPersistentBehavior
    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {

  public static Behavior<Command> create(PersistenceId persistenceId) {
    return Behaviors.setup(ctx -> new MyPersistentBehavior(persistenceId, ctx));
  }

  // this makes the context available to the command handler etc.
  private final ActorContext<Command> context;

  // optionally if you only need `ActorContext.getSelf()`
  private final ActorRef<Command> self;

  public MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> ctx) {
    super(persistenceId);
    this.context = ctx;
    this.self = ctx.getSelf();
  }

}




Changing Behavior


After processing a message, actors are able to return the 
Behavior
 that is used for the next message.


As you can see in the above examples this is not supported by persistent actors. Instead, the state is persisted as an 
Effect
 by the 
commandHandler
. 


The reason a new behavior can’t be returned is that behavior is part of the actor’s state and must also carefully be reconstructed during recovery from the persisted state. This would imply that the state needs to be encoded such that the behavior can also be restored from it. That would be very prone to mistakes which is why it is not allowed in Akka Persistence.


For basic actors you can use the same set of command handlers independent of what state the entity is in. For more complex actors it’s useful to be able to change the behavior in the sense that different functions for processing commands may be defined depending on what state the actor is in. This is useful when implementing finite state machine (FSM) like entities.


The next example demonstrates how to define different behavior based on the current 
State
. It shows an actor that represents the state of a blog post. Before a post is started the only command it can process is to 
AddPost
. Once it is started then one can look it up with 
GetPost
, modify it with 
ChangeBody
 or publish it with 
Publish
.


The state is captured by:




Scala




copy
source
sealed trait State

case object BlankState extends State

final case class DraftState(content: PostContent) extends State {
  def withBody(newBody: String): DraftState =
    copy(content = content.copy(body = newBody))

  def postId: String = content.postId
}

final case class PublishedState(content: PostContent) extends State {
  def postId: String = content.postId
}


Java




copy
source
interface State {}

enum BlankState implements State {
  INSTANCE
}

static class DraftState implements State {
  final PostContent content;

  DraftState(PostContent content) {
    this.content = content;
  }

  DraftState withContent(PostContent newContent) {
    return new DraftState(newContent);
  }

  DraftState withBody(String newBody) {
    return withContent(new PostContent(postId(), content.title, newBody));
  }

  String postId() {
    return content.postId;
  }
}

static class PublishedState implements State {
  final PostContent content;

  PublishedState(PostContent content) {
    this.content = content;
  }

  PublishedState withContent(PostContent newContent) {
    return new PublishedState(newContent);
  }

  PublishedState withBody(String newBody) {
    return withContent(new PostContent(postId(), content.title, newBody));
  }

  String postId() {
    return content.postId;
  }
}




The commands, of which only a subset are valid depending on the state:




Scala




copy
source
sealed trait Command
final case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command
final case class AddPostDone(postId: String)
final case class GetPost(replyTo: ActorRef[PostContent]) extends Command
final case class ChangeBody(newBody: String, replyTo: ActorRef[Done]) extends Command
final case class Publish(replyTo: ActorRef[Done]) extends Command
final case class PostContent(postId: String, title: String, body: String)


Java




copy
source
public interface Command {}
public static class AddPost implements Command {
  final PostContent content;
  final ActorRef<AddPostDone> replyTo;

  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {
    this.content = content;
    this.replyTo = replyTo;
  }
}

public static class AddPostDone implements Command {
  final String postId;

  public AddPostDone(String postId) {
    this.postId = postId;
  }
}
public static class GetPost implements Command {
  final ActorRef<PostContent> replyTo;

  public GetPost(ActorRef<PostContent> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class ChangeBody implements Command {
  final String newBody;
  final ActorRef<Done> replyTo;

  public ChangeBody(String newBody, ActorRef<Done> replyTo) {
    this.newBody = newBody;
    this.replyTo = replyTo;
  }
}

public static class Publish implements Command {
  final ActorRef<Done> replyTo;

  public Publish(ActorRef<Done> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class PostContent implements Command {
  final String postId;
  final String title;
  final String body;

  public PostContent(String postId, String title, String body) {
    this.postId = postId;
    this.title = title;
    this.body = body;
  }
}




The command handler to process each command is decided by the state class (or state predicate) that is given to the 
forStateType
 of the 
CommandHandlerBuilder
 and the match cases in the builders.
 
The command handler to process each command is decided by first looking at the state and then the command. It typically becomes two levels of pattern matching, first on the state and then on the command.
 Delegating to methods like 
addPost
, 
changeBody
, 
publish
 etc. is a good practice because the one-line cases give a nice overview of the message dispatch.




Scala




copy
source
private val commandHandler: (State, Command) => Effect[State] = { (state, command) =>
  state match {

    case BlankState =>
      command match {
        case cmd: AddPost => addPost(cmd)
        case _            => Effect.unhandled
      }

    case draftState: DraftState =>
      command match {
        case cmd: ChangeBody  => changeBody(draftState, cmd)
        case Publish(replyTo) => publish(draftState, replyTo)
        case GetPost(replyTo) => getPost(draftState, replyTo)
        case AddPost(_, replyTo) =>
          Effect.unhandled[State].thenRun(_ => replyTo ! StatusReply.Error("Cannot add post while in draft state"))
      }

    case publishedState: PublishedState =>
      command match {
        case GetPost(replyTo) => getPost(publishedState, replyTo)
        case AddPost(_, replyTo) =>
          Effect.unhandled[State].thenRun(_ => replyTo ! StatusReply.Error("Cannot add post, already published"))
        case _ => Effect.unhandled
      }
  }
}

private def addPost(cmd: AddPost): Effect[State] = {
  Effect.persist(DraftState(cmd.content)).thenRun { _ =>
    // After persist is done additional side effects can be performed
    cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))
  }
}

private def changeBody(state: DraftState, cmd: ChangeBody): Effect[State] = {
  Effect.persist(state.withBody(cmd.newBody)).thenRun { _ =>
    cmd.replyTo ! Done
  }
}

private def publish(state: DraftState, replyTo: ActorRef[Done]): Effect[State] = {
  Effect.persist(PublishedState(state.content)).thenRun { _ =>
    println(s"Blog post ${state.postId} was published")
    replyTo ! Done
  }
}

private def getPost(state: DraftState, replyTo: ActorRef[PostContent]): Effect[State] = {
  replyTo ! state.content
  Effect.none
}

private def getPost(state: PublishedState, replyTo: ActorRef[PostContent]): Effect[State] = {
  replyTo ! state.content
  Effect.none
}


Java




copy
source
@Override
public CommandHandler<Command, State> commandHandler() {
  CommandHandlerBuilder<Command, State> builder = newCommandHandlerBuilder();

  builder.forStateType(BlankState.class).onCommand(AddPost.class, this::onAddPost);

  builder
      .forStateType(DraftState.class)
      .onCommand(ChangeBody.class, this::onChangeBody)
      .onCommand(Publish.class, this::onPublish)
      .onCommand(GetPost.class, this::onGetPost);

  builder
      .forStateType(PublishedState.class)
      .onCommand(ChangeBody.class, this::onChangeBody)
      .onCommand(GetPost.class, this::onGetPost);

  builder.forAnyState().onCommand(AddPost.class, (state, cmd) -> Effect().unhandled());

  return builder.build();
}

private Effect<State> onAddPost(AddPost cmd) {
  return Effect()
      .persist(new DraftState(cmd.content))
      .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));
}

private Effect<State> onChangeBody(DraftState state, ChangeBody cmd) {
  return Effect()
      .persist(state.withBody(cmd.newBody))
      .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
}

private Effect<State> onChangeBody(PublishedState state, ChangeBody cmd) {
  return Effect()
      .persist(state.withBody(cmd.newBody))
      .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
}

private Effect<State> onPublish(DraftState state, Publish cmd) {
  return Effect()
      .persist(new PublishedState(state.content))
      .thenRun(
          () -> {
            System.out.println("Blog post published: " + state.postId());
            cmd.replyTo.tell(Done.getInstance());
          });
}

private Effect<State> onGetPost(DraftState state, GetPost cmd) {
  cmd.replyTo.tell(state.content);
  return Effect().none();
}

private Effect<State> onGetPost(PublishedState state, GetPost cmd) {
  cmd.replyTo.tell(state.content);
  return Effect().none();
}




And finally the behavior is created 
from the 
DurableStateBehavior.apply
:




Scala




copy
source
object BlogPostEntityDurableState {
  // commands, state defined here

  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {
    Behaviors.setup { context =>
      context.log.info("Starting BlogPostEntityDurableState {}", entityId)
      DurableStateBehavior[Command, State](persistenceId, emptyState = BlankState, commandHandler)
    }
  }

  // commandHandler defined here
}


Java




copy
source
public class BlogPostEntityDurableState
    extends DurableStateBehavior<
        BlogPostEntityDurableState.Command, BlogPostEntityDurableState.State> {
  // commands and state as in above snippets

  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {
    return Behaviors.setup(
        context -> {
          context.getLog().info("Starting BlogPostEntityDurableState {}", entityId);
          return new BlogPostEntityDurableState(persistenceId);
        });
  }

  private BlogPostEntityDurableState(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return BlankState.INSTANCE;
  }

  // commandHandler, eventHandler as in above snippets
}




This can be refactored one or two steps further by defining the command handlers in the state class as illustrated in 
command handlers in the state
.


There is also an example illustrating an 
optional initial state
.


Replies


The 
Request-Response interaction pattern
 is very common for persistent actors, because you typically want to know if the command was rejected due to validation errors and when accepted you want a confirmation when the events have been successfully stored.


Therefore you typically include a 
ActorRef[ReplyMessageType]
ActorRef<ReplyMessageType>
. If the command can either have a successful response or a validation error returned, the generic response type 
StatusReply[ReplyType]]
 
StatusReply<ReplyType>
 can be used. If the successful reply does not contain a value but is more of an acknowledgement a pre defined 
StatusReply.Ack
StatusReply.ack()
 of type 
StatusReply[Done]
StatusReply<Done>
 can be used.


After validation errors or after persisting events, using a 
thenRun
 side effect, the reply message can be sent to the 
ActorRef
.




Scala




copy
source
final case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command
final case class AddPostDone(postId: String)


Java




copy
source
public static class AddPost implements Command {
  final PostContent content;
  final ActorRef<AddPostDone> replyTo;

  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {
    this.content = content;
    this.replyTo = replyTo;
  }
}

public static class AddPostDone implements Command {
  final String postId;

  public AddPostDone(String postId) {
    this.postId = postId;
  }
}






Scala




copy
source
Effect.persist(DraftState(cmd.content)).thenRun { _ =>
  // After persist is done additional side effects can be performed
  cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))
}


Java




copy
source
return Effect()
    .persist(new DraftState(cmd.content))
    .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));




Since this is such a common pattern there is a reply effect for this purpose. It has the nice property that it can be used to enforce that you do not forget to specify replies when implementing the 
DurableStateBehavior
. If it’s defined with 
DurableStateBehavior.withEnforcedReplies
DurableStateBehaviorWithEnforcedReplies
 there will be compilation errors if the returned effect isn’t a 
ReplyEffect
, which can be created with 
Effect.reply
Effect().reply
, 
Effect.noReply
Effect().noReply
, 
Effect.thenReply
Effect().thenReply
, or 
Effect.thenNoReply
Effect().thenNoReply
.




Scala




copy
source
def apply(persistenceId: PersistenceId): Behavior[Command] = {
  DurableStateBehavior
    .withEnforcedReplies[Command, Account](persistenceId, EmptyAccount, (state, cmd) => state.applyCommand(cmd))
}


Java




copy
source
public class AccountEntity
    extends DurableStateBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Account> {




The commands must have a field of 
ActorRef[ReplyMessageType]
ActorRef<ReplyMessageType>
 that can then be used to send a reply.




Scala




copy
source
sealed trait Command extends CborSerializable
final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command


Java




copy
source
interface Command extends CborSerializable {}




The 
ReplyEffect
 is created with 
Effect.reply
Effect().reply
, 
Effect.noReply
Effect().noReply
, 
Effect.thenReply
Effect().thenReply
, or 
Effect.thenNoReply
Effect().thenNoReply
.


Note that command handlers are defined with 
newCommandHandlerWithReplyBuilder
 when using 
EventSourcedBehaviorWithEnforcedReplies
, as opposed to newCommandHandlerBuilder when using 
EventSourcedBehavior
.




Scala




copy
source
private def deposit(cmd: Deposit) = {
  Effect.persist(copy(balance = balance + cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
}

private def withdraw(cmd: Withdraw) = {
  if (canWithdraw(cmd.amount))
    Effect.persist(copy(balance = balance - cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
  else
    Effect.reply(cmd.replyTo)(
      StatusReply.Error(s"Insufficient balance ${balance} to be able to withdraw ${cmd.amount}"))
}


Java




copy
source
private ReplyEffect<Account> withdraw(OpenedAccount account, Withdraw command) {
  if (!account.canWithdraw(command.amount)) {
    return Effect()
        .reply(
            command.replyTo,
            StatusReply.error("not enough funds to withdraw " + command.amount));
  } else {
    return Effect()
        .persist(account.makeWithdraw(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }
}




These effects will send the reply message even when 
DurableStateBehavior.withEnforcedReplies
DurableStateBehaviorWithEnforcedReplies
 is not used, but then there will be no compilation errors if the reply decision is left out.


Note that the 
noReply
 is a way of making a conscious decision that a reply shouldn’t be sent for a specific command or that a reply will be sent later, perhaps after some asynchronous interaction with other actors or services.


Serialization


The same 
serialization
 mechanism as for actor messages is also used for persistent actors.


You need to enable 
serialization
 for your commands (messages) and state. 
Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference.


Tagging


Persistence allows you to use tags in persistence query. Tagging allows you to identify a subset of states in the durable store and separately consume them as a stream through the 
DurableStateStoreQuery
 interface. 




Scala




copy
source
DurableStateBehavior[Command[_], State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(0),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"))
  .withTag("tag1")


Java




copy
source
public class MyPersistentBehavior
    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {
  @Override
  public String tag() {
    return "tag1";
  }




Wrapping DurableStateBehavior


When creating a 
DurableStateBehavior
, it is possible to wrap 
DurableStateBehavior
 in other behaviors such as 
Behaviors.setup
 in order to access the 
ActorContext
 object. For instance to access the logger from within the 
ActorContext
 to log for debugging the 
commandHandler
.




Scala




copy
source
Behaviors.setup[Command[_]] { context =>
  DurableStateBehavior[Command[_], State](
    persistenceId = PersistenceId.ofUniqueId("abc"),
    emptyState = State(0),
    commandHandler = CommandHandler.command { cmd =>
      context.log.info("Got command {}", cmd)
      Effect.none
    })
}


Java




copy
source
public class MyPersistentBehavior
    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {


  public static Behavior<Command> create(PersistenceId persistenceId) {
    return Behaviors.setup(context -> new MyPersistentBehavior(persistenceId, context));
  }

  private final ActorContext<Command> context;

  private MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> context) {
    super(
        persistenceId,
        SupervisorStrategy.restartWithBackoff(
            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));
    this.context = context;
  }

  @Override
  public CommandHandler<Command, State> commandHandler() {
    return (state, command) -> {
      context.getLog().info("In command handler");
      return Effect().none();
    };
  }
















 
Persistence (Durable State)






Style Guide 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/other-modules.html#akka-management
Other Akka modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic




















Other Akka modules


This page describes modules that compliment libraries from the Akka core. See 
this overview
 instead for a guide on the core modules.


Akka HTTP


A full server- and client-side HTTP stack on top of akka-actor and akka-stream.


Akka gRPC


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams.


Alpakka


Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka.


Alpakka Kafka Connector


The Alpakka Kafka Connector connects Apache Kafka with Akka Streams.


Akka Projections


Akka Projections let you process a stream of events or records from a source to a projected model or external system.


Cassandra Plugin for Akka Persistence


An Akka Persistence journal and snapshot store backed by Apache Cassandra.


JDBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on 
Slick
.


R2DBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on 
R2DBC
.


Akka Management




Akka Management
 provides a central HTTP endpoint for Akka management extensions.


Akka Cluster Bootstrap
 helps bootstrapping an Akka cluster using Akka Discovery.


Akka Management Kubernetes Rolling Updates
 for smooth rolling updates.


Akka Management Cluster HTTP
 provides HTTP endpoints for introspecting and managing Akka clusters.


Akka Discovery for Kubernetes, Consul, Marathon, and AWS


Kubernetes Lease




Akka Diagnostics




Akka Thread Starvation Detector


Akka Configuration Checker




Akka Insights


Intelligent monitoring and observability purpose-built for Akka: 
Lightbend Telemetry


Community Projects


Akka has a vibrant and passionate user community, the members of which have created many independent projects using Akka as well as extensions to it. See 
Community Projects
.














 
Extending Akka






Package, Deploy and Run 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/actor-discovery.html
Actor discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery




Dependency


Obtaining Actor references


Receptionist


Cluster Receptionist


Receptionist Scalability




Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery




Dependency


Obtaining Actor references


Receptionist


Cluster Receptionist


Receptionist Scalability




Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actor discovery


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Actors
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Obtaining Actor references


There are two general ways to obtain 
Actor references
: by 
creating actors
 and by discovery using the 
Receptionist
.


You can pass actor references between actors as constructor parameters or part of messages.


Sometimes you need something to bootstrap the interaction, for example when actors are running on different nodes in the Cluster or when “dependency injection” with constructor parameters is not applicable.


Receptionist


When an actor needs to be discovered by another actor but you are unable to put a reference to it in an incoming message, you can use the 
Receptionist
Receptionist
. It supports both local and cluster (see 
cluster
). You register the specific actors that should be discoverable from each node in the local 
Receptionist
 instance. The API of the receptionist is also based on actor messages. This registry of actor references is then automatically distributed to all other nodes in the case of a cluster. You can lookup such actors with the key that was used when they were registered. The reply to such a 
Find
 request is a 
Listing
, which contains a 
Set
 of actor references that are registered for the key. Note that several actors can be registered to the same key.


The registry is dynamic. New actors can be registered during the lifecycle of the system. Entries are removed when registered actors are stopped, manually deregistered or the node they live on is removed from the 
Cluster
. To facilitate this dynamic aspect you can also subscribe to changes with the 
Receptionist.Subscribe
 message. It will send 
Listing
 messages to the subscriber, first with the set of entries upon subscription, then whenever the entries for a key are changed.


These imports are used in the following example:




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.receptionist.Receptionist
import akka.actor.typed.receptionist.ServiceKey
import akka.actor.typed.scaladsl.Behaviors


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.receptionist.Receptionist;
import akka.actor.typed.receptionist.ServiceKey;




First we create a 
PingService
 actor and register it with the 
Receptionist
 against a 
ServiceKey
ServiceKey
 that will later be used to lookup the reference:




Scala




copy
source
object PingService {
  val PingServiceKey = ServiceKey[Ping]("pingService")

  final case class Ping(replyTo: ActorRef[Pong.type])
  case object Pong

  def apply(): Behavior[Ping] = {
    Behaviors.setup { context =>
      context.system.receptionist ! Receptionist.Register(PingServiceKey, context.self)

      Behaviors.receiveMessage {
        case Ping(replyTo) =>
          context.log.info("Pinged by {}", replyTo)
          replyTo ! Pong
          Behaviors.same
      }
    }
  }
}


Java




copy
source
public class PingService {

  public static final ServiceKey<Ping> pingServiceKey =
      ServiceKey.create(Ping.class, "pingService");

  public static class Pong {}

  public static class Ping {
    private final ActorRef<Pong> replyTo;

    public Ping(ActorRef<Pong> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static Behavior<Ping> create() {
    return Behaviors.setup(
        context -> {
          context
              .getSystem()
              .receptionist()
              .tell(Receptionist.register(pingServiceKey, context.getSelf()));

          return new PingService(context).behavior();
        });
  }

  private final ActorContext<Ping> context;

  private PingService(ActorContext<Ping> context) {
    this.context = context;
  }

  private Behavior<Ping> behavior() {
    return Behaviors.receive(Ping.class).onMessage(Ping.class, this::onPing).build();
  }

  private Behavior<Ping> onPing(Ping msg) {
    context.getLog().info("Pinged by {}", msg.replyTo);
    msg.replyTo.tell(new Pong());
    return Behaviors.same();
  }
}




Then we have another actor that requires a 
PingService
 to be constructed:




Scala




copy
source
object Pinger {
  def apply(pingService: ActorRef[PingService.Ping]): Behavior[PingService.Pong.type] = {
    Behaviors.setup { context =>
      pingService ! PingService.Ping(context.self)

      Behaviors.receiveMessage { _ =>
        context.log.info("{} was ponged!!", context.self)
        Behaviors.stopped
      }
    }
  }
}


Java




copy
source
public class Pinger {
  private final ActorContext<PingService.Pong> context;
  private final ActorRef<PingService.Ping> pingService;

  private Pinger(ActorContext<PingService.Pong> context, ActorRef<PingService.Ping> pingService) {
    this.context = context;
    this.pingService = pingService;
  }

  public static Behavior<PingService.Pong> create(ActorRef<PingService.Ping> pingService) {
    return Behaviors.setup(
        ctx -> {
          pingService.tell(new PingService.Ping(ctx.getSelf()));
          return new Pinger(ctx, pingService).behavior();
        });
  }

  private Behavior<PingService.Pong> behavior() {
    return Behaviors.receive(PingService.Pong.class)
        .onMessage(PingService.Pong.class, this::onPong)
        .build();
  }

  private Behavior<PingService.Pong> onPong(PingService.Pong msg) {
    context.getLog().info("{} was ponged!!", context.getSelf());
    return Behaviors.stopped();
  }
}




Finally in the guardian actor we spawn the service as well as subscribing to any actors registering against the 
ServiceKey
ServiceKey
. Subscribing means that the guardian actor will be informed of any new registrations via a 
Listing
 message:




Scala




copy
source
object Guardian {
  def apply(): Behavior[Nothing] = {
    Behaviors
      .setup[Receptionist.Listing] { context =>
        context.spawnAnonymous(PingService())
        context.system.receptionist ! Receptionist.Subscribe(PingService.PingServiceKey, context.self)

        Behaviors.receiveMessagePartial[Receptionist.Listing] {
          case PingService.PingServiceKey.Listing(listings) =>
            listings.foreach(ps => context.spawnAnonymous(Pinger(ps)))
            Behaviors.same
        }
      }
      .narrow
  }
}


Java




copy
source
public class Guardian {

  public static Behavior<Void> create() {
    return Behaviors.setup(
            (ActorContext<Receptionist.Listing> context) -> {
              context
                  .getSystem()
                  .receptionist()
                  .tell(
                      Receptionist.subscribe(
                          PingService.pingServiceKey, context.getSelf().narrow()));
              context.spawnAnonymous(PingService.create());

              return new Guardian(context).behavior();
            })
        .unsafeCast(); // Void
  }

  private final ActorContext<Receptionist.Listing> context;

  private Guardian(ActorContext<Receptionist.Listing> context) {
    this.context = context;
  }

  private Behavior<Receptionist.Listing> behavior() {
    return Behaviors.receive(Receptionist.Listing.class)
        .onMessage(Receptionist.Listing.class, this::onListing)
        .build();
  }

  private Behavior<Receptionist.Listing> onListing(Receptionist.Listing msg) {
    msg.getServiceInstances(PingService.pingServiceKey)
        .forEach(pingService -> context.spawnAnonymous(Pinger.create(pingService)));
    return Behaviors.same();
  }
}




Each time a new (which is just a single time in this example) 
PingService
 is registered the guardian actor spawns a 
Pinger
 for each currently known 
PingService
. The 
Pinger
 sends a 
Ping
 message and when receiving the 
Pong
 reply it stops.


In above example we used 
Receptionist.Subscribe
, but it’s also possible to request a single 
Listing
 of the current state without receiving further updates by sending the 
Receptionist.Find
 message to the receptionist. An example of using 
Receptionist.Find
:




Scala




copy
source
object PingManager {
  sealed trait Command
  case object PingAll extends Command
  private case class ListingResponse(listing: Receptionist.Listing) extends Command

  def apply(): Behavior[Command] = {
    Behaviors.setup[Command] { context =>
      val listingResponseAdapter = context.messageAdapter[Receptionist.Listing](ListingResponse.apply)

      context.spawnAnonymous(PingService())

      Behaviors.receiveMessagePartial {
        case PingAll =>
          context.system.receptionist ! Receptionist.Find(PingService.PingServiceKey, listingResponseAdapter)
          Behaviors.same
        case ListingResponse(PingService.PingServiceKey.Listing(listings)) =>
          listings.foreach(ps => context.spawnAnonymous(Pinger(ps)))
          Behaviors.same
      }
    }
  }
}


Java




copy
source
public class PingManager {

  interface Command {}

  enum PingAll implements Command {
    INSTANCE
  }

  private static class ListingResponse implements Command {
    final Receptionist.Listing listing;

    private ListingResponse(Receptionist.Listing listing) {
      this.listing = listing;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(context -> new PingManager(context).behavior());
  }

  private final ActorContext<Command> context;
  private final ActorRef<Receptionist.Listing> listingResponseAdapter;

  private PingManager(ActorContext<Command> context) {
    this.context = context;
    this.listingResponseAdapter =
        context.messageAdapter(Receptionist.Listing.class, ListingResponse::new);

    context.spawnAnonymous(PingService.create());
  }

  private Behavior<Command> behavior() {
    return Behaviors.receive(Command.class)
        .onMessage(PingAll.class, notUsed -> onPingAll())
        .onMessage(ListingResponse.class, response -> onListing(response.listing))
        .build();
  }

  private Behavior<Command> onPingAll() {
    context
        .getSystem()
        .receptionist()
        .tell(Receptionist.find(PingService.pingServiceKey, listingResponseAdapter));
    return Behaviors.same();
  }

  private Behavior<Command> onListing(Receptionist.Listing msg) {
    msg.getServiceInstances(PingService.pingServiceKey)
        .forEach(pingService -> context.spawnAnonymous(Pinger.create(pingService)));
    return Behaviors.same();
  }
}




Also note how a 
messageAdapter
messageAdapter
 is used to convert the 
Receptionist.Listing
 to a message type that the 
PingManager
 understands.


If a server no longer wish to be associated with a service key it can deregister using the command 
Receptionist.Deregister
 which will remove the association and inform all subscribers.


The command can optionally send an acknowledgement once the local receptionist has removed the registration. The acknowledgement does not guarantee that all subscribers has seen that the instance has been removed, it may still receive messages from subscribers for some time after this.




Scala




copy
source
context.system.receptionist ! Receptionist.Deregister(PingService.PingServiceKey, context.self)


Java




copy
source
context
    .getSystem()
    .receptionist()
    .tell(Receptionist.deregister(PingService.pingServiceKey, context.getSelf()));




Cluster Receptionist


The 
Receptionist
 also works in a cluster, an actor registered to the receptionist will appear in the receptionist of the other nodes of the cluster.


The state for the receptionist is propagated via 
distributed data
 which means that each node will eventually reach the same set of actors per 
ServiceKey
.


Subscription
s and 
Find
 queries to a clustered receptionist will keep track of cluster reachability and only list registered actors that are reachable. The full set of actors, including unreachable ones, is available through 
Listing.allServiceInstances
Listing.getAllServiceInstances
.


One important difference from local only receptions are the serialization concerns, all messages sent to and back from an actor on another node must be serializable, see 
serialization
.


Receptionist Scalability


The receptionist does not scale up to any number of services or very high turnaround of services. It will likely handle up to thousands or tens of thousands of services. Use cases with higher demands the receptionist for initial contact between actors on the nodes while the actual logic of those is up to the applications own actors. 














 
Fault Tolerance






Routers 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/replicated-eventsourcing.html
Replicated Event Sourcing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing




Relaxing the single-writer principle for availability


API


Resolving conflicting updates


Side effects


How it works


Running projections


Examples


Journal Support


Migrating from non-replicated




CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing




Relaxing the single-writer principle for availability


API


Resolving conflicting updates


Side effects


How it works


Running projections


Examples


Journal Support


Migrating from non-replicated




CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Replicated Event Sourcing


Event Sourcing
 with 
EventSourcedBehavior
s is based on the single writer principle, which means that there can only be one active instance of a 
EventSourcedBehavior
 with a given 
persistenceId
. Otherwise, multiple instances would store interleaving events based on different states, and when these events would later be replayed it would not be possible to reconstruct the correct state.


This restriction means that in the event of network partitions, and for a short time during rolling re-deploys, some 
EventSourcedBehavior
 actors are unavailable.


Replicated Event Sourcing enables running multiple replicas of each entity. There is automatic replication of every event persisted to all replicas. This makes it possible to implement patterns such as active-active and hot standby.


For instance, a replica can be run per:




Cloud provider region


Data Center


Availability zone or rack




The motivations are:




Redundancy to tolerate failures in one location and still be operational


Serve requests from a location near the user to provide better responsiveness


Allow updates to an entity from several locations


Balance the load over many servers




However, the event handler must be able to 
handle concurrent events
 as when replication is enabled the single-writer guarantee is not maintained like it is with a normal 
EventSourcedBehavior
.


The state of a replicated 
EventSourcedBehavior
 is 
eventually consistent
. Event replication may be delayed due to network partitions and outages, which means that the event handler and those reading the state must be designed to handle this.


To be able to use Replicated Event Sourcing the journal and snapshot store used is required to have specific support for the metadata that the replication needs (see 
Journal Support
).


The 
Replicated Event Sourcing video
 is a good starting point describing the use cases and motivation for when to use Replicated Event Sourcing. The second part, 
Replicated Event Sourcing data modelling
 guides you to find a suitable model for your use-case.


Relaxing the single-writer principle for availability


Taking the example of using Replicated Event Sourcing to run a replica per region.


When there is no network partitions and no concurrent writes the events stored by an 
EventSourcedBehavior
 at one replica can be replicated and consumed by another (corresponding) replica in another region without any concerns. Such replicated events can simply be applied to the local state.




The interesting part begins when there are concurrent writes by 
EventSourcedBehavior
 replicas. That is more likely to happen when there is a network partition, but it can also happen when there are no network issues. They simply write at the “same time” before the events from the other side have been replicated and consumed.




The event handler logic for applying events to the state of the entity must be aware of that such concurrent updates can occur, and it must be modeled to handle such conflicts. This means that it should typically have the same characteristics as a Conflict Free Replicated Data Type (CRDT). With a CRDT there are by definition no conflicts, the events can always be applied. The library provides some general purpose CRDTs, but the logic of how to apply events can also be defined by an application specific function.


For example, sometimes it’s enough to use application specific timestamps to decide which update should win.


To assist in implementing the event handler the Replicated Event Sourcing detects these conflicts.


API


The same API as regular 
EventSourcedBehavior
s
A very similar API to the regular 
EventSourcedBehavior
 or 
EventSourcedOnCommandBehavior
 found in 
ReplicatedEventSourcedBehavior
 and 
ReplicatedEventSourcedOnCommandBehavior
 is used to define the logic. 


Consuming events via gRPC transport


Since Akka 2.8.0 a gRPC based transport is the recommended way to set up the replication of events between the replicas.


The functionality is provided through the Akka Projection gRPC module, see the details about how to use it up in the 
Akka Projection gRPC documentation


Complete samples of the gRPC transport set up can be found in the 
Akka Distributed Cluster Guide
.


Consuming events via direct access to replica databases


It is also possible to consume events with a direct connection to the database of each replica.


See 
Replicated Event Sourcing replication via direct access to replica databases


Resolving conflicting updates


Conflict free replicated data types


Writing code to resolve conflicts can be complicated to get right. One well-understood technique to create eventually-consistent systems is to model your state as a Conflict Free Replicated Data Type, a CRDT. There are two types of CRDTs; operation-based and state-based. For Replicated Event Sourcing the operation-based is a good fit, since the events represent the operations. Note that this is distinct from the CRDT’s implemented in 
Akka Distributed Data
, which are state-based rather than operation-based.


The rule for operation-based CRDT’s is that the operations must be commutative â in other words, applying the same events (which represent the operations) in any order should always produce the same final state. You may assume each event is applied only once, with 
causal delivery order
.


The following CRDTs are included that can you can use as the state or part of the state in the entity:




LwwTime
LwwTime


Counter
Counter


ORSet
ORSet




Akka serializers are included for all these types and can be used to serialize when 
embedded in Jackson
.


An example would be a movies watch list that is represented by the general purpose 
ORSet
ORSet
 CRDT. 
ORSet
 is short for Observed Remove Set. Elements can be added and removed any number of times. Concurrent add wins over remove. It is an operation based CRDT where the delta of an operation (add/remove) can be represented as an event.


Such movies watch list example:




Scala




copy
source
object MovieWatchList {
  sealed trait Command
  final case class AddMovie(movieId: String) extends Command
  final case class RemoveMovie(movieId: String) extends Command
  final case class GetMovieList(replyTo: ActorRef[MovieList]) extends Command
  final case class MovieList(movieIds: Set[String])

  def apply(entityId: String, replicaId: ReplicaId, allReplicaIds: Set[ReplicaId]): Behavior[Command] = {
    ReplicatedEventSourcing.commonJournalConfig(
      ReplicationId("movies", entityId, replicaId),
      allReplicaIds,
      PersistenceTestKitReadJournal.Identifier) { replicationContext =>
      EventSourcedBehavior[Command, ORSet.DeltaOp, ORSet[String]](
        replicationContext.persistenceId,
        ORSet.empty(replicationContext.replicaId),
        (state, cmd) => commandHandler(state, cmd),
        (state, event) => eventHandler(state, event))
    }
  }

  private def commandHandler(state: ORSet[String], cmd: Command): Effect[ORSet.DeltaOp, ORSet[String]] = {
    cmd match {
      case AddMovie(movieId) =>
        Effect.persist(state + movieId)
      case RemoveMovie(movieId) =>
        Effect.persist(state - movieId)
      case GetMovieList(replyTo) =>
        replyTo ! MovieList(state.elements)
        Effect.none
    }
  }

  private def eventHandler(state: ORSet[String], event: ORSet.DeltaOp): ORSet[String] = {
    state.applyOperation(event)
  }

}


Java




copy
source
public final class MovieWatchList
    extends ReplicatedEventSourcedBehavior<MovieWatchList.Command, ORSet.DeltaOp, ORSet<String>> {

  interface Command {}

  public static class AddMovie implements Command {
    public final String movieId;

    public AddMovie(String movieId) {
      this.movieId = movieId;
    }
  }

  public static class RemoveMovie implements Command {
    public final String movieId;

    public RemoveMovie(String movieId) {
      this.movieId = movieId;
    }
  }

  public static class GetMovieList implements Command {
    public final ActorRef<MovieList> replyTo;

    public GetMovieList(ActorRef<MovieList> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class MovieList {
    public final Set<String> movieIds;

    public MovieList(Set<String> movieIds) {
      this.movieIds = Collections.unmodifiableSet(movieIds);
    }
  }

  public static Behavior<Command> create(
      String entityId, ReplicaId replicaId, Set<ReplicaId> allReplicas) {
    return ReplicatedEventSourcing.commonJournalConfig(
        new ReplicationId("movies", entityId, replicaId),
        allReplicas,
        PersistenceTestKitReadJournal.Identifier(),
        MovieWatchList::new);
  }

  private MovieWatchList(ReplicationContext replicationContext) {
    super(replicationContext);
  }

  @Override
  public ORSet<String> emptyState() {
    return ORSet.empty(getReplicationContext().replicaId());
  }

  @Override
  public CommandHandler<Command, ORSet.DeltaOp, ORSet<String>> commandHandler() {
    return newCommandHandlerBuilder()
        .forAnyState()
        .onCommand(
            AddMovie.class, (state, command) -> Effect().persist(state.add(command.movieId)))
        .onCommand(
            RemoveMovie.class,
            (state, command) -> Effect().persist(state.remove(command.movieId)))
        .onCommand(
            GetMovieList.class,
            (state, command) -> {
              command.replyTo.tell(new MovieList(state.getElements()));
              return Effect().none();
            })
        .build();
  }

  @Override
  public EventHandler<ORSet<String>, ORSet.DeltaOp> eventHandler() {
    return newEventHandlerBuilder().forAnyState().onAnyEvent(ORSet::applyOperation);
  }
}




The 
Auction example
 is a more comprehensive example that illustrates how application-specific rules can be used to implement an entity with CRDT semantics.


Last writer wins


Sometimes it is enough to use timestamps to decide which update should win. Such approach relies on synchronized clocks, and clocks of different machines will always be slightly out of sync. Timestamps should therefore only be used when the choice of value is not important for concurrent updates occurring within the clock skew.


In general, last writer wins means that the event is used if the timestamp of the event is later (higher) than the timestamp of previous local update, otherwise the event is discarded. There is no built-in support for last writer wins, because it must often be combined with more application specific aspects.




There is a small utility class 
LwwTime
LwwTime
 that can be useful for implementing last writer wins semantics. It contains a timestamp representing current time when the event was persisted and an identifier of the replica that persisted it. When comparing two 
LwwTime
LwwTime
 the greatest timestamp wins. The replica identifier is used if the two timestamps are equal, and then the one from the 
replicaId
 sorted first in alphanumeric order wins.




Scala




copy
source
private def eventHandler(
    ctx: ActorContext[Command],
    replicationContext: ReplicationContext,
    state: BlogState,
    event: Event): BlogState = {
  ctx.log.info(s"${replicationContext.entityId}:${replicationContext.replicaId} Received event $event")
  event match {
    case PostAdded(_, content, timestamp) =>
      if (timestamp.isAfter(state.contentTimestamp)) {
        val s = state.withContent(content, timestamp)
        ctx.log.info("Updating content. New content is {}", s)
        s
      } else {
        ctx.log.info("Ignoring event as timestamp is older")
        state
      }
    case BodyChanged(_, newContent, timestamp) =>
      if (timestamp.isAfter(state.contentTimestamp))
        state.withContent(newContent, timestamp)
      else state
    case Published(_) =>
      state.copy(published = true)
  }
}


Java




copy
source
@Override
public EventHandler<BlogState, Event> eventHandler() {
  return newEventHandlerBuilder()
      .forAnyState()
      .onEvent(PostAdded.class, this::onPostAdded)
      .onEvent(BodyChanged.class, this::onBodyChanged)
      .onEvent(Published.class, this::onPublished)
      .build();
}

private BlogState onPostAdded(BlogState state, PostAdded event) {
  if (event.timestamp.isAfter(state.contentTimestamp)) {
    BlogState s = state.withContent(event.content, event.timestamp);
    context.getLog().info("Updating content. New content is {}", s);
    return s;
  } else {
    context.getLog().info("Ignoring event as timestamp is older");
    return state;
  }
}

private BlogState onBodyChanged(BlogState state, BodyChanged event) {
  if (event.timestamp.isAfter(state.contentTimestamp)) {
    return state.withContent(event.content, event.timestamp);
  } else {
    return state;
  }
}

private BlogState onPublished(BlogState state, Published event) {
  return state.publish();
}




When creating the 
LwwTime
 it is good to have a monotonically increasing timestamp, and for that the 
increase
 method in 
LwwTime
 can be used:




Scala




copy
source
private def commandHandler(
    ctx: ActorContext[Command],
    replicationContext: ReplicationContext,
    state: BlogState,
    cmd: Command): Effect[Event, BlogState] = {
  cmd match {
    case AddPost(_, content, replyTo) =>
      val evt =
        PostAdded(
          replicationContext.entityId,
          content,
          state.contentTimestamp.increase(replicationContext.currentTimeMillis(), replicationContext.replicaId))
      Effect.persist(evt).thenRun { _ =>
        replyTo ! AddPostDone(replicationContext.entityId)
      }
    case ChangeBody(_, newContent, replyTo) =>
      val evt =
        BodyChanged(
          replicationContext.entityId,
          newContent,
          state.contentTimestamp.increase(replicationContext.currentTimeMillis(), replicationContext.replicaId))
      Effect.persist(evt).thenRun { _ =>
        replyTo ! Done
      }
    case p: Publish =>
      Effect.persist(Published("id")).thenRun { _ =>
        p.replyTo ! Done
      }
    case gp: GetPost =>
      ctx.log.info("GetPost {}", state.content)
      state.content.foreach(content => gp.replyTo ! content)
      Effect.none
  }
}


Java




copy
source
@Override
public CommandHandler<Command, Event, BlogState> commandHandler() {
  return newCommandHandlerBuilder()
      .forAnyState()
      .onCommand(AddPost.class, this::onAddPost)
      .onCommand(ChangeBody.class, this::onChangeBody)
      .onCommand(Publish.class, this::onPublish)
      .onCommand(GetPost.class, this::onGetPost)
      .build();
}

private Effect<Event, BlogState> onAddPost(BlogState state, AddPost command) {
  PostAdded evt =
      new PostAdded(
          getReplicationContext().entityId(),
          command.content,
          state.contentTimestamp.increase(
              getReplicationContext().currentTimeMillis(),
              getReplicationContext().replicaId()));
  return Effect()
      .persist(evt)
      .thenRun(() -> command.replyTo.tell(new AddPostDone(getReplicationContext().entityId())));
}

private Effect<Event, BlogState> onChangeBody(BlogState state, ChangeBody command) {
  BodyChanged evt =
      new BodyChanged(
          getReplicationContext().entityId(),
          command.newContent,
          state.contentTimestamp.increase(
              getReplicationContext().currentTimeMillis(),
              getReplicationContext().replicaId()));
  return Effect().persist(evt).thenRun(() -> command.replyTo.tell(Done.getInstance()));
}

private Effect<Event, BlogState> onPublish(BlogState state, Publish command) {
  Published evt = new Published(getReplicationContext().entityId());
  return Effect().persist(evt).thenRun(() -> command.replyTo.tell(Done.getInstance()));
}

private Effect<Event, BlogState> onGetPost(BlogState state, GetPost command) {
  context.getLog().info("GetPost {}", state.content);
  if (state.content.isPresent()) command.replyTo.tell(state.content.get());
  return Effect().none();
}




The nature of last writer wins means that if you only have one timestamp for the state the events must represent an update of the full state. Otherwise, there is a risk that the state in different replicas will be different and not eventually converge.


An example of that would be an entity representing a blog post and the fields 
author
 and 
title
 could be updated separately with events 
AuthorChanged(newAuthor: String)
new AuthorChanged(newAuthor)
 and 
TitleChanged(newTitle: String)
new TitleChanged(newTitle)
.


Let’s say the blog post is created and the initial state of 
title=Akka, author=unknown
 is in sync in both replicas 
DC-A
 and `DC-B.


In 
DC-A
 author is changed to “Bob” at time 
100
. Before that event has been replicated over to 
DC-B
 the title is updated to “Akka News” at time 
101
 in 
DC-B
. When the events have been replicated the result will be:


DC-A
: The title update is later so the event is used and new state is 
title=Akka News, author=Bob


DC-B
: The author update is earlier so the event is discarded and state is 
title=Akka News, author=unknown


The problem here is that the partial update of the state is not applied on both sides, so the states have diverged and will not become the same.


To solve this with last writer wins the events must carry the full state, such as 
AuthorChanged(newContent: PostContent)
new AuthorChanged(newContent)
 and 
TitleChanged(newContent: PostContent)
new TitleChanged(newContent)
. Then the result would eventually be 
title=Akka News, author=unknown
 on both sides. The author update is lost but that is because the changes were performed concurrently. More important is that the state is eventually consistent.


Including the full state in each event is often not desired. An event typically represent a change, a delta. Then one can use several timestamps, one for each set of fields that can be updated together. In the above example one could use one timestamp for the title and another for the author. Then the events could represent changes to parts of the full state, such as 
AuthorChanged(newAuthor: String)
new AuthorChanged(newAuthor)
 and 
TitleChanged(newTitle: String)
new TitleChanged(newTitle)
.


Side effects


In most cases it is recommended to do side effects as 
described for 
EventSourcedBehavior
s
.


Side effects from the event handler are generally discouraged because the event handlers are also used during replay and when consuming replicated events and that would result in undesired re-execution of the side effects.


Uses cases for doing side effects in the event handler:




Doing a side effect only in a single replica


Doing a side effect once all replicas have seen an event


A side effect for a replicated event


A side effect when a conflict has occurred




There is no built in support for knowing an event has been replicated to all replicas but it can be modelled in your state. For some use cases you may need to trigger side effects after consuming replicated events. For example when an auction has been closed in all regions and all bids have been replicated. 


The  contains the current replica, the origin replica for the event processes, and if a recovery is running. These can be used to implement side effects that take place once events are fully replicated. If the side effect should happen only once then a particular replica can be designated to do it. The 
Auction example
 uses these techniques.


How it works


You donât have to read this section to be able to use the feature, but to use the abstraction efficiently and for the right type of use cases it can be good to understand how itâs implemented. For example, it should give you the right expectations of the overhead that the solution introduces compared to using just 
EventSourcedBehavior
s.


Causal delivery order


Causal delivery order means that events persisted in one replica are read in the same order in other replicas. The order of concurrent events is undefined, which should be no problem when using 
CRDT’s
 and otherwise will be detected via the 
ReplicationContext
 concurrent method.


For example:


DC-1: write e1
DC-2: read e1, write e2
DC-1: read e2, write e3



In the above example the causality is 
e1 -> e2 -> e3
. Also in a third replica DC-3 these events will be read in the same order e1, e2, e3.


Another example with concurrent events:


DC1: write e1
DC2: read e1, write e2
DC1: write e3 (e2 and e3 are concurrent)
DC1: read e2
DC2: read e3



e2 and e3 are concurrent, i.e. they don’t have a causal relation: DC1 sees them in the order “e1, e3, e2”, while DC2 sees them as “e1, e2, e3”.


A third replica may also see the events as either “e1, e3, e2” or as “e1, e2, e3”.


Concurrent updates


Replicated Event Sourcing automatically tracks causality between events from different replicas using 
version vectors
.




Each replica “owns” a slot in the version vector and increases its counter when an event is persisted. The version vector is stored with the event, and when a replicated event is consumed the version vector of the event is merged with the local version vector.


When comparing two version vectors 
v1
 and 
v2
: 




v1
 is SAME as 
v2
 iff for all i 
v1(i) == v2(i)


v1
is BEFORE 
v2
 iff for all i 
v1(i) <= v2(i)
 and there exist a j such that 
v1(j) < v2(j)


v1
is AFTER 
v2
 iff for all i 
v1(i) >= v2(i)
 and there exist a j such that 
v1(j) > v2(j)


v1
is CONCURRENT with 
v2
 otherwise




Running projections


Just like for regular 
EventSourcedBehavior
s it is possible to create read side projections with 
Akka Projections
.


When running a Projection in each replica you should be aware of that it will receive all events, from all replicas. Since the events can arrive in different order, they can also come in different order per replica.


Another approach is to only process events from the local replica. Then you would need to filter events in the Projection. The origin replica id is available in the 
ReplicationContext
ReplicationContext
 and you could add this information to the event itself or include it as a tag. The Projection handler can then use this to decide how to process the event.


Examples


More examples can be found in 
Replicated Event Sourcing Examples


Journal Support


For a journal plugin to support replication it needs to store and read metadata for each event if it is defined in the   
metadata
 field. To attach the metadata after writing it, 
PersistentRepr.withMetadata
 is used. The 
JournalSpec
JournalSpec
 in the Persistence TCK provides  a capability flag 
supportsMetadata
 to toggle verification that metadata is handled correctly.


For a snapshot plugin to support replication it needs to store and read metadata for the snapshot if it is defined in the  
metadata
 field. To attach the metadata when reading the snapshot the 
akka.persistence.SnapshotMetadata.apply
 factory overload taking a 
metadata
 parameter is used. The 
SnapshotStoreSpec
SnapshotStoreSpec
 in the Persistence TCK provides a capability flag 
supportsMetadata
 to toggle verification that metadata is handled correctly.


The following plugins support Replicated Event Sourcing over gRPC:




Akka Persistence R2DBC
 versions 1.1.0+




Migrating from non-replicated


It is possible to migrate from an ordinary single-writer 
EventSourcedBehavior
 to a 
ReplicatedEventSourcedBehavior
. The events are stored in the same way, aside from some metadata that is filled in automatically if it’s missing.


The 
ReplicaId
 for the where the original entity was located should be empty. This makes sure that the same 
PersistenceId
 and same events are used for the original replica.


The aspects of 
Resolving conflicting updates
 must be considered in the logic of the event handler when migrating to Replicated Event Sourcing.














 
Event Sourcing






CQRS 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/stream/stream-design.html
Design Principles behind Akka Streams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams




What shall users of Akka Streams expect?


Interoperation with other Reactive Streams implementations


What shall users of streaming libraries expect?


The difference between Error and Failure




Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams




What shall users of Akka Streams expect?


Interoperation with other Reactive Streams implementations


What shall users of streaming libraries expect?


The difference between Error and Failure




Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Design Principles behind Akka Streams


It took quite a while until we were reasonably happy with the look and feel of the API and the architecture of the implementation, and while being guided by intuition the design phase was very much exploratory research. This section details the findings and codifies them into a set of principles that have emerged during the process.
Note


As detailed in the introduction keep in mind that the Akka Streams API is completely decoupled from the Reactive Streams interfaces which are an implementation detail for how to pass stream data between individual operators.


What shall users of Akka Streams expect?


Akka is built upon a conscious decision to offer APIs that are minimal and consistentâas opposed to easy or intuitive. The credo is that we favor explicitness over magic, and if we provide a feature then it must work always, no exceptions. Another way to say this is that we minimize the number of rules a user has to learn instead of trying to keep the rules close to what we think users might expect.


From this follows that the principles implemented by Akka Streams are:




all features are explicit in the API, no magic


supreme compositionality: combined pieces retain the function of each part


exhaustive model of the domain of distributed bounded stream processing




This means that we provide all the tools necessary to express any stream processing topology, that we model all the essential aspects of this domain (back-pressure, buffering, transformations, failure recovery, etc.) and that whatever the user builds is reusable in a larger context.


Akka Streams does not send dropped stream elements to the dead letter office


One important consequence of offering only features that can be relied upon is the restriction that Akka Streams cannot ensure that all objects sent through a processing topology will be processed. Elements can be dropped for a number of reasons:




plain user code can consume one element in a 
map(…)
 operator and produce an entirely different one as its result


common stream operators drop elements intentionally, e.g. take/drop/filter/conflate/buffer/â¦


stream failure will tear down the stream without waiting for processing to finish, all elements that are in flight will be discarded


stream cancellation will propagate upstream (e.g. from a 
take
 operator) leading to upstream processing steps being terminated without having processed all of their inputs




This means that sending JVM objects into a stream that need to be cleaned up will require the user to ensure that this happens outside of the Akka Streams facilities (e.g. by cleaning them up after a timeout or when their results are observed on the stream output, or by using other means like finalizers etc.).


Resulting Implementation Considerations


Compositionality entails reusability of partial stream topologies, which led us to the lifted approach of describing data flows as (partial) graphs that can act as composite sources, flows (a.k.a. pipes) and sinks of data. These building blocks shall then be freely shareable, with the ability to combine them freely to form larger graphs. The representation of these pieces must therefore be an immutable blueprint that is materialized in an explicit step in order to start the stream processing. The resulting stream processing engine is then also immutable in the sense of having a fixed topology that is prescribed by the blueprint. Dynamic networks need to be modeled by explicitly using the Reactive Streams interfaces for plugging different engines together.


The process of materialization will often create specific objects that are useful to interact with the processing engine once it is running, for example for shutting it down or for extracting metrics. This means that the materialization function produces a result termed the 
materialized value of a graph
.


Interoperation with other Reactive Streams implementations


Akka Streams fully implement the Reactive Streams specification and interoperate with all other conformant implementations. We chose to completely separate the Reactive Streams interfaces from the user-level API because we regard them to be an SPI that is not targeted at endusers. In order to obtain a 
Publisher
 or 
Subscriber
 from an Akka Stream topology, a corresponding 
Sink.asPublisher
Sink.asPublisher
 or 
Source.asSubscriber
Source.asSubscriber
 element must be used.


All stream Processors produced by the default materialization of Akka Streams are restricted to having a single Subscriber, additional Subscribers will be rejected. The reason for this is that the stream topologies described using our DSL never require fan-out behavior from the Publisher sides of the elements, all fan-out is done using explicit elements like 
Broadcast<T>
Broadcast[T]
.


This means that 
Sink.asPublisher(true)
Sink.asPublisher(WITH_FANOUT)
 must be used where broadcast behavior is needed for interoperation with other Reactive Streams implementations.


Rationale and benefits from Sink/Source/Flow not directly extending Reactive Streams interfaces


A sometimes overlooked crucial piece of information about 
Reactive Streams
 is that they are a 
Service Provider Interface
, as explained in depth in one of the 
early discussions
 about the specification. Akka Streams was designed during the development of Reactive Streams, so they both heavily influenced one another.


It may be enlightening to learn that even within the Reactive Specification the types had initially attempted to hide 
Publisher
, 
Subscriber
 and the other SPI types from users of the API. Though since those internal SPI types would end up surfacing to end users of the standard in some cases, it was decided to 
remove the API types, and only keep the SPI types
 which are the 
Publisher
, 
Subscriber
 et al.


With this historical knowledge and context about the purpose of the standard â being an internal detail of interoperable libraries - we can with certainty say that it can’t be really said that a direct 
inheritance
 relationship with these types can be considered some form of advantage or meaningful differentiator between libraries. Rather, it could be seen that APIs which expose those SPI types to end-users are leaking internal implementation details accidentally. 


The 
Source
Source
, 
Sink
Sink
 and 
Flow
Flow
 types which are part of Akka Streams have the purpose of providing the fluent DSL, as well as to be “factories” for running those streams. Their direct counterparts in Reactive Streams are, respectively, 
Publisher
, 
Subscriber
` and 
Processor
. In other words, Akka Streams operate on a lifted representation of the computing graph, which then is materialized and executed in accordance to Reactive Streams rules. This also allows Akka Streams to perform optimizations like fusing and dispatcher configuration during the materialization step.


Another not obvious gain from hiding the Reactive Streams interfaces comes from the fact that 
org.reactivestreams.Subscriber
 (et al) have now been included in Java 9+, and thus become part of Java itself, so libraries should migrate to using the 
java.util.concurrent.Flow.Subscriber
 instead of 
org.reactivestreams.Subscriber
. Libraries which selected to expose and directly extend the Reactive Streams types will now have a tougher time to adapt the JDK9+ types – all their classes that extend Subscriber and friends will need to be copied or changed to extend the exact same interface, but from a different package. In Akka we simply expose the new type when asked to – already supporting JDK9 types, from the day JDK9 was released.


The other, and perhaps more important reason for hiding the Reactive Streams interfaces comes back to the first points of this explanation: the fact of Reactive Streams being an SPI, and as such is hard to “get right” in ad-hoc implementations. Thus Akka Streams discourages the use of the hard to implement pieces of the underlying infrastructure, and offers simpler, more type-safe, yet more powerful abstractions for users to work with: 
GraphStage
GraphStage
s and operators. It is of course still (and easily) possible to accept or obtain Reactive Streams (or JDK+ Flow) representations of the stream operators by using methods like 
Sink.asPublisher
Sink.asPublisher
 or 
fromSubscriber
fromSubscriber
.


What shall users of streaming libraries expect?


We expect libraries to be built on top of Akka Streams, in fact Akka HTTP is one such example that lives within the Akka project itself. In order to allow users to profit from the principles that are described for Akka Streams above, the following rules are established:




libraries shall provide their users with reusable pieces, i.e. expose factories that return operators, allowing full compositionality


libraries may optionally and additionally provide facilities that consume and materialize operators




The reasoning behind the first rule is that compositionality would be destroyed if different libraries only accepted operators and expected to materialize them: using two of these together would be impossible because materialization can only happen once. As a consequence, the functionality of a library must be expressed such that materialization can be done by the user, outside of the libraryâs control.


The second rule allows a library to additionally provide nice sugar for the common case, an example of which is the Akka HTTP API that provides a 
handleWith
 method for convenient materialization.
Note


One important consequence of this is that a reusable flow description cannot be bound to âliveâ resources, any connection to or allocation of such resources must be deferred until materialization time. Examples of âliveâ resources are already existing TCP connections, a multicast Publisher, etc.; a TickSource does not fall into this category if its timer is created only upon materialization (as is the case for our implementation).


Exceptions from this need to be well-justified and carefully documented.


Resulting Implementation Constraints


Akka Streams must enable a library to express any stream processing utility in terms of immutable blueprints. The most common building blocks are




Source
Source
: something with exactly one output stream


Sink
Sink
: something with exactly one input stream


Flow
Flow
: something with exactly one input and one output stream


BidiFlow
BidiFlow
: something with exactly two input streams and two output streams that conceptually behave like two Flows of opposite direction


Graph
Graph
: a packaged stream processing topology that exposes a certain set of input and output ports, characterized by an object of type 
Shape
Shape
.


Note


A source that emits a stream of streams is still a normal Source, the kind of elements that are produced does not play a role in the static stream topology that is being expressed.


The difference between Error and Failure


The starting point for this discussion is the 
definition given by the Reactive Manifesto
. Translated to streams this means that an error is accessible within the stream as a normal data element, while a failure means that the stream itself has failed and is collapsing. In concrete terms, on the Reactive Streams interface level data elements (including errors) are signaled via 
onNext
 while failures raise the 
onError
 signal.
Note


Unfortunately the method name for signaling 
failure
 to a Subscriber is called 
onError
 for historical reasons. Always keep in mind that the Reactive Streams interfaces (Publisher/Subscription/Subscriber) are modeling the low-level infrastructure for passing streams between execution units, and errors on this level are precisely the failures that we are talking about on the higher level that is modeled by Akka Streams.


There is only limited support for treating 
onError
 in Akka Streams compared to the operators that are available for the transformation of data elements, which is intentional in the spirit of the previous paragraph. Since 
onError
 signals that the stream is collapsing, its ordering semantics are not the same as for stream completion: transformation operators of any kind will collapse with the stream, possibly still holding elements in implicit or explicit buffers. This means that data elements emitted before a failure can still be lost if the 
onError
 overtakes them.


The ability for failures to propagate faster than data elements is essential for tearing down streams that are back-pressuredâespecially since back-pressure can be the failure mode (e.g. by tripping upstream buffers which then abort because they cannot do anything else; or if a dead-lock occurred).


The semantics of stream recovery


A recovery element (i.e. any transformation that absorbs an 
onError
 signal and turns that into possibly more data elements followed normal stream completion) acts as a bulkhead that confines a stream collapse to a given region of the stream topology. Within the collapsed region buffered elements may be lost, but the outside is not affected by the failure.


This works in the same fashion as a 
try
â
catch
 expression: it marks a region in which exceptions are caught, but the exact amount of code that was skipped within this region in case of a failure might not be known preciselyâthe placement of statements matters.














 
Streams Quickstart Guide






Basics and working with Flows 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-graphs.html
Working with Graphs • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs




Dependency


Introduction


Constructing Graphs


Constructing and combining Partial Graphs


Constructing Sources, Sinks and Flows from Partial Graphs


Combining Sources and Sinks with simplified API


Building reusable Graph components


Predefined shapes


Bidirectional Flows


Accessing the materialized value inside the Graph


Graph cycles, liveness and deadlocks




Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs




Dependency


Introduction


Constructing Graphs


Constructing and combining Partial Graphs


Constructing Sources, Sinks and Flows from Partial Graphs


Combining Sources and Sinks with simplified API


Building reusable Graph components


Predefined shapes


Bidirectional Flows


Accessing the materialized value inside the Graph


Graph cycles, liveness and deadlocks




Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Working with Graphs


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


In Akka Streams computation graphs are not expressed using a fluent DSL like linear computations are, instead they are written in a more graph-resembling DSL which aims to make translating graph drawings (e.g. from notes taken from design discussions, or illustrations in protocol specifications) to and from code simpler. In this section we’ll dive into the multiple ways of constructing and re-using graphs, as well as explain common pitfalls and how to avoid them.


Graphs are needed whenever you want to perform any kind of fan-in (“multiple inputs”) or fan-out (“multiple outputs”) operations. Considering linear Flows to be like roads, we can picture graph operations as junctions: multiple flows being connected at a single point. Some operators which are common enough and fit the linear style of Flows, such as 
concat
 (which concatenates two streams, such that the second one is consumed after the first one has completed), may have shorthand methods defined on 
Flow
 or 
Source
 themselves, however you should keep in mind that those are also implemented as graph junctions.




Constructing Graphs


Graphs are built from simple Flows which serve as the linear connections within the graphs as well as junctions which serve as fan-in and fan-out points for Flows. Thanks to the junctions having meaningful types based on their behavior and making them explicit elements these elements should be rather straightforward to use.


Akka Streams currently provide these junctions (for a detailed list see the 
operator index
):






Fan-out




Broadcast[T]
Broadcast<T>
 â 
(1 input, N outputs)
 given an input element emits to each output


Balance[T]
Balance<T>
 â 
(1 input, N outputs)
 given an input element emits to one of its output ports


Partition[T]]
Partition<T>
 â 
(1 input, N outputs)
 given an input element emits to specified output based on a partition function


UnzipWith[In,A,B,...]
UnzipWith<In,A,B,...>
 â 
(1 input, N outputs)
 takes a function of 1 input that given a value for each input emits N output elements (where N <= 20)


UnZip[A,B]
UnZip<A,B>
 â 
(1 input, 2 outputs)
 splits a stream of 
(A,B)
Pair<A,B>
 tuples into two streams, one of type 
A
 and one of type 
B








Fan-in




Merge[In]
Merge<In>
 â 
(N inputs , 1 output)
 picks randomly from inputs pushing them one by one to its output


MergePreferred[In]
MergePreferred<In>
 â like 
Merge
 but if elements are available on 
preferred
 port, it picks from it, otherwise randomly from 
others


MergePrioritized[In]
MergePrioritized<In>
 â like 
Merge
 but if elements are available on all input ports, it picks from them randomly based on their 
priority


MergeLatest[In]
MergeLatest<In>
 â 
(N inputs, 1 output)
 emits 
List[In]
, when i-th input stream emits element, then i-th element in emitted list is updated


MergeSequence[In]
MergeSequence<In>
 â 
(N inputs, 1 output)
 emits 
List[In]
, where the input streams must represent a partitioned sequence that must be merged back together in order


ZipWith[A,B,...,Out]
ZipWith<A,B,...,Out>
 â 
(N inputs, 1 output)
 which takes a function of N inputs that given a value for each input emits 1 output element


Zip[A,B]
Zip<A,B>
 â 
(2 inputs, 1 output)
 is a 
ZipWith
 specialised to zipping input streams of 
A
 and 
B
 into a 
(A,B)
Pair(A,B)
 tuple stream


Concat[A]
Concat<A>
 â 
(2 inputs, 1 output)
 concatenates two streams (first consume one, then the second one)








One of the goals of the GraphDSL DSL is to look similar to how one would draw a graph on a whiteboard, so that it is simple to translate a design from whiteboard to code and be able to relate those two. Let’s illustrate this by translating the below hand drawn graph into Akka Streams:




Such graph is simple to translate to the Graph DSL since each linear element corresponds to a 
Flow
, and each circle corresponds to either a 
Junction
 or a 
Source
 or 
Sink
 if it is beginning or ending a 
Flow
. 
Junctions must always be created with defined type parameters, as otherwise the 
Nothing
 type will be inferred.




Scala




copy
source
val g = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder: GraphDSL.Builder[NotUsed] =>
  import GraphDSL.Implicits._
  val in = Source(1 to 10)
  val out = Sink.ignore

  val bcast = builder.add(Broadcast[Int](2))
  val merge = builder.add(Merge[Int](2))

  val f1, f2, f3, f4 = Flow[Int].map(_ + 10)

  in ~> f1 ~> bcast ~> f2 ~> merge ~> f3 ~> out
  bcast ~> f4 ~> merge
  ClosedShape
})


Java




copy
source
final Source<Integer, NotUsed> in = Source.from(Arrays.asList(1, 2, 3, 4, 5));
final Sink<List<String>, CompletionStage<List<String>>> sink = Sink.head();
final Flow<Integer, Integer, NotUsed> f1 = Flow.of(Integer.class).map(elem -> elem + 10);
final Flow<Integer, Integer, NotUsed> f2 = Flow.of(Integer.class).map(elem -> elem + 20);
final Flow<Integer, String, NotUsed> f3 = Flow.of(Integer.class).map(elem -> elem.toString());
final Flow<Integer, Integer, NotUsed> f4 = Flow.of(Integer.class).map(elem -> elem + 30);

final RunnableGraph<CompletionStage<List<String>>> result =
    RunnableGraph.fromGraph(
        GraphDSL // create() function binds sink, out which is sink's out port and builder DSL
            .create( // we need to reference out's shape in the builder DSL below (in to()
            // function)
            sink, // previously created sink (Sink)
            (builder, out) -> { // variables: builder (GraphDSL.Builder) and out (SinkShape)
              final UniformFanOutShape<Integer, Integer> bcast =
                  builder.add(Broadcast.create(2));
              final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));

              final Outlet<Integer> source = builder.add(in).out();
              builder
                  .from(source)
                  .via(builder.add(f1))
                  .viaFanOut(bcast)
                  .via(builder.add(f2))
                  .viaFanIn(merge)
                  .via(builder.add(f3.grouped(1000)))
                  .to(out); // to() expects a SinkShape
              builder.from(bcast).via(builder.add(f4)).toFanIn(merge);
              return ClosedShape.getInstance();
            }));


Note


Junction 
reference equality
 defines 
graph node equality
 (i.e. the same merge 
instance
 used in a GraphDSL refers to the same location in the resulting graph).


Notice the 
importÂ GraphDSL.Implicits._
 which brings into scope the 
~>
 operator (read as “edge”, “via” or “to”) and its inverted counterpart 
<~
 (for noting down flows in the opposite direction where appropriate).


By looking at the snippets above, it should be apparent that the 
GraphDSL.Builder
builder
 object is 
mutable
. 
It is used (implicitly) by the 
~>
 operator, also making it a mutable operation as well.
 The reason for this design choice is to enable simpler creation of complex graphs, which may even contain cycles. Once the GraphDSL has been constructed though, the 
GraphDSL
RunnableGraph
 instance 
is immutable, thread-safe, and freely shareable
. The same is true of all operatorsâsources, sinks, and flowsâonce they are constructed. This means that you can safely re-use one given Flow or junction in multiple places in a processing graph.


We have seen examples of such re-use already above: the merge and broadcast junctions were imported into the graph using 
builder.add(...)
, an operation that will make a copy of the blueprint that is passed to it and return the inlets and outlets of the resulting copy so that they can be wired up. Another alternative is to pass existing graphsâof any shapeâinto the factory method that produces a new graph. The difference between these approaches is that importing using 
builder.add(...)
 ignores the materialized value of the imported graph while importing via the factory method allows its inclusion; for more details see 
Stream Materialization
.


In the example below we prepare a graph that consists of two parallel streams, in which we re-use the same instance of 
Flow
, yet it will properly be materialized as two connections between the corresponding Sources and Sinks:




Scala




copy
source
val topHeadSink = Sink.head[Int]
val bottomHeadSink = Sink.head[Int]
val sharedDoubler = Flow[Int].map(_ * 2)

RunnableGraph.fromGraph(GraphDSL.createGraph(topHeadSink, bottomHeadSink)((_, _)) { implicit builder =>
  (topHS, bottomHS) =>
  import GraphDSL.Implicits._
  val broadcast = builder.add(Broadcast[Int](2))
  Source.single(1) ~> broadcast.in

  broadcast ~> sharedDoubler ~> topHS.in
  broadcast ~> sharedDoubler ~> bottomHS.in
  ClosedShape
})


Java




copy
source
final Sink<Integer, CompletionStage<Integer>> topHeadSink = Sink.head();
final Sink<Integer, CompletionStage<Integer>> bottomHeadSink = Sink.head();
final Flow<Integer, Integer, NotUsed> sharedDoubler =
    Flow.of(Integer.class).map(elem -> elem * 2);

final RunnableGraph<Pair<CompletionStage<Integer>, CompletionStage<Integer>>> g =
    RunnableGraph.fromGraph(
        GraphDSL.create(
            topHeadSink, // import this sink into the graph
            bottomHeadSink, // and this as well
            Keep.both(),
            (b, top, bottom) -> {
              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));

              b.from(b.add(Source.single(1)))
                  .viaFanOut(bcast)
                  .via(b.add(sharedDoubler))
                  .to(top);
              b.from(bcast).via(b.add(sharedDoubler)).to(bottom);
              return ClosedShape.getInstance();
            }));




In some cases we may have a list of graph elements, for example if they are dynamically created. If these graphs have similar signatures, we can construct a graph collecting all their materialized values as a collection:




Scala




copy
source
val sinks = immutable
  .Seq("a", "b", "c")
  .map(prefix => Flow[String].filter(str => str.startsWith(prefix)).toMat(Sink.head[String])(Keep.right))

val g: RunnableGraph[Seq[Future[String]]] = RunnableGraph.fromGraph(GraphDSL.create(sinks) {
  implicit b => sinkList =>
    import GraphDSL.Implicits._
    val broadcast = b.add(Broadcast[String](sinkList.size))

    Source(List("ax", "bx", "cx")) ~> broadcast
    sinkList.foreach(sink => broadcast ~> sink)

    ClosedShape
})

val matList: Seq[Future[String]] = g.run()


Java




copy
source
// create the source
final Source<String, NotUsed> in = Source.from(Arrays.asList("ax", "bx", "cx"));
// generate the sinks from code
List<String> prefixes = Arrays.asList("a", "b", "c");
final List<Sink<String, CompletionStage<String>>> list = new ArrayList<>();
for (String prefix : prefixes) {
  final Sink<String, CompletionStage<String>> sink =
      Flow.of(String.class)
          .filter(str -> str.startsWith(prefix))
          .toMat(Sink.head(), Keep.right());
  list.add(sink);
}

final RunnableGraph<List<CompletionStage<String>>> g =
    RunnableGraph.fromGraph(
        GraphDSL.create(
            list,
            (GraphDSL.Builder<List<CompletionStage<String>>> builder,
                List<SinkShape<String>> outs) -> {
              final UniformFanOutShape<String, String> bcast =
                  builder.add(Broadcast.create(outs.size()));

              final Outlet<String> source = builder.add(in).out();
              builder.from(source).viaFanOut(bcast);

              for (SinkShape<String> sink : outs) {
                builder.from(bcast).to(sink);
              }

              return ClosedShape.getInstance();
            }));
List<CompletionStage<String>> result = g.run(system);






Constructing and combining Partial Graphs


Sometimes it is not possible (or needed) to construct the entire computation graph in one place, but instead construct all of its different phases in different places and in the end connect them all into a complete graph and run it.


This can be achieved by 
returning a different 
Shape
 than 
ClosedShape
, for example 
FlowShape(in, out)
, from the function given to 
GraphDSL.create
. See 
Predefined shapes
 for a list of such predefined shapes. Making a 
Graph
 a 
RunnableGraph
using the returned 
Graph
 from 
GraphDSL.create()
 rather than passing it to 
RunnableGraph.fromGraph()
 to wrap it in a 
RunnableGraph
.The reason of representing it as a different type is that a 
RunnableGraph
 requires all ports to be connected, and if they are not it will throw an exception at construction time, which helps to avoid simple wiring errors while working with graphs. A partial graph however allows you to return the set of yet to be connected ports from the code block that performs the internal wiring.


Let’s imagine we want to provide users with a specialized element that given 3 inputs will pick the greatest int value of each zipped triple. We’ll want to expose 3 input ports (unconnected sources) and one output port (unconnected sink).




Scala




copy
source
val pickMaxOfThree = GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val zip1 = b.add(ZipWith[Int, Int, Int](math.max _))
  val zip2 = b.add(ZipWith[Int, Int, Int](math.max _))
  zip1.out ~> zip2.in0

  UniformFanInShape(zip2.out, zip1.in0, zip1.in1, zip2.in1)
}

val resultSink = Sink.head[Int]

val g = RunnableGraph.fromGraph(GraphDSL.createGraph(resultSink) { implicit b => sink =>
  import GraphDSL.Implicits._

  // importing the partial graph will return its shape (inlets & outlets)
  val pm3 = b.add(pickMaxOfThree)

  Source.single(1) ~> pm3.in(0)
  Source.single(2) ~> pm3.in(1)
  Source.single(3) ~> pm3.in(2)
  pm3.out ~> sink.in
  ClosedShape
})

val max: Future[Int] = g.run()
Await.result(max, 300.millis) should equal(3)


Java




copy
source
final Graph<FanInShape2<Integer, Integer, Integer>, NotUsed> zip =
    ZipWith.create((Integer left, Integer right) -> Math.max(left, right));

final Graph<UniformFanInShape<Integer, Integer>, NotUsed> pickMaxOfThree =
    GraphDSL.create(
        builder -> {
          final FanInShape2<Integer, Integer, Integer> zip1 = builder.add(zip);
          final FanInShape2<Integer, Integer, Integer> zip2 = builder.add(zip);

          builder.from(zip1.out()).toInlet(zip2.in0());
          // return the shape, which has three inputs and one output
          return UniformFanInShape.<Integer, Integer>create(
              zip2.out(), Arrays.asList(zip1.in0(), zip1.in1(), zip2.in1()));
        });

final Sink<Integer, CompletionStage<Integer>> resultSink = Sink.<Integer>head();

final RunnableGraph<CompletionStage<Integer>> g =
    RunnableGraph.<CompletionStage<Integer>>fromGraph(
        GraphDSL.create(
            resultSink,
            (builder, sink) -> {
              // import the partial graph explicitly
              final UniformFanInShape<Integer, Integer> pm = builder.add(pickMaxOfThree);

              builder.from(builder.add(Source.single(1))).toInlet(pm.in(0));
              builder.from(builder.add(Source.single(2))).toInlet(pm.in(1));
              builder.from(builder.add(Source.single(3))).toInlet(pm.in(2));
              builder.from(pm.out()).to(sink);
              return ClosedShape.getInstance();
            }));

final CompletionStage<Integer> max = g.run(system);


Note


While the above example shows composing two 2-input 
ZipWith
s, in reality ZipWith already provides numerous overloads including a 3 (and many more) parameter versions. So this could be implemented using one ZipWith using the 3 parameter version, like this: 
ZipWith((a, b, c) => out)
ZipWith.create((a, b, c) -> out)
. (The ZipWith with N input has N+1 type parameter; the last type param is the output type.)


As you can see, first we construct the partial graph that 
contains all the zipping and comparing of stream elements. This partial graph will have three inputs and one output, wherefore we use the 
UniformFanInShape
describes how to compute the maximum of two input streams. then we reuse that twice while constructing the partial graph that extends this to three input streams
. Then we import it (all of its nodes and connections) explicitly into the 
closed graph built in the second step
last graph
 in which all the undefined elements are rewired to real sources and sinks. The graph can then be run and yields the expected result.
Warning


Please note that 
GraphDSL
 is not able to provide compile time type-safety about whether or not all elements have been properly connectedâthis validation is performed as a runtime check during the graph’s instantiation.


A partial graph also verifies that all ports are either connected or part of the returned 
Shape
.




Constructing Sources, Sinks and Flows from Partial Graphs


Instead of treating a 
partial graph
Graph
 as a collection of flows and junctions which may not yet all be connected it is sometimes useful to expose such a complex graph as a simpler structure, such as a 
Source
, 
Sink
 or 
Flow
.


In fact, these concepts can be expressed as special cases of a partially connected graph:




Source
 is a partial graph with 
exactly one
 output, that is it returns a 
SourceShape
.


Sink
 is a partial graph with 
exactly one
 input, that is it returns a 
SinkShape
.


Flow
 is a partial graph with 
exactly one
 input and 
exactly one
 output, that is it returns a 
FlowShape
.




Being able to hide complex graphs inside of simple elements such as Sink / Source / Flow enables you to create one complex element and from there on treat it as simple compound operator for linear computations.


In order to create a Source from a graph the method 
Source.fromGraph
 is used, to use it we must have a 
Graph[SourceShape, T]
Graph
 with a 
SourceShape
. This is constructed using 
GraphDSL.create
 and returning a 
SourceShape
 from the function passed in
GraphDSL.create
 and providing building a 
SourceShape
 graph
. The single outlet must be provided to the 
SourceShape.of
 method and will become âthe sink that must be attached before this Source can runâ.


Refer to the example below, in which we create a Source that zips together two numbers, to see this graph construction in action:




Scala




copy
source
val pairs = Source.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  // prepare graph elements
  val zip = b.add(Zip[Int, Int]())
  def ints = Source.fromIterator(() => Iterator.from(1))

  // connect the graph
  ints.filter(_ % 2 != 0) ~> zip.in0
  ints.filter(_ % 2 == 0) ~> zip.in1

  // expose port
  SourceShape(zip.out)
})

val firstPair: Future[(Int, Int)] = pairs.runWith(Sink.head)


Java




copy
source
// first create an indefinite source of integer numbers
class Ints implements Iterator<Integer> {
  private int next = 0;

  @Override
  public boolean hasNext() {
    return true;
  }

  @Override
  public Integer next() {
    return next++;
  }
}
  final Source<Integer, NotUsed> ints = Source.fromIterator(() -> new Ints());

  final Source<Pair<Integer, Integer>, NotUsed> pairs =
      Source.fromGraph(
          GraphDSL.create(
              builder -> {
                final FanInShape2<Integer, Integer, Pair<Integer, Integer>> zip =
                    builder.add(Zip.create());

                builder.from(builder.add(ints.filter(i -> i % 2 == 0))).toInlet(zip.in0());
                builder.from(builder.add(ints.filter(i -> i % 2 == 1))).toInlet(zip.in1());

                return SourceShape.of(zip.out());
              }));

  final CompletionStage<Pair<Integer, Integer>> firstPair =
      pairs.runWith(Sink.<Pair<Integer, Integer>>head(), system);




Similarly the same can be done for a 
Sink[T]
Sink<T>
, using 
SinkShape.of
 in which case the provided value must be an 
Inlet[T]
Inlet<T>
. For defining a 
Flow[T]
Flow<T>
 we need to expose both an 
inlet and an outlet
undefined source and sink
:




Scala




copy
source
val pairUpWithToString =
  Flow.fromGraph(GraphDSL.create() { implicit b =>
    import GraphDSL.Implicits._

    // prepare graph elements
    val broadcast = b.add(Broadcast[Int](2))
    val zip = b.add(Zip[Int, String]())

    // connect the graph
    broadcast.out(0).map(identity) ~> zip.in0
    broadcast.out(1).map(_.toString) ~> zip.in1

    // expose ports
    FlowShape(broadcast.in, zip.out)
  })

pairUpWithToString.runWith(Source(List(1)), Sink.head)


Java




copy
source
final Flow<Integer, Pair<Integer, String>, NotUsed> pairs =
    Flow.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
              final FanInShape2<Integer, String, Pair<Integer, String>> zip =
                  b.add(Zip.create());

              b.from(bcast).toInlet(zip.in0());
              b.from(bcast)
                  .via(b.add(Flow.of(Integer.class).map(i -> i.toString())))
                  .toInlet(zip.in1());

              return FlowShape.of(bcast.in(), zip.out());
            }));

    Source.single(1).via(pairs).runWith(Sink.<Pair<Integer, String>>head(), system);




Combining Sources and Sinks with simplified API


There is a simplified API you can use to combine sources and sinks with junctions like: 
Broadcast[T]
, 
Balance[T]
, 
Merge[In]
 and 
Concat[A]
Broadcast<T>
, 
Balance<T>
, 
Merge<In>
 and 
Concat<A>
 without the need for using the Graph DSL. The combine method takes care of constructing the necessary graph underneath. In following example we combine two sources into one (fan-in):




Scala




copy
source
val sourceOne = Source(List(1))
val sourceTwo = Source(List(2))
val merged = Source.combine(sourceOne, sourceTwo)(Merge(_))

val mergedResult: Future[Int] = merged.runWith(Sink.fold(0)(_ + _))


Java




copy
source
Source<Integer, NotUsed> source1 = Source.single(1);
Source<Integer, NotUsed> source2 = Source.single(2);

final Source<Integer, NotUsed> sources =
    Source.combine(source1, source2, new ArrayList<>(), i -> Merge.<Integer>create(i));
    sources.runWith(Sink.<Integer, Integer>fold(0, (a, b) -> a + b), system);




The same can be done for a 
Sink[T]
Sink
 but in this case it will be fan-out:




Scala




copy
source
val sendRemotely = Sink.actorRef(actorRef, "Done", _ => "Failed")
val localProcessing = Sink.foreach[Int](_ => /* do something useful */ ())

val sink = Sink.combine(sendRemotely, localProcessing)(Broadcast[Int](_))

Source(List(0, 1, 2)).runWith(sink)


Java




copy
source
Sink<Integer, NotUsed> sendRemotely = Sink.actorRef(actorRef, "Done");
Sink<Integer, CompletionStage<Done>> localProcessing =
    Sink.<Integer>foreach(
        a -> {
          /*do something useful*/
        });
Sink<Integer, NotUsed> sinks =
    Sink.combine(sendRemotely, localProcessing, new ArrayList<>(), a -> Broadcast.create(a));

Source.<Integer>from(Arrays.asList(new Integer[] {0, 1, 2})).runWith(sinks, system);




Building reusable Graph components


It is possible to build reusable, encapsulated components of arbitrary input and output ports using the graph DSL.


As an example, we will build a graph junction that represents a pool of workers, where a worker is expressed as a 
Flow[I,O,_]
Flow<I,O,M>
, i.e. a simple transformation of jobs of type 
I
 to results of type 
O
 (as you have seen already, this flow can actually contain a complex graph inside). Our reusable worker pool junction will not preserve the order of the incoming jobs (they are assumed to have a proper ID field) and it will use a 
Balance
 junction to schedule jobs to available workers. On top of this, our junction will feature a “fastlane”, a dedicated port where jobs of higher priority can be sent.


Altogether, our junction will have two input ports of type 
I
 (for the normal and priority jobs) and an output port of type 
O
. To represent this interface, we need to define a custom 
Shape
. The following lines show how to do that.




Scala




copy
source
// A shape represents the input and output ports of a reusable
// processing module
case class PriorityWorkerPoolShape[In, Out](jobsIn: Inlet[In], priorityJobsIn: Inlet[In], resultsOut: Outlet[Out])
    extends Shape {

  // It is important to provide the list of all input and output
  // ports with a stable order. Duplicates are not allowed.
  override val inlets: immutable.Seq[Inlet[_]] =
    jobsIn :: priorityJobsIn :: Nil
  override val outlets: immutable.Seq[Outlet[_]] =
    resultsOut :: Nil

  // A Shape must be able to create a copy of itself. Basically
  // it means a new instance with copies of the ports
  override def deepCopy() =
    PriorityWorkerPoolShape(jobsIn.carbonCopy(), priorityJobsIn.carbonCopy(), resultsOut.carbonCopy())

}




Predefined shapes


In general a custom 
Shape
 needs to be able to provide all its input and output ports, be able to copy itself, and also be able to create a new instance from given ports. There are some predefined shapes provided to avoid unnecessary boilerplate:




SourceShape
, 
SinkShape
, 
FlowShape
 for simpler shapes,


UniformFanInShape
 and 
UniformFanOutShape
 for junctions with multiple input (or output) ports of the same type,


FanInShape1
, 
FanInShape2
, …, 
FanOutShape1
, 
FanOutShape2
, … for junctions with multiple input (or output) ports of different types.




Since our shape has two input ports and one output port, we can use the 
FanInShape
 DSL to define our custom shape:




Scala




copy
source
import FanInShape.{ Init, Name }

class PriorityWorkerPoolShape2[In, Out](_init: Init[Out] = Name("PriorityWorkerPool"))
    extends FanInShape[Out](_init) {
  protected override def construct(i: Init[Out]) = new PriorityWorkerPoolShape2(i)

  val jobsIn = newInlet[In]("jobsIn")
  val priorityJobsIn = newInlet[In]("priorityJobsIn")
  // Outlet[Out] with name "out" is automatically created
}




Now that we have a 
Shape
 we can wire up a Graph that represents our worker pool. First, we will merge incoming normal and priority jobs using 
MergePreferred
, then we will send the jobs to a 
Balance
 junction which will fan-out to a configurable number of workers (flows), finally we merge all these results together and send them out through our only output port. This is expressed by the following code:




Scala




copy
source
object PriorityWorkerPool {
  def apply[In, Out](
      worker: Flow[In, Out, Any],
      workerCount: Int): Graph[PriorityWorkerPoolShape[In, Out], NotUsed] = {

    GraphDSL.create() { implicit b =>
      import GraphDSL.Implicits._

      val priorityMerge = b.add(MergePreferred[In](1))
      val balance = b.add(Balance[In](workerCount))
      val resultsMerge = b.add(Merge[Out](workerCount))

      // After merging priority and ordinary jobs, we feed them to the balancer
      priorityMerge ~> balance

      // Wire up each of the outputs of the balancer to a worker flow
      // then merge them back
      for (i <- 0 until workerCount)
        balance.out(i) ~> worker ~> resultsMerge.in(i)

      // We now expose the input ports of the priorityMerge and the output
      // of the resultsMerge as our PriorityWorkerPool ports
      // -- all neatly wrapped in our domain specific Shape
      PriorityWorkerPoolShape(
        jobsIn = priorityMerge.in(0),
        priorityJobsIn = priorityMerge.preferred,
        resultsOut = resultsMerge.out)
    }

  }

}




All we need to do now is to use our custom junction in a graph. The following code simulates some simple workers and jobs using plain strings and prints out the results. Actually we used 
two
 instances of our worker pool junction using 
add()
 twice.




Scala




copy
source
val worker1 = Flow[String].map("step 1 " + _)
val worker2 = Flow[String].map("step 2 " + _)

RunnableGraph
  .fromGraph(GraphDSL.create() { implicit b =>
    import GraphDSL.Implicits._

    val priorityPool1 = b.add(PriorityWorkerPool(worker1, 4))
    val priorityPool2 = b.add(PriorityWorkerPool(worker2, 2))

    Source(1 to 100).map("job: " + _) ~> priorityPool1.jobsIn
    Source(1 to 100).map("priority job: " + _) ~> priorityPool1.priorityJobsIn

    priorityPool1.resultsOut ~> priorityPool2.jobsIn
    Source(1 to 100).map("one-step, priority " + _) ~> priorityPool2.priorityJobsIn

    priorityPool2.resultsOut ~> Sink.foreach(println)
    ClosedShape
  })
  .run()






Bidirectional Flows


A graph topology that is often useful is that of two flows going in opposite directions. Take for example a codec operator that serializes outgoing messages and deserializes incoming octet streams. Another such operator could add a framing protocol that attaches a length header to outgoing data and parses incoming frames back into the original octet stream chunks. These two operators are meant to be composed, applying one atop the other as part of a protocol stack. For this purpose exists the special type 
BidiFlow
 which is a graph that has exactly two open inlets and two open outlets. The corresponding shape is called 
BidiShape
 and is defined like this:


copy
source
/**
 * A bidirectional flow of elements that consequently has two inputs and two
 * outputs, arranged like this:
 *
 * {{{
 *        +------+
 *  In1 ~>|      |~> Out1
 *        | bidi |
 * Out2 <~|      |<~ In2
 *        +------+
 * }}}
 */
final case class BidiShape[-In1, +Out1, -In2, +Out2](
    in1: Inlet[In1 @uncheckedVariance],
    out1: Outlet[Out1 @uncheckedVariance],
    in2: Inlet[In2 @uncheckedVariance],
    out2: Outlet[Out2 @uncheckedVariance])
    extends Shape {
  override val inlets: immutable.Seq[Inlet[_]] = in1 :: in2 :: Nil
  override val outlets: immutable.Seq[Outlet[_]] = out1 :: out2 :: Nil

  /**
   * Java API for creating from a pair of unidirectional flows.
   */
  def this(top: FlowShape[In1, Out1], bottom: FlowShape[In2, Out2]) = this(top.in, top.out, bottom.in, bottom.out)

  override def deepCopy(): BidiShape[In1, Out1, In2, Out2] =
    BidiShape(in1.carbonCopy(), out1.carbonCopy(), in2.carbonCopy(), out2.carbonCopy())

}


A bidirectional flow is defined just like a unidirectional 
Flow
 as demonstrated for the codec mentioned above:




Scala




copy
source
trait Message
case class Ping(id: Int) extends Message
case class Pong(id: Int) extends Message

def toBytes(msg: Message): ByteString = {
  implicit val order = ByteOrder.LITTLE_ENDIAN
  msg match {
    case Ping(id) => ByteString.newBuilder.putByte(1).putInt(id).result()
    case Pong(id) => ByteString.newBuilder.putByte(2).putInt(id).result()
  }
}

def fromBytes(bytes: ByteString): Message = {
  implicit val order = ByteOrder.LITTLE_ENDIAN
  val it = bytes.iterator
  it.getByte match {
    case 1     => Ping(it.getInt)
    case 2     => Pong(it.getInt)
    case other => throw new RuntimeException(s"parse error: expected 1|2 got $other")
  }
}

val codecVerbose = BidiFlow.fromGraph(GraphDSL.create() { b =>
  // construct and add the top flow, going outbound
  val outbound = b.add(Flow[Message].map(toBytes))
  // construct and add the bottom flow, going inbound
  val inbound = b.add(Flow[ByteString].map(fromBytes))
  // fuse them together into a BidiShape
  BidiShape.fromFlows(outbound, inbound)
})

// this is the same as the above
val codec = BidiFlow.fromFunctions(toBytes _, fromBytes _)


Java




copy
source
static interface Message {}

static class Ping implements Message {
  final int id;

  public Ping(int id) {
    this.id = id;
  }

  @Override
  public boolean equals(Object o) {
    if (o instanceof Ping) {
      return ((Ping) o).id == id;
    } else return false;
  }

  @Override
  public int hashCode() {
    return id;
  }
}

static class Pong implements Message {
  final int id;

  public Pong(int id) {
    this.id = id;
  }

  @Override
  public boolean equals(Object o) {
    if (o instanceof Pong) {
      return ((Pong) o).id == id;
    } else return false;
  }

  @Override
  public int hashCode() {
    return id;
  }
}

public static ByteString toBytes(Message msg) {
  if (msg instanceof Ping) {
    final int id = ((Ping) msg).id;
    return new ByteStringBuilder().putByte((byte) 1).putInt(id, ByteOrder.LITTLE_ENDIAN).result();
  } else {
    final int id = ((Pong) msg).id;
    return new ByteStringBuilder().putByte((byte) 2).putInt(id, ByteOrder.LITTLE_ENDIAN).result();
  }
}

public static Message fromBytes(ByteString bytes) {
  final ByteIterator it = bytes.iterator();
  switch (it.getByte()) {
    case 1:
      return new Ping(it.getInt(ByteOrder.LITTLE_ENDIAN));
    case 2:
      return new Pong(it.getInt(ByteOrder.LITTLE_ENDIAN));
    default:
      throw new RuntimeException("message format error");
  }
}

public final BidiFlow<Message, ByteString, ByteString, Message, NotUsed> codecVerbose =
    BidiFlow.fromGraph(
        GraphDSL.create(
            b -> {
              final FlowShape<Message, ByteString> top =
                  b.add(Flow.of(Message.class).map(BidiFlowDocTest::toBytes));
              final FlowShape<ByteString, Message> bottom =
                  b.add(Flow.of(ByteString.class).map(BidiFlowDocTest::fromBytes));
              return BidiShape.fromFlows(top, bottom);
            }));

public final BidiFlow<Message, ByteString, ByteString, Message, NotUsed> codec =
    BidiFlow.fromFunctions(BidiFlowDocTest::toBytes, BidiFlowDocTest::fromBytes);




The first version resembles the partial graph constructor, while for the simple case of a functional 1:1 transformation there is a concise convenience method as shown on the last line. The implementation of the two functions is not difficult either:




Scala




copy
source
def toBytes(msg: Message): ByteString = {
  implicit val order = ByteOrder.LITTLE_ENDIAN
  msg match {
    case Ping(id) => ByteString.newBuilder.putByte(1).putInt(id).result()
    case Pong(id) => ByteString.newBuilder.putByte(2).putInt(id).result()
  }
}

def fromBytes(bytes: ByteString): Message = {
  implicit val order = ByteOrder.LITTLE_ENDIAN
  val it = bytes.iterator
  it.getByte match {
    case 1     => Ping(it.getInt)
    case 2     => Pong(it.getInt)
    case other => throw new RuntimeException(s"parse error: expected 1|2 got $other")
  }
}


Java




copy
source
public static ByteString toBytes(Message msg) {
  if (msg instanceof Ping) {
    final int id = ((Ping) msg).id;
    return new ByteStringBuilder().putByte((byte) 1).putInt(id, ByteOrder.LITTLE_ENDIAN).result();
  } else {
    final int id = ((Pong) msg).id;
    return new ByteStringBuilder().putByte((byte) 2).putInt(id, ByteOrder.LITTLE_ENDIAN).result();
  }
}

public static Message fromBytes(ByteString bytes) {
  final ByteIterator it = bytes.iterator();
  switch (it.getByte()) {
    case 1:
      return new Ping(it.getInt(ByteOrder.LITTLE_ENDIAN));
    case 2:
      return new Pong(it.getInt(ByteOrder.LITTLE_ENDIAN));
    default:
      throw new RuntimeException("message format error");
  }
}




In this way you can integrate any other serialization library that turns an object into a sequence of bytes.


The other operator that we talked about is a little more involved since reversing a framing protocol means that any received chunk of bytes may correspond to zero or more messages. This is best implemented using 
GraphStage
 (see also 
Custom processing with GraphStage
).




Scala




copy
source
val framing = BidiFlow.fromGraph(GraphDSL.create() { b =>
  implicit val order = ByteOrder.LITTLE_ENDIAN

  def addLengthHeader(bytes: ByteString) = {
    val len = bytes.length
    ByteString.newBuilder.putInt(len).append(bytes).result()
  }

  class FrameParser extends GraphStage[FlowShape[ByteString, ByteString]] {

    val in = Inlet[ByteString]("FrameParser.in")
    val out = Outlet[ByteString]("FrameParser.out")
    override val shape = FlowShape.of(in, out)

    override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {

      // this holds the received but not yet parsed bytes
      var stash = ByteString.empty
      // this holds the current message length or -1 if at a boundary
      var needed = -1

      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          if (isClosed(in)) run()
          else pull(in)
        }
      })
      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val bytes = grab(in)
          stash = stash ++ bytes
          run()
        }

        override def onUpstreamFinish(): Unit = {
          // either we are done
          if (stash.isEmpty) completeStage()
          // or we still have bytes to emit
          // wait with completion and let run() complete when the
          // rest of the stash has been sent downstream
          else if (isAvailable(out)) run()
        }
      })

      private def run(): Unit = {
        if (needed == -1) {
          // are we at a boundary? then figure out next length
          if (stash.length < 4) {
            if (isClosed(in)) completeStage()
            else pull(in)
          } else {
            needed = stash.iterator.getInt
            stash = stash.drop(4)
            run() // cycle back to possibly already emit the next chunk
          }
        } else if (stash.length < needed) {
          // we are in the middle of a message, need more bytes,
          // or have to stop if input closed
          if (isClosed(in)) completeStage()
          else pull(in)
        } else {
          // we have enough to emit at least one message, so do it
          val emit = stash.take(needed)
          stash = stash.drop(needed)
          needed = -1
          push(out, emit)
        }
      }
    }
  }

  val outbound = b.add(Flow[ByteString].map(addLengthHeader))
  val inbound = b.add(Flow[ByteString].via(new FrameParser))
  BidiShape.fromFlows(outbound, inbound)
})


Java




copy
source
public static ByteString addLengthHeader(ByteString bytes) {
  final int len = bytes.size();
  return new ByteStringBuilder().putInt(len, ByteOrder.LITTLE_ENDIAN).append(bytes).result();
}

public static class FrameParser extends GraphStage<FlowShape<ByteString, ByteString>> {
  public Inlet<ByteString> in = Inlet.create("FrameParser.in");
  public Outlet<ByteString> out = Outlet.create("FrameParser.out");
  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<ByteString, ByteString> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {

      // this holds the received but not yet parsed bytes
      private ByteString stash = emptyByteString();
      // this holds the current message length or -1 if at a boundary
      private int needed = -1;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                ByteString bytes = grab(in);
                stash = stash.concat(bytes);
                run();
              }

              @Override
              public void onUpstreamFinish() throws Exception {
                // either we are done
                if (stash.isEmpty()) completeStage();
                // or we still have bytes to emit
                // wait with completion and let run() complete when the
                // rest of the stash has been sent downstream
                else if (isAvailable(out)) run();
              }
            });

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                if (isClosed(in)) run();
                else pull(in);
              }
            });
      }

      private void run() {
        if (needed == -1) {
          // are we at a boundary? then figure out next length
          if (stash.size() < 4) {
            if (isClosed(in)) completeStage();
            else pull(in);
          } else {
            needed = stash.iterator().getInt(ByteOrder.LITTLE_ENDIAN);
            stash = stash.drop(4);
            run(); // cycle back to possibly already emit the next chunk
          }
        } else if (stash.size() < needed) {
          // we are in the middle of a message, need more bytes
          // or in is already closed and we cannot pull any more
          if (isClosed(in)) completeStage();
          else pull(in);
        } else {
          // we have enough to emit at least one message, so do it
          final ByteString emit = stash.take(needed);
          stash = stash.drop(needed);
          needed = -1;
          push(out, emit);
        }
      }
    };
  }
}

public final BidiFlow<ByteString, ByteString, ByteString, ByteString, NotUsed> framing =
    BidiFlow.fromGraph(
        GraphDSL.create(
            b -> {
              final FlowShape<ByteString, ByteString> top =
                  b.add(Flow.of(ByteString.class).map(BidiFlowDocTest::addLengthHeader));
              final FlowShape<ByteString, ByteString> bottom =
                  b.add(Flow.of(ByteString.class).via(new FrameParser()));
              return BidiShape.fromFlows(top, bottom);
            }));




With these implementations we can build a protocol stack and test it:




Scala




copy
source
/* construct protocol stack
 *         +------------------------------------+
 *         | stack                              |
 *         |                                    |
 *         |  +-------+            +---------+  |
 *    ~>   O~~o       |     ~>     |         o~~O    ~>
 * Message |  | codec | ByteString | framing |  | ByteString
 *    <~   O~~o       |     <~     |         o~~O    <~
 *         |  +-------+            +---------+  |
 *         +------------------------------------+
 */
val stack = codec.atop(framing)

// test it by plugging it into its own inverse and closing the right end
val pingpong = Flow[Message].collect { case Ping(id) => Pong(id) }
val flow = stack.atop(stack.reversed).join(pingpong)
val result = Source((0 to 9).map(Ping(_))).via(flow).limit(20).runWith(Sink.seq)
Await.result(result, 1.second) should ===((0 to 9).map(Pong(_)))


Java




copy
source
/* construct protocol stack
 *         +------------------------------------+
 *         | stack                              |
 *         |                                    |
 *         |  +-------+            +---------+  |
 *    ~>   O~~o       |     ~>     |         o~~O    ~>
 * Message |  | codec | ByteString | framing |  | ByteString
 *    <~   O~~o       |     <~     |         o~~O    <~
 *         |  +-------+            +---------+  |
 *         +------------------------------------+
 */
final BidiFlow<Message, ByteString, ByteString, Message, NotUsed> stack = codec.atop(framing);

// test it by plugging it into its own inverse and closing the right end
final Flow<Message, Message, NotUsed> pingpong =
    Flow.of(Message.class)
        .collect(
            new PFBuilder<Message, Message>().match(Ping.class, p -> new Pong(p.id)).build());
final Flow<Message, Message, NotUsed> flow = stack.atop(stack.reversed()).join(pingpong);
final CompletionStage<List<Message>> result =
    Source.from(Arrays.asList(0, 1, 2))
        .<Message>map(id -> new Ping(id))
        .via(flow)
        .grouped(10)
        .runWith(Sink.<List<Message>>head(), system);
assertArrayEquals(
    new Message[] {new Pong(0), new Pong(1), new Pong(2)},
    result.toCompletableFuture().get(1, TimeUnit.SECONDS).toArray(new Message[0]));




This example demonstrates how 
BidiFlow
 subgraphs can be hooked together and also turned around with the 
.reversed
.reversed()
 method. The test simulates both parties of a network communication protocol without actually having to open a network connectionâthe flows can be connected directly.




Accessing the materialized value inside the Graph


In certain cases it might be necessary to feed back the materialized value of a Graph (partial, closed or backing a Source, Sink, Flow or BidiFlow). This is possible by using 
builder.materializedValue
 which gives an 
Outlet
 that can be used in the graph as an ordinary source or outlet, and which will eventually emit the materialized value. If the materialized value is needed at more than one place, it is possible to call 
materializedValue
 any number of times to acquire the necessary number of outlets.




Scala




copy
source
import GraphDSL.Implicits._
val foldFlow: Flow[Int, Int, Future[Int]] = Flow.fromGraph(GraphDSL.createGraph(Sink.fold[Int, Int](0)(_ + _)) {
  implicit builder => fold =>
    FlowShape(fold.in, builder.materializedValue.mapAsync(4)(identity).outlet)
})


Java




copy
source
final Sink<Integer, CompletionStage<Integer>> foldSink =
    Sink.fold(
        0,
        (a, b) -> {
          return a + b;
        });

final Flow<CompletionStage<Integer>, Integer, NotUsed> flatten =
    Flow.<CompletionStage<Integer>>create().mapAsync(4, x -> x);

final Flow<Integer, Integer, CompletionStage<Integer>> foldingFlow =
    Flow.fromGraph(
        GraphDSL.create(
            foldSink,
            (b, fold) -> {
              return FlowShape.of(
                  fold.in(), b.from(b.materializedValue()).via(b.add(flatten)).out());
            }));




Be careful not to introduce a cycle where the materialized value actually contributes to the materialized value. The following example demonstrates a case where the materialized 
Future
CompletionStage
 of a fold is fed back to the fold itself.




Scala




copy
source
import GraphDSL.Implicits._
// This cannot produce any value:
val cyclicFold: Source[Int, Future[Int]] =
  Source.fromGraph(GraphDSL.createGraph(Sink.fold[Int, Int](0)(_ + _)) { implicit builder => fold =>
    // - Fold cannot complete until its upstream mapAsync completes
    // - mapAsync cannot complete until the materialized Future produced by
    //   fold completes
    // As a result this Source will never emit anything, and its materialited
    // Future will never complete
    builder.materializedValue.mapAsync(4)(identity) ~> fold
    SourceShape(builder.materializedValue.mapAsync(4)(identity).outlet)
  })


Java




copy
source
// This cannot produce any value:
final Source<Integer, CompletionStage<Integer>> cyclicSource =
    Source.fromGraph(
        GraphDSL.create(
            foldSink,
            (b, fold) -> {
              // - Fold cannot complete until its upstream mapAsync completes
              // - mapAsync cannot complete until the materialized Future produced by
              //   fold completes
              // As a result this Source will never emit anything, and its materialited
              // Future will never complete
              b.from(b.materializedValue()).via(b.add(flatten)).to(fold);
              return SourceShape.of(b.from(b.materializedValue()).via(b.add(flatten)).out());
            }));







Graph cycles, liveness and deadlocks


Cycles in bounded stream topologies need special considerations to avoid potential deadlocks and other liveness issues. This section shows several examples of problems that can arise from the presence of feedback arcs in stream processing graphs.


In the following examples runnable graphs are created but do not run because each have some issue and will deadlock after start. 
Source
 variable is not defined as the nature and number of element does not matter for described problems.


The first example demonstrates a graph that contains a naÃ¯ve cycle. The graph takes elements from the source, prints them, then broadcasts those elements to a consumer (we just used 
Sink.ignore
 for now) and to a feedback arc that is merged back into the main stream via a 
Merge
 junction.
Note


The graph DSL allows the connection arrows to be reversed, which is particularly handy when writing cyclesâas we will see there are cases where this is very helpful.




Scala




copy
source
// WARNING! The graph below deadlocks!
RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val merge = b.add(Merge[Int](2))
  val bcast = b.add(Broadcast[Int](2))

  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore
            merge                    <~                      bcast
  ClosedShape
})


Java




copy
source
// WARNING! The graph below deadlocks!
final Flow<Integer, Integer, NotUsed> printFlow =
    Flow.of(Integer.class)
        .map(
            s -> {
              System.out.println(s);
              return s;
            });

RunnableGraph.fromGraph(
    GraphDSL.create(
        b -> {
          final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));
          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
          final Outlet<Integer> src = b.add(source).out();
          final FlowShape<Integer, Integer> printer = b.add(printFlow);
          final SinkShape<Integer> ignore = b.add(Sink.ignore());

          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);
          b.to(merge).fromFanOut(bcast);
          return ClosedShape.getInstance();
        }));




Running this we observe that after a few numbers have been printed, no more elements are logged to the console - all processing stops after some time. After some investigation we observe that:




through merging from 
source
 we increase the number of elements flowing in the cycle


by broadcasting back to the cycle we do not decrease the number of elements in the cycle




Since Akka Streams (and Reactive Streams in general) guarantee bounded processing (see the “Buffering” section for more details) it means that only a bounded number of elements are buffered over any time span. Since our cycle gains more and more elements, eventually all of its internal buffers become full, backpressuring 
source
 forever. To be able to process more elements from 
source
 elements would need to leave the cycle somehow.


If we modify our feedback loop by replacing the 
Merge
 junction with a 
MergePreferred
 we can avoid the deadlock. 
MergePreferred
 is unfair as it always tries to consume from a preferred input port if there are elements available before trying the other lower priority input ports. Since we feed back through the preferred port it is always guaranteed that the elements in the cycles can flow.




Scala




copy
source
// WARNING! The graph below stops consuming from "source" after a few steps
RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val merge = b.add(MergePreferred[Int](1))
  val bcast = b.add(Broadcast[Int](2))

  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore
            merge.preferred              <~                  bcast
  ClosedShape
})


Java




copy
source
// WARNING! The graph below stops consuming from "source" after a few steps
RunnableGraph.fromGraph(
    GraphDSL.create(
        b -> {
          final MergePreferredShape<Integer> merge = b.add(MergePreferred.create(1));
          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
          final Outlet<Integer> src = b.add(source).out();
          final FlowShape<Integer, Integer> printer = b.add(printFlow);
          final SinkShape<Integer> ignore = b.add(Sink.ignore());

          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);
          b.to(merge.preferred()).fromFanOut(bcast);
          return ClosedShape.getInstance();
        }));




If we run the example we see that the same sequence of numbers are printed over and over again, but the processing does not stop. Hence, we avoided the deadlock, but 
source
 is still back-pressured forever, because buffer space is never recovered: the only action we see is the circulation of a couple of initial elements from 
source
.
Note


What we see here is that in certain cases we need to choose between boundedness and liveness. Our first example would not deadlock if there were an infinite buffer in the loop, or vice versa, if the elements in the cycle were balanced (as many elements are removed as many are injected) then there would be no deadlock.


To make our cycle both live (not deadlocking) and fair we can introduce a dropping element on the feedback arc. In this case we chose the 
buffer()
 operation giving it a dropping strategy 
OverflowStrategy.dropHead
.




Scala




copy
source
RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val merge = b.add(Merge[Int](2))
  val bcast = b.add(Broadcast[Int](2))

  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore
      merge <~ Flow[Int].buffer(10, OverflowStrategy.dropHead) <~ bcast
  ClosedShape
})


Java




copy
source
RunnableGraph.fromGraph(
    GraphDSL.create(
        b -> {
          final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));
          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
          final FlowShape<Integer, Integer> droppyFlow =
              b.add(Flow.of(Integer.class).buffer(10, OverflowStrategy.dropHead()));
          final Outlet<Integer> src = b.add(source).out();
          final FlowShape<Integer, Integer> printer = b.add(printFlow);
          final SinkShape<Integer> ignore = b.add(Sink.ignore());

          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);
          b.to(merge).via(droppyFlow).fromFanOut(bcast);
          return ClosedShape.getInstance();
        }));




If we run this example we see that




The flow of elements does not stop, there are always elements printed


We see that some of the numbers are printed several times over time (due to the feedback loop) but on average the numbers are increasing in the long term




This example highlights that one solution to avoid deadlocks in the presence of potentially unbalanced cycles (cycles where the number of circulating elements are unbounded) is to drop elements. An alternative would be to define a larger buffer with 
OverflowStrategy.fail
 which would fail the stream instead of deadlocking it after all buffer space has been consumed.


As we discovered in the previous examples, the core problem was the unbalanced nature of the feedback loop. We circumvented this issue by adding a dropping element, but now we want to build a cycle that is balanced from the beginning instead. To achieve this we modify our first graph by replacing the 
Merge
 junction with a 
ZipWith
. Since 
ZipWith
 takes one element from 
source
 
and
 from the feedback arc to inject one element into the cycle, we maintain the balance of elements.




Scala




copy
source
// WARNING! The graph below never processes any elements
RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val zip = b.add(ZipWith[Int, Int, Int]((left, right) => right))
  val bcast = b.add(Broadcast[Int](2))

  source ~> zip.in0
  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore
  zip.in1             <~                bcast
  ClosedShape
})


Java




copy
source
// WARNING! The graph below never processes any elements
RunnableGraph.fromGraph(
    GraphDSL.create(
        b -> {
          final FanInShape2<Integer, Integer, Integer> zip =
              b.add(ZipWith.create((Integer left, Integer right) -> left));
          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
          final FlowShape<Integer, Integer> printer = b.add(printFlow);
          final SinkShape<Integer> ignore = b.add(Sink.ignore());

          b.from(b.add(source)).toInlet(zip.in0());
          b.from(zip.out()).via(printer).viaFanOut(bcast).to(ignore);
          b.to(zip.in1()).fromFanOut(bcast);
          return ClosedShape.getInstance();
        }));




Still, when we try to run the example it turns out that no element is printed at all! After some investigation we realize that:




In order to get the first element from 
source
 into the cycle we need an already existing element in the cycle


In order to get an initial element in the cycle we need an element from 
source




These two conditions are a typical “chicken-and-egg” problem. The solution is to inject an initial element into the cycle that is independent from 
source
. We do this by using a 
Concat
 junction on the backwards arc that injects a single element using 
Source.single
.




Scala




copy
source
RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  val zip = b.add(ZipWith((left: Int, right: Int) => left))
  val bcast = b.add(Broadcast[Int](2))
  val concat = b.add(Concat[Int]())
  val start = Source.single(0)

  source ~> zip.in0
  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore
  zip.in1 <~ concat <~ start
             concat         <~          bcast
  ClosedShape
})


Java




copy
source
RunnableGraph.fromGraph(
    GraphDSL.create(
        b -> {
          final FanInShape2<Integer, Integer, Integer> zip =
              b.add(ZipWith.create((Integer left, Integer right) -> left));
          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));
          final UniformFanInShape<Integer, Integer> concat = b.add(Concat.create());
          final FlowShape<Integer, Integer> printer = b.add(printFlow);
          final SinkShape<Integer> ignore = b.add(Sink.ignore());

          b.from(b.add(source)).toInlet(zip.in0());
          b.from(zip.out()).via(printer).viaFanOut(bcast).to(ignore);
          b.to(zip.in1()).viaFanIn(concat).from(b.add(Source.single(1)));
          b.to(concat).fromFanOut(bcast);
          return ClosedShape.getInstance();
        }));




When we run the above example we see that processing starts and never stops. The important takeaway from this example is that balanced cycles often need an initial “kick-off” element to be injected into the cycle.














 
Basics and working with Flows






Modularity, Composition and Hierarchy 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-parallelism.html
Pipelining and Parallelism • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism




Dependency


Introduction


Pipelining


Parallel processing


Combining pipelining and parallel processing




Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism




Dependency


Introduction


Pipelining


Parallel processing


Combining pipelining and parallel processing




Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Pipelining and Parallelism


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


Akka Streams operators (be it simple operators on Flows and Sources or graph junctions) are “fused” together and executed sequentially by default. This avoids the overhead of events crossing asynchronous boundaries but limits the flow to execute at most one operator at any given time.


In many cases it is useful to be able to concurrently execute the operators of a flow, this is done by explicitly marking them as asynchronous using the 
async
async()
 method. Each operator marked as asynchronous will run in a dedicated actor internally, while all operators not marked asynchronous will run in one single actor.


We will illustrate through the example of pancake cooking how streams can be used for various processing patterns, exploiting the available parallelism on modern computers. The setting is the following: both Patrik and Roland like to make pancakes, but they need to produce sufficient amount in a cooking session to make all of the children happy. To increase their pancake production throughput they use two frying pans. How they organize their pancake processing is markedly different.


Pipelining


Roland uses the two frying pans in an asymmetric fashion. The first pan is only used to fry one side of the pancake then the half-finished pancake is flipped into the second pan for the finishing fry on the other side. Once the first frying pan becomes available it gets a new scoop of batter. As an effect, most of the time there are two pancakes being cooked at the same time, one being cooked on its first side and the second being cooked to completion. This is how this setup would look like implemented as a stream:




Scala




copy
source
// Takes a scoop of batter and creates a pancake with one side cooked
val fryingPan1: Flow[ScoopOfBatter, HalfCookedPancake, NotUsed] =
  Flow[ScoopOfBatter].map { batter =>
    HalfCookedPancake()
  }

// Finishes a half-cooked pancake
val fryingPan2: Flow[HalfCookedPancake, Pancake, NotUsed] =
  Flow[HalfCookedPancake].map { halfCooked =>
    Pancake()
  }

  // With the two frying pans we can fully cook pancakes
  val pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] =
    Flow[ScoopOfBatter].via(fryingPan1.async).via(fryingPan2.async)


Java




copy
source
Flow<ScoopOfBatter, HalfCookedPancake, NotUsed> fryingPan1 =
    Flow.of(ScoopOfBatter.class).map(batter -> new HalfCookedPancake());

Flow<HalfCookedPancake, Pancake, NotUsed> fryingPan2 =
    Flow.of(HalfCookedPancake.class).map(halfCooked -> new Pancake());

  // With the two frying pans we can fully cook pancakes
  Flow<ScoopOfBatter, Pancake, NotUsed> pancakeChef = fryingPan1.async().via(fryingPan2.async());




The two 
map
 operators in sequence (encapsulated in the “frying pan” flows) will be executed in a pipelined way, the same way that Roland was using his frying pans:




A 
ScoopOfBatter
 enters 
fryingPan1


fryingPan1
 emits a HalfCookedPancake once 
fryingPan2
 becomes available


fryingPan2
 takes the HalfCookedPancake


at this point fryingPan1 already takes the next scoop, without waiting for fryingPan2 to finish




The benefit of pipelining is that it can be applied to any sequence of processing steps that are otherwise not parallelisable (for example because the result of a processing step depends on all the information from the previous step). One drawback is that if the processing times of the operators are very different then some of the operators will not be able to operate at full throughput because they will wait on a previous or subsequent operator most of the time. In the pancake example frying the second half of the pancake is usually faster than frying the first half, 
fryingPan2
 will not be able to operate at full capacity 
[1]
.
Note


Asynchronous stream operators have internal buffers to make communication between them more efficient. For more details about the behavior of these and how to add additional buffers refer to 
Buffers and working with rate
.


Parallel processing


Patrik uses the two frying pans symmetrically. He uses both pans to fully fry a pancake on both sides, then puts the results on a shared plate. Whenever a pan becomes empty, he takes the next scoop from the shared bowl of batter. In essence he parallelizes the same process over multiple pans. This is how this setup will look like if implemented using streams:




Scala




copy
source
val fryingPan: Flow[ScoopOfBatter, Pancake, NotUsed] =
  Flow[ScoopOfBatter].map { batter =>
    Pancake()
  }

val pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] = Flow.fromGraph(GraphDSL.create() { implicit builder =>
  val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))
  val mergePancakes = builder.add(Merge[Pancake](2))

  // Using two frying pans in parallel, both fully cooking a pancake from the batter.
  // We always put the next scoop of batter to the first frying pan that becomes available.
  dispatchBatter.out(0) ~> fryingPan.async ~> mergePancakes.in(0)
  // Notice that we used the "fryingPan" flow without importing it via builder.add().
  // Flows used this way are auto-imported, which in this case means that the two
  // uses of "fryingPan" mean actually different stages in the graph.
  dispatchBatter.out(1) ~> fryingPan.async ~> mergePancakes.in(1)

  FlowShape(dispatchBatter.in, mergePancakes.out)
})



Java




copy
source
Flow<ScoopOfBatter, Pancake, NotUsed> fryingPan =
    Flow.of(ScoopOfBatter.class).map(batter -> new Pancake());

Flow<ScoopOfBatter, Pancake, NotUsed> pancakeChef =
    Flow.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));
              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =
                  b.add(Balance.create(2));

              // Using two frying pans in parallel, both fully cooking a pancake from the
              // batter.
              // We always put the next scoop of batter to the first frying pan that becomes
              // available.
              b.from(dispatchBatter.out(0))
                  .via(b.add(fryingPan.async()))
                  .toInlet(mergePancakes.in(0));
              // Notice that we used the "fryingPan" flow without importing it via
              // builder.add().
              // Flows used this way are auto-imported, which in this case means that the two
              // uses of "fryingPan" mean actually different stages in the graph.
              b.from(dispatchBatter.out(1))
                  .via(b.add(fryingPan.async()))
                  .toInlet(mergePancakes.in(1));

              return FlowShape.of(dispatchBatter.in(), mergePancakes.out());
            }));




The benefit of parallelizing is that it is easy to scale. In the pancake example it is easy to add a third frying pan with Patrik’s method, but Roland cannot add a third frying pan, since that would require a third processing step, which is not practically possible in the case of frying pancakes.


One drawback of the example code above is it does not preserve the ordering of pancakes. This might be a problem if children like to track their “own” pancakes. In those cases the 
Balance
Balance
 and 
Merge
Merge
 operators should be replaced by round-robin balancing and merging operators which put in and take out pancakes in a strict order.


A more detailed example of creating a worker pool can be found in the cookbook: 
Balancing jobs to a fixed pool of workers


Combining pipelining and parallel processing


The two concurrency patterns that we demonstrated as means to increase throughput are not exclusive. In fact, it is rather simple to combine the two approaches and streams provide a nice unifying language to express and compose them.


First, let’s look at how we can parallelize pipelined operators. In the case of pancakes this means that we will employ two chefs, each working using Roland’s pipelining method, but we use the two chefs in parallel, just like Patrik used the two frying pans. This is how it looks like if expressed as streams:




Scala




copy
source
val pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] =
  Flow.fromGraph(GraphDSL.create() { implicit builder =>
    val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))
    val mergePancakes = builder.add(Merge[Pancake](2))

    // Using two pipelines, having two frying pans each, in total using
    // four frying pans
    dispatchBatter.out(0) ~> fryingPan1.async ~> fryingPan2.async ~> mergePancakes.in(0)
    dispatchBatter.out(1) ~> fryingPan1.async ~> fryingPan2.async ~> mergePancakes.in(1)

    FlowShape(dispatchBatter.in, mergePancakes.out)
  })


Java




copy
source
Flow<ScoopOfBatter, Pancake, NotUsed> pancakeChef =
    Flow.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));
              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =
                  b.add(Balance.create(2));

              // Using two pipelines, having two frying pans each, in total using
              // four frying pans
              b.from(dispatchBatter.out(0))
                  .via(b.add(fryingPan1.async()))
                  .via(b.add(fryingPan2.async()))
                  .toInlet(mergePancakes.in(0));

              b.from(dispatchBatter.out(1))
                  .via(b.add(fryingPan1.async()))
                  .via(b.add(fryingPan2.async()))
                  .toInlet(mergePancakes.in(1));

              return FlowShape.of(dispatchBatter.in(), mergePancakes.out());
            }));




The above pattern works well if there are many independent jobs that do not depend on the results of each other, but the jobs themselves need multiple processing steps where each step builds on the result of the previous one. In our case individual pancakes do not depend on each other, they can be cooked in parallel, on the other hand it is not possible to fry both sides of the same pancake at the same time, so the two sides have to be fried in sequence.


It is also possible to organize parallelized operators into pipelines. This would mean employing four chefs:




the first two chefs prepare half-cooked pancakes from batter, in parallel, then putting those on a large enough flat surface.


the second two chefs take these and fry their other side in their own pans, then they put the pancakes on a shared plate.




This is again straightforward to implement with the streams API:




Scala




copy
source
val pancakeChefs1: Flow[ScoopOfBatter, HalfCookedPancake, NotUsed] =
  Flow.fromGraph(GraphDSL.create() { implicit builder =>
    val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))
    val mergeHalfPancakes = builder.add(Merge[HalfCookedPancake](2))

    // Two chefs work with one frying pan for each, half-frying the pancakes then putting
    // them into a common pool
    dispatchBatter.out(0) ~> fryingPan1.async ~> mergeHalfPancakes.in(0)
    dispatchBatter.out(1) ~> fryingPan1.async ~> mergeHalfPancakes.in(1)

    FlowShape(dispatchBatter.in, mergeHalfPancakes.out)
  })

val pancakeChefs2: Flow[HalfCookedPancake, Pancake, NotUsed] =
  Flow.fromGraph(GraphDSL.create() { implicit builder =>
    val dispatchHalfPancakes = builder.add(Balance[HalfCookedPancake](2))
    val mergePancakes = builder.add(Merge[Pancake](2))

    // Two chefs work with one frying pan for each, finishing the pancakes then putting
    // them into a common pool
    dispatchHalfPancakes.out(0) ~> fryingPan2.async ~> mergePancakes.in(0)
    dispatchHalfPancakes.out(1) ~> fryingPan2.async ~> mergePancakes.in(1)

    FlowShape(dispatchHalfPancakes.in, mergePancakes.out)
  })

val kitchen: Flow[ScoopOfBatter, Pancake, NotUsed] = pancakeChefs1.via(pancakeChefs2)


Java




copy
source
Flow<ScoopOfBatter, HalfCookedPancake, NotUsed> pancakeChefs1 =
    Flow.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanInShape<HalfCookedPancake, HalfCookedPancake> mergeHalfCooked =
                  b.add(Merge.create(2));
              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =
                  b.add(Balance.create(2));

              // Two chefs work with one frying pan for each, half-frying the pancakes then
              // putting
              // them into a common pool
              b.from(dispatchBatter.out(0))
                  .via(b.add(fryingPan1.async()))
                  .toInlet(mergeHalfCooked.in(0));
              b.from(dispatchBatter.out(1))
                  .via(b.add(fryingPan1.async()))
                  .toInlet(mergeHalfCooked.in(1));

              return FlowShape.of(dispatchBatter.in(), mergeHalfCooked.out());
            }));

Flow<HalfCookedPancake, Pancake, NotUsed> pancakeChefs2 =
    Flow.fromGraph(
        GraphDSL.create(
            b -> {
              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));
              final UniformFanOutShape<HalfCookedPancake, HalfCookedPancake>
                  dispatchHalfCooked = b.add(Balance.create(2));

              // Two chefs work with one frying pan for each, finishing the pancakes then
              // putting
              // them into a common pool
              b.from(dispatchHalfCooked.out(0))
                  .via(b.add(fryingPan2.async()))
                  .toInlet(mergePancakes.in(0));
              b.from(dispatchHalfCooked.out(1))
                  .via(b.add(fryingPan2.async()))
                  .toInlet(mergePancakes.in(1));

              return FlowShape.of(dispatchHalfCooked.in(), mergePancakes.out());
            }));

Flow<ScoopOfBatter, Pancake, NotUsed> kitchen = pancakeChefs1.via(pancakeChefs2);




This usage pattern is less common but might be usable if a certain step in the pipeline might take wildly different times to finish different jobs. The reason is that there are more balance-merge steps in this pattern compared to the parallel pipelines. This pattern rebalances after each step, while the previous pattern only balances at the entry point of the pipeline. This only matters however if the processing time distribution has a large deviation.




[1]
 Roland’s reason for this seemingly suboptimal procedure is that he prefers the temperature of the second pan to be slightly lower than the first in order to achieve a more homogeneous result.
















 
StreamRefs - Reactive Streams over the network






Testing streams 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-customize.html
Custom stream processing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing




Dependency


Introduction


Custom processing with GraphStage


Thread safety of custom operators


Resources and the operator lifecycle


Extending Flow Operators with Custom Operators




Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing




Dependency


Introduction


Custom processing with GraphStage


Thread safety of custom operators


Resources and the operator lifecycle


Extending Flow Operators with Custom Operators




Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Custom stream processing


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


While the processing vocabulary of Akka Streams is quite rich (see the 
Streams Cookbook
 for examples) it is sometimes necessary to define new transformation operators either because some functionality is missing from the stock operations, or for performance reasons. In this part we show how to build custom operators and graph junctions of various kinds.
Note


A custom operator should not be the first tool you reach for, defining operators using flows and the graph DSL is in general easier and does to a larger extent protect you from mistakes that might be easy to make with a custom 
GraphStage




Custom processing with GraphStage


The 
GraphStage
GraphStage
 abstraction can be used to create arbitrary operators with any number of input or output ports. It is a counterpart of the 
GraphDSL.create()
GraphDSL.create()
 method which creates new stream processing operators by composing others. Where 
GraphStage
 differs is that it creates an operator that is itself not divisible into smaller ones, and allows state to be maintained inside it in a safe way.


As a first motivating example, we will build a new 
Source
Source
 that will emit numbers from 1 until it is cancelled. To start, we need to define the “interface” of our operator, which is called 
shape
 in Akka Streams terminology (this is explained in more detail in the section 
Modularity, Composition and Hierarchy
). This is how it looks:




Scala




copy
source
import akka.stream.SourceShape
import akka.stream.stage.GraphStage

class NumbersSource extends GraphStage[SourceShape[Int]] {
  // Define the (sole) output port of this stage
  val out: Outlet[Int] = Outlet("NumbersSource")
  // Define the shape of this stage, which is SourceShape with the port we defined above
  override val shape: SourceShape[Int] = SourceShape(out)

  // This is where the actual (possibly stateful) logic will live
  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = ???
}


Java




copy
source
public class NumbersSource extends GraphStage<SourceShape<Integer>> {
  // Define the (sole) output port of this stage
  public final Outlet<Integer> out = Outlet.create("NumbersSource.out");

  // Define the shape of this stage, which is SourceShape with the port we defined above
  private final SourceShape<Integer> shape = SourceShape.of(out);

  @Override
  public SourceShape<Integer> shape() {
    return shape;
  }

  // This is where the actual (possibly stateful) logic is created
  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape()) {
      // All state MUST be inside the GraphStageLogic,
      // never inside the enclosing GraphStage.
      // This state is safe to access and modify from all the
      // callbacks that are provided by GraphStageLogic and the
      // registered handlers.
      private int counter = 1;

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                push(out, counter);
                counter += 1;
              }
            });
      }
    };
  }
}




As you see, in itself the 
GraphStage
 only defines the ports of this operator and a shape that contains the ports. It also has, a currently unimplemented method called 
createLogic
createLogic
. If you recall, operators are reusable in multiple materializations, each resulting in a different executing entity. In the case of 
GraphStage
 the actual running logic is modeled as an instance of a 
GraphStageLogic
GraphStageLogic
 which will be created by the materializer by calling the 
createLogic
 method. In other words, all we need to do is to create a suitable logic that will emit the numbers we want.
Note


It is very important to keep the GraphStage object itself immutable and reusable. All mutable state needs to be confined to the GraphStageLogic that is created for every materialization.


In order to emit from a 
Source
Source
 in a backpressured stream one needs first to have demand from downstream. To receive the necessary events one needs to register a subclass of 
OutHandler
 
AbstractOutHandler
 with the output port (
Outlet
Outlet
). This handler will receive events related to the lifecycle of the port. In our case we need to override 
onPull()
 which indicates that we are free to emit a single element. There is another callback, 
onDownstreamFinish()
 which is called if the downstream cancelled. Since the default behavior of that callback is to stop the operator, we don’t need to override it. In the 
onPull
 callback we will emit the next number. This is how it looks like in the end:




Scala




copy
source
import akka.stream.Attributes
import akka.stream.Outlet
import akka.stream.SourceShape
import akka.stream.stage.GraphStage
import akka.stream.stage.GraphStageLogic
import akka.stream.stage.OutHandler

class NumbersSource extends GraphStage[SourceShape[Int]] {
  val out: Outlet[Int] = Outlet("NumbersSource")
  override val shape: SourceShape[Int] = SourceShape(out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {
      // All state MUST be inside the GraphStageLogic,
      // never inside the enclosing GraphStage.
      // This state is safe to access and modify from all the
      // callbacks that are provided by GraphStageLogic and the
      // registered handlers.
      private var counter = 1

      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          push(out, counter)
          counter += 1
        }
      })
    }
}




Instances of the above 
GraphStage
GraphStage
 are subclasses of 
Graph[SourceShape[Int],NotUsed]
 
Graph<SourceShape<Integer>,NotUsed>
 which means that they are already usable in many situations, but do not provide the DSL methods we usually have for other 
Source
 s. In order to convert this 
Graph
 to a proper 
Source
 we need to wrap it using 
Source.fromGraph
 (see 
Modularity, Composition and Hierarchy
 for more details about operators and DSLs). Now we can use the source as any other built-in one:




Scala




copy
source
// A GraphStage is a proper Graph, just like what GraphDSL.create would return
val sourceGraph: Graph[SourceShape[Int], NotUsed] = new NumbersSource

// Create a Source from the Graph to access the DSL
val mySource: Source[Int, NotUsed] = Source.fromGraph(sourceGraph)

// Returns 55
val result1: Future[Int] = mySource.take(10).runFold(0)(_ + _)

// The source is reusable. This returns 5050
val result2: Future[Int] = mySource.take(100).runFold(0)(_ + _)


Java




copy
source
// A GraphStage is a proper Graph, just like what GraphDSL.create would return
Graph<SourceShape<Integer>, NotUsed> sourceGraph = new NumbersSource();

// Create a Source from the Graph to access the DSL
Source<Integer, NotUsed> mySource = Source.fromGraph(sourceGraph);

// Returns 55
CompletionStage<Integer> result1 =
    mySource.take(10).runFold(0, (sum, next) -> sum + next, system);

// The source is reusable. This returns 5050
CompletionStage<Integer> result2 =
    mySource.take(100).runFold(0, (sum, next) -> sum + next, system);




Similarly, to create a custom 
Sink
Sink
 one can register a subclass 
InHandler
InHandler
 with the operator 
Inlet
Inlet
. The 
onPush()
onPush()
 callback is used to signal the handler a new element has been pushed to the operator, and can hence be grabbed and used. 
onPush()
 can be overridden to provide custom behavior. Please note, most Sinks would need to request upstream elements as soon as they are created: this can be done by calling 
pull(inlet)
pull(inlet)
 in the 
preStart()
preStart()
 callback.




Scala




copy
source
import akka.stream.Attributes
import akka.stream.Inlet
import akka.stream.SinkShape
import akka.stream.stage.GraphStage
import akka.stream.stage.GraphStageLogic
import akka.stream.stage.InHandler

class StdoutSink extends GraphStage[SinkShape[Int]] {
  val in: Inlet[Int] = Inlet("StdoutSink")
  override val shape: SinkShape[Int] = SinkShape(in)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {

      // This requests one element at the Sink startup.
      override def preStart(): Unit = pull(in)

      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          println(grab(in))
          pull(in)
        }
      })
    }
}


Java




copy
source
public class StdoutSink extends GraphStage<SinkShape<Integer>> {
  public final Inlet<Integer> in = Inlet.create("StdoutSink.in");

  private final SinkShape<Integer> shape = SinkShape.of(in);

  @Override
  public SinkShape<Integer> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape()) {

      // This requests one element at the Sink startup.
      @Override
      public void preStart() {
        pull(in);
      }

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                Integer element = grab(in);
                System.out.println(element);
                pull(in);
              }
            });
      }
    };
  }
}




Port states, 
InHandler
 
AbstractInHandler
 and 
OutHandler
 
AbstractOutHandler


In order to interact with a port (
Inlet
Inlet
 or 
Outlet
Outlet
) of the operator we need to be able to receive events and generate new events belonging to the port.


Output port


From the 
GraphStageLogic
GraphStageLogic
 the following operations are available on an output port:




push(out,elem)
 pushes an element to the output port. Only possible after the port has been pulled by downstream.


complete(out)
 closes the output port normally.


fail(out,exception)
 closes the port with a failure signal.




The events corresponding to an 
output
 port can be received in an 
OutHandler
AbstractOutHandler
 instance registered to the output port using 
setHandler(out,handler)
. This handler has two callbacks:




onPull()
onPull()
 is called when the output port is ready to emit the next element, 
push(out, elem)
 is now allowed to be called on this port.


onDownstreamFinish()
onDownstreamFinish()
 is called once the downstream has cancelled and no longer allows messages to be pushed to it. No more 
onPull()
 will arrive after this event. If not overridden this will default to stopping the operator.




Also, there are two query methods available for output ports:




isAvailable(out)
isAvailable(out)
 returns true if the port can be pushed


isClosed(out)
 returns true if the port is closed. At this point the port can not be pushed and will not be pulled anymore.




The relationship of the above operations, events and queries are summarized in the state machine below. Green shows the initial state while orange indicates the end state. If an operation is not listed for a state, then it is invalid to call it while the port is in that state. If an event is not listed for a state, then that event cannot happen in that state.




Input port


The following operations are available for 
input
 ports:




pull(in)
 requests a new element from an input port. This is only possible after the port has been pushed by upstream.


grab(in)
 acquires the element that has been received during an 
onPush()
. It cannot be called again until the port is pushed again by the upstream.


cancel(in)
 closes the input port.




The events corresponding to an 
input
 port can be received in an 
InHandler
 
AbstractInHandler
 instance registered to the input port using 
setHandler(in, handler)
. This handler has three callbacks:




onPush()
 is called when the input port has now a new element. Now it is possible to acquire this element using 
grab(in)
 and/or call 
pull(in)
 on the port to request the next element. It is not mandatory to grab the element, but if it is pulled while the element has not been grabbed it will drop the buffered element.


onUpstreamFinish()
 is called once the upstream has completed and no longer can be pulled for new elements. No more 
onPush()
 will arrive after this event. If not overridden this will default to stopping the operator.


onUpstreamFailure()
 is called if the upstream failed with an exception and no longer can be pulled for new elements. No more 
onPush()
 will arrive after this event. If not overridden this will default to failing the operator.




Also, there are three query methods available for input ports:




isAvailable(in)
 returns true if the port can be grabbed.


hasBeenPulled(in)
 returns true if the port has been already pulled. Calling 
pull(in)
 in this state is illegal.


isClosed(in)
 returns true if the port is closed. At this point the port can not be pulled and will not be pushed anymore.




The relationship of the above operations, events and queries are summarized in the state machine below. Green shows the initial state while orange indicates the end state. If an operation is not listed for a state, then it is invalid to call it while the port is in that state. If an event is not listed for a state, then that event cannot happen in that state.




Complete and fail


Finally, there are two methods available for convenience to complete the operator and all of its ports:




completeStage()
completeStage()
 is equivalent to closing all output ports and cancelling all input ports.


failStage(exception)
failStage(exception)
 is equivalent to failing all output ports and cancelling all input ports.




Emit


In some cases it is inconvenient and error prone to react on the regular state machine events with the signal based API described above. For those cases there is an API which allows for a more declarative sequencing of actions which will greatly simplify some use cases at the cost of some extra allocations. The difference between the two APIs could be described as that the first one is signal driven from the outside, while this API is more active and drives its surroundings.


The operations of this part of the 
GraphStage
GraphStage
 API are:




emit(out, elem)
 and 
emitMultiple(out, Iterable(elem1, elem2))
 replaces the 
OutHandler
 with a handler that emits one or more elements when there is demand, and then reinstalls the current handlers


read(in)(andThen)
 and 
readN(in, n)(andThen)
 replaces the 
InHandler
 with a handler that reads one or more elements as they are pushed and allows the handler to react once the requested number of elements has been read.


abortEmitting()
 and 
abortReading()
 which will cancel an ongoing emit or read




Note that since the above methods are implemented by temporarily replacing the handlers of the operator you should never call 
setHandler
 while they are running 
emit
 or 
read
 as that interferes with how they are implemented. The following methods are safe to call after invoking 
emit
 and 
read
 (and will lead to actually running the operation when those are done): 
complete(out)
, 
completeStage()
, 
emit
, 
emitMultiple
, 
abortEmitting()
 and 
abortReading()


An example of how this API simplifies an operator can be found below in the second version of the 
Duplicator
.


Custom linear operators using GraphStage


To define custom linear operators, you should extend 
GraphStage
GraphStage
 using 
FlowShape
FlowShape
 which has one input and one output.


Such an operator can be illustrated as a box with two flows as it is seen in the illustration below. Demand flowing upstream leading to elements flowing downstream.




To illustrate these concepts we create a small 
GraphStage
 that implements the 
map
 transformation.




Map calls 
push(out)
 from the 
onPush()
 handler and it also calls 
pull()
 from the 
onPull
 handler resulting in the conceptual wiring above, and fully expressed in code below:




Scala




copy
source
class Map[A, B](f: A => B) extends GraphStage[FlowShape[A, B]] {

  val in = Inlet[A]("Map.in")
  val out = Outlet[B]("Map.out")

  override val shape = FlowShape.of(in, out)

  override def createLogic(attr: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {
      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          push(out, f(grab(in)))
        }
      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          pull(in)
        }
      })
    }
}


Java




copy
source
public class Map<A, B> extends GraphStage<FlowShape<A, B>> {

  private final Function<A, B> f;

  public Map(Function<A, B> f) {
    this.f = f;
  }

  public final Inlet<A> in = Inlet.create("Map.in");
  public final Outlet<B> out = Outlet.create("Map.out");

  private final FlowShape<A, B> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, B> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                push(out, f.apply(grab(in)));
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
      }
    };
  }
}




Map is a typical example of a one-to-one transformation of a stream where demand is passed along upstream elements passed on downstream.


To demonstrate a many-to-one operator we will implement filter. The conceptual wiring of 
Filter
 looks like this:




As we see above, if the given predicate matches the current element we are propagating it downwards, otherwise we return the âballâ to our upstream so that we get the new element. This is achieved by modifying the map example by adding a conditional in the 
onPush
 handler and decide between a 
pull(in)
 or 
push(out)
 call (and not having a mapping 
f
 function).




Scala




copy
source
class Filter[A](p: A => Boolean) extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("Filter.in")
  val out = Outlet[A]("Filter.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {
      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          if (p(elem)) push(out, elem)
          else pull(in)
        }
      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          pull(in)
        }
      })
    }
}


Java




copy
source
public final class Filter<A> extends GraphStage<FlowShape<A, A>> {

  private final Predicate<A> p;

  public Filter(Predicate<A> p) {
    this.p = p;
  }

  public final Inlet<A> in = Inlet.create("Filter.in");
  public final Outlet<A> out = Outlet.create("Filter.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                A elem = grab(in);
                if (p.test(elem)) {
                  push(out, elem);
                } else {
                  pull(in);
                }
              }
            });

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
      }
    };
  }
}




To complete the picture we define a one-to-many transformation as the next step. We chose a straightforward example operator that emits every upstream element twice downstream. The conceptual wiring of this operator looks like this:




This is an operator that has state: an option with the last element it has seen indicating if it has duplicated this last element already or not. We must also make sure to emit the extra element if the upstream completes.




Scala




copy
source
class Duplicator[A] extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("Duplicator.in")
  val out = Outlet[A]("Duplicator.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {
      // Again: note that all mutable state
      // MUST be inside the GraphStageLogic
      var lastElem: Option[A] = None

      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          lastElem = Some(elem)
          push(out, elem)
        }

        override def onUpstreamFinish(): Unit = {
          if (lastElem.isDefined) emit(out, lastElem.get)
          complete(out)
        }

      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          if (lastElem.isDefined) {
            push(out, lastElem.get)
            lastElem = None
          } else {
            pull(in)
          }
        }
      })
    }
}


Java




copy
source
public class Duplicator<A> extends GraphStage<FlowShape<A, A>> {

  public final Inlet<A> in = Inlet.create("Duplicator.in");
  public final Outlet<A> out = Outlet.create("Duplicator.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      // Again: note that all mutable state
      // MUST be inside the GraphStageLogic
      Option<A> lastElem = Option.none();

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                A elem = grab(in);
                lastElem = Option.some(elem);
                push(out, elem);
              }

              @Override
              public void onUpstreamFinish() {
                if (lastElem.isDefined()) {
                  emit(out, lastElem.get());
                }
                complete(out);
              }
            });

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                if (lastElem.isDefined()) {
                  push(out, lastElem.get());
                  lastElem = Option.none();
                } else {
                  pull(in);
                }
              }
            });
      }
    };
  }
}




In this case a pull from downstream might be consumed by the operator itself rather than passed along upstream as the operator might contain an element it wants to push. Note that we also need to handle the case where the upstream closes while the operator still has elements it wants to push downstream. This is done by overriding 
onUpstreamFinish
 in the 
InHandler
AbstractInHandler
 and provide custom logic that should happen when the upstream has been finished.


This example can be simplified by replacing the usage of a mutable state with calls to 
emitMultiple
 which will replace the handlers, emit each of multiple elements and then reinstate the original handlers:




Scala




copy
source
class Duplicator[A] extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("Duplicator.in")
  val out = Outlet[A]("Duplicator.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {

      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          // this will temporarily suspend this handler until the two elems
          // are emitted and then reinstates it
          emitMultiple(out, Iterable(elem, elem))
        }
      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          pull(in)
        }
      })
    }
}


Java




copy
source
public class Duplicator2<A> extends GraphStage<FlowShape<A, A>> {

  public final Inlet<A> in = Inlet.create("Duplicator.in");
  public final Outlet<A> out = Outlet.create("Duplicator.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                A elem = grab(in);
                // this will temporarily suspend this handler until the two elems
                // are emitted and then reinstates it
                emitMultiple(out, Arrays.asList(elem, elem).iterator());
              }
            });

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
      }
    };
  }
}




Finally, to demonstrate all of the operators above, we put them together into a processing chain, which conceptually would correspond to the following structure:




In code this is only a few lines, using the 
via
via
 use our custom operators in a stream:




Scala




copy
source
val resultFuture =
  Source(1 to 5).via(new Filter(_ % 2 == 0)).via(new Duplicator()).via(new Map(_ / 2)).runWith(sink)



Java




copy
source
CompletionStage<String> resultFuture =
    Source.from(Arrays.asList(1, 2, 3, 4, 5))
        .via(new Filter<Integer>((n) -> n % 2 == 0))
        .via(new Duplicator<Integer>())
        .via(new Map<Integer, Integer>((n) -> n / 2))
        .runWith(sink, system);





If we attempt to draw the sequence of events, it shows that there is one “event token” in circulation in a potential chain of operators, just like our conceptual “railroad tracks” representation predicts.




Completion


Completion handling usually (but not exclusively) comes into the picture when operators need to emit a few more elements after their upstream source has been completed. We have seen an example of this in our first 
Duplicator
 implementation where the last element needs to be doubled even after the upstream neighbor operator has been completed. This can be done by overriding the 
onUpstreamFinish
 method in 
InHandler
AbstractInHandler
.


Operators by default automatically stop once all of their ports (input and output) have been closed externally or internally. It is possible to opt out from this behavior by invoking 
setKeepGoing(true)
 (which is not supported from the operatorâs constructor and usually done in 
preStart
). In this case the operator 
must
 be explicitly closed by calling 
completeStage()
completeStage()
 or `
failStage(exception)
failStage(exception)
. This feature carries the risk of leaking streams and actors, therefore it should be used with care.


Logging inside GraphStages


Logging debug or other important information in your operators is often a very good idea, especially when developing more advanced operators which may need to be debugged at some point.


The helper trait 
StageLogging
 is provided to enable you to obtain a 
LoggingAdapter
LoggingAdapter
 inside of a 
GraphStage
GraphStage
 as long as the 
Materializer
Materializer
 you’re using is able to provide you with a logger. In that sense, it serves a very similar purpose as 
ActorLogging
ActorLogging
 does for Actors. 


You can extend the 
GraphStageLogicWithLogging
GraphStageLogicWithLogging
 or 
TimerGraphStageLogicWithLogging
TimerGraphStageLogicWithLogging
 classes instead of the usual 
GraphStageLogic
GraphStageLogic
 to enable you to obtain a 
LoggingAdapter
LoggingAdapter
 inside your operator as long as the 
Materializer
Materializer
 you’re using is able to provide you with a logger.
Note


Please note that you can always use a logging library directly inside an operator. Make sure to use an asynchronous appender however, to not accidentally block the operator when writing to files etc. See 
Using the SLF4J API directly
 for more details on setting up async appenders in SLF4J.


The operator then gets access to the 
log
 field which it can safely use from any 
GraphStage
 callbacks:




Scala




copy
source
import akka.stream.stage.{ GraphStage, GraphStageLogic, OutHandler, StageLogging }

final class RandomLettersSource extends GraphStage[SourceShape[String]] {
  val out = Outlet[String]("RandomLettersSource.out")
  override val shape: SourceShape[String] = SourceShape(out)

  override def createLogic(inheritedAttributes: Attributes) =
    new GraphStageLogic(shape) with StageLogging {
      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          val c = nextChar() // ASCII lower case letters

          // `log` is obtained from materializer automatically (via StageLogging)
          log.debug("Randomly generated: [{}]", c)

          push(out, c.toString)
        }
      })
    }

  def nextChar(): Char =
    ThreadLocalRandom.current().nextInt('a', 'z'.toInt + 1).toChar
}


Java




copy
source
public class RandomLettersSource extends GraphStage<SourceShape<String>> {
  public final Outlet<String> out = Outlet.create("RandomLettersSource.in");

  private final SourceShape<String> shape = SourceShape.of(out);

  @Override
  public SourceShape<String> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogicWithLogging(shape()) {

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                final String s = nextChar(); // ASCII lower case letters

                // `log` is obtained from materializer automatically (via StageLogging)
                log().debug("Randomly generated: [{}]", s);

                push(out, s);
              }

              private String nextChar() {
                final char i = (char) ThreadLocalRandom.current().nextInt('a', 'z' + 1);
                return String.valueOf(i);
              }
            });
      }
    };
  }
}


Note


SPI Note:
 If you’re implementing a Materializer, you can add this ability to your materializer by implementing 
MaterializerLoggingProvider
MaterializerLoggingProvider
 in your 
Materializer
Materializer
.


Using timers


It is possible to use timers in 
GraphStages
 by using 
TimerGraphStageLogic
TimerGraphStageLogic
 as the base class for the returned logic. Timers can be scheduled by calling one of 
scheduleOnce(timerKey,delay)
, 
scheduleAtFixedRate(timerKey,initialDelay,interval)
 or 
scheduleWithFixedDelay(timerKey,initialDelay,interval)
 and passing an object as a key for that timer (can be any object, for example a 
String
). The 
onTimer(timerKey)
 method needs to be overridden, and it will be called once the timer of 
timerKey
 fires. It is possible to cancel a timer using 
cancelTimer(timerKey)
 and check the status of a timer with 
isTimerActive(timerKey)
. Timers will be automatically cleaned up when the operator completes.


Timers can not be scheduled from the constructor of the logic, but it is possible to schedule them from the 
preStart()
 lifecycle hook.


In this sample the operator toggles between open and closed, where open means no elements are passed through. The operator starts out as closed but as soon as an element is pushed downstream the gate becomes open for a duration of time during which it will consume and drop upstream messages:




Scala




copy
source
// each time an event is pushed through it will trigger a period of silence
class TimedGate[A](silencePeriod: FiniteDuration) extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("TimedGate.in")
  val out = Outlet[A]("TimedGate.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new TimerGraphStageLogic(shape) {

      var open = false

      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          if (open) pull(in)
          else {
            push(out, elem)
            open = true
            scheduleOnce(None, silencePeriod)
          }
        }
      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = { pull(in) }
      })

      override protected def onTimer(timerKey: Any): Unit = {
        open = false
      }
    }
}


Java




copy
source
// each time an event is pushed through it will trigger a period of silence
public class TimedGate<A> extends GraphStage<FlowShape<A, A>> {

  private final int silencePeriodInSeconds;

  public TimedGate(int silencePeriodInSeconds) {
    this.silencePeriodInSeconds = silencePeriodInSeconds;
  }

  public final Inlet<A> in = Inlet.create("TimedGate.in");
  public final Outlet<A> out = Outlet.create("TimedGate.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new TimerGraphStageLogic(shape) {

      private boolean open = false;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                A elem = grab(in);
                if (open) pull(in);
                else {
                  push(out, elem);
                  open = true;
                  scheduleOnce("key", java.time.Duration.ofSeconds(silencePeriodInSeconds));
                }
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
      }

      @Override
      public void onTimer(Object key) {
        if (key.equals("key")) {
          open = false;
        }
      }
    };
  }
}




Using asynchronous side-channels


In order to receive asynchronous events that are not arriving as stream elements (for example a completion of a future or a callback from a 3rd party API) one must acquire a 
AsyncCallback
AsyncCallback
 by calling 
getAsyncCallback()
getAsyncCallback()
 from the operator logic. The method 
getAsyncCallback
 takes as a parameter a callback that will be called once the asynchronous event fires. It is important to 
not call the callback directly
, instead, the external API must call the 
invoke(event)
 method on the returned 
AsyncCallback
. The execution engine will take care of calling the provided callback in a thread-safe way. The callback can safely access the state of the 
GraphStageLogic
GraphStageLogic
 implementation.


Sharing the AsyncCallback from the constructor risks race conditions, therefore it is recommended to use the 
preStart()
 lifecycle hook instead.


This example shows an asynchronous side channel operator that starts dropping elements when a future completes:




Scala




copy
source
// will close upstream in all materializations of the graph stage instance
// when the future completes
class KillSwitch[A](switch: Future[Unit]) extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("KillSwitch.in")
  val out = Outlet[A]("KillSwitch.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {

      override def preStart(): Unit = {
        val callback = getAsyncCallback[Unit] { (_) =>
          completeStage()
        }
        switch.foreach(callback.invoke)
      }

      setHandler(in, new InHandler {
        override def onPush(): Unit = { push(out, grab(in)) }
      })
      setHandler(out, new OutHandler {
        override def onPull(): Unit = { pull(in) }
      })
    }
}


Java




copy
source
// will close upstream in all materializations of the stage instance
// when the completion stage completes
public class KillSwitch<A> extends GraphStage<FlowShape<A, A>> {

  private final CompletionStage<Done> switchF;

  public KillSwitch(CompletionStage<Done> switchF) {
    this.switchF = switchF;
  }

  public final Inlet<A> in = Inlet.create("KillSwitch.in");
  public final Outlet<A> out = Outlet.create("KillSwitch.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                push(out, grab(in));
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
      }

      @Override
      public void preStart() {
        AsyncCallback<Done> callback =
            createAsyncCallback(
                new Procedure<Done>() {
                  @Override
                  public void apply(Done param) throws Exception {
                    completeStage();
                  }
                });

        ExecutionContext ec = system.dispatcher();
        switchF.thenAccept(callback::invoke);
      }
    };
  }
}




Integration with actors


This section is a stub and will be extended in the next release
 
This is a 
may change
 feature
*


It is possible to acquire an ActorRef that can be addressed from the outside of the operator, similarly how 
AsyncCallback
AsyncCallback
 allows injecting asynchronous events into an operator logic. This reference can be obtained by calling 
getStageActor(receive)
 passing in a function that takes a 
Pair
 of the sender 
ActorRef
ActorRef
 and the received message. This reference can be used to watch other actors by calling its 
watch(ref)
 or 
unwatch(ref)
 methods. The reference can be also watched by external actors. The current limitations of this 
ActorRef
 are:




they are not location transparent, they cannot be accessed via remoting.


they cannot be returned as materialized values.


they cannot be accessed from the constructor of the 
GraphStageLogic
, but they can be accessed from the 
preStart()
 method.




Custom materialized values


Custom operators can return materialized values instead of 
NotUsed
NotUsed
 by inheriting from 
GraphStageWithMaterializedValue
AbstractGraphStageWithMaterializedValue
 instead of the simpler 
GraphStage
. The difference is that in this case the method 
createLogicAndMaterializedValue(inheritedAttributes)
createLogicAndMaterializedValue(inheritedAttributes
 needs to be overridden, and in addition to the operator logic the materialized value must be provided
Warning


There is no built-in synchronization of accessing this value from both of the thread where the logic runs and the thread that got hold of the materialized value. It is the responsibility of the programmer to add the necessary (non-blocking) synchronization and visibility guarantees to this shared object.


In this sample the materialized value is a future containing the first element to go through the stream:




Scala




copy
source
class FirstValue[A] extends GraphStageWithMaterializedValue[FlowShape[A, A], Future[A]] {

  val in = Inlet[A]("FirstValue.in")
  val out = Outlet[A]("FirstValue.out")

  val shape = FlowShape.of(in, out)

  override def createLogicAndMaterializedValue(inheritedAttributes: Attributes): (GraphStageLogic, Future[A]) = {
    val promise = Promise[A]()
    val logic = new GraphStageLogic(shape) {

      setHandler(in, new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          promise.success(elem)
          push(out, elem)

          // replace handler with one that only forwards elements
          setHandler(in, new InHandler {
            override def onPush(): Unit = {
              push(out, grab(in))
            }
          })
        }
      })

      setHandler(out, new OutHandler {
        override def onPull(): Unit = {
          pull(in)
        }
      })

    }

    (logic, promise.future)
  }
}


Java




copy
source
public class FirstValue<A>
    extends AbstractGraphStageWithMaterializedValue<FlowShape<A, A>, CompletionStage<A>> {

  public final Inlet<A> in = Inlet.create("FirstValue.in");
  public final Outlet<A> out = Outlet.create("FirstValue.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  @Override
  public Pair<GraphStageLogic, CompletionStage<A>> createLogicAndMaterializedValuePair(
      Attributes inheritedAttributes) {
    CompletableFuture<A> promise = new CompletableFuture<>();

    GraphStageLogic logic =
        new GraphStageLogic(shape) {
          {
            setHandler(
                in,
                new AbstractInHandler() {
                  @Override
                  public void onPush() {
                    A elem = grab(in);
                    promise.complete(elem);
                    push(out, elem);

                    // replace handler with one that only forwards elements
                    setHandler(
                        in,
                        new AbstractInHandler() {
                          @Override
                          public void onPush() {
                            push(out, grab(in));
                          }
                        });
                  }
                });

            setHandler(
                out,
                new AbstractOutHandler() {
                  @Override
                  public void onPull() throws Exception {
                    pull(in);
                  }
                });
          }
        };

    return new Pair<>(logic, promise);
  }
}




Using attributes to affect the behavior of an operator


This section is a stub and will be extended in the next release


Operators can access the 
Attributes
Attributes
 object created by the materializer. This contains all the applied (inherited) attributes applying to the operator, ordered from least specific (outermost) towards the most specific (innermost) attribute. It is the responsibility of the operator to decide how to reconcile this inheritance chain to a final effective decision.


See 
Modularity, Composition and Hierarchy
 for an explanation on how attributes work.


Rate decoupled operators


Sometimes it is desirable to 
decouple
 the rate of the upstream and downstream of an operator, synchronizing only when needed.


This is achieved in the model by representing a 
GraphStage
 as a 
boundary
 between two regions where the demand sent upstream is decoupled from the demand that arrives from downstream. One immediate consequence of this difference is that an 
onPush
 call does not always lead to calling 
push
 and an 
onPull
 call does not always lead to calling 
pull
.


One of the important use-case for this is to build buffer-like entities, that allow independent progress of upstream and downstream operators when the buffer is not full or empty, and slowing down the appropriate side if the buffer becomes empty or full.


The next diagram illustrates the event sequence for a buffer with capacity of two elements in a setting where the downstream demand is slow to start and the buffer will fill up with upstream elements before any demand is seen from downstream.




Another scenario would be where the demand from downstream starts coming in before any element is pushed into the buffer operator.




The first difference we can notice is that our 
Buffer
 operator is automatically pulling its upstream on initialization. The buffer has demand for up to two elements without any downstream demand.


The following code example demonstrates a buffer class corresponding to the message sequence chart above.




Scala




copy
source
class TwoBuffer[A] extends GraphStage[FlowShape[A, A]] {

  val in = Inlet[A]("TwoBuffer.in")
  val out = Outlet[A]("TwoBuffer.out")

  val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =
    new GraphStageLogic(shape) {

      val buffer = mutable.Queue[A]()
      def bufferFull = buffer.size == 2
      var downstreamWaiting = false

      override def preStart(): Unit = {
        // a detached stage needs to start upstream demand
        // itself as it is not triggered by downstream demand
        pull(in)
      }

      setHandler(
        in,
        new InHandler {
          override def onPush(): Unit = {
            val elem = grab(in)
            buffer.enqueue(elem)
            if (downstreamWaiting) {
              downstreamWaiting = false
              val bufferedElem = buffer.dequeue()
              push(out, bufferedElem)
            }
            if (!bufferFull) {
              pull(in)
            }
          }

          override def onUpstreamFinish(): Unit = {
            if (buffer.nonEmpty) {
              // emit the rest if possible
              emitMultiple(out, buffer.iterator)
            }
            completeStage()
          }
        })

      setHandler(
        out,
        new OutHandler {
          override def onPull(): Unit = {
            if (buffer.isEmpty) {
              downstreamWaiting = true
            } else {
              val elem = buffer.dequeue()
              push(out, elem)
            }
            if (!bufferFull && !hasBeenPulled(in)) {
              pull(in)
            }
          }
        })
    }

}


Java




copy
source
public class TwoBuffer<A> extends GraphStage<FlowShape<A, A>> {

  public final Inlet<A> in = Inlet.create("TwoBuffer.in");
  public final Outlet<A> out = Outlet.create("TwoBuffer.out");

  private final FlowShape<A, A> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<A, A> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {

      private final int SIZE = 2;
      private Queue<A> buffer = new ArrayDeque<>(SIZE);
      private boolean downstreamWaiting = false;

      private boolean isBufferFull() {
        return buffer.size() == SIZE;
      }

      @Override
      public void preStart() {
        // a detached stage needs to start upstream demand
        // itself as it is not triggered by downstream demand
        pull(in);
      }

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                A elem = grab(in);
                buffer.add(elem);
                if (downstreamWaiting) {
                  downstreamWaiting = false;
                  A bufferedElem = buffer.poll();
                  push(out, bufferedElem);
                }
                if (!isBufferFull()) {
                  pull(in);
                }
              }

              @Override
              public void onUpstreamFinish() {
                if (!buffer.isEmpty()) {
                  // emit the rest if possible
                  emitMultiple(out, buffer.iterator());
                }
                completeStage();
              }
            });

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                if (buffer.isEmpty()) {
                  downstreamWaiting = true;
                } else {
                  A elem = buffer.poll();
                  push(out, elem);
                }
                if (!isBufferFull() && !hasBeenPulled(in)) {
                  pull(in);
                }
              }
            });
      }
    };
  }
}




Thread safety of custom operators




All of the above custom operators (linear or graph) provide a few simple guarantees that implementors can rely on.
  








The callbacks exposed by all of these classes are never called concurrently.


The state encapsulated by these classes can be safely modified from the provided callbacks, without any further synchronization.




In essence, the above guarantees are similar to what 
Actor
 s provide, if one thinks of the state of a custom operator as state of an actor, and the callbacks as the 
receive
 block of the actor.
Warning


It is 
not safe
 to access the state of any custom operator outside of the callbacks that it provides, just like it is unsafe to access the state of an actor from the outside. This means that Future callbacks should 
not close over
 internal state of custom operators because such access can be concurrent with the provided callbacks, leading to undefined behavior.


Resources and the operator lifecycle


If an operator manages a resource with a lifecycle, for example objects that need to be shutdown when they are not used anymore it is important to make sure this will happen in all circumstances when the operator shuts down.


Cleaning up resources should be done in 
GraphStageLogic.postStop
GraphStageLogic.postStop
 and not in the 
InHandler
AbstractInHandler
 and 
OutHandler
 
AbstractOutHandler
 callbacks. The reason for this is that when the operator itself completes or is failed there is no signal from the upstreams or the downstreams. Even for operators that do not complete or fail in this manner, this can happen when the 
Materializer
Materializer
 is shutdown or the 
ActorSystem
ActorSystem
 is terminated while a stream is still running, what is called an “abrupt termination”.


Extending Flow Operators with Custom Operators


The most general way of extending any 
Source
Source
, 
Flow
Flow
 or 
SubFlow
SubFlow
 (e.g. from 
groupBy
) is demonstrated above: create an operator of flow-shape like the 
Duplicator
 example given above and use the 
.via(...)
.via(...)
 operator to integrate it into your stream topology. This works with all 
FlowOps
 sub-types, including the ports that you connect with the graph DSL.


Advanced Scala users may wonder whether it is possible to write extension methods that enrich 
FlowOps
 to allow nicer syntax. The short answer is that Scala 2 does not support this in a fully generic fashion, the problem is that it is impossible to abstract over the kind of stream that is being extended because 
Source
, 
Flow
 and 
SubFlow
 differ in the number and kind of their type parameters. While it would be possible to write an implicit class that enriches them generically, this class would require explicit instantiation with all type parameters due to 
SI-2712
. For a partial workaround that unifies extensions to 
Source
 and 
Flow
 see 
this sketch by R. Kuhn
.


A lot simpler is the task of adding an extension method to 
Source
 as shown below:


copy
source
implicit class SourceDuplicator[Out, Mat](s: Source[Out, Mat]) {
  def duplicateElements: Source[Out, Mat] = s.via(new Duplicator)
}

val s = Source(1 to 3).duplicateElements

s.runWith(Sink.seq).futureValue should ===(Seq(1, 1, 2, 2, 3, 3))


The analog works for 
Flow
 as well:


copy
source
implicit class FlowDuplicator[In, Out, Mat](s: Flow[In, Out, Mat]) {
  def duplicateElements: Flow[In, Out, Mat] = s.via(new Duplicator)
}

val f = Flow[Int].duplicateElements

Source(1 to 3).via(f).runWith(Sink.seq).futureValue should ===(Seq(1, 1, 2, 2, 3, 3))


If you try to write this for 
SubFlow
, though, you will run into the same issue as when trying to unify the two solutions above, only on a higher level (the type constructors needed for that unification would have rank two, meaning that some of their type arguments are type constructors themselvesâwhen trying to extend the solution shown in the linked sketch the author encountered such a density of compiler StackOverflowErrors and IDE failures that he gave up).


It is interesting to note that a simplified form of this problem has found its way into the 
dotty test suite
. Dotty is the development version of Scala on its way to Scala 3.














 
Dynamic stream handling






Futures interop 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/actor-systems.html
Actor Systems • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems




Hierarchical Structure


Configuration Container


Actor Best Practices


What you should not concern yourself with


Terminating ActorSystem




What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems




Hierarchical Structure


Configuration Container


Actor Best Practices


What you should not concern yourself with


Terminating ActorSystem




What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actor Systems


Actors are objects which encapsulate state and behavior, they communicate exclusively by exchanging messages which are placed into the recipientâs mailbox. In a sense, actors are the most stringent form of object-oriented programming, but it serves better to view them as persons: while modeling a solution with actors, envision a group of people and assign sub-tasks to them, arrange their functions into an organizational structure and think about how to escalate failure (all with the benefit of not actually dealing with people, which means that we need not concern ourselves with their emotional state or moral issues). The result can then serve as a mental scaffolding for building the software implementation.
Note


An ActorSystem is a heavyweight structure that will allocate 1â¦N Threads, so create one per logical application.


Hierarchical Structure


Like in an economic organization, actors naturally form hierarchies. One actor, which is to oversee a certain function in the program might want to split up its task into smaller, more manageable pieces. For this purpose, it starts child actors.


The quintessential feature of actor systems is that tasks are split up and delegated until they become small enough to be handled in one piece. In doing so, not only is the task itself clearly structured, but the resulting actors can be reasoned about in terms of which messages they should process, how they should react normally and how failure should be handled.


Compare this to layered software design which easily devolves into defensive programming with the aim of not leaking any failure out: if the problem is communicated to the right person, a better solution can be found than if trying to keep everything âunder the carpetâ.


Now, the difficulty in designing such a system is how to decide how to structure the work. There is no single best solution, but there are a few guidelines which might be helpful:




If one actor carries very important data (i.e. its state shall not be lost  if avoidable), this actor should source out any possibly dangerous sub-tasks  to children and handle failures of these children as appropriate. Depending on  the nature of the requests, it may be best to create a new child for each request,  which simplifies state management for collecting the replies. This is known as the  âError Kernel Patternâ from Erlang.


If one actor depends on another actor for carrying out its duty, it should  watch that other actorâs liveness and act upon receiving a termination  notice.


If one actor has multiple responsibilities each responsibility can often be pushed  into a separate child to make the logic and state more simple.




Configuration Container


The actor system as a collaborating ensemble of actors is the natural unit for managing shared facilities like scheduling services, configuration, logging, etc. Several actor systems with different configurations may co-exist within the same JVM without problems, there is no global shared state within Akka itself, however the most common scenario will only involve a single actor system per JVM.


Couple this with the transparent communication between actor systems â within one node or across a network connection â and actor systems are a perfect fit to form a distributed application.


Actor Best Practices




Actors should be like nice co-workers: do their job efficiently without bothering everyone else needlessly and avoid hogging resources. Translated to programming this means to process events and generate responses (or more requests) in an event-driven manner. Actors should not block (i.e. passively wait while occupying a Thread) on some external entityâwhich might be a lock, a network socket, etc.âunless it is unavoidable; in the latter case see 
Blocking Needs Careful Management
.


Do not pass mutable objects between actors. In order to ensure that, prefer immutable messages. If the encapsulation of actors is broken by exposing their mutable state to the outside, you are back in normal Java concurrency land with all the drawbacks.


Actors are made to be containers for behavior and state, embracing this means to not routinely send behavior within messages (which may be tempting using Scala closures). One of the risks is to accidentally share mutable state between actors, and this violation of the actor model unfortunately breaks all the properties which make programming in actors such a nice experience.


The top-level actor of the actor system is the innermost part of your Error Kernel, it should only be responsible for starting the various sub systems of your application, and not contain much logic in itself, prefer truly hierarchical systems. This has benefits with respect to fault-handling (both considering the granularity of configuration and the performance) and it also reduces the strain on the guardian actor, which is a single point of contention if over-used.




What you should not concern yourself with


An actor system manages the resources it is configured to use in order to run the actors which it contains. There may be millions of actors within one such system, after all the mantra is to view them as abundant and they weigh in at an overhead of only roughly 300 bytes per instance. Naturally, the exact order in which messages are processed in large systems is not controllable by the application author, but this is also not intended. Take a step back and relax while Akka does the heavy lifting under the hood.


Terminating ActorSystem


When you know everything is done for your application, you can have the user guardian  actor stop, or call the 
terminate
terminate()
 method of 
ActorSystem
ActorSystem
. That will run 
CoordinatedShutdown
 stopping all running actors.


If you want to execute some operations while terminating 
ActorSystem
ActorSystem
, look at 
CoordinatedShutdown
.
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.














 
Terminology, Concepts






What is an Actor? 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/actor-lifecycle.html
Actor lifecycle • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle




Dependency


Introduction


Creating Actors


Stopping Actors


Watching Actors




Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle




Dependency


Introduction


Creating Actors


Stopping Actors


Watching Actors




Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actor lifecycle


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Actors
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


An actor is a stateful resource that has to be explicitly started and stopped.


It is important to note that actors do not stop automatically when no longer referenced, every Actor that is created must also explicitly be destroyed. The only simplification is that stopping a parent Actor will also recursively stop all the child Actors that this parent has created. All actors are also stopped automatically when the 
ActorSystem
ActorSystem
 is shut down.
Note


An 
ActorSystem
 is a heavyweight structure that will allocate threads, so create one per logical application. Typically one 
ActorSystem
 per JVM process.


Creating Actors


An actor can create, or 
spawn
, an arbitrary number of child actors, which in turn can spawn children of their own, thus forming an actor hierarchy. 
ActorSystem
ActorSystem
 hosts the hierarchy and there can be only one 
root actor
, an actor at the top of the hierarchy of the 
ActorSystem
. The lifecycle of a child actor is tied to the parent – a child can stop itself or be stopped at any time but it can never outlive its parent.


The ActorContext


The 
ActorContext
ActorContext
 can be accessed for many purposes such as:




Spawning child actors and supervision


Watching other actors to receive a 
Terminated(otherActor)
Terminated(otherActor)
 event should the watched actor stop permanently


Logging


Creating message adapters


Request-response interactions (ask) with another actor


Access to the 
self
getSelf()
 ActorRef




If a behavior needs to use the 
ActorContext
, for example to spawn child actors, or use 
context.self
context.getSelf()
, it can be obtained by wrapping construction with 
Behaviors.setup
Behaviors.setup
:




Scala




copy
source
object HelloWorldMain {

  final case class SayHello(name: String)

  def apply(): Behavior[SayHello] =
    Behaviors.setup { context =>
      val greeter = context.spawn(HelloWorld(), "greeter")

      Behaviors.receiveMessage { message =>
        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)
        greeter ! HelloWorld.Greet(message.name, replyTo)
        Behaviors.same
      }
    }

}


Java




copy
source
public class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {

  public static record SayHello(String name) {}

  public static Behavior<SayHello> create() {
    return Behaviors.setup(HelloWorldMain::new);
  }

  private final ActorRef<HelloWorld.Greet> greeter;

  private HelloWorldMain(ActorContext<SayHello> context) {
    super(context);
    greeter = context.spawn(HelloWorld.create(), "greeter");
  }

  @Override
  public Receive<SayHello> createReceive() {
    return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build();
  }

  private Behavior<SayHello> onSayHello(SayHello command) {
    ActorRef<HelloWorld.Greeted> replyTo =
        getContext().spawn(HelloWorldBot.create(3), command.name);
    greeter.tell(new HelloWorld.Greet(command.name, replyTo));
    return this;
  }
}




ActorContext Thread Safety


Many of the methods in 
ActorContext
ActorContext
 are not thread-safe and




Must not be accessed by threads from 
scala.concurrent.Future
java.util.concurrent.CompletionStage
 callbacks


Must not be shared between several actor instances


Must only be used in the ordinary actor message processing thread




The Guardian Actor


The top level actor, also called the user guardian actor, is created along with the 
ActorSystem
ActorSystem
. Messages sent to the actor system are directed to the root actor. The root actor is defined by the behavior used to create the 
ActorSystem
, named 
HelloWorldMain
 in the example below:




Scala




copy
source
val system: ActorSystem[HelloWorldMain.SayHello] =
  ActorSystem(HelloWorldMain(), "hello")

system ! HelloWorldMain.SayHello("World")
system ! HelloWorldMain.SayHello("Akka")


Java




copy
source
final ActorSystem<SayHello> system =
    ActorSystem.create(HelloWorldMain.create(), "hello");

system.tell(new HelloWorldMain.SayHello("World"));
system.tell(new HelloWorldMain.SayHello("Akka"));




For very simple applications the guardian may contain the actual application logic and handle messages. As soon as the application handles more than one concern the guardian should instead just bootstrap the application, spawn the various subsystems as children and monitor their lifecycles.


When the guardian actor stops this will stop the 
ActorSystem
.


When 
ActorSystem.terminate
ActorSystem.terminate
 is invoked the 
Coordinated Shutdown
 process will stop actors and services in a specific order.


Spawning Children


Child actors are created and started with 
ActorContext
’s 
spawn
spawn
. In the example below, when the root actor is started, it spawns a child actor described by the 
HelloWorld
 behavior. Additionally, when the root actor receives a 
SayHello
 message, it creates a child actor defined by the behavior 
HelloWorldBot
:




Scala




copy
source
object HelloWorldMain {

  final case class SayHello(name: String)

  def apply(): Behavior[SayHello] =
    Behaviors.setup { context =>
      val greeter = context.spawn(HelloWorld(), "greeter")

      Behaviors.receiveMessage { message =>
        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)
        greeter ! HelloWorld.Greet(message.name, replyTo)
        Behaviors.same
      }
    }

}


Java




copy
source
public class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {

  public static record SayHello(String name) {}

  public static Behavior<SayHello> create() {
    return Behaviors.setup(HelloWorldMain::new);
  }

  private final ActorRef<HelloWorld.Greet> greeter;

  private HelloWorldMain(ActorContext<SayHello> context) {
    super(context);
    greeter = context.spawn(HelloWorld.create(), "greeter");
  }

  @Override
  public Receive<SayHello> createReceive() {
    return newReceiveBuilder().onMessage(SayHello.class, this::onSayHello).build();
  }

  private Behavior<SayHello> onSayHello(SayHello command) {
    ActorRef<HelloWorld.Greeted> replyTo =
        getContext().spawn(HelloWorldBot.create(3), command.name);
    greeter.tell(new HelloWorld.Greet(command.name, replyTo));
    return this;
  }
}




To specify a dispatcher when spawning an actor use 
DispatcherSelector
DispatcherSelector
. If not specified, the actor will use the default dispatcher, see 
Default dispatcher
 for details.




Scala




copy
source
def apply(): Behavior[SayHello] =
  Behaviors.setup { context =>
    val dispatcherPath = "akka.actor.default-blocking-io-dispatcher"

    val props = DispatcherSelector.fromConfig(dispatcherPath)
    val greeter = context.spawn(HelloWorld(), "greeter", props)

    Behaviors.receiveMessage { message =>
      // ...
      Behaviors.same
    }
  }


Java




copy
source
public class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {

  // Start message...

  public static Behavior<SayHello> create() {
    return Behaviors.setup(HelloWorldMain::new);
  }

  private final ActorRef<HelloWorld.Greet> greeter;

  private HelloWorldMain(ActorContext<SayHello> context) {
    super(context);

    final String dispatcherPath = "akka.actor.default-blocking-io-dispatcher";
    Props greeterProps = DispatcherSelector.fromConfig(dispatcherPath);
    greeter = getContext().spawn(HelloWorld.create(), "greeter", greeterProps);
  }

  // createReceive ...
}




Refer to 
Actors
 for a walk-through of the above examples.


SpawnProtocol


The guardian actor should be responsible for initialization of tasks and create the initial actors of the application, but sometimes you might want to spawn new actors from the outside of the guardian actor. For example creating one actor per HTTP request.


That is not difficult to implement in your behavior, but since this is a common pattern there is a predefined message protocol and implementation of a behavior for this. It can be used as the guardian actor of the 
ActorSystem
ActorSystem
, possibly combined with 
Behaviors.setup
Behaviors.setup
 to start some initial tasks or actors. Child actors can then be started from the outside by 
tell
tell
ing or 
ask
ask
ing 
SpawnProtocol.Spawn
SpawnProtocol.Spawn
 to the actor reference of the system. Using 
ask
 is similar to how 
ActorSystem.actorOf
 can be used in classic actors with the difference that a 
Future
CompletionStage
 of the 
ActorRef
ActorRef
 is returned.


The guardian behavior can be defined as:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.SpawnProtocol
import akka.actor.typed.scaladsl.Behaviors

object HelloWorldMain {
  def apply(): Behavior[SpawnProtocol.Command] =
    Behaviors.setup { context =>
      // Start initial tasks
      // context.spawn(...)

      SpawnProtocol()
    }
}


Java




copy
source
import akka.actor.typed.Behavior;
import akka.actor.typed.SpawnProtocol;
import akka.actor.typed.javadsl.Behaviors;

public abstract class HelloWorldMain {
  private HelloWorldMain() {}

  public static Behavior<SpawnProtocol.Command> create() {
    return Behaviors.setup(
        context -> {
          // Start initial tasks
          // context.spawn(...)

          return SpawnProtocol.create();
        });
  }
}




and the 
ActorSystem
ActorSystem
 can be created with that 
main
 behavior and asked to spawn other actors:




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.ActorSystem
import akka.actor.typed.Props
import akka.util.Timeout


implicit val system: ActorSystem[SpawnProtocol.Command] =
  ActorSystem(HelloWorldMain(), "hello")

// needed in implicit scope for ask (?)
import akka.actor.typed.scaladsl.AskPattern._
implicit val ec: ExecutionContext = system.executionContext
implicit val timeout: Timeout = Timeout(3.seconds)

val greeter: Future[ActorRef[HelloWorld.Greet]] =
  system.ask(SpawnProtocol.Spawn(behavior = HelloWorld(), name = "greeter", props = Props.empty, _))

val greetedBehavior = Behaviors.receive[HelloWorld.Greeted] { (context, message) =>
  context.log.info("Greeting for {} from {}", message.whom, message.from)
  Behaviors.stopped
}

val greetedReplyTo: Future[ActorRef[HelloWorld.Greeted]] =
  system.ask(SpawnProtocol.Spawn(greetedBehavior, name = "", props = Props.empty, _))

for (greeterRef <- greeter; replyToRef <- greetedReplyTo) {
  greeterRef ! HelloWorld.Greet("Akka", replyToRef)
}



Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.ActorSystem;
import akka.actor.typed.Props;
import akka.actor.typed.javadsl.AskPattern;

final ActorSystem<SpawnProtocol.Command> system =
    ActorSystem.create(HelloWorldMain.create(), "hello");
final Duration timeout = Duration.ofSeconds(3);

CompletionStage<ActorRef<HelloWorld.Greet>> greeter =
    AskPattern.ask(
        system,
        replyTo ->
            new SpawnProtocol.Spawn<>(HelloWorld.create(), "greeter", Props.empty(), replyTo),
        timeout,
        system.scheduler());

Behavior<HelloWorld.Greeted> greetedBehavior =
    Behaviors.receive(
        (context, message) -> {
          context.getLog().info("Greeting for {} from {}", message.whom, message.from);
          return Behaviors.stopped();
        });

CompletionStage<ActorRef<HelloWorld.Greeted>> greetedReplyTo =
    AskPattern.ask(
        system,
        replyTo -> new SpawnProtocol.Spawn<>(greetedBehavior, "", Props.empty(), replyTo),
        timeout,
        system.scheduler());

greeter.whenComplete(
    (greeterRef, exc) -> {
      if (exc == null) {
        greetedReplyTo.whenComplete(
            (greetedReplyToRef, exc2) -> {
              if (exc2 == null) {
                greeterRef.tell(new HelloWorld.Greet("Akka", greetedReplyToRef));
              }
            });
      }
    });





The 
SpawnProtocol
SpawnProtocol
 can also be used at other places in the actor hierarchy. It doesn’t have to be the root guardian actor.


A way to find running actors is described in 
Actor discovery
.


Stopping Actors


An actor can stop itself by returning 
Behaviors.stopped
Behaviors.stopped
 as the next behavior.


A child actor can be forced to stop after it finishes processing its current message by using the 
stop
stop
 method of the 
ActorContext
 from the parent actor. Only child actors can be stopped in that way.


All child actors will be stopped when their parent is stopped.


When an actor is stopped, it receives the 
PostStop
PostStop
 signal that can be used for cleaning up resources.


Here is an illustrating example:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors
import akka.actor.typed.{ ActorSystem, PostStop }


object MasterControlProgram {
  sealed trait Command
  final case class SpawnJob(name: String) extends Command
  case object GracefulShutdown extends Command

  def apply(): Behavior[Command] = {
    Behaviors
      .receive[Command] { (context, message) =>
        message match {
          case SpawnJob(jobName) =>
            context.log.info("Spawning job {}!", jobName)
            context.spawn(Job(jobName), name = jobName)
            Behaviors.same
          case GracefulShutdown =>
            context.log.info("Initiating graceful shutdown...")
            // Here it can perform graceful stop (possibly asynchronous) and when completed
            // return `Behaviors.stopped` here or after receiving another message.
            Behaviors.stopped
        }
      }
      .receiveSignal {
        case (context, PostStop) =>
          context.log.info("Master Control Program stopped")
          Behaviors.same
      }
  }
}

object Job {
  sealed trait Command

  def apply(name: String): Behavior[Command] = {
    Behaviors.receiveSignal[Command] {
      case (context, PostStop) =>
        context.log.info("Worker {} stopped", name)
        Behaviors.same
    }
  }
}


Java




copy
source
import java.util.concurrent.TimeUnit;

import akka.actor.typed.ActorSystem;
import akka.actor.typed.Behavior;
import akka.actor.typed.PostStop;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;


public class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {

  interface Command {}

  public static final class SpawnJob implements Command {
    public final String name;

    public SpawnJob(String name) {
      this.name = name;
    }
  }

  public enum GracefulShutdown implements Command {
    INSTANCE
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(MasterControlProgram::new);
  }

  public MasterControlProgram(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(SpawnJob.class, this::onSpawnJob)
        .onMessage(GracefulShutdown.class, message -> onGracefulShutdown())
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private Behavior<Command> onSpawnJob(SpawnJob message) {
    getContext().getSystem().log().info("Spawning job {}!", message.name);
    getContext().spawn(Job.create(message.name), message.name);
    return this;
  }

  private Behavior<Command> onGracefulShutdown() {
    getContext().getSystem().log().info("Initiating graceful shutdown...");

    // Here it can perform graceful stop (possibly asynchronous) and when completed
    // return `Behaviors.stopped()` here or after receiving another message.
    return Behaviors.stopped();
  }

  private Behavior<Command> onPostStop() {
    getContext().getSystem().log().info("Master Control Program stopped");
    return this;
  }
}

public class Job extends AbstractBehavior<Job.Command> {

  interface Command {}

  public static Behavior<Command> create(String name) {
    return Behaviors.setup(context -> new Job(context, name));
  }

  private final String name;

  public Job(ActorContext<Command> context, String name) {
    super(context);
    this.name = name;
  }

  @Override
  public Receive<Job.Command> createReceive() {
    return newReceiveBuilder().onSignal(PostStop.class, postStop -> onPostStop()).build();
  }

  private Behavior<Command> onPostStop() {
    getContext().getSystem().log().info("Worker {} stopped", name);
    return this;
  }
}




When cleaning up resources from 
PostStop
 you should also consider doing the same for the 
PreRestart
PreRestart
 signal, which is emitted when the 
actor is restarted
. Note that 
PostStop
 is not emitted for a restart. 


Watching Actors


In order to be notified when another actor terminates (i.e. stops permanently, not temporary failure and restart), an actor can 
watch
watch
 another actor. It will receive the 
Terminated
Terminated
 signal upon termination (see 
Stopping Actors
) of the watched actor.




Scala




copy
source
object MasterControlProgram {
  sealed trait Command
  final case class SpawnJob(name: String) extends Command

  def apply(): Behavior[Command] = {
    Behaviors
      .receive[Command] { (context, message) =>
        message match {
          case SpawnJob(jobName) =>
            context.log.info("Spawning job {}!", jobName)
            val job = context.spawn(Job(jobName), name = jobName)
            context.watch(job)
            Behaviors.same
        }
      }
      .receiveSignal {
        case (context, Terminated(ref)) =>
          context.log.info("Job stopped: {}", ref.path.name)
          Behaviors.same
      }
  }
}


Java




copy
source
public class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {

  interface Command {}

  public static final class SpawnJob implements Command {
    public final String name;

    public SpawnJob(String name) {
      this.name = name;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(MasterControlProgram::new);
  }

  public MasterControlProgram(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(SpawnJob.class, this::onSpawnJob)
        .onSignal(Terminated.class, this::onTerminated)
        .build();
  }

  private Behavior<Command> onSpawnJob(SpawnJob message) {
    getContext().getSystem().log().info("Spawning job {}!", message.name);
    ActorRef<Job.Command> job = getContext().spawn(Job.create(message.name), message.name);
    getContext().watch(job);
    return this;
  }

  private Behavior<Command> onTerminated(Terminated terminated) {
    getContext().getSystem().log().info("Job stopped: {}", terminated.getRef().path().name());
    return this;
  }
}




An alternative to 
watch
watch
 is 
watchWith
watchWith
, which allows specifying a custom message instead of the 
Terminated
. This is often preferred over using 
watch
 and the 
Terminated
 signal because additional information can be included in the message that can be used later when receiving it.


Similar example as above, but using 
watchWith
 and replies to the original requestor when the job has finished.




Scala




copy
source
object MasterControlProgram {
  sealed trait Command
  final case class SpawnJob(name: String, replyToWhenDone: ActorRef[JobDone]) extends Command
  final case class JobDone(name: String)
  private final case class JobTerminated(name: String, replyToWhenDone: ActorRef[JobDone]) extends Command

  def apply(): Behavior[Command] = {
    Behaviors.receive { (context, message) =>
      message match {
        case SpawnJob(jobName, replyToWhenDone) =>
          context.log.info("Spawning job {}!", jobName)
          val job = context.spawn(Job(jobName), name = jobName)
          context.watchWith(job, JobTerminated(jobName, replyToWhenDone))
          Behaviors.same
        case JobTerminated(jobName, replyToWhenDone) =>
          context.log.info("Job stopped: {}", jobName)
          replyToWhenDone ! JobDone(jobName)
          Behaviors.same
      }
    }
  }
}


Java




copy
source
public class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {

  interface Command {}

  public static final class SpawnJob implements Command {
    public final String name;
    public final ActorRef<JobDone> replyToWhenDone;

    public SpawnJob(String name, ActorRef<JobDone> replyToWhenDone) {
      this.name = name;
      this.replyToWhenDone = replyToWhenDone;
    }
  }

  public static final class JobDone {
    public final String name;

    public JobDone(String name) {
      this.name = name;
    }
  }

  private static final class JobTerminated implements Command {
    final String name;
    final ActorRef<JobDone> replyToWhenDone;

    JobTerminated(String name, ActorRef<JobDone> replyToWhenDone) {
      this.name = name;
      this.replyToWhenDone = replyToWhenDone;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(MasterControlProgram::new);
  }

  public MasterControlProgram(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(SpawnJob.class, this::onSpawnJob)
        .onMessage(JobTerminated.class, this::onJobTerminated)
        .build();
  }

  private Behavior<Command> onSpawnJob(SpawnJob message) {
    getContext().getSystem().log().info("Spawning job {}!", message.name);
    ActorRef<Job.Command> job = getContext().spawn(Job.create(message.name), message.name);
    getContext().watchWith(job, new JobTerminated(message.name, message.replyToWhenDone));
    return this;
  }

  private Behavior<Command> onJobTerminated(JobTerminated terminated) {
    getContext().getSystem().log().info("Job stopped: {}", terminated.name);
    terminated.replyToWhenDone.tell(new JobDone(terminated.name));
    return this;
  }
}




Note how the 
replyToWhenDone
 is included in the 
watchWith
 message and then used later when receiving the 
JobTerminated
 message. 


The watched actor can be any 
ActorRef
ActorRef
, it doesn’t have to be a child actor as in the above example.


It should be noted that the terminated message is generated independent of the order in which registration and termination occur. In particular, the watching actor will receive a terminated message even if the watched actor has already been terminated at the time of registration.


Registering multiple times does not necessarily lead to multiple messages being generated, but there is no guarantee that only exactly one such message is received: if termination of the watched actor has generated and queued the message, and another registration is done before this message has been processed, then a second message will be queued, because registering for monitoring of an already terminated actor leads to the immediate generation of the terminated message.


It is also possible to deregister from watching another actorâs liveliness using 
context.unwatch(target)
context.unwatch(target)
. This works even if the terminated message has already been enqueued in the mailbox; after calling 
unwatch
 no terminated message for that actor will be processed anymore.


The terminated message is also sent when the watched actor is on a node that has been removed from the 
Cluster
.














 
Introduction to Actors






Interaction Patterns 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/persistence-snapshot.html
Snapshotting • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting




Snapshots


Snapshot failures


Snapshot deletion


Event deletion




Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting




Snapshots


Snapshot failures


Snapshot deletion


Event deletion




Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Snapshotting


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Akka Persistence
.


Snapshots


As you model your domain using 
event sourced actors
, you may notice that some actors may be prone to accumulating extremely long event logs and experiencing long recovery times. Sometimes, the right approach may be to split out into a set of shorter lived actors. However, when this is not an option, you can use snapshots to reduce recovery times drastically.


Persistent actors can save snapshots of internal state every N events or when a given predicate of the state is fulfilled.




Scala




copy
source
import akka.persistence.typed.scaladsl.Effect

EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => state) // do something based on a particular state
  .snapshotWhen {
    case (state, BookingCompleted(_), sequenceNumber) => true
    case (state, event, sequenceNumber)               => false
  }
  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2))


Java




copy
source
@Override // override retentionCriteria in EventSourcedBehavior
public RetentionCriteria retentionCriteria() {
  return RetentionCriteria.snapshotEvery(100, 2);
}






Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .snapshotWhen {
    case (state, BookingCompleted(_), sequenceNumber) => true
    case (state, event, sequenceNumber)               => false
  }


Java




copy
source
@Override // override shouldSnapshot in EventSourcedBehavior
public boolean shouldSnapshot(State state, Event event, long sequenceNr) {
  return event instanceof BookingCompleted;
}




When a snapshot is triggered, incoming commands are stashed until the snapshot has been saved. This means that the state can safely be mutable although the serialization and storage of the state is performed asynchronously. The state instance will not be updated by new events until after the snapshot has been saved.


During recovery, the persistent actor is using the latest saved snapshot to initialize the state. Thereafter the events after the snapshot are replayed using the event handler to recover the persistent actor to its current (i.e. latest) state.


If not specified, they default to 
SnapshotSelectionCriteria.Latest
SnapshotSelectionCriteria.latest()
 which selects the latest (youngest) snapshot. It’s possible to override the selection of which snapshot to use for recovery like this:




Scala




copy
source
import akka.persistence.typed.SnapshotSelectionCriteria

EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withRecovery(Recovery.withSnapshotSelectionCriteria(SnapshotSelectionCriteria.none))


Java




copy
source
@Override
public Recovery recovery() {
  return Recovery.withSnapshotSelectionCriteria(SnapshotSelectionCriteria.none());
}




To disable snapshot-based recovery, applications can use 
SnapshotSelectionCriteria.None
SnapshotSelectionCriteria.none()
. A recovery where no saved snapshot matches the specified 
SnapshotSelectionCriteria
 will replay all journaled events. This can be useful if snapshot serialization format has changed in an incompatible way. It should typically not be used when events have been deleted.


In order to use snapshots, a default snapshot-store (
akka.persistence.snapshot-store.plugin
) must be configured, or you can pick a snapshot store for for a specific 
EventSourcedBehavior
 by 
defining it with 
withSnapshotPluginId
 of the 
EventSourcedBehavior
overriding 
snapshotPluginId
 in the 
EventSourcedBehavior
.


Because some use cases may not benefit from or need snapshots, it is perfectly valid not to not configure a snapshot store. However, Akka will log a warning message when this situation is detected and then continue to operate until an actor tries to store a snapshot, at which point the operation will fail.


Snapshot failures


Saving snapshots can either succeed or fail â this information is reported back to the persistent actor via the 
SnapshotCompleted
 or 
SnapshotFailed
 signal. Snapshot failures are logged by default but do not cause the actor to stop or restart.


If there is a problem with recovering the state of the actor from the journal when the actor is started, 
RecoveryFailed
 signal is emitted (logging the error by default), and the actor will be stopped. Note that failure to load snapshot is also treated like this, but you can disable loading of snapshots if you for example know that serialization format has changed in an incompatible way.


Optional snapshots


By default, the persistent actor will unconditionally be stopped if the snapshot can’t be loaded in the recovery. It is possible to make snapshot loading optional. This can be useful when it is alright to ignore snapshot in case of for example deserialization errors. When snapshot loading fails it will instead recover by replaying all events.


Enable this feature by setting 
snapshot-is-optional = true
 in the snapshot store configuration.
Warning


Don’t set 
snapshot-is-optional = true
 if events have been deleted because that would result in wrong recovered state if snapshot load fails.


Snapshot deletion


To free up space, an event sourced actor can automatically delete older snapshots based on the given 
RetentionCriteria
.




Scala




copy
source
import akka.persistence.typed.scaladsl.Effect

EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => state) // do something based on a particular state
  .snapshotWhen {
    case (state, BookingCompleted(_), sequenceNumber) => true
    case (state, event, sequenceNumber)               => false
  }
  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2))


Java




copy
source
@Override // override retentionCriteria in EventSourcedBehavior
public RetentionCriteria retentionCriteria() {
  return RetentionCriteria.snapshotEvery(100, 2);
}
@Override // override shouldSnapshot in EventSourcedBehavior
public boolean shouldSnapshot(State state, Event event, long sequenceNr) {
  return event instanceof BookingCompleted;
}




Snapshot deletion is triggered after successfully saving a new snapshot.


The above example will save snapshots automatically every 
numberOfEvents = 100
. Snapshots that have sequence number less than the sequence number of the saved snapshot minus 
keepNSnapshots * numberOfEvents
 (
100 * 2
) are automatically deleted.


In addition, it will also save a snapshot when the persisted event is 
BookingCompleted
. Automatic snapshotting based on 
numberOfEvents
 can be used without specifying 
snapshotWhen
shouldSnapshot
. Snapshots triggered by the 
snapshotWhen
shouldSnapshot
 predicate will not trigger deletion of old snapshots.


On async deletion, either a 
DeleteSnapshotsCompleted
 or 
DeleteSnapshotsFailed
 signal is emitted. You can react to signal outcomes by using 
with 
receiveSignal
 handler
 
by overriding 
receiveSignal
. By default, successful completion is logged by the system at log level 
debug
, failures at log level 
warning
.




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2))
  .receiveSignal { // optionally respond to signals
    case (state, _: SnapshotFailed)        => // react to failure
    case (state, _: DeleteSnapshotsFailed) => // react to failure
  }


Java




copy
source
@Override
public SignalHandler<State> signalHandler() {
  return newSignalHandlerBuilder()
      .onSignal(
          SnapshotFailed.class,
          (state, completed) -> {
            throw new RuntimeException("TODO: add some on-snapshot-failed side-effect here");
          })
      .onSignal(
          DeleteSnapshotsFailed.class,
          (state, completed) -> {
            throw new RuntimeException(
                "TODO: add some on-delete-snapshot-failed side-effect here");
          })
      .onSignal(
          DeleteEventsFailed.class,
          (state, completed) -> {
            throw new RuntimeException(
                "TODO: add some on-delete-snapshot-failed side-effect here");
          })
      .build();
}




Event deletion


Deleting events in Event Sourcing based applications is typically either not used at all, or used in conjunction with snapshotting. By deleting events you will lose the history of how the system changed before it reached current state, which is one of the main reasons for using Event Sourcing in the first place.


Immediate event deletion should not be used together with Projections and for 
Replicated Event Sourcing
 event deletion via snapshot predicate or retention criteria is not allowed. Deleting all events immediately when an entity has reached its terminal deleted state would have the consequence that Projections might not have consumed all previous events yet and will not be notified of the deleted event. Instead, it’s recommended to emit a final deleted event and store the fact that the entity is deleted separately via a Projection. Then a background task can clean up the events and snapshots for the deleted entities. The entity itself knows about the terminal state from the deleted event and should not emit further events after that and typically stop itself if it receives more commands.


If snapshots are triggered using the predicate-based api (
snapshotWhen
shouldSnapshot
), events are not deleted by default. Event deletion can be enabled using the following api:




Scala




copy
source
.snapshotWhen(predicate, deleteEventsOnSnapshot = true)


Java




copy
source
@Override // override deleteEventsOnSnapshot in EventSourcedBehavior
public boolean deleteEventsOnSnapshot() {
  return true;
}




If snapshot-based retention is enabled, after a snapshot has been successfully stored, a delete of the events (journaled by a single event sourced actor) up until the sequence number of the data held by that snapshot can be issued.


To elect to use this, enable 
withDeleteEventsOnSnapshot
 of the 
RetentionCriteria
 which is disabled by default.




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2).withDeleteEventsOnSnapshot)
  .receiveSignal { // optionally respond to signals
    case (state, _: SnapshotFailed)        => // react to failure
    case (state, _: DeleteSnapshotsFailed) => // react to failure
    case (state, _: DeleteEventsFailed)    => // react to failure
  }


Java




copy
source
@Override // override retentionCriteria in EventSourcedBehavior
public RetentionCriteria retentionCriteria() {
  return RetentionCriteria.snapshotEvery(100, 2).withDeleteEventsOnSnapshot();
}




Event deletion is triggered after saving a new snapshot. Old events would be deleted prior to old snapshots being deleted.


On async deletion, either a 
DeleteEventsCompleted
 or 
DeleteEventsFailed
 signal is emitted. You can react to signal outcomes by using 
with 
receiveSignal
 handler
 
by overriding 
receiveSignal
. By default, successful completion is logged by the system at log level 
debug
, failures at log level 
warning
.


Message deletion does not affect the highest sequence number of the journal, even if all messages were deleted from it after a delete occurs.
Note


It is up to the journal implementation whether events are actually removed from storage.














 
Style Guide






Testing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-introduction.html
Introduction • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction




Motivation


How to read these docs


Module info




Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction




Motivation


How to read these docs


Module info




Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Introduction


Motivation


The way we consume services from the Internet today includes many instances of streaming data, both downloading from a service as well as uploading to it or peer-to-peer data transfers. Regarding data as a stream of elements instead of in its entirety is very useful because it matches the way computers send and receive them (for example via TCP), but it is often also a necessity because data sets frequently become too large to be handled as a whole. We spread computations or analyses over large clusters and call it âbig dataâ, where the whole principle of processing them is by feeding those data sequentiallyâas a streamâthrough some CPUs.


Actors can be seen as dealing with streams as well: they send and receive series of messages in order to transfer knowledge (or data) from one place to another. We have found it tedious and error-prone to implement all the proper measures in order to achieve stable streaming between actors, since in addition to sending and receiving we also need to take care to not overflow any buffers or mailboxes in the process. Another pitfall is that Actor messages can be lost and must be retransmitted in that case. Failure to do so would lead to holes at the receiving side.


For these reasons we decided to bundle up a solution to these problems as an Akka Streams API. The purpose is to offer an intuitive and safe way to formulate stream processing setups such that we can then execute them efficiently and with bounded resource usageâno more OutOfMemoryErrors. In order to achieve this our streams need to be able to limit the buffering that they employ, they need to be able to slow down producers if the consumers cannot keep up. This feature is called back-pressure and is at the core of the 
Reactive Streams
 initiative of which Akka is a founding member. For you this means that the hard problem of propagating and reacting to back-pressure has been incorporated in the design of Akka Streams already, so you have one less thing to worry about; it also means that Akka Streams interoperate seamlessly with all other Reactive Streams implementations (where Reactive Streams interfaces define the interoperability SPI while implementations like Akka Streams offer a nice user API).


Relationship with Reactive Streams


The Akka Streams API is completely decoupled from the Reactive Streams interfaces. While Akka Streams focus on the formulation of transformations on data streams the scope of Reactive Streams is to define a common mechanism of how to move data across an asynchronous boundary without losses, buffering or resource exhaustion.


The relationship between these two is that the Akka Streams API is geared towards end-users while the Akka Streams implementation uses the Reactive Streams interfaces internally to pass data between the different operators. For this reason you will not find any resemblance between the Reactive Streams interfaces and the Akka Streams API. This is in line with the expectations of the Reactive Streams project, whose primary purpose is to define interfaces such that different streaming implementation can interoperate; it is not the purpose of Reactive Streams to describe an end-user API.


How to read these docs


Stream processing is a different paradigm to the Actor Model or to Future composition, therefore it may take some careful study of this subject until you feel familiar with the tools and techniques. The documentation is here to help and for best results we recommend the following approach:




Read the 
Quick Start Guide
 to get a feel for how streams look like and what they can do.


The top-down learners may want to peruse the 
Design Principles behind Akka Streams
 at this point.


The bottom-up learners may feel more at home rummaging through the 
Streams Cookbook
.


For a complete overview of the built-in processing operators you can look at the 
operator index


The other sections can be read sequentially or as needed during the previous steps, each digging deeper into specific topics.




Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-stream" % AkkaVersion,
  "com.typesafe.akka" %% "akka-stream-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-stream-testkit_${versions.ScalaBinary}"
}




Project Info: Akka Streams


Artifact
com.typesafe.akka


akka-stream


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.stream


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.0, 2017-04-13




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka
















 
Streams






Streams Quickstart Guide 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/other-modules.html#akka-insights
Other Akka modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic




















Other Akka modules


This page describes modules that compliment libraries from the Akka core. See 
this overview
 instead for a guide on the core modules.


Akka HTTP


A full server- and client-side HTTP stack on top of akka-actor and akka-stream.


Akka gRPC


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams.


Alpakka


Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka.


Alpakka Kafka Connector


The Alpakka Kafka Connector connects Apache Kafka with Akka Streams.


Akka Projections


Akka Projections let you process a stream of events or records from a source to a projected model or external system.


Cassandra Plugin for Akka Persistence


An Akka Persistence journal and snapshot store backed by Apache Cassandra.


JDBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on 
Slick
.


R2DBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on 
R2DBC
.


Akka Management




Akka Management
 provides a central HTTP endpoint for Akka management extensions.


Akka Cluster Bootstrap
 helps bootstrapping an Akka cluster using Akka Discovery.


Akka Management Kubernetes Rolling Updates
 for smooth rolling updates.


Akka Management Cluster HTTP
 provides HTTP endpoints for introspecting and managing Akka clusters.


Akka Discovery for Kubernetes, Consul, Marathon, and AWS


Kubernetes Lease




Akka Diagnostics




Akka Thread Starvation Detector


Akka Configuration Checker




Akka Insights


Intelligent monitoring and observability purpose-built for Akka: 
Lightbend Telemetry


Community Projects


Akka has a vibrant and passionate user community, the members of which have created many independent projects using Akka as well as extensions to it. See 
Community Projects
.














 
Extending Akka






Package, Deploy and Run 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/other-modules.html#community-projects
Other Akka modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic




















Other Akka modules


This page describes modules that compliment libraries from the Akka core. See 
this overview
 instead for a guide on the core modules.


Akka HTTP


A full server- and client-side HTTP stack on top of akka-actor and akka-stream.


Akka gRPC


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams.


Alpakka


Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka.


Alpakka Kafka Connector


The Alpakka Kafka Connector connects Apache Kafka with Akka Streams.


Akka Projections


Akka Projections let you process a stream of events or records from a source to a projected model or external system.


Cassandra Plugin for Akka Persistence


An Akka Persistence journal and snapshot store backed by Apache Cassandra.


JDBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on 
Slick
.


R2DBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on 
R2DBC
.


Akka Management




Akka Management
 provides a central HTTP endpoint for Akka management extensions.


Akka Cluster Bootstrap
 helps bootstrapping an Akka cluster using Akka Discovery.


Akka Management Kubernetes Rolling Updates
 for smooth rolling updates.


Akka Management Cluster HTTP
 provides HTTP endpoints for introspecting and managing Akka clusters.


Akka Discovery for Kubernetes, Consul, Marathon, and AWS


Kubernetes Lease




Akka Diagnostics




Akka Thread Starvation Detector


Akka Configuration Checker




Akka Insights


Intelligent monitoring and observability purpose-built for Akka: 
Lightbend Telemetry


Community Projects


Akka has a vibrant and passionate user community, the members of which have created many independent projects using Akka as well as extensions to it. See 
Community Projects
.














 
Extending Akka






Package, Deploy and Run 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial_3.html
Part 3: Working with Device Actors • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors




Introduction


Identifying messages for devices


Adding flexibility to device messages


Implementing the device actor and its read protocol


Testing the actor


Adding a write protocol


Actor with read and write messages


What’s Next?




Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors




Introduction


Identifying messages for devices


Adding flexibility to device messages


Implementing the device actor and its read protocol


Testing the actor


Adding a write protocol


Actor with read and write messages


What’s Next?




Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Part 3: Working with Device Actors


Introduction


In the previous topics we explained how to view actor systems 
in the large
, that is, how components should be represented, how actors should be arranged in the hierarchy. In this part, we will look at actors 
in the small
 by implementing the device actor.


If we were working with objects, we would typically design the API as 
interfaces
, a collection of abstract methods to be filled out by the actual implementation. In the world of actors, protocols take the place of interfaces. While it is not possible to formalize general protocols in the programming language, we can compose their most basic element, messages. So, we will start by identifying the messages we will want to send to device actors.


Typically, messages fall into categories, or patterns. By identifying these patterns, you will find that it becomes easier to choose between them and to implement them. The first example demonstrates the 
request-respond
 message pattern.


Identifying messages for devices


The tasks of a device actor will be simple:




Collect temperature measurements


When asked, report the last measured temperature




However, a device might start without immediately having a temperature measurement. Hence, we need to account for the case where a temperature is not present. This also allows us to test the query part of the actor without the write part present, as the device actor can report an empty result.


The protocol for obtaining the current temperature from the device actor is simple. The actor:




Waits for a request for the current temperature.


Responds to the request with a reply that either:
    


contains the current temperature or,


indicates that a temperature is not yet available.








We need two messages, one for the request, and one for the reply. Our first attempt might look like the following:




Scala




copy
source
package com.example

  import akka.actor.typed.ActorRef

  object Device {
    sealed trait Command
    final case class ReadTemperature(replyTo: ActorRef[RespondTemperature]) extends Command
    final case class RespondTemperature(value: Option[Double])
  }


Java




copy
source
package com.example;

import akka.actor.typed.ActorRef;
import java.util.Optional;

public class Device {

  public interface Command {}

  public static final class ReadTemperature implements Command {
    final ActorRef<RespondTemperature> replyTo;

    public ReadTemperature(ActorRef<RespondTemperature> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static final class RespondTemperature {
    final Optional<Double> value;

    public RespondTemperature(Optional<Double> value) {
      this.value = value;
    }
  }
}




Note that the 
ReadTemperature
 message contains the 
ActorRef[RespondTemperature]
ActorRef<RespondTemperature>
 that the device actor will use when replying to the request.


These two messages seem to cover the required functionality. However, the approach we choose must take into account the distributed nature of the application. While the basic mechanism is the same for communicating with an actor on the local JVM as with a remote actor, we need to keep the following in mind:




There will be observable differences in the latency of delivery between local and remote messages, because factors like network link bandwidth and the message size also come into play.


Reliability is a concern because a remote message send involves more steps, which means that more can go wrong.


A local send will pass a reference to the message inside the same JVM, without any restrictions on the underlying object which is sent, whereas a remote transport will place a limit on the message size.




In addition, while sending inside the same JVM is significantly more reliable, if an actor fails due to a programmer error while processing the message, the effect is the same as if a remote network request fails due to the remote host crashing while processing the message. Even though in both cases, the service recovers after a while (the actor is restarted by its supervisor, the host is restarted by an operator or by a monitoring system) individual requests are lost during the crash. 
Therefore, writing your actors such that every message could possibly be lost is the safe, pessimistic bet.


But to further understand the need for flexibility in the protocol, it will help to consider Akka message ordering and message delivery guarantees. Akka provides the following behavior for message sends:




At-most-once delivery, that is, no guaranteed delivery.


Message ordering is maintained per sender, receiver pair.




The following sections discuss this behavior in more detail:




Message delivery


Message ordering




Message delivery


The delivery semantics provided by messaging subsystems typically fall into the following categories:




At-most-once delivery
 — each message is delivered zero or one time; in more causal terms it means that messages can be lost, but are never duplicated.


At-least-once delivery
 — potentially multiple attempts are made to deliver each message, until at least one succeeds; again, in more causal terms this means that messages can be duplicated but are never lost.


Exactly-once delivery
 — each message is delivered exactly once to the recipient; the message can neither be lost nor be duplicated.




The first behavior, the one used by Akka, is the cheapest and results in the highest performance. It has the least implementation overhead because it can be done in a fire-and-forget fashion without keeping the state at the sending end or in the transport mechanism. The second, at-least-once, requires retries to counter transport losses. This adds the overhead of keeping the state at the sending end and having an acknowledgment mechanism at the receiving end. Exactly-once delivery is most expensive, and results in the worst performance: in addition to the overhead added by at-least-once delivery, it requires the state to be kept at the receiving end in order to filter out duplicate deliveries.


In an actor system, we need to determine exact meaning of a guarantee — at which point does the system consider the delivery as accomplished:




When the message is sent out on the network?


When the message is received by the target actor’s host?


When the message is put into the target actor’s mailbox?


When the message target actor starts to process the message?


When the target actor has successfully processed the message?




Most frameworks and protocols that claim guaranteed delivery actually provide something similar to points 4 and 5. While this sounds reasonable, 
is it actually useful?
 To understand the implications, consider a simple, practical example: a user attempts to place an order and we only want to claim that it has successfully processed once it is actually on disk in the orders database.


If we rely on the successful processing of the message, the actor will report success as soon as the order has been submitted to the internal API that has the responsibility to validate it, process it and put it into the database. Unfortunately, immediately after the API has been invoked any of the following can happen:




The host can crash.


Deserialization can fail.


Validation can fail.


The database might be unavailable.


A programming error might occur.




This illustrates that the 
guarantee of delivery
 does not translate to the 
domain level guarantee
. We only want to report success once the order has been actually fully processed and persisted. 
The only entity that can report success is the application itself, since only it has any understanding of the domain guarantees required. No generalized framework can figure out the specifics of a particular domain and what is considered a success in that domain
.


In this particular example, we only want to signal success after a successful database write, where the database acknowledged that the order is now safely stored. 
For these reasons Akka lifts the responsibilities of guarantees to the application itself, i.e. you have to implement them yourself with the tools that Akka provides. This gives you full control of the guarantees that you want to provide
. Now, let’s consider the message ordering that Akka provides to make it easy to reason about application logic.


Message Ordering


In Akka, for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order. The word directly emphasizes that this guarantee only applies when sending with the tell operator directly to the final destination, but not when employing mediators.


If:




Actor 
A1
 sends messages 
M1
, 
M2
, 
M3
 to 
A2
.


Actor 
A3
 sends messages 
M4
, 
M5
, 
M6
 to 
A2
.




This means that, for Akka messages:




If 
M1
 is delivered it must be delivered before 
M2
 and 
M3
.


If 
M2
 is delivered it must be delivered before 
M3
.


If 
M4
 is delivered it must be delivered before 
M5
 and 
M6
.


If 
M5
 is delivered it must be delivered before 
M6
.


A2
 can see messages from 
A1
 interleaved with messages from 
A3
.


Since there is no guaranteed delivery, any of the messages may be dropped, i.e. not arrive at 
A2
.




These guarantees strike a good balance: having messages from one actor arrive in-order is convenient for building systems that can be easily reasoned about, while on the other hand allowing messages from different actors to arrive interleaved provides sufficient freedom for an efficient implementation of the actor system.


For the full details on delivery guarantees please refer to the 
reference page
.


Adding flexibility to device messages


Our first query protocol was correct, but did not take into account distributed application execution. If we want to implement resends in the actor that queries a device actor (because of timed out requests), or if we want to query multiple actors, we need to be able to correlate requests and responses. Hence, we add one more field to our messages, so that an ID can be provided by the requester (we will add this code to our app in a later step):




Scala




copy
source
sealed trait Command
final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command
final case class RespondTemperature(requestId: Long, value: Option[Double])


Java




copy
source
public class Device extends AbstractBehavior<Device.Command> {
  public interface Command {}

  public static final class ReadTemperature implements Command {
    final long requestId;
    final ActorRef<RespondTemperature> replyTo;

    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {
      this.requestId = requestId;
      this.replyTo = replyTo;
    }
  }

  public static final class RespondTemperature {
    final long requestId;
    final Optional<Double> value;

    public RespondTemperature(long requestId, Optional<Double> value) {
      this.requestId = requestId;
      this.value = value;
    }
  }
}




Implementing the device actor and its read protocol


As we learned in the Hello World example, each actor defines the type of messages it will accept. Our device actor has the responsibility to use the same ID parameter for the response of a given query, which would make it look like the following.




Scala




copy
source
package com.example

  import akka.actor.typed.ActorRef
  import akka.actor.typed.Behavior
  import akka.actor.typed.PostStop
  import akka.actor.typed.Signal
  import akka.actor.typed.scaladsl.AbstractBehavior
  import akka.actor.typed.scaladsl.ActorContext
  import akka.actor.typed.scaladsl.Behaviors

  object Device {
    def apply(groupId: String, deviceId: String): Behavior[Command] =
      Behaviors.setup(context => new Device(context, groupId, deviceId))

    sealed trait Command
    final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command
    final case class RespondTemperature(requestId: Long, value: Option[Double])
  }

  class Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)
      extends AbstractBehavior[Device.Command](context) {
    import Device._

    var lastTemperatureReading: Option[Double] = None

    context.log.info("Device actor {}-{} started", groupId, deviceId)

    override def onMessage(msg: Command): Behavior[Command] = {
      msg match {
        case ReadTemperature(id, replyTo) =>
          replyTo ! RespondTemperature(id, lastTemperatureReading)
          this
      }
    }

    override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
      case PostStop =>
        context.log.info("Device actor {}-{} stopped", groupId, deviceId)
        this
    }

  }


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.PostStop;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.util.Optional;

public class Device extends AbstractBehavior<Device.Command> {
  public interface Command {}

  public static final class ReadTemperature implements Command {
    final long requestId;
    final ActorRef<RespondTemperature> replyTo;

    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {
      this.requestId = requestId;
      this.replyTo = replyTo;
    }
  }

  public static final class RespondTemperature {
    final long requestId;
    final Optional<Double> value;

    public RespondTemperature(long requestId, Optional<Double> value) {
      this.requestId = requestId;
      this.value = value;
    }
  }

  public static Behavior<Command> create(String groupId, String deviceId) {
    return Behaviors.setup(context -> new Device(context, groupId, deviceId));
  }

  private final String groupId;
  private final String deviceId;

  private Optional<Double> lastTemperatureReading = Optional.empty();

  private Device(ActorContext<Command> context, String groupId, String deviceId) {
    super(context);
    this.groupId = groupId;
    this.deviceId = deviceId;

    context.getLog().info("Device actor {}-{} started", groupId, deviceId);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(ReadTemperature.class, this::onReadTemperature)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private Behavior<Command> onReadTemperature(ReadTemperature r) {
    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));
    return this;
  }

  private Device onPostStop() {
    getContext().getLog().info("Device actor {}-{} stopped", groupId, deviceId);
    return this;
  }
}





Note in the code that:




The 
apply
 method in the companion object
static 
create
 method
 defines how to construct the 
Behavior
 for the 
Device
 actor. The parameters include an ID for the device and the group to which it belongs, which we will use later.


The messages we reasoned about previously are defined in 
the companion object.
Device class that was shown earlier.


In the 
Device
 class, the value of 
lastTemperatureReading
 is initially set to 
None
Optional.empty()
, and the actor will report it back if queried.




Testing the actor


Based on the actor above, we could write a test. In the 
com.example
 package in the test tree of your project, add the following code to a 
DeviceSpec.scala
DeviceTest.java
 file. 
(We use ScalaTest but any other test framework can be used with the Akka Testkit)
.


You can run this test 
by running 
test
 at the sbt prompt
by running 
mvn test
.




Scala




copy
source
package com.example

import akka.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit
import org.scalatest.wordspec.AnyWordSpecLike

class DeviceSpec extends ScalaTestWithActorTestKit with AnyWordSpecLike {
  import Device._

  "Device actor" must {

    "reply with empty reading if no temperature is known" in {
      val probe = createTestProbe[RespondTemperature]()
      val deviceActor = spawn(Device("group", "device"))

      deviceActor ! Device.ReadTemperature(requestId = 42, probe.ref)
      val response = probe.receiveMessage()
      response.requestId should ===(42)
      response.value should ===(None)
    }
  }
}


Java




copy
source
import static org.junit.Assert.assertEquals;

import akka.actor.testkit.typed.javadsl.TestKitJunitResource;
import akka.actor.testkit.typed.javadsl.TestProbe;
import akka.actor.typed.ActorRef;
import java.util.Optional;
import org.junit.ClassRule;
import org.junit.Test;


public class DeviceTest {

  @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource();

  @Test
  public void testReplyWithEmptyReadingIfNoTemperatureIsKnown() {
    TestProbe<Device.RespondTemperature> probe =
        testKit.createTestProbe(Device.RespondTemperature.class);
    ActorRef<Device.Command> deviceActor = testKit.spawn(Device.create("group", "device"));
    deviceActor.tell(new Device.ReadTemperature(42L, probe.getRef()));
    Device.RespondTemperature response = probe.receiveMessage();
    assertEquals(42L, response.requestId);
    assertEquals(Optional.empty(), response.value);
  }
}




Now, the actor needs a way to change the state of the temperature when it receives a message from the sensor.


Adding a write protocol


The purpose of the write protocol is to update the 
currentTemperature
 field when the actor receives a message that contains the temperature. Again, it is tempting to define the write protocol as a very simple message, something like this:




Scala




copy
source
sealed trait Command
final case class RecordTemperature(value: Double) extends Command


Java




copy
source
public static final class RecordTemperature implements Command {
  final double value;

  public RecordTemperature(double value) {
    this.value = value;
  }
}




However, this approach does not take into account that the sender of the record temperature message can never be sure if the message was processed or not. We have seen that Akka does not guarantee delivery of these messages and leaves it to the application to provide success notifications. In our case, we would like to send an acknowledgment to the sender once we have updated our last temperature recording, e.g. replying with a 
TemperatureRecorded
 message. Just like in the case of temperature queries and responses, it is also a good idea to include an ID field to provide maximum flexibility.




Scala




copy
source
final case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])
    extends Command
final case class TemperatureRecorded(requestId: Long)


Java




copy
source
public static final class RecordTemperature implements Command {
  final long requestId;
  final double value;
  final ActorRef<TemperatureRecorded> replyTo;

  public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {
    this.requestId = requestId;
    this.value = value;
    this.replyTo = replyTo;
  }
}

public static final class TemperatureRecorded {
  final long requestId;

  public TemperatureRecorded(long requestId) {
    this.requestId = requestId;
  }
}




Actor with read and write messages


Putting the read and write protocol together, the device actor looks like the following example:




Scala




copy
source
package com.example

import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.PostStop
import akka.actor.typed.Signal
import akka.actor.typed.scaladsl.AbstractBehavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object Device {
  def apply(groupId: String, deviceId: String): Behavior[Command] =
    Behaviors.setup(context => new Device(context, groupId, deviceId))

  sealed trait Command

  final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command
  final case class RespondTemperature(requestId: Long, value: Option[Double])

  final case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])
      extends Command
  final case class TemperatureRecorded(requestId: Long)
}

class Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)
    extends AbstractBehavior[Device.Command](context) {
  import Device._

  var lastTemperatureReading: Option[Double] = None

  context.log.info("Device actor {}-{} started", groupId, deviceId)

  override def onMessage(msg: Command): Behavior[Command] = {
    msg match {
      case RecordTemperature(id, value, replyTo) =>
        context.log.info("Recorded temperature reading {} with {}", value, id)
        lastTemperatureReading = Some(value)
        replyTo ! TemperatureRecorded(id)
        this

      case ReadTemperature(id, replyTo) =>
        replyTo ! RespondTemperature(id, lastTemperatureReading)
        this
    }
  }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("Device actor {}-{} stopped", groupId, deviceId)
      this
  }

}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.PostStop;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.util.Optional;

public class Device extends AbstractBehavior<Device.Command> {

  public interface Command {}

  public static final class RecordTemperature implements Command {
    final long requestId;
    final double value;
    final ActorRef<TemperatureRecorded> replyTo;

    public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {
      this.requestId = requestId;
      this.value = value;
      this.replyTo = replyTo;
    }
  }

  public static final class TemperatureRecorded {
    final long requestId;

    public TemperatureRecorded(long requestId) {
      this.requestId = requestId;
    }
  }

  public static final class ReadTemperature implements Command {
    final long requestId;
    final ActorRef<RespondTemperature> replyTo;

    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {
      this.requestId = requestId;
      this.replyTo = replyTo;
    }
  }

  public static final class RespondTemperature {
    final long requestId;
    final Optional<Double> value;

    public RespondTemperature(long requestId, Optional<Double> value) {
      this.requestId = requestId;
      this.value = value;
    }
  }

  public static Behavior<Command> create(String groupId, String deviceId) {
    return Behaviors.setup(context -> new Device(context, groupId, deviceId));
  }

  private final String groupId;
  private final String deviceId;

  private Optional<Double> lastTemperatureReading = Optional.empty();

  private Device(ActorContext<Command> context, String groupId, String deviceId) {
    super(context);
    this.groupId = groupId;
    this.deviceId = deviceId;

    context.getLog().info("Device actor {}-{} started", groupId, deviceId);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(RecordTemperature.class, this::onRecordTemperature)
        .onMessage(ReadTemperature.class, this::onReadTemperature)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private Behavior<Command> onRecordTemperature(RecordTemperature r) {
    getContext().getLog().info("Recorded temperature reading {} with {}", r.value, r.requestId);
    lastTemperatureReading = Optional.of(r.value);
    r.replyTo.tell(new TemperatureRecorded(r.requestId));
    return this;
  }

  private Behavior<Command> onReadTemperature(ReadTemperature r) {
    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));
    return this;
  }

  private Behavior<Command> onPostStop() {
    getContext().getLog().info("Device actor {}-{} stopped", groupId, deviceId);
    return Behaviors.stopped();
  }
}




We should also write a new test case now, exercising both the read/query and write/record functionality together:




Scala




copy
source
"reply with latest temperature reading" in {
  val recordProbe = createTestProbe[TemperatureRecorded]()
  val readProbe = createTestProbe[RespondTemperature]()
  val deviceActor = spawn(Device("group", "device"))

  deviceActor ! Device.RecordTemperature(requestId = 1, 24.0, recordProbe.ref)
  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 1))

  deviceActor ! Device.ReadTemperature(requestId = 2, readProbe.ref)
  val response1 = readProbe.receiveMessage()
  response1.requestId should ===(2)
  response1.value should ===(Some(24.0))

  deviceActor ! Device.RecordTemperature(requestId = 3, 55.0, recordProbe.ref)
  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 3))

  deviceActor ! Device.ReadTemperature(requestId = 4, readProbe.ref)
  val response2 = readProbe.receiveMessage()
  response2.requestId should ===(4)
  response2.value should ===(Some(55.0))
}


Java




copy
source
@Test
public void testReplyWithLatestTemperatureReading() {
  TestProbe<Device.TemperatureRecorded> recordProbe =
      testKit.createTestProbe(Device.TemperatureRecorded.class);
  TestProbe<Device.RespondTemperature> readProbe =
      testKit.createTestProbe(Device.RespondTemperature.class);
  ActorRef<Device.Command> deviceActor = testKit.spawn(Device.create("group", "device"));

  deviceActor.tell(new Device.RecordTemperature(1L, 24.0, recordProbe.getRef()));
  assertEquals(1L, recordProbe.receiveMessage().requestId);

  deviceActor.tell(new Device.ReadTemperature(2L, readProbe.getRef()));
  Device.RespondTemperature response1 = readProbe.receiveMessage();
  assertEquals(2L, response1.requestId);
  assertEquals(Optional.of(24.0), response1.value);

  deviceActor.tell(new Device.RecordTemperature(3L, 55.0, recordProbe.getRef()));
  assertEquals(3L, recordProbe.receiveMessage().requestId);

  deviceActor.tell(new Device.ReadTemperature(4L, readProbe.getRef()));
  Device.RespondTemperature response2 = readProbe.receiveMessage();
  assertEquals(4L, response2.requestId);
  assertEquals(Optional.of(55.0), response2.value);
}




What’s Next?


So far, we have started designing our overall architecture, and we wrote the first actor that directly corresponds to the domain. We now have to create the component that is responsible for maintaining groups of devices and the device actors themselves.














 
Part 2: Creating the First Actor






Part 4: Working with Device Groups 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/contact-us
Contact | Akka
































































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



































































          Talk to an expert.
        






We will 
respond
 almost as fast as your app will.
































































Other Contacts



















                  Support
                






If you are a 
Akka
 subscriber with a technical question, please 
 
log into support
.























                  Partnerships
                






For all partnership requests, please complete the 
partner form
.























                  Press/Analyst Inquiry
                






To request a press or analyst briefing, please email 
pr@akka.io
.























                  Meetup/Event
                






To request a speaker or sponsorship for an upcoming event, please email 
programs@akka.io
.























                  Sales
                






For all sales requests, please complete the form above or call 
+1 877.989.7372























                  Social Media
                












































































































































Our Offices





























                  USA / Headquarters
                






San Francisco


Akka.io


580 California, #1231 
San Francisco, CA 94104

































                  Switzerland
                






Lusanne


Akka.io


Lightbend Sàrl, EPFL Innovation Park 
Bâtiment D, 1015 Lausanne, Switzerland

































                  Canada
                






Quebec


Akka.io


260 Saint-Raymond Blvd. Unit 205 
Gatineau, Quebec J9A 3G7 Canada

































                  Sweden
                






Uppsala


Akka.io


c/o Upb, Box 3076, 750 03 Uppsala 
Sweden










































Stay Responsive 
to Change.










GET STARTED










































REQUEST A DEMO








































































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://console.akka.io/register
Akka Registration Page

URL: https://doc.akka.io/libraries/akka/snapshot/general/addressing.html
Actor References, Paths and Addresses • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses




What is an Actor Reference


What is an Actor Path?


How are Actor References obtained?


Actor Reference and Path Equality


Reusing Actor Paths


What is the Address part used for?


Top-Level Scopes for Actor Paths




Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses




What is an Actor Reference


What is an Actor Path?


How are Actor References obtained?


Actor Reference and Path Equality


Reusing Actor Paths


What is the Address part used for?


Top-Level Scopes for Actor Paths




Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actor References, Paths and Addresses


This chapter describes how actors are identified and located within a possibly distributed Akka application. 




The above image displays the relationship between the most important entities within an actor system, please read on for the details.


What is an Actor Reference


An actor reference is a subtype of 
ActorRef
ActorRef
, whose foremost purpose is to support sending messages to the actor it represents. Each actor has access to its canonical (local) reference through the 
ActorContext.self
ActorContext.self
 field; this reference can be included in messages to other actors to get replies back.


There are several different types of actor references that are supported depending on the configuration of the actor system:




Purely local actor references are used by actor systems which are not configured to support networking functions. These actor references will not function if sent across a network connection to a remote JVM.


Local actor references when remoting is enabled are used by actor systems which support networking functions for those references which represent actors within the same JVM. In order to also be reachable when sent to other network nodes, these references include protocol and remote addressing information.


Remote actor references represent actors which are reachable using remote communication, i.e. sending messages to them will serialize the messages transparently and send them to the remote JVM.


There are several special types of actor references which behave like local actor references for all practical purposes:
    


PromiseActorRef
 is the special representation of a 
Promise
 for the purpose of being completed by the response from an actor. 
akka.pattern.ask
 creates this actor reference.


DeadLetterActorRef
 is the default implementation of the dead letters service to which Akka routes all messages whose destinations are shut down or non-existent.


EmptyLocalActorRef
 is what Akka returns when looking up a non-existent local actor path: it is equivalent to a 
DeadLetterActorRef
, but it retains its path so that Akka can send it over the network and compare it to other existing actor references for that path, some of which might have been obtained before the actor died.






And then there are some one-off internal implementations which you should never really see:
    


There is an actor reference which does not represent an actor but acts only as a pseudo-supervisor for the root guardian, we call it âthe one who walks the bubbles of space-timeâ.


The first logging service started before actually firing up actor creation facilities is a fake actor reference which accepts log events and prints them directly to standard output; it is 
Logging.StandardOutLogger
Logging.StandardOutLogger
.








What is an Actor Path?


Since actors are created in a strictly hierarchical fashion, there exists a unique sequence of actor names given by recursively following the supervision links between child and parent down towards the root of the actor system. This sequence can be seen as enclosing folders in a file system, hence we adopted the name âpathâ to refer to it, although actor hierarchy has some fundamental difference from file system hierarchy.


An actor path consists of an anchor, which identifies the actor system, followed by the concatenation of the path elements, from root guardian to the designated actor; the path elements are the names of the traversed actors and are separated by slashes.


What is the Difference Between Actor Reference and Path?


An actor reference designates a single actor and the life-cycle of the reference matches that actorâs life-cycle; an actor path represents a name which may or may not be inhabited by an actor and the path itself does not have a life-cycle, it never becomes invalid. You can create an actor path without creating an actor, but you cannot create an actor reference without creating a corresponding actor.


You can create an actor, terminate it, and then create a new actor with the same actor path. The newly created actor is a new incarnation of the actor. It is not the same actor. An actor reference to the old incarnation is not valid for the new incarnation. Messages sent to the old actor reference will not be delivered to the new incarnation even though they have the same path.


Actor Path Anchors


Each actor path has an address component, describing the protocol and location by which the corresponding actor is reachable, followed by the names of the actors in the hierarchy from the root up. Examples are:


"akka://my-sys/user/service-a/worker1"               // purely local
"akka://
[email protected]
:5678/user/service-b" // remote



The interpretation of the host and port part (i.e. 
host.example.com:5678
 in the example) depends on the transport mechanism used, but it must abide by the URI structural rules.


Logical Actor Paths


The unique path obtained by following the parental supervision links towards the root guardian is called the logical actor path. This path matches exactly the creation ancestry of an actor, so it is completely deterministic as soon as the actor systemâs remoting configuration (and with it the address component of the path) is set.


Actor path alias or symbolic link?


As in some real file-systems you might think of a âpath aliasâ or âsymbolic linkâ for an actor, i.e. one actor may be reachable using more than one path. However, you should note that actor hierarchy is different from file system hierarchy. You cannot freely create actor paths like symbolic links to refer to arbitrary actors.


How are Actor References obtained?


There are two general categories to how actor references may be obtained: by 
creating actors
 or by looking them up through the 
Receptionist
.


Actor Reference and Path Equality


Equality of 
ActorRef
ActorRef
 match the intention that an 
ActorRef
ActorRef
 corresponds to the target actor incarnation. Two actor references are compared equal when they have the same path and point to the same actor incarnation. A reference pointing to a terminated actor does not compare equal to a reference pointing to another (re-created) actor with the same path. Note that a restart of an actor caused by a failure still means that it is the same actor incarnation, i.e. a restart is not visible for the consumer of the 
ActorRef
ActorRef
.


If you need to keep track of actor references in a collection and do not care about the exact actor incarnation you can use the 
ActorPath
ActorPath
 as key, because the identifier of the target actor is not taken into account when comparing actor paths.


Reusing Actor Paths


When an actor is terminated, its reference will point to the dead letter mailbox, DeathWatch will publish its final transition and in general it is not expected to come back to life again (since the actor life cycle does not allow this).


What is the Address part used for?


When sending an actor reference across the network, it is represented by its path. Hence, the path must fully encode all information necessary to send messages to the underlying actor. This is achieved by encoding protocol, host and port in the address part of the path string. When an actor system receives an actor path from a remote node, it checks whether that pathâs address matches the address of this actor system, in which case it will be resolved to the actorâs local reference. Otherwise, it will be represented by a remote actor reference.




Top-Level Scopes for Actor Paths


At the root of the path hierarchy resides the root guardian above which all other actors are found; its name is 
"/"
. The next level consists of the following:




"/user"
 is the guardian actor for all user-created top-level actors; actors created using 
ActorSystem.actorOf
ActorSystem.actorOf
 are found below this one.


"/system"
 is the guardian actor for all system-created top-level actors, e.g. logging listeners or actors automatically deployed by configuration at the start of the actor system.


"/deadLetters"
 is the dead letter actor, which is where all messages sent to stopped or non-existing actors are re-routed (on a best-effort basis: messages may be lost even within the local JVM).


"/temp"
 is the guardian for all short-lived system-created actors, e.g. those which are used in the implementation of 
ActorRef.ask
Patterns.ask
.


"/remote"
 is an artificial path below which all actors reside whose supervisors are remote actor references




The need to structure the name space for actors like this arises from a central and very simple design goal: everything in the hierarchy is an actor, and all actors function in the same way. 
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.














 
Supervision and Monitoring






Location Transparency 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/futures.html
Futures patterns • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns




Dependency


After


Retry




Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns




Dependency


After


Retry




Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Futures patterns


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Akka offers tiny helpers for use with 
Future
s
CompletionStage
. These are part of Akka’s core module:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor_${versions.ScalaBinary}"
}


After


akka.pattern.after
akka.pattern.Patterns.after
 makes it easy to complete a 
Future
CompletionStage
 with a value or exception after a timeout.




Scala




copy
source
val delayed =
  akka.pattern.after(200.millis)(Future.failed(new IllegalStateException("OHNOES")))

val future = Future { Thread.sleep(1000); "foo" }
val result = Future.firstCompletedOf(Seq(future, delayed))


Java




copy
source
import akka.pattern.Patterns;

CompletionStage<String> failWithException =
    CompletableFuture.supplyAsync(
        () -> {
          throw new IllegalStateException("OHNOES1");
        });
CompletionStage<String> delayed =
    Patterns.after(Duration.ofMillis(200), system, () -> failWithException);




Retry


akka.pattern.retry
akka.pattern.Patterns.retry
 will retry a 
Future
CompletionStage
 some number of times with a delay between each attempt.




Scala




copy
source
import akka.actor.typed.scaladsl.adapter._
implicit val scheduler: akka.actor.Scheduler = system.scheduler.toClassic
implicit val ec: ExecutionContext = system.executionContext

//Given some future that will succeed eventually
@volatile var failCount = 0
def futureToAttempt() = {
  if (failCount < 5) {
    failCount += 1
    Future.failed(new IllegalStateException(failCount.toString))
  } else Future.successful(5)
}

//Return a new future that will retry up to 10 times
val retried: Future[Int] = akka.pattern.retry(() => futureToAttempt(), attempts = 10, 100 milliseconds)


Java




copy
source
import akka.pattern.Patterns;

Callable<CompletionStage<String>> attempt = () -> CompletableFuture.completedFuture("test");

CompletionStage<String> retriedFuture =
    Patterns.retry(attempt, 3, java.time.Duration.ofMillis(200), system);
















 
Circuit Breaker






Extending Akka 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/failure-detector.html
Phi Accrual Failure Detector • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector




Introduction


Failure Detector Heartbeats


Logging


Failure Detector Threshold




Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector




Introduction


Failure Detector Heartbeats


Logging


Failure Detector Threshold




Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Phi Accrual Failure Detector


Introduction


Remote DeathWatch uses heartbeat messages and the failure detector to detect network failures and JVM crashes. 


The heartbeat arrival times are interpreted by an implementation of 
The Phi Accrual Failure Detector
 by Hayashibara et al.


Failure Detector Heartbeats


Heartbeats are sent every second by default, which is configurable. They are performed in a request/reply handshake, and the replies are input to the failure detector.


The suspicion level of failure is represented by a value called 
phi
. The basic idea of the phi failure detector is to express the value of 
phi
 on a scale that is dynamically adjusted to reflect current network conditions.


The value of 
phi
 is calculated as:


phi = -log10(1 - F(timeSinceLastHeartbeat))



where F is the cumulative distribution function of a normal distribution with mean and standard deviation estimated from historical heartbeat inter-arrival times.


An accrual failure detector decouples monitoring and interpretation. That makes them applicable to a wider area of scenarios and more adequate to build generic failure detection services. The idea is that it is keeping a history of failure statistics, calculated from heartbeats received from other nodes, and is trying to do educated guesses by taking multiple factors, and how they accumulate over time, into account in order to come up with a better guess if a specific node is up or down. Rather than only answering “yes” or “no” to the question “is the node down?” it returns a 
phi
 value representing the likelihood that the node is down.


The following chart illustrates how 
phi
 increase with increasing time since the previous heartbeat.




Phi is calculated from the mean and standard deviation of historical inter arrival times. The previous chart is an example for standard deviation of 200 ms. If the heartbeats arrive with less deviation the curve becomes steeper, i.e. it is possible to determine failure more quickly. The curve looks like this for a standard deviation of 100 ms.




To be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is configured with a margin, which you may want to adjust depending on you environment. This is how the curve looks like for 
failure-detector.acceptable-heartbeat-pause
 configured to 3 seconds.




Logging


When the Cluster failure detector observes another node as unreachable it will log:


Marking node(s) as UNREACHABLE



and if it becomes reachable again:


Marking node(s) as REACHABLE



There is also a warning when the heartbeat arrival interval exceeds 2/3 of the 
acceptable-heartbeat-pause


heartbeat interval is growing too large



If you see false positives, as indicated by frequent 
UNREACHABLE
 followed by 
REACHABLE
 logging, you can increase the 
acceptable-heartbeat-pause
 if you suspect that your environment is more unstable than what is tolerated by the default value. However, it can be good to investigate the reason so that it is not caused by long (unexpected) garbage collection pauses, overloading the system, too restrictive CPU quotas settings, and similar. 


akka.cluster.failure-detector.acceptable-heartbeat-pause = 7s



Another log message to watch out for that typically requires investigation of the root cause:


Scheduled sending of heartbeat was delayed



Failure Detector Threshold


The 
threshold
 that is the basis for the calculation is configurable by the user, but typically it’s enough to configure the 
acceptable-heartbeat-pause
 as described above.




A low 
threshold
 is prone to generate many false positives but ensures a quick detection in the event of a real crash.


Conversely, a high 
threshold
 generates fewer mistakes but needs more time to detect actual crashes.


The default 
threshold
 is 8 and is appropriate for most situations. However in cloud environments, such as Amazon EC2, the value could be increased to 12 in order to account for network issues that sometimes occur on such platforms.
















 
Cluster Membership Service






Distributed Data 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/persistence-journals.html
Persistence - Building a storage backend • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend




Journal plugin API


Snapshot store plugin API


Plugin TCK


Corrupt event logs




Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend




Journal plugin API


Snapshot store plugin API


Plugin TCK


Corrupt event logs




Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence - Building a storage backend


Storage backends for journals and snapshot stores are pluggable in the Akka persistence extension. A directory of persistence journal and snapshot store plugins is available at the Akka Community Projects page, see 
Community plugins
 This documentation described how to build a new storage backend.


Applications can provide their own plugins by implementing a plugin API and activating them by configuration. Plugin development requires the following imports:




Scala




copy
source
import akka.persistence._
import akka.persistence.journal._
import akka.persistence.snapshot._



Java




copy
source
import akka.dispatch.Futures;
import akka.persistence.*;
import akka.persistence.journal.japi.*;
import akka.persistence.snapshot.japi.*;




Journal plugin API


A journal plugin extends 
AsyncWriteJournal
.


AsyncWriteJournal
 is an actor and the methods to be implemented are:




Scala




copy
source
/**
 * Plugin API: asynchronously writes a batch (`Seq`) of persistent messages to the
 * journal.
 *
 * The batch is only for performance reasons, i.e. all messages don't have to be written
 * atomically. Higher throughput can typically be achieved by using batch inserts of many
 * records compared to inserting records one-by-one, but this aspect depends on the
 * underlying data store and a journal implementation can implement it as efficient as
 * possible. Journals should aim to persist events in-order for a given `persistenceId`
 * as otherwise in case of a failure, the persistent state may be end up being inconsistent.
 *
 * Each `AtomicWrite` message contains the single `PersistentRepr` that corresponds to
 * the event that was passed to the `persist` method of the `PersistentActor`, or it
 * contains several `PersistentRepr` that corresponds to the events that were passed
 * to the `persistAll` method of the `PersistentActor`. All `PersistentRepr` of the
 * `AtomicWrite` must be written to the data store atomically, i.e. all or none must
 * be stored. If the journal (data store) cannot support atomic writes of multiple
 * events it should reject such writes with a `Try` `Failure` with an
 * `UnsupportedOperationException` describing the issue. This limitation should
 * also be documented by the journal plugin.
 *
 * If there are failures when storing any of the messages in the batch the returned
 * `Future` must be completed with failure. The `Future` must only be completed with
 * success when all messages in the batch have been confirmed to be stored successfully,
 * i.e. they will be readable, and visible, in a subsequent replay. If there is
 * uncertainty about if the messages were stored or not the `Future` must be completed
 * with failure.
 *
 * Data store connection problems must be signaled by completing the `Future` with
 * failure.
 *
 * The journal can also signal that it rejects individual messages (`AtomicWrite`) by
 * the returned `immutable.Seq[Try[Unit]]`. It is possible but not mandatory to reduce
 * number of allocations by returning `Future.successful(Nil)` for the happy path,
 * i.e. when no messages are rejected. Otherwise the returned `Seq` must have as many elements
 * as the input `messages` `Seq`. Each `Try` element signals if the corresponding
 * `AtomicWrite` is rejected or not, with an exception describing the problem. Rejecting
 * a message means it was not stored, i.e. it must not be included in a later replay.
 * Rejecting a message is typically done before attempting to store it, e.g. because of
 * serialization error.
 *
 * Data store connection problems must not be signaled as rejections.
 *
 * It is possible but not mandatory to reduce number of allocations by returning
 * `Future.successful(Nil)` for the happy path, i.e. when no messages are rejected.
 *
 * Calls to this method are serialized by the enclosing journal actor. If you spawn
 * work in asynchronous tasks it is alright that they complete the futures in any order,
 * but the actual writes for a specific persistenceId should be serialized to avoid
 * issues such as events of a later write are visible to consumers (query side, or replay)
 * before the events of an earlier write are visible.
 * A PersistentActor will not send a new WriteMessages request before the previous one
 * has been completed.
 *
 * Please note that the `sender` field of the contained PersistentRepr objects has been
 * nulled out (i.e. set to `ActorRef.noSender`) in order to not use space in the journal
 * for a sender reference that will likely be obsolete during replay.
 *
 * Please also note that requests for the highest sequence number may be made concurrently
 * to this call executing for the same `persistenceId`, in particular it is possible that
 * a restarting actor tries to recover before its outstanding writes have completed. In
 * the latter case it is highly desirable to defer reading the highest sequence number
 * until all outstanding writes have completed, otherwise the PersistentActor may reuse
 * sequence numbers.
 *
 * This call is protected with a circuit-breaker.
 */
def asyncWriteMessages(messages: immutable.Seq[AtomicWrite]): Future[immutable.Seq[Try[Unit]]]

/**
 * Plugin API: asynchronously deletes all persistent messages up to `toSequenceNr`
 * (inclusive).
 *
 * This call is protected with a circuit-breaker.
 * Message deletion doesn't affect the highest sequence number of messages,
 * journal must maintain the highest sequence number and never decrease it.
 */
def asyncDeleteMessagesTo(persistenceId: String, toSequenceNr: Long): Future[Unit]

/**
 * Plugin API
 *
 * Allows plugin implementers to use `f pipeTo self` and
 * handle additional messages for implementing advanced features
 *
 */
def receivePluginInternal: Actor.Receive = Actor.emptyBehavior


Java




copy
source
/**
 * Java API, Plugin API: asynchronously writes a batch (`Iterable`) of persistent messages to the
 * journal.
 *
 * <p>The batch is only for performance reasons, i.e. all messages don't have to be written
 * atomically. Higher throughput can typically be achieved by using batch inserts of many records
 * compared to inserting records one-by-one, but this aspect depends on the underlying data store
 * and a journal implementation can implement it as efficient as possible. Journals should aim to
 * persist events in-order for a given `persistenceId` as otherwise in case of a failure, the
 * persistent state may be end up being inconsistent.
 *
 * <p>Each `AtomicWrite` message contains the single `PersistentRepr` that corresponds to the
 * event that was passed to the `persist` method of the `PersistentActor`, or it contains several
 * `PersistentRepr` that corresponds to the events that were passed to the `persistAll` method of
 * the `PersistentActor`. All `PersistentRepr` of the `AtomicWrite` must be written to the data
 * store atomically, i.e. all or none must be stored. If the journal (data store) cannot support
 * atomic writes of multiple events it should reject such writes with an `Optional` with an
 * `UnsupportedOperationException` describing the issue. This limitation should also be documented
 * by the journal plugin.
 *
 * <p>If there are failures when storing any of the messages in the batch the returned `Future`
 * must be completed with failure. The `Future` must only be completed with success when all
 * messages in the batch have been confirmed to be stored successfully, i.e. they will be
 * readable, and visible, in a subsequent replay. If there is uncertainty about if the messages
 * were stored or not the `Future` must be completed with failure.
 *
 * <p>Data store connection problems must be signaled by completing the `Future` with failure.
 *
 * <p>The journal can also signal that it rejects individual messages (`AtomicWrite`) by the
 * returned `Iterable&lt;Optional&lt;Exception&gt;&gt;`. The returned `Iterable` must have as many
 * elements as the input `messages` `Iterable`. Each `Optional` element signals if the
 * corresponding `AtomicWrite` is rejected or not, with an exception describing the problem.
 * Rejecting a message means it was not stored, i.e. it must not be included in a later replay.
 * Rejecting a message is typically done before attempting to store it, e.g. because of
 * serialization error.
 *
 * <p>Data store connection problems must not be signaled as rejections.
 *
 * <p>Note that it is possible to reduce number of allocations by caching some result `Iterable`
 * for the happy path, i.e. when no messages are rejected.
 *
 * <p>Calls to this method are serialized by the enclosing journal actor. If you spawn work in
 * asynchronous tasks it is alright that they complete the futures in any order, but the actual
 * writes for a specific persistenceId should be serialized to avoid issues such as events of a
 * later write are visible to consumers (query side, or replay) before the events of an earlier
 * write are visible. This can also be done with consistent hashing if it is too fine grained to
 * do it on the persistenceId level. Normally a `PersistentActor` will only have one outstanding
 * write request to the journal but it may emit several write requests when `persistAsync` is used
 * and the max batch size is reached.
 *
 * <p>This call is protected with a circuit-breaker.
 */
Future<Iterable<Optional<Exception>>> doAsyncWriteMessages(Iterable<AtomicWrite> messages);

/**
 * Java API, Plugin API: synchronously deletes all persistent messages up to `toSequenceNr`.
 *
 * <p>This call is protected with a circuit-breaker.
 *
 * @see AsyncRecoveryPlugin
 */
Future<Void> doAsyncDeleteMessagesTo(String persistenceId, long toSequenceNr);




If the storage backend API only supports synchronous, blocking writes, the methods should be implemented as:




Scala




copy
source
def asyncWriteMessages(messages: immutable.Seq[AtomicWrite]): Future[immutable.Seq[Try[Unit]]] =
  Future.fromTry(Try {
    // blocking call here
    ???
  })


Java




copy
source
@Override
public Future<Iterable<Optional<Exception>>> doAsyncWriteMessages(
    Iterable<AtomicWrite> messages) {
  try {
    Iterable<Optional<Exception>> result = new ArrayList<Optional<Exception>>();
    // blocking call here...
    // result.add(..)
    return Futures.successful(result);
  } catch (Exception e) {
    return Futures.failed(e);
  }
}




A journal plugin must also implement the methods defined in 
AsyncRecovery
 for replays and sequence number recovery:




Scala




copy
source
/**
 * Plugin API: asynchronously replays persistent messages. Implementations replay
 * a message by calling `replayCallback`. The returned future must be completed
 * when all messages (matching the sequence number bounds) have been replayed.
 * The future must be completed with a failure if any of the persistent messages
 * could not be replayed.
 *
 * The `replayCallback` must also be called with messages that have been marked
 * as deleted. In this case a replayed message's `deleted` method must return
 * `true`.
 *
 * The `toSequenceNr` is the lowest of what was returned by [[#asyncReadHighestSequenceNr]]
 * and what the user specified as recovery [[akka.persistence.Recovery]] parameter.
 * This does imply that this call is always preceded by reading the highest sequence
 * number for the given `persistenceId`.
 *
 * A special case is `fromSequenceNr` of -1, which means that only the last message if any
 * should be replayed.
 *
 * This call is NOT protected with a circuit-breaker because it may take long time
 * to replay all events. The plugin implementation itself must protect against
 * an unresponsive backend store and make sure that the returned Future is
 * completed with success or failure within reasonable time. It is not allowed
 * to ignore completing the future.
 *
 * @param persistenceId persistent actor id.
 * @param fromSequenceNr sequence number where replay should start (inclusive).
 * @param toSequenceNr sequence number where replay should end (inclusive).
 * @param max maximum number of messages to be replayed.
 * @param recoveryCallback called to replay a single message. Can be called from any
 *                       thread.
 * @see [[AsyncWriteJournal]]
 */
def asyncReplayMessages(persistenceId: String, fromSequenceNr: Long, toSequenceNr: Long, max: Long)(
    recoveryCallback: PersistentRepr => Unit): Future[Unit]

/**
 * Plugin API: asynchronously reads the highest stored sequence number for the
 * given `persistenceId`. The persistent actor will use the highest sequence
 * number after recovery as the starting point when persisting new events.
 * This sequence number is also used as `toSequenceNr` in subsequent call
 * to [[#asyncReplayMessages]] unless the user has specified a lower `toSequenceNr`.
 * Journal must maintain the highest sequence number and never decrease it.
 *
 * This call is protected with a circuit-breaker.
 *
 * Please also note that requests for the highest sequence number may be made concurrently
 * to writes executing for the same `persistenceId`, in particular it is possible that
 * a restarting actor tries to recover before its outstanding writes have completed.
 *
 * @param persistenceId persistent actor id.
 * @param fromSequenceNr hint where to start searching for the highest sequence
 *                       number. When a persistent actor is recovering this
 *                       `fromSequenceNr` will be the sequence number of the used
 *                       snapshot or `0L` if no snapshot is used.
 */
def asyncReadHighestSequenceNr(persistenceId: String, fromSequenceNr: Long): Future[Long]


Java




copy
source
/**
 * Java API, Plugin API: asynchronously replays persistent messages. Implementations replay a
 * message by calling `replayCallback`. The returned future must be completed when all messages
 * (matching the sequence number bounds) have been replayed. The future must be completed with a
 * failure if any of the persistent messages could not be replayed.
 *
 * <p>The `replayCallback` must also be called with messages that have been marked as deleted. In
 * this case a replayed message's `deleted` method must return `true`.
 *
 * <p>The `toSequenceNr` is the lowest of what was returned by {@link
 * #doAsyncReadHighestSequenceNr} and what the user specified as recovery {@link
 * akka.persistence.Recovery} parameter.
 *
 * @param persistenceId id of the persistent actor.
 * @param fromSequenceNr sequence number where replay should start (inclusive).
 * @param toSequenceNr sequence number where replay should end (inclusive).
 * @param max maximum number of messages to be replayed.
 * @param replayCallback called to replay a single message. Can be called from any thread.
 */
Future<Void> doAsyncReplayMessages(
    String persistenceId,
    long fromSequenceNr,
    long toSequenceNr,
    long max,
    Consumer<PersistentRepr> replayCallback);

/**
 * Java API, Plugin API: asynchronously reads the highest stored sequence number for the given
 * `persistenceId`. The persistent actor will use the highest sequence number after recovery as
 * the starting point when persisting new events. This sequence number is also used as
 * `toSequenceNr` in subsequent call to [[#asyncReplayMessages]] unless the user has specified a
 * lower `toSequenceNr`.
 *
 * @param persistenceId id of the persistent actor.
 * @param fromSequenceNr hint where to start searching for the highest sequence number.
 */
Future<Long> doAsyncReadHighestSequenceNr(String persistenceId, long fromSequenceNr);




A journal plugin can be activated with the following minimal configuration:


copy
source
# Path to the journal plugin to be used
akka.persistence.journal.plugin = "my-journal"

# My custom journal plugin
my-journal {
  # Class name of the plugin.
  class = "docs.persistence.MyJournal"
  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"
}


The journal plugin instance is an actor so the methods corresponding to requests from persistent actors are executed sequentially. It may delegate to asynchronous libraries, spawn futures, or delegate to other actors to achieve parallelism.


The journal plugin class must have a constructor with one of these signatures:




constructor with one 
com.typesafe.config.Config
 parameter and a 
String
 parameter for the config path


constructor with one 
com.typesafe.config.Config
 parameter


constructor without parameters




The plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the 
String
 parameter.


The 
plugin-dispatcher
 is the dispatcher used for the plugin actor. If not specified, it defaults to 
akka.actor.default-dispatcher
.


Don’t run journal tasks/futures on the system default dispatcher, since that might starve other tasks.


Snapshot store plugin API


A snapshot store plugin must extend the 
SnapshotStore
 actor and implement the following methods:




Scala




copy
source
/**
 * Plugin API: asynchronously loads a snapshot.
 *
 * If the future `Option` is `None` then all events will be replayed,
 * i.e. there was no snapshot. If snapshot could not be loaded the `Future`
 * should be completed with failure. That is important because events may
 * have been deleted and just replaying the events might not result in a valid
 * state.
 *
 * This call is protected with a circuit-breaker.
 *
 * @param persistenceId id of the persistent actor.
 * @param criteria selection criteria for loading.
 */
def loadAsync(persistenceId: String, criteria: SnapshotSelectionCriteria): Future[Option[SelectedSnapshot]]

/**
 * Plugin API: asynchronously saves a snapshot.
 *
 * This call is protected with a circuit-breaker.
 *
 * @param metadata snapshot metadata.
 * @param snapshot snapshot.
 */
def saveAsync(metadata: SnapshotMetadata, snapshot: Any): Future[Unit]

/**
 * Plugin API: deletes the snapshot identified by `metadata`.
 *
 * This call is protected with a circuit-breaker.
 *
 * @param metadata snapshot metadata.
 */
def deleteAsync(metadata: SnapshotMetadata): Future[Unit]

/**
 * Plugin API: deletes all snapshots matching `criteria`.
 *
 * This call is protected with a circuit-breaker.
 *
 * @param persistenceId id of the persistent actor.
 * @param criteria selection criteria for deleting.
 */
def deleteAsync(persistenceId: String, criteria: SnapshotSelectionCriteria): Future[Unit]

/**
 * Plugin API
 * Allows plugin implementers to use `f pipeTo self` and
 * handle additional messages for implementing advanced features
 */
def receivePluginInternal: Actor.Receive = Actor.emptyBehavior


Java




copy
source
/**
 * Java API, Plugin API: asynchronously loads a snapshot.
 *
 * @param persistenceId id of the persistent actor.
 * @param criteria selection criteria for loading.
 */
Future<Optional<SelectedSnapshot>> doLoadAsync(
    String persistenceId, SnapshotSelectionCriteria criteria);

/**
 * Java API, Plugin API: asynchronously saves a snapshot.
 *
 * @param metadata snapshot metadata.
 * @param snapshot snapshot.
 */
Future<Void> doSaveAsync(SnapshotMetadata metadata, Object snapshot);

/**
 * Java API, Plugin API: deletes the snapshot identified by `metadata`.
 *
 * @param metadata snapshot metadata.
 */
Future<Void> doDeleteAsync(SnapshotMetadata metadata);

/**
 * Java API, Plugin API: deletes all snapshots matching `criteria`.
 *
 * @param persistenceId id of the persistent actor.
 * @param criteria selection criteria for deleting.
 */
Future<Void> doDeleteAsync(String persistenceId, SnapshotSelectionCriteria criteria);




A snapshot store plugin can be activated with the following minimal configuration:


copy
source
# Path to the snapshot store plugin to be used
akka.persistence.snapshot-store.plugin = "my-snapshot-store"

# My custom snapshot store plugin
my-snapshot-store {
  # Class name of the plugin.
  class = "docs.persistence.MySnapshotStore"
  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"
}


The snapshot store instance is an actor so the methods corresponding to requests from persistent actors are executed sequentially. It may delegate to asynchronous libraries, spawn futures, or delegate to other actors to achieve parallelism.


The snapshot store plugin class must have a constructor with one of these signatures:




constructor with one 
com.typesafe.config.Config
 parameter and a 
String
 parameter for the config path


constructor with one 
com.typesafe.config.Config
 parameter


constructor without parameters




The plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the 
String
 parameter.


The 
plugin-dispatcher
 is the dispatcher used for the plugin actor. If not specified, it defaults to 
akka.actor.default-dispatcher
.


Don’t run snapshot store tasks/futures on the system default dispatcher, since that might starve other tasks.


Plugin TCK


In order to help developers build correct and high quality storage plugins, we provide a Technology Compatibility Kit (
TCK
 for short).


The TCK is usable from Java as well as Scala projects. To test your implementation (independently of language) you need to include the akka-persistence-tck dependency.


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-tck" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-tck_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-tck_${versions.ScalaBinary}"
}


To include the Journal TCK tests in your test suite simply extend the provided 
JournalSpec
JavaJournalSpec
:




Scala




copy
source
class MyJournalSpec
    extends JournalSpec(
      config = ConfigFactory.parseString("""akka.persistence.journal.plugin = "my.journal.plugin"""")) {

  override def supportsRejectingNonSerializableObjects: CapabilityFlag =
    false // or CapabilityFlag.off

  override def supportsSerialization: CapabilityFlag =
    true // or CapabilityFlag.on
}


Java




copy
source
@RunWith(JUnitRunner.class)
class MyJournalSpecTest extends JavaJournalSpec {

  public MyJournalSpecTest() {
    super(
        ConfigFactory.parseString(
            "akka.persistence.journal.plugin = "
                + "\"akka.persistence.journal.leveldb-shared\""));
  }

  @Override
  public CapabilityFlag supportsRejectingNonSerializableObjects() {
    return CapabilityFlag.off();
  }
}




Please note that some of the tests are optional, and by overriding the 
supports...
 methods you give the TCK the needed information about which tests to run. You can implement these methods using 
boolean values or
 the provided 
CapabilityFlag.on
 / 
CapabilityFlag.off
 values.


We also provide a simple benchmarking class 
JournalPerfSpec
JavaJournalPerfSpec
 which includes all the tests that 
JournalSpec
JavaJournalSpec
 has, and also performs some longer operations on the Journal while printing its performance stats. While it is NOT aimed to provide a proper benchmarking environment it can be used to get a rough feel about your journal’s performance in the most typical scenarios.


In order to include the 
SnapshotStore
 TCK tests in your test suite extend the 
SnapshotStoreSpec
:




Scala




copy
source
class MySnapshotStoreSpec
    extends SnapshotStoreSpec(
      config = ConfigFactory.parseString("""
    akka.persistence.snapshot-store.plugin = "my.snapshot-store.plugin"
    """)) {

  override def supportsSerialization: CapabilityFlag =
    true // or CapabilityFlag.on
}


Java




copy
source
@RunWith(JUnitRunner.class)
class MySnapshotStoreTest extends JavaSnapshotStoreSpec {

  public MySnapshotStoreTest() {
    super(
        ConfigFactory.parseString(
            "akka.persistence.snapshot-store.plugin = "
                + "\"akka.persistence.snapshot-store.local\""));
  }
}




In case your plugin requires some setting up (starting a mock database, removing temporary files etc.) you can override the 
beforeAll
 and 
afterAll
 methods to hook into the tests lifecycle:




Scala




copy
source
class MyJournalSpec
    extends JournalSpec(config = ConfigFactory.parseString("""
    akka.persistence.journal.plugin = "my.journal.plugin"
    """)) {

  override def supportsRejectingNonSerializableObjects: CapabilityFlag =
    true // or CapabilityFlag.on

  val storageLocations = List(
    new File(system.settings.config.getString("akka.persistence.journal.leveldb.dir")),
    new File(config.getString("akka.persistence.snapshot-store.local.dir")))

  override def beforeAll(): Unit = {
    super.beforeAll()
    storageLocations.foreach(FileUtils.deleteRecursively)
  }

  override def afterAll(): Unit = {
    storageLocations.foreach(FileUtils.deleteRecursively)
    super.afterAll()
  }

}


Java




copy
source
@RunWith(JUnitRunner.class)
class MyJournalSpecTest extends JavaJournalSpec {

  List<File> storageLocations = new ArrayList<File>();

  public MyJournalSpecTest() {
    super(
        ConfigFactory.parseString(
            "persistence.journal.plugin = "
                + "\"akka.persistence.journal.leveldb-shared\""));

    Config config = system().settings().config();
    storageLocations.add(
        new File(config.getString("akka.persistence.journal.leveldb.dir")));
    storageLocations.add(
        new File(config.getString("akka.persistence.snapshot-store.local.dir")));
  }

  @Override
  public CapabilityFlag supportsRejectingNonSerializableObjects() {
    return CapabilityFlag.on();
  }

  @Override
  public void beforeAll() {
    for (File storageLocation : storageLocations) {
      FileUtils.deleteRecursively(storageLocation);
    }
    super.beforeAll();
  }

  @Override
  public void afterAll() {
    super.afterAll();
    for (File storageLocation : storageLocations) {
      FileUtils.deleteRecursively(storageLocation);
    }
  }
}




We 
highly recommend
 including these specifications in your test suite, as they cover a broad range of cases you might have otherwise forgotten to test for when writing a plugin from scratch.


Corrupt event logs


If a journal can’t prevent users from running persistent actors with the same 
persistenceId
 concurrently it is likely that an event log will be corrupted by having events with the same sequence number.


It is recommended that journals should still deliver these events during recovery so that a 
replay-filter
 can be used to decide what to do about it in a journal agnostic way.














 
Persistence Plugins






Replicated Event Sourcing replication via direct access to replica databases 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/guide/
Akka Guide :: Akka Guide




























 














 


























Developers








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started


 












































Akka Guide














Tutorial: Microservices






Shopping Cart example architecture






Section 1: Set up your development environment






Section 2: Start a project






Section 3: Create the gRPC Cart service






Section 4: Create the Event Sourced Cart entity






Section 5: Complete Event Sourced entity






Section 6: Projection for queries






Section 7: Service-to-Service Eventing






Section 8: Projection calling gRPC service














Security






Implementing Zero Trust with Akka


















Lightbend Telemetry






Getting started with Telemetry






Using Telemetry with OpenTelemetry






Using Telemetry with Prometheus






AWS X-Ray tracing support


















How to …​






Use Cassandra instead of PostgreSQL






Use Kafka between two services






Migrate from CRUD to Event Sourcing






Scale command and query-side






Improve SBR’s decision process






Enable TLS for gRPC






Inspect the health


















Akka modules for Microservices






Actor model






Akka Streams






Akka gRPC






Akka Cluster






Akka Persistence






Akka Projections






Akka Management














What is Reactive?






Reactive (disambiguation)






Isolation






Autonomy






Mobility and Addressability






Event-Driven






State Ownership














Designing Reactive Microsystems






Event Sourcing






Command Query Responsibility Segregation (CQRS)






Eventual Consistency






Commands and Events






Memory Image Pattern






Circuit Breakers






Reactive Streams






Brief Summary of DDD concepts






Internal and External Communication






What makes up a service?






Deployment and Container Orchestration
























default
































Akka Guide












Akka Guide








What is this guide about?




This guide provides information to aid in understanding and using Akka.








What is Akka?




Akka is a toolkit for building highly concurrent systems that are scalable, highly efficient, and resilient by using 
Reactive Principles
. Akka allows you to focus on meeting business needs instead of writing low-level code to provide reliable behavior, fault tolerance, and high performance.






Akka provides:










Multi-threaded behavior
 â without the use of low-level concurrency constructs like atomics or locks; relieving you from even thinking about memory visibility issues.






Transparent remote communication between systems and their components
 â relieving you from writing and maintaining difficult networking code.






Clustered, high-availability, elastic architecture, that scales on demand
 â enabling you to deliver a truly reactive system.










Akkaâs use of the actor model provides a level of abstraction that makes it easier to write correct concurrent, parallel and distributed systems. The actor model spans the full set of Akka libraries, providing you with a consistent way of understanding and using them.








Who are the intended audience of this guide?




This guide is for developers, administrators, evaluators and dev/ops. For evaluators the guide will provide a solid demonstration the power and value available via Akka. For developers the guide focuses on getting you up and running by covering the basics of Microservices and Cloud deployment, as well as the fundamental concepts that underlay 
Reactive
 development. For administrators, and devops the guide lays out the requirements, operations and patterns of usage.






Whether you are new to 
Reactive principles
, or an experienced Akka user, you will find something of use in this guide. The guide complements the 
Akka reference documentation 
, which presents all Akka features in detail.








How to get the most out of this guide






For a code-first experience, have a look at the Implementing Microservices with Akka tutorial. The tutorial walks you through an example project and explains how features from the Akka ecosystem connect to build Reactive Systems. It will point you to the relevant concepts where applicable.






If this is your first experience with Reactive frameworks or Akka, we recommend reviewing the 
What is Reactive?
 section. This section presents some background and ideas behind Reactive. You can read about those concepts when introduced in the tutorial, or read them first if you prefer a more theoretical overview before diving into the code. There you will learn, for example:










The challenges that Reactive Architecture can help you overcome






A summary of Reactive Principles






The benefits of using Akka










If you are looking for a deeper dive into the conceptual framework it is also advisable to review 
Designing Reactive Microsystems
.






The 
Akka modules for Microservices
 section provides a conceptual overview for each of the Akka modules used with Microservices.










Akka Microservices tutorial






The 
Implementing Microservices with Akka tutorial
 illustrates how to implement an Event Sourced CQRS application with Akka Persistence and Akka Projections.






The tutorial shows how to build and run each service step by step:










Shopping Cart example architecture






Section 1: Set up your development environment






Section 2: Start a project






Section 3: Create the gRPC Cart service






Section 4: Create the Event Sourced Cart entity






Section 5: Complete Event Sourced entity






Section 6: Projection for queries






Section 7: Service-to-Service Eventing






Section 8: Projection calling gRPC service














Lightbend Telemetry






Lightbend Telemetry 
 can capture data for applications using 
Akka libraries and modules
. For those interested in adding telemetry to their deployments, we advise reviewing the 
Lightbend Telemetry
 section of this guide.










How to…​






Some development tasks arise from requirements on existing projects. The 
How to …​
 section contains guidance on introducing new features or architectural improvements to an existing codebase.






This documentation was last published: 2025-01-20












Tutorial: Microservices












































© 2011 - 
, Lightbend, Inc. All rights reserved. | 
Licenses
 | 
Terms
 | 
Privacy Policy
 | 
Cookie Listing
 | 
Cookie Settings
 | 
RSS

URL: https://doc.akka.io/libraries/akka/snapshot/persistence-query-leveldb.html
Persistence Query for LevelDB • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB




Dependency


Introduction


How to get the ReadJournal


Supported Queries


Configuration




Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB




Dependency


Introduction


How to get the ReadJournal


Supported Queries


Configuration




Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence Query for LevelDB


The LevelDB journal and query plugin is deprecated and it is not advised to build new applications with it. As a replacement we recommend using 
Akka Persistence JDBC
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Persistence Query, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-query" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-query_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-query_${versions.ScalaBinary}"
}


This will also add dependency on the 
akka-persistence
 module.


Introduction


This is documentation for the LevelDB implementation of the 
Persistence Query
 API. Note that implementations for other journals may have different semantics.


How to get the ReadJournal


The 
ReadJournal
 is retrieved via the 
akka.persistence.query.PersistenceQuery
 extension:




Scala




copy
source
import akka.persistence.query.PersistenceQuery
import akka.persistence.query.journal.leveldb.scaladsl.LeveldbReadJournal

val queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)


Java




copy
source
LeveldbReadJournal queries =
    PersistenceQuery.get(system)
        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());




Supported Queries


EventsByPersistenceIdQuery and CurrentEventsByPersistenceIdQuery


eventsByPersistenceId
 is used for retrieving events for a specific 
PersistentActor
 identified by 
persistenceId
.




Scala




copy
source
val queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)

val src: Source[EventEnvelope, NotUsed] =
  queries.eventsByPersistenceId("some-persistence-id", 0L, Long.MaxValue)

val events: Source[Any, NotUsed] = src.map(_.event)


Java




copy
source
LeveldbReadJournal queries =
    PersistenceQuery.get(system)
        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());

Source<EventEnvelope, NotUsed> source =
    queries.eventsByPersistenceId("some-persistence-id", 0, Long.MAX_VALUE);




You can retrieve a subset of all events by specifying 
fromSequenceNr
 and 
toSequenceNr
 or use 
0L
 and 
Long.MaxValue
Long.MAX_VALUE
 respectively to retrieve all events. Note that the corresponding sequence number of each event is provided in the 
EventEnvelope
, which makes it possible to resume the stream at a later point from a given sequence number.


The returned event stream is ordered by sequence number, i.e. the same order as the 
PersistentActor
 persisted the events. The same prefix of stream elements (in same order) are returned for multiple executions of the query, except for when events have been deleted.


The stream is not completed when it reaches the end of the currently stored events, but it continues to push new events when new events are persisted. Corresponding query that is completed when it reaches the end of the currently stored events is provided by 
currentEventsByPersistenceId
.


The LevelDB write journal is notifying the query side as soon as events are persisted, but for efficiency reasons the query side retrieves the events in batches that sometimes can be delayed up to the configured 
refresh-interval
 or given 
RefreshInterval
 hint.


The stream is completed with failure if there is a failure in executing the query in the backend journal.


PersistenceIdsQuery and CurrentPersistenceIdsQuery


persistenceIds
 is used for retrieving all 
persistenceIds
 of all persistent actors.




Scala




copy
source
val queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)

val src: Source[String, NotUsed] = queries.persistenceIds()


Java




copy
source
LeveldbReadJournal queries =
    PersistenceQuery.get(system)
        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());

Source<String, NotUsed> source = queries.persistenceIds();




The returned event stream is unordered and you can expect different order for multiple executions of the query.


The stream is not completed when it reaches the end of the currently used 
persistenceIds
, but it continues to push new 
persistenceIds
 when new persistent actors are created. Corresponding query that is completed when it reaches the end of the currently used 
persistenceIds
 is provided by 
currentPersistenceIds
.


The LevelDB write journal is notifying the query side as soon as new 
persistenceIds
 are created and there is no periodic polling or batching involved in this query.


The stream is completed with failure if there is a failure in executing the query in the backend journal.


EventsByTag and CurrentEventsByTag


eventsByTag
 is used for retrieving events that were marked with a given tag, e.g. all domain events of an Aggregate Root type.




Scala




copy
source
val queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)

val src: Source[EventEnvelope, NotUsed] =
  queries.eventsByTag(tag = "green", offset = Sequence(0L))


Java




copy
source
LeveldbReadJournal queries =
    PersistenceQuery.get(system)
        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());

Source<EventEnvelope, NotUsed> source = queries.eventsByTag("green", new Sequence(0L));




To tag events you create an 
Event Adapters
 that wraps the events in a 
akka.persistence.journal.Tagged
 with the given 
tags
.




Scala




copy
source
import akka.persistence.journal.WriteEventAdapter
import akka.persistence.journal.Tagged

class MyTaggingEventAdapter extends WriteEventAdapter {
  val colors = Set("green", "black", "blue")
  override def toJournal(event: Any): Any = event match {
    case s: String =>
      val tags = colors.foldLeft(Set.empty[String]) { (acc, c) =>
        if (s.contains(c)) acc + c else acc
      }
      if (tags.isEmpty) event
      else Tagged(event, tags)
    case _ => event
  }

  override def manifest(event: Any): String = ""
}


Java




copy
source
static class MyTaggingEventAdapter implements WriteEventAdapter {

  @Override
  public Object toJournal(Object event) {
    if (event instanceof String) {
      String s = (String) event;
      Set<String> tags = new HashSet<String>();
      if (s.contains("green")) tags.add("green");
      if (s.contains("black")) tags.add("black");
      if (s.contains("blue")) tags.add("blue");
      if (tags.isEmpty()) return event;
      else return new Tagged(event, tags);
    } else {
      return event;
    }
  }

  @Override
  public String manifest(Object event) {
    return "";
  }
}




You can use 
NoOffset
 to retrieve all events with a given tag or retrieve a subset of all events by specifying a 
Sequence
 
offset
. The 
offset
 corresponds to an ordered sequence number for the specific tag. Note that the corresponding offset of each event is provided in the 
EventEnvelope
, which makes it possible to resume the stream at a later point from a given offset.


The 
offset
 is exclusive, i.e. the event with the exact same sequence number will not be included in the returned stream. This means that you can use the offset that is returned in 
EventEnvelope
 as the 
offset
 parameter in a subsequent query.


In addition to the 
offset
 the 
EventEnvelope
 also provides 
persistenceId
 and 
sequenceNr
 for each event. The 
sequenceNr
 is the sequence number for the persistent actor with the 
persistenceId
 that persisted the event. The 
persistenceId
 + 
sequenceNr
 is an unique identifier for the event.


The returned event stream is ordered by the offset (tag sequence number), which corresponds to the same order as the write journal stored the events. The same stream elements (in same order) are returned for multiple executions of the query. Deleted events are not deleted from the tagged event stream.
Note


Events deleted using 
deleteMessages(toSequenceNr)
 are not deleted from the “tagged stream”.


The stream is not completed when it reaches the end of the currently stored events, but it continues to push new events when new events are persisted. Corresponding query that is completed when it reaches the end of the currently stored events is provided by 
currentEventsByTag
.


The LevelDB write journal is notifying the query side as soon as tagged events are persisted, but for efficiency reasons the query side retrieves the events in batches that sometimes can be delayed up to the configured 
refresh-interval
 or given 
RefreshInterval
 hint.


The stream is completed with failure if there is a failure in executing the query in the backend journal.


Configuration


Configuration settings can be defined in the configuration section with the absolute path corresponding to the identifier, which is 
"akka.persistence.query.journal.leveldb"
 for the default 
LeveldbReadJournal.Identifier
.


It can be configured with the following properties:


copy
source
# Configuration for the LeveldbReadJournal
akka.persistence.query.journal.leveldb {
  # Implementation class of the LevelDB ReadJournalProvider
  class = "akka.persistence.query.journal.leveldb.LeveldbReadJournalProvider"
  
  # Absolute path to the write journal plugin configuration entry that this 
  # query journal will connect to. That must be a LeveldbJournal or SharedLeveldbJournal.
  # If undefined (or "") it will connect to the default journal as specified by the
  # akka.persistence.journal.plugin property.
  write-plugin = ""
  
  # The LevelDB write journal is notifying the query side as soon as things
  # are persisted, but for efficiency reasons the query side retrieves the events 
  # in batches that sometimes can be delayed up to the configured `refresh-interval`.
  refresh-interval = 3s
  
  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = 100
}














 
Persistence Query






Persistence Plugins 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/index.html#security-related-documentation
Security Announcements • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Security Announcements
Note


Security announcements has moved to a shared page for all Akka projects and can now be found at 
akka.io/security


Receiving Security Advisories


The best way to receive any and all security announcements is to subscribe to the 
Akka security list
.


The mailing list is very low traffic, and receives notifications only after security reports have been managed by the core team and fixes are publicly available.


Reporting Vulnerabilities


We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.


Following best practice, we strongly encourage anyone to report potential security vulnerabilities to 
[email protected]
 before disclosing them in a public forum like the mailing list or as a GitHub issue.


Reports to this email address will be handled by our security team, who will work together with you to ensure that a fix can be provided without delay.


Security Related Documentation




Java Serialization


Remote deployment allow list


Remote Security




Fixed Security Vulnerabilities






Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


















 
Akka Documentation






Java Serialization, Fixed in Akka 2.4.17 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-sharded-daemon-process.html
Sharded Daemon Process • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process




Module info


Introduction


Basic example


Addressing the actors


Dynamic scaling of number of workers


Scalability


Configuration




Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process




Module info


Introduction


Basic example


Addressing the actors


Dynamic scaling of number of workers


Scalability


Configuration




Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Sharded Daemon Process


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Sharded Daemon Process, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-sharding-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-sharding-typed_${versions.ScalaBinary}"
}




Project Info: Akka Cluster Sharding (typed)


Artifact
com.typesafe.akka


akka-cluster-sharding-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster.sharding.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


Sharded Daemon Process provides a way to run 
N
 actors, each given a numeric id starting from 
0
 that are then kept alive and balanced across the cluster. When a rebalance is needed the actor is stopped and, triggered by a keep alive from a Cluster Singleton (the keep alive should be seen as an implementation detail and may change in future versions).


The intended use case is for splitting data processing workloads across a set number of workers that each get to work on a subset of the data that needs to be processed. This is commonly needed to create projections based on the event streams available from all the 
EventSourcedBehaviors
 in a CQRS application. Events are tagged with one out of 
N
 tags used to split the workload of consuming and updating a projection between 
N
 workers.


For cases where a single actor needs to be kept alive see 
Cluster Singleton


Basic example


To set up a set of actors running with Sharded Daemon process each node in the cluster needs to run the same initialization when starting up:




Scala




copy
source
val tags = Vector("tag-1", "tag-2", "tag-3")
ShardedDaemonProcess(system).init("TagProcessors", tags.size, id => TagProcessor(tags(id)))


Java




copy
source
List<String> tags = Arrays.asList("tag-1", "tag-2", "tag-3");
ShardedDaemonProcess.get(system)
    .init(
        TagProcessor.Command.class,
        "TagProcessors",
        tags.size(),
        id -> TagProcessor.create(tags.get(id)));




An additional factory method is provided for further configurability and providing a graceful stop message for the actor.


Addressing the actors


In use cases where you need to send messages to the daemon process actors it is recommended to use the 
system receptionist
 either with a single 
ServiceKey
 which all daemon process actors register themeselves to for broadcasts or individual keys if more fine grained messaging is needed.


Dynamic scaling of number of workers


Starting the sharded daemon process with 
initWithContext
 returns an 
ActorRef[ShardedDaemonProcessCommand]
 that accepts a 
ChangeNumberOfProcesses
ChangeNumberOfProcesses
 command to rescale the process to a new number of workers.


The rescaling process among other things includes the process actors stopping themselves in response to a stop message so may be a relatively slow operation. If a subsequent request to rescale is sent while one is in progress it is responded to with a failure response.


A rolling upgrade switching from a static number of workers to a dynamic number is possible. It is not safe to do a rolling upgrade from dynamic number of workers to static without a full cluster shutdown.


Colocate processes


When using the default shard allocation strategy the processes for different names are allocated independent of each other, i.e. the same process index for different process names may be allocated to different nodes. Colocating processes can be useful to share resources, such as Projections with 
EventsBySliceFirehoseQuery


To colocate such processes you can use the 
ConsistentHashingShardAllocationStrategy
ConsistentHashingShardAllocationStrategy
 as 
shardAllocationStrategy
 parameter of the 
init
 or 
initWithContext
 methods. 
Note


Create a new instance of the 
ConsistentHashingShardAllocationStrategy
 for each Sharded Daemon Process name, i.e. a 
ConsistentHashingShardAllocationStrategy
 instance must not be shared.


The shard identifier that is used by Sharded Daemon Process is the same as the process index, i.e. processes with the same index will be colocated.


The allocation strategy is using 
Consistent Hashing
 of the Cluster membership ring to assign a shard to a node. When adding or removing nodes it will rebalance according to the new consistent hashing, but that means that only a few shards will be rebalanced and others remain on the same location. When there are changes to the Cluster membership the shards may be on different nodes for a while, but eventually, when the membership is stable, the shards with the same identifier will end up on the same node.


Scalability


This cluster tool is intended for up to thousands of processes. Running with larger sets of processes might cause problems with Akka Distributed Data replication or process keepalive messages.


Configuration


The following configuration properties are read by the 
ShardedDaemonProcessSettings
ShardedDaemonProcessSettings
 when created with a 
ActorSystem
ActorSystem
 parameter:


copy
source
akka.cluster.sharded-daemon-process {
  # Settings for the sharded daemon process internal usage of sharding are using the akka.cluste.sharding defaults.
  # Some of the settings can be overridden specifically for the sharded daemon process here. For example can the
  # `role` setting limit what nodes the daemon processes and the keep alive pingers will run on.
  # Some settings can not be changed (remember-entities and related settings, passivation, number-of-shards),
  # overriding those settings will be ignored.
  sharding = ${akka.cluster.sharding}

  # Each entity is pinged at this interval from a few nodes in the
  # cluster to trigger a start if it has stopped, for example during
  # rebalancing.
  # See also keep-alive-from-number-of-nodes and keep-alive-throttle-interval
  # Note: How the set of actors is kept alive may change in the future meaning this setting may go away.
  keep-alive-interval = 10s

  # Keep alive messages from this number of nodes.
  keep-alive-from-number-of-nodes = 3

  # Keep alive messages are sent with this delay between each message.
  keep-alive-throttle-interval = 100 ms
}














 
Cluster Sharding concepts






Distributed Publish Subscribe in Cluster 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/
Build and run apps that react to change






















































































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



















































































        Build and run apps that react to change
      








Responsive by design, Akka apps are elastic, agile, and resilient.
Simple dev. Automated ops.










GET STARTED










































WHAT IS AKKA










































99.9999%




MULTI-REGION AVAILABILITY






99.9999% multi-region availability




only with Akka!












































Responsive by Design






Akka is relied upon when application responsiveness must be guaranteed. Engineering teams use a simple SDK and powerful libraries to build transactional, durable, and real-time services that distribute logic and data together. Operations are fully automated in serverless and BYOC environments, and a self-hosted option is available. 


























































What


is Akka






A platform to build and run apps that are elastic, agile, and resilient.












EXPLORE




















































Why


choose Akka






To guarantee responsiveness with apps that maintain responsibility for their own outcomes.












ADAPT CONTINUOUSLY




















































How


Akka works






By distributing logic and data together in movable apps that act as their own in-memory, durable database.












LOOK INSIDE
















































































It's not what we say...

























                  HPE redefines customer experiences processing petabytes of data
                










CASE STUDY

























































                  Verizon drives 235% sales growth and cuts its
hardware by half
                










CASE STUDY

























































                  Akka powers CERN’s NXCALS system for groundbreaking physics
                










CASE STUDY

























































                  Dream11 cuts cloud
costs by 30%
                










CASE STUDY

























































                  Tubi boots ad revenue with unique hyper-personalized experiences
                










CASE STUDY

























































                  Norwegian sets sail for growth with agility and stability boosts
                










CASE STUDY








































































































...it's the apps you build...






Use Akka to build transactional systems, AI inference, real-time data, edge and IoT, workflows, and globally distributed apps.  




























Transactional




Embed in-memory databases










KNOW MORE






























































Event-Sourced 




Track and replay state










KNOW MORE






























































AI




Inference for AI models










KNOW MORE






























































Digital Twin




Converge replicated data










KNOW MORE






























































Durable Execution




Orchestrate persisted processes










KNOW MORE






























































Analytics




Analyze real-time data points










KNOW MORE






























































Streaming




Process continuous data










KNOW MORE






























































Edge




Daisy chain device-to-cloud data










KNOW MORE






































































































...and it's guaranteed.




























The Akka Resilience Guarantee


Akka will indemnify against losses caused by an Akka App becoming unreliable.  Psst, it’s never happened.
















Akka IP Protections


Akka indemnifies our IP and dependencies when open source cannot.
















Akka Data Protections


Akka is audited annually against the AICPA SOC2 standard.
















Akka Cybersecurity Protections


We develop, maintain, and support Akka according to detailed cybersecurity processes, including NIST.










































Stay Responsive 
to Change.










GET STARTED










































REQUEST A DEMO








































































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/futures-interop.html
Futures interop • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop




Dependency


Overview




Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop




Dependency


Overview




Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Futures interop


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Overview


Stream transformations and side effects involving external non-stream based services can be performed with 
mapAsync
 or 
mapAsyncUnordered
.


For example, sending emails to the authors of selected tweets using an external email service:




Scala




copy
source
def send(email: Email): Future[Unit] = {
  // ...
}


Java




copy
source
public CompletionStage<Email> send(Email email) {
  // ...
}




We start with the tweet stream of authors:




Scala




copy
source
val authors: Source[Author, NotUsed] =
  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)


Java




copy
source
final Source<Author, NotUsed> authors =
    tweets.filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);





Assume that we can look up their email address using:




Scala




copy
source
def lookupEmail(handle: String): Future[Option[String]] =


Java




copy
source
public CompletionStage<Optional<String>> lookupEmail(String handle)




Transforming the stream of authors to a stream of email addresses by using the 
lookupEmail
 service can be done with 
mapAsync
:




Scala




copy
source
val emailAddresses: Source[String, NotUsed] =
  authors.mapAsync(4)(author => addressSystem.lookupEmail(author.handle)).collect {
    case Some(emailAddress) => emailAddress
  }


Java




copy
source
final Source<String, NotUsed> emailAddresses =
    authors
        .mapAsync(4, author -> addressSystem.lookupEmail(author.handle))
        .filter(o -> o.isPresent())
        .map(o -> o.get());





Finally, sending the emails:




Scala




copy
source
val sendEmails: RunnableGraph[NotUsed] =
  emailAddresses
    .mapAsync(4)(address => {
      emailServer.send(Email(to = address, title = "Akka", body = "I like your tweet"))
    })
    .to(Sink.ignore)

sendEmails.run()


Java




copy
source
final RunnableGraph<NotUsed> sendEmails =
    emailAddresses
        .mapAsync(
            4, address -> emailServer.send(new Email(address, "Akka", "I like your tweet")))
        .to(Sink.ignore());

sendEmails.run(system);




mapAsync
 is applying the given function that is calling out to the external service to each of the elements as they pass through this processing step. The function returns a 
Future
CompletionStage
 and the value of that future will be emitted downstream. The number of Futures that shall run in parallel is given as the first argument to 
mapAsync
. These Futures may complete in any order, but the elements that are emitted downstream are in the same order as received from upstream.


That means that back-pressure works as expected. For example if the 
emailServer.send
 is the bottleneck it will limit the rate at which incoming tweets are retrieved and email addresses looked up.


The final piece of this pipeline is to generate the demand that pulls the tweet authors information through the emailing pipeline: we attach a 
Sink.ignore
 which makes it all run. If our email process would return some interesting data for further transformation then we would not ignore it but send that result stream onwards for further processing or storage.


Note that 
mapAsync
 preserves the order of the stream elements. In this example the order is not important and then we can use the more efficient 
mapAsyncUnordered
:




Scala




copy
source
val authors: Source[Author, NotUsed] =
  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)

val emailAddresses: Source[String, NotUsed] =
  authors.mapAsyncUnordered(4)(author => addressSystem.lookupEmail(author.handle)).collect {
    case Some(emailAddress) => emailAddress
  }

val sendEmails: RunnableGraph[NotUsed] =
  emailAddresses
    .mapAsyncUnordered(4)(address => {
      emailServer.send(Email(to = address, title = "Akka", body = "I like your tweet"))
    })
    .to(Sink.ignore)

sendEmails.run()


Java




copy
source
final Source<Author, NotUsed> authors =
    tweets.filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);

final Source<String, NotUsed> emailAddresses =
    authors
        .mapAsyncUnordered(4, author -> addressSystem.lookupEmail(author.handle))
        .filter(o -> o.isPresent())
        .map(o -> o.get());

final RunnableGraph<NotUsed> sendEmails =
    emailAddresses
        .mapAsyncUnordered(
            4, address -> emailServer.send(new Email(address, "Akka", "I like your tweet")))
        .to(Sink.ignore());

sendEmails.run(system);




In the above example the services conveniently returned a 
Future
CompletionStage
 of the result. If that is not the case you need to wrap the call in a 
Future
CompletionStage
. If the service call involves blocking you must also make sure that you run it on a dedicated execution context, to avoid starvation and disturbance of other tasks in the system.




Scala




copy
source
val blockingExecutionContext = system.dispatchers.lookup("blocking-dispatcher")

val sendTextMessages: RunnableGraph[NotUsed] =
  phoneNumbers
    .mapAsync(4)(phoneNo => {
      Future {
        smsServer.send(TextMessage(to = phoneNo, body = "I like your tweet"))
      }(blockingExecutionContext)
    })
    .to(Sink.ignore)

sendTextMessages.run()


Java




copy
source
final Executor blockingEc = system.dispatchers().lookup("blocking-dispatcher");

final RunnableGraph<NotUsed> sendTextMessages =
    phoneNumbers
        .mapAsync(
            4,
            phoneNo ->
                CompletableFuture.supplyAsync(
                    () -> smsServer.send(new TextMessage(phoneNo, "I like your tweet")),
                    blockingEc))
        .to(Sink.ignore());

sendTextMessages.run(system);




The configuration of the 
"blocking-dispatcher"
 may look something like:


copy
source
blocking-dispatcher {
  executor = "thread-pool-executor"
  thread-pool-executor {
    core-pool-size-min    = 10
    core-pool-size-max    = 10
  }
}


An alternative for blocking calls is to perform them in a 
map
 operation, still using a dedicated dispatcher for that operation.




Scala




copy
source
val send = Flow[String]
  .map { phoneNo =>
    smsServer.send(TextMessage(to = phoneNo, body = "I like your tweet"))
  }
  .withAttributes(ActorAttributes.dispatcher("blocking-dispatcher"))
val sendTextMessages: RunnableGraph[NotUsed] =
  phoneNumbers.via(send).to(Sink.ignore)

sendTextMessages.run()


Java




copy
source
final Flow<String, Boolean, NotUsed> send =
    Flow.of(String.class)
        .map(phoneNo -> smsServer.send(new TextMessage(phoneNo, "I like your tweet")))
        .withAttributes(ActorAttributes.dispatcher("blocking-dispatcher"));
final RunnableGraph<?> sendTextMessages = phoneNumbers.via(send).to(Sink.ignore());

sendTextMessages.run(system);




However, that is not exactly the same as 
mapAsync
, since the 
mapAsync
 may run several calls concurrently, but 
map
 performs them one at a time.


For a service that is exposed as an actor, or if an actor is used as a gateway in front of an external service, you can use 
ask
:




Scala




copy
source
import akka.pattern.ask

val akkaTweets: Source[Tweet, NotUsed] = tweets.filter(_.hashtags.contains(akkaTag))

implicit val timeout: Timeout = 3.seconds
val saveTweets: RunnableGraph[NotUsed] =
  akkaTweets.mapAsync(4)(tweet => database ? Save(tweet)).to(Sink.ignore)


Java




copy
source
final Source<Tweet, NotUsed> akkaTweets = tweets.filter(t -> t.hashtags().contains(AKKA));

final RunnableGraph<NotUsed> saveTweets =
    akkaTweets
        .mapAsync(4, tweet -> ask(database, new Save(tweet), Duration.ofMillis(300L)))
        .to(Sink.ignore());




Note that if the 
ask
 is not completed within the given timeout the stream is completed with failure. If that is not desired outcome you can use 
recover
 on the 
ask
 
Future
CompletionStage
.


Illustrating ordering and parallelism


Let us look at another example to get a better understanding of the ordering and parallelism characteristics of 
mapAsync
 and 
mapAsyncUnordered
.


Several 
mapAsync
 and 
mapAsyncUnordered
 futures may run concurrently. The number of concurrent futures are limited by the downstream demand. For example, if 5 elements have been requested by downstream there will be at most 5 futures in progress.


mapAsync
 emits the future results in the same order as the input elements were received. That means that completed results are only emitted downstream when earlier results have been completed and emitted. One slow call will thereby delay the results of all successive calls, even though they are completed before the slow call.


mapAsyncUnordered
 emits the future results as soon as they are completed, i.e. it is possible that the elements are not emitted downstream in the same order as received from upstream. One slow call will thereby not delay the results of faster successive calls as long as there is downstream demand of several elements.


Here is a fictive service that we can use to illustrate these aspects.




Scala




copy
source
class SometimesSlowService(implicit ec: ExecutionContext) {

  private val runningCount = new AtomicInteger

  def convert(s: String): Future[String] = {
    println(s"running: $s (${runningCount.incrementAndGet()})")
    Future {
      if (s.nonEmpty && s.head.isLower)
        Thread.sleep(500)
      else
        Thread.sleep(20)
      println(s"completed: $s (${runningCount.decrementAndGet()})")
      s.toUpperCase
    }
  }
}


Java




copy
source
static class SometimesSlowService {
  private final Executor ec;

  public SometimesSlowService(Executor ec) {
    this.ec = ec;
  }

  private final AtomicInteger runningCount = new AtomicInteger();

  public CompletionStage<String> convert(String s) {
    System.out.println("running: " + s + "(" + runningCount.incrementAndGet() + ")");
    return CompletableFuture.supplyAsync(
        () -> {
          if (!s.isEmpty() && Character.isLowerCase(s.charAt(0)))
            try {
              Thread.sleep(500);
            } catch (InterruptedException e) {
            }
          else
            try {
              Thread.sleep(20);
            } catch (InterruptedException e) {
            }
          System.out.println("completed: " + s + "(" + runningCount.decrementAndGet() + ")");
          return s.toUpperCase();
        },
        ec);
  }
}




Elements starting with a lower case character are simulated to take longer time to process.


Here is how we can use it with 
mapAsync
:




Scala




copy
source
implicit val blockingExecutionContext = system.dispatchers.lookup("blocking-dispatcher")
val service = new SometimesSlowService

Source(List("a", "B", "C", "D", "e", "F", "g", "H", "i", "J"))
  .map(elem => { println(s"before: $elem"); elem })
  .mapAsync(4)(service.convert)
  .to(Sink.foreach(elem => println(s"after: $elem")))
  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))
  .run()


Java




copy
source
final Executor blockingEc = system.dispatchers().lookup("blocking-dispatcher");
final SometimesSlowService service = new SometimesSlowService(blockingEc);

Source.from(Arrays.asList("a", "B", "C", "D", "e", "F", "g", "H", "i", "J"))
    .map(
        elem -> {
          System.out.println("before: " + elem);
          return elem;
        })
    .mapAsync(4, service::convert)
    .to(Sink.foreach(elem -> System.out.println("after: " + elem)))
    .withAttributes(Attributes.inputBuffer(4, 4))
    .run(system);




The output may look like this:


before: a
before: B
before: C
before: D
running: a (1)
running: B (2)
before: e
running: C (3)
before: F
running: D (4)
before: g
before: H
completed: C (3)
completed: B (2)
completed: D (1)
completed: a (0)
after: A
after: B
running: e (1)
after: C
after: D
running: F (2)
before: i
before: J
running: g (3)
running: H (4)
completed: H (2)
completed: F (3)
completed: e (1)
completed: g (0)
after: E
after: F
running: i (1)
after: G
after: H
running: J (2)
completed: J (1)
completed: i (0)
after: I
after: J



Note that 
after
 lines are in the same order as the 
before
 lines even though elements are 
completed
 in a different order. For example 
H
 is 
completed
 before 
g
, but still emitted afterwards.


The numbers in parentheses illustrate how many calls that are in progress at the same time. Here the downstream demand and thereby the number of concurrent calls are limited by the buffer size (4) set with an attribute.


Here is how we can use the same service with 
mapAsyncUnordered
:




Scala




copy
source
implicit val blockingExecutionContext = system.dispatchers.lookup("blocking-dispatcher")
val service = new SometimesSlowService

Source(List("a", "B", "C", "D", "e", "F", "g", "H", "i", "J"))
  .map(elem => { println(s"before: $elem"); elem })
  .mapAsyncUnordered(4)(service.convert)
  .to(Sink.foreach(elem => println(s"after: $elem")))
  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))
  .run()


Java




copy
source
final Executor blockingEc = system.dispatchers().lookup("blocking-dispatcher");
final SometimesSlowService service = new SometimesSlowService(blockingEc);

Source.from(Arrays.asList("a", "B", "C", "D", "e", "F", "g", "H", "i", "J"))
    .map(
        elem -> {
          System.out.println("before: " + elem);
          return elem;
        })
    .mapAsyncUnordered(4, service::convert)
    .to(Sink.foreach(elem -> System.out.println("after: " + elem)))
    .withAttributes(Attributes.inputBuffer(4, 4))
    .run(system);




The output may look like this:


before: a
before: B
before: C
before: D
running: a (1)
running: B (2)
before: e
running: C (3)
before: F
running: D (4)
before: g
before: H
completed: B (3)
completed: C (1)
completed: D (2)
after: B
after: D
running: e (2)
after: C
running: F (3)
before: i
before: J
completed: F (2)
after: F
running: g (3)
running: H (4)
completed: H (3)
after: H
completed: a (2)
after: A
running: i (3)
running: J (4)
completed: J (3)
after: J
completed: e (2)
after: E
completed: g (1)
after: G
completed: i (0)
after: I



Note that 
after
 lines are not in the same order as the 
before
 lines. For example 
H
 overtakes the slow 
G
.


The numbers in parentheses illustrate how many calls that are in progress at the same time. Here the downstream demand and thereby the number of concurrent calls are limited by the buffer size (4) set with an attribute.














 
Custom stream processing






Actors interop 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/from-classic.html
Learning Akka Typed from Classic • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Dependencies


Package names


Actor definition


actorOf and Props


ActorRef


ActorSystem


become


sender


parent


Supervision


Lifecycle hooks


watch


Stopping


ActorSelection


ask


pipeTo


ActorContext


ActorContext.children


Remote deployment


Routers


FSM


Timers


Stash


PersistentActor


Asynchronous Testing


Synchronous Testing






Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Dependencies


Package names


Actor definition


actorOf and Props


ActorRef


ActorSystem


become


sender


parent


Supervision


Lifecycle hooks


watch


Stopping


ActorSelection


ask


pipeTo


ActorContext


ActorContext.children


Remote deployment


Routers


FSM


Timers


Stash


PersistentActor


Asynchronous Testing


Synchronous Testing






Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Learning Akka Typed from Classic


Akka Classic is the original Actor APIs, which have been improved by more type safe and guided Actor APIs, known as Akka Typed.


If you already know the classic Actor APIs and would like to learn Akka Typed, this reference is a good resource. Many concepts are the same and this page tries to highlight differences and how to do certain things in Typed compared to classic.


You should probably learn some of the basics of Akka Typed to see how it looks like before diving into the differences and details described here. A good starting point for that is the 
IoT example
 in the Getting Started Guide or the examples shown in 
Introduction to Actors
.


Another good resource to learning Akka Typed is Manuel Bernhardt’s 
Tour of Akka Typed
.


Note that Akka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use Akka Typed together with classic actors within the same ActorSystem, see 
coexistence
. For new projects we recommend using the new Actor APIs.


Dependencies


The dependencies of the Typed modules are named by adding 
-typed
 suffix of the corresponding classic module, with a few exceptions.


For example 
akka-cluster-typed
:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}


Artifact names:








Classic 


Typed 










akka-actor 


akka-actor-typed 






akka-cluster 


akka-cluster-typed 






akka-cluster-sharding 


akka-cluster-sharding-typed 






akka-cluster-tools 


akka-cluster-typed 






akka-distributed-data 


akka-cluster-typed 






akka-persistence 


akka-persistence-typed 






akka-stream 


akka-stream-typed 






akka-testkit 


akka-actor-testkit-typed 








Cluster Singleton and Distributed Data are included in 
akka-cluster-typed
.


Artifacts not listed in above table don’t have a specific API for Akka Typed.


Package names


The convention of the package names in Akka Typed is to add 
typed.scaladsl
 and 
typed.javadsl
 to the corresponding Akka classic package name. 
scaladsl
 and 
javadsl
 is the convention to separate Scala and Java APIs, which is familiar from Akka Streams.


Examples of a few package names:








Classic 


Typed for Scala 


Typed for Java 










akka.actor 


akka.actor.typed.scaladsl 


akka.actor.typed.javadsl 






akka.cluster 


akka.cluster.typed 


akka.cluster.typed 






akka.cluster.sharding 


akka.cluster.sharding.typed.scaladsl 


akka.cluster.sharding.typed.javadsl 






akka.persistence 


akka.persistence.typed.scaladsl 


akka.persistence.typed.javadsl 








Actor definition


A classic actor is defined by a class extending 
akka.actor.Actor
akka.actor.AbstractActor
.


An actor in Typed is defined by a class extending 
akka.actor.typed.scaladsl.AbstractBehavior
akka.actor.typed.javadsl.AbstractBehavior
.


It’s also possible to define an actor in Typed from functions instead of extending a class. This is called the 
functional style
.


Classic HelloWorld actor:




Scala




copy
source
import akka.actor.Actor
import akka.actor.ActorLogging
import akka.actor.Props

object HelloWorld {
  final case class Greet(whom: String)
  final case class Greeted(whom: String)

  def props(): Props =
    Props(new HelloWorld)
}

class HelloWorld extends Actor with ActorLogging {
  import HelloWorld._

  override def receive: Receive = {
    case Greet(whom) =>
      log.info("Hello {}!", whom)
      sender() ! Greeted(whom)
  }
}


Java




copy
source
import akka.actor.AbstractActor;
import akka.actor.Props;
import akka.event.Logging;
import akka.event.LoggingAdapter;

public class HelloWorld extends AbstractActor {

  public static final class Greet {
    public final String whom;

    public Greet(String whom) {
      this.whom = whom;
    }
  }

  public static final class Greeted {
    public final String whom;

    public Greeted(String whom) {
      this.whom = whom;
    }
  }

  public static Props props() {
    return Props.create(HelloWorld.class, HelloWorld::new);
  }

  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);

  @Override
  public Receive createReceive() {
    return receiveBuilder().match(Greet.class, this::onGreet).build();
  }

  private void onGreet(Greet command) {
    log.info("Hello {}!", command.whom);
    getSender().tell(new Greeted(command.whom), getSelf());
  }
}




Typed HelloWorld actor:




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.AbstractBehavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object HelloWorld {
  final case class Greet(whom: String, replyTo: ActorRef[Greeted])
  final case class Greeted(whom: String, from: ActorRef[Greet])

  def apply(): Behavior[HelloWorld.Greet] =
    Behaviors.setup(context => new HelloWorld(context))
}

class HelloWorld(context: ActorContext[HelloWorld.Greet]) extends AbstractBehavior[HelloWorld.Greet](context) {
  import HelloWorld._

  override def onMessage(message: Greet): Behavior[Greet] = {
    context.log.info("Hello {}!", message.whom)
    message.replyTo ! Greeted(message.whom, context.self)
    this
  }
}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.util.HashMap;
import java.util.Map;

public class HelloWorld extends AbstractBehavior<HelloWorld.Greet> {

  public static final class Greet {
    public final String whom;
    public final ActorRef<Greeted> replyTo;

    public Greet(String whom, ActorRef<Greeted> replyTo) {
      this.whom = whom;
      this.replyTo = replyTo;
    }
  }

  public static final class Greeted {
    public final String whom;
    public final ActorRef<Greet> from;

    public Greeted(String whom, ActorRef<Greet> from) {
      this.whom = whom;
      this.from = from;
    }
  }

  public static Behavior<Greet> create() {
    return Behaviors.setup(HelloWorld::new);
  }

  private HelloWorld(ActorContext<Greet> context) {
    super(context);
  }

  @Override
  public Receive<Greet> createReceive() {
    return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build();
  }

  private Behavior<Greet> onGreet(Greet command) {
    getContext().getLog().info("Hello {}!", command.whom);
    command.replyTo.tell(new Greeted(command.whom, getContext().getSelf()));
    return this;
  }
}




Why is it called 
Behavior
 and not 
Actor
?


In Typed, the 
Behavior
 defines how to handle incoming messages. After processing a message, a different 
Behavior
 may be returned for processing the next message. This means that an actor is started with an initial 
Behavior
 and may change 
Behavior
 over its lifecycle. This is described more in the section about 
become
.


Note that the 
Behavior
 has a type parameter describing the type of messages that it can handle. This information is not defined explicitly for a classic actor.


Links to reference documentation:




Classic


Typed




actorOf and Props


A classic actor is started with the 
actorOf
 method of the 
ActorContext
 or 
ActorSystem
.


Corresponding method in Typed is called 
spawn
 in the 
akka.actor.typed.scaladsl.ActorContext
akka.actor.typed.javadsl.ActorContext
.


There is no 
spawn
 method in the 
akka.actor.typed.scaladsl.ActorSystem
akka.actor.typed.javadsl.ActorSystem
 for creating top level actors. Instead, there is a single top level actor defined by a user guardian 
Behavior
 that is given when starting the 
ActorSystem
. Other actors are started as children of that user guardian actor or children of other actors in the actor hierarchy. This is explained more in 
ActorSystem
.


Note that when mixing classic and typed and have a classic system, spawning top level actors from the side is possible, see 
Coexistence
.


The 
actorOf
 method takes an 
akka.actor.Props
 parameter, which is like a factory for creating the actor instance, and it’s also used when creating a new instance when the actor is restarted. The 
Props
 may also define additional properties such as which dispatcher to use for the actor.


In typed, the 
spawn
 method creates an actor directly from a given 
Behavior
 without using a 
Props
 factory. It does however accept an optional 
akka.actor.typed.Props
 for specifying Actor metadata. The factory aspect is instead defined via 
Behaviors.setup
 when using the object-oriented style with a class extending 
AbstractBehavior
. For the function style there is typically no need for the factory.


Additional properties such as which dispatcher to use for the actor can still be given via an optional 
akka.actor.typed.Props
 parameter of the 
spawn
 method.


The 
name
 parameter of 
actorOf
 is optional and if not defined the actor will have a generated name. Corresponding in Typed is achieved with the 
spawnAnonymous
 method.


Links to reference documentation:




Classic


Typed




ActorRef


akka.actor.ActorRef
 has its correspondence in 
akka.actor.typed.ActorRef
. The difference being that the latter has a type parameter describing which messages the actor can handle. This information is not defined for a classic actor and you can send any type of message to a classic 
ActorRef
 even though the actor may not understand it.


ActorSystem


akka.actor.ActorSystem
 has its correspondence in 
akka.actor.typed.ActorSystem
. One difference is that when creating an 
ActorSystem
 in Typed you give it a 
Behavior
 that will be used as the top level actor, also known as the user guardian.


Additional actors for an application are created from the user guardian alongside performing the initialization of Akka components such as Cluster Sharding. In contrast, in a classic 
ActorSystem
, such initialization is typically performed from the “outside”.


The 
actorOf
 method of the classic 
ActorSystem
 is typically used to create a few (or many) top level actors. The 
ActorSystem
 in Typed doesn’t have that capability. Instead, such actors are started as children of the user guardian actor or children of other actors in the actor hierarchy. The rationale for this is partly about consistency. In a typed system you canât create children to an arbitrary actor from anywhere in your app without messaging it, so this will also hold true for the user guardian actor. That noted, in cases where you do need to spawn outside of this guardian then you can use the 
SpawnProtocol
 to spawn as needed.


become


A classic actor can change its message processing behavior by using 
become
 in 
ActorContext
. In Typed this is done by returning a new 
Behavior
 after processing a message. The returned 
Behavior
 will be used for the next received message.


There is no correspondence to 
unbecome
 in Typed. Instead you must explicitly keep track of and return the “previous” 
Behavior
.


Links to reference documentation:




Classic




sender


There is no 
sender()
getSender()
 in Typed. Instead you have to explicitly include an 
ActorRef
 representing the senderâor rather representing where to send a reply toâin the messages.


The reason for not having an implicit sender in Typed is that it wouldn’t be possible to know the type for the sender 
ActorRef[T]
ActorRef<T>
 at compile time. It’s also much better to define this explicitly in the messages as it becomes more clear what the message protocol expects.


Links to reference documentation:




Classic


Typed




parent


There is no 
parent
getParent
 in Typed. Instead you have to explicitly include the 
ActorRef
 of the parent as a parameter when constructing the 
Behavior
.


The reason for not having a parent in Typed is that it wouldn’t be possible to know the type for the parent 
ActorRef[T]
ActorRef<T>
 at compile time without having an additional type parameter in the 
Behavior
. For testing purposes it’s also better to pass in the 
parent
 since it can be replaced by a probe or being stubbed out in tests.


Supervision


An important difference between classic and typed is that in typed, actors are stopped by default if an exception is thrown and no supervision strategy is defined. In contrast, in classic, by default, actors are restarted.


In classic actors the supervision strategy for child actors are defined by overriding the 
supervisorStrategy
 method in the parent actor.


In Typed the supervisor strategy is defined by wrapping the 
Behavior
 of the child actor with 
Behaviors.supervise
.


The classic 
BackoffSupervisor
 is supported by 
SupervisorStrategy.restartWithBackoff
 as an ordinary 
SupervisorStrategy
 in Typed.


SupervisorStrategy.Escalate
 isn’t supported in Typed, but similar can be achieved as described in 
Bubble failures up through the hierarchy
.


Links to reference documentation:




Classic


Typed




Lifecycle hooks


Classic actors have methods 
preStart
, 
preRestart
, 
postRestart
 and 
postStop
 that can be overridden to act on changes to the actor’s lifecycle.


This is supported with corresponding 
PreRestart
 and 
PostStop
 signal messages in Typed. There are no 
PreStart
 and 
PostRestart
 signals because such action can be done from 
Behaviors.setup
 or the constructor of the 
AbstractBehavior
 class.


Note that in classic, the 
postStop
 lifecycle hook is also called when the actor is restarted. That is not the case in Typed, only the 
PreRestart
 signal is emitted. If you need to do resource cleanup on both restart and stop, you have to do that for both 
PreRestart
 and 
PostStop
.


Links to reference documentation:




Classic


Typed




watch


watch
 and the 
Terminated
 message are pretty much the same, with some additional capabilities in Typed.


Terminated
 is a signal in Typed since it is a different type than the declared message type of the 
Behavior
.


The 
watchWith
 method of the 
ActorContext
 in Typed can be used to send a message instead of the 
Terminated
 signal.


When watching child actors it’s possible to see if the child terminated voluntarily or due to a failure via the 
ChildFailed
 signal, which is a subclass of 
Terminated
.


Links to reference documentation:




Classic


Typed




Stopping


Classic actors can be stopped with the 
stop
 method of 
ActorContext
 or 
ActorSystem
. In Typed an actor stops itself by returning 
Behaviors.stopped
. There is also a 
stop
 method in the 
ActorContext
 but it can only be used for stopping direct child actors and not any arbitrary actor.


PoisonPill
 is not supported in Typed. Instead, if you need to request an actor to stop you should define a message that the actor understands and let it return 
Behaviors.stopped
 when receiving that message.


Links to reference documentation:




Classic


Typed




ActorSelection


ActorSelection
 isn’t supported in Typed. Instead the 
Receptionist
 is supposed to be used for finding actors by a registered key.


ActorSelection
 can be used for sending messages to a path without having an 
ActorRef
 of the destination. Note that a 
Group Router
 can be used for that.


Links to reference documentation:




Classic


Typed




ask


The classic 
ask
 pattern returns a 
Future
CompletionStage
 for the response.


Corresponding 
ask
 exists in Typed and is good when the requester itself isn’t an actor. It is located in 
akka.actor.typed.scaladsl.AskPattern
akka.actor.typed.javadsl.AskPattern
.


When the requester is an actor it is better to use the 
ask
 method of the 
ActorContext
 in Typed. It has the advantage of not having to mix 
Future
CompletionStage
 callbacks that are running on different threads with actor code.


Links to reference documentation:




Classic


Typed




pipeTo


pipeTo
 is typically used together with 
ask
 in an actor. The 
ask
 method of the 
ActorContext
 in Typed removes the need for 
pipeTo
. However, for interactions with other APIs that return 
Future
CompletionStage
 it is still useful to send the result as a message to the actor. For this purpose there is a 
pipeToSelf
 method in the 
ActorContext
 in Typed.


ActorContext


The 
ActorContext
 is always 1:1 with instance of an actor, when an actor has been started it exists even if you do not access it in any behavior that the actor has during its lifetime. When the actor is stopped and is garbage collected, so is the actor context. Multiple nested setup blocks will just give access to the same actor context and is not a problem.


You can think of the 
ActorContext
 as parallel to what 
ActorRef
 is for the actor from the âoutsideâ, but from the âinsideâ. It gives access to operations that are associated with the actor instance, for example spawning children, emitting log entries etc. that should only ever be used by the current behavior of the actor.


The definition of an actor is a computational entity that in response to a message can:




send messages to other actors


create new actors


change its state


designate the behavior to be used for the next message it receives




In Akka this boils down to a running actor having a current behavior to use when receiving the next message, access to a way to spawn children and optional state.


In the classic API this is directly and always modelled as a class 
Actor
AbstractActor
. While this at first glance seems simple the running actor is in fact more of a pair of the actor class instance and the actor context (which for example contains the current behavior 
Receive
 and 
self
).


In the new APIs this can be modelled both in the same way using a class based 
AbstractBehavior
 which the actor keeps for its entire life with state modelled as mutable fields, but also with a more FP:ish-style where the behavior and state is separated and the actor often returns a new behavior and state pair in response to a message. The running actor is in this case also essentially a pair of the actor context and the current behavior.


If you look at the current implementation of the new APIs you can see that it is in fact built on top of the classic APIs, spawning a typed actor always spawns a classic actor under the hood.


ActorContext.children


The 
ActorContext
 has methods 
children
 and 
child
getChildren
 and 
getChild
 to retrieve the 
ActorRef
 of started child actors in both Typed and Classic.


The type of the returned 
ActorRef
 is unknown, since different types can be used for different children. Therefore, this is not a useful way to lookup children when the purpose is to send messages to them.


Instead of finding children via the 
ActorContext
, it is recommended to use an application specific collection for bookkeeping of children, such as a 
Map[String, ActorRef[Child.Command]]
 
Map<String, ActorRef<Child.Command>>
. It can look like this:




Scala




copy
source
object Parent {
  sealed trait Command
  case class DelegateToChild(name: String, message: Child.Command) extends Command
  private case class ChildTerminated(name: String) extends Command

  def apply(): Behavior[Command] = {
    def updated(children: Map[String, ActorRef[Child.Command]]): Behavior[Command] = {
      Behaviors.receive { (context, command) =>
        command match {
          case DelegateToChild(name, childCommand) =>
            children.get(name) match {
              case Some(ref) =>
                ref ! childCommand
                Behaviors.same
              case None =>
                val ref = context.spawn(Child(), name)
                context.watchWith(ref, ChildTerminated(name))
                ref ! childCommand
                updated(children + (name -> ref))
            }

          case ChildTerminated(name) =>
            updated(children - name)
        }
      }
    }

    updated(Map.empty)
  }
}


Java




copy
source
public class Parent extends AbstractBehavior<Parent.Command> {

  public interface Command {}

  public static class DelegateToChild implements Command {
    public final String name;
    public final Child.Command message;

    public DelegateToChild(String name, Child.Command message) {
      this.name = name;
      this.message = message;
    }
  }

  private static class ChildTerminated implements Command {
    final String name;

    ChildTerminated(String name) {
      this.name = name;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(Parent::new);
  }

  private Map<String, ActorRef<Child.Command>> children = new HashMap<>();

  private Parent(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(DelegateToChild.class, this::onDelegateToChild)
        .onMessage(ChildTerminated.class, this::onChildTerminated)
        .build();
  }

  private Behavior<Command> onDelegateToChild(DelegateToChild command) {
    ActorRef<Child.Command> ref = children.get(command.name);
    if (ref == null) {
      ref = getContext().spawn(Child.create(), command.name);
      getContext().watchWith(ref, new ChildTerminated(command.name));
      children.put(command.name, ref);
    }
    ref.tell(command.message);
    return this;
  }

  private Behavior<Command> onChildTerminated(ChildTerminated command) {
    children.remove(command.name);
    return this;
  }
}




Remember to remove entries from the 
Map
 when the children are terminated. For that purpose it’s convenient to use 
watchWith
, as illustrated in the example above, because then you can include the key to the 
Map
 in the termination message. In that way the name of the actor doesn’t have to be the same as identifier used for bookkeeping.


Retrieving the children from the 
ActorContext
 can still be useful for some use cases, such as:




see if a child name is in use


stopping children


the type of the child is well known and 
unsafeUpcast
 of the 
ActorRef
 is considered “safe enough”




Remote deployment


Starting an actor on a remote nodeâso called remote deploymentâisn’t supported in Typed.


This feature would be discouraged because it often results in tight coupling between nodes and undesirable failure handling. For example if the node of the parent actor crashes, all remote deployed child actors are brought down with it. Sometimes that can be desired but many times it is used without realizing. This can be achieved by other means, such as using 
watch
.


Routers


Routers are provided in Typed, but in a much simplified form compared to the classic routers.


Destinations of group routers are registered in the 
Receptionist
, which makes them Cluster aware and also more dynamic than classic group routers.


Pool routers are only for local actor destinations in Typed, since 
remote deployment isn’t supported
.


Links to reference documentation:




Classic


Typed




FSM


With classic actors there is explicit support for building Finite State Machines. No support is needed in Akka Typed as it is straightforward to represent FSMs with behaviors.


Links to reference documentation:




Classic


Typed




Timers


In classic actors you 
mixin 
with Timers
extend AbstractActorWithTimers
 to gain access to delayed and periodic scheduling of messages. In Typed you have access to similar capabilities via 
Behaviors.withTimers
.


Links to reference documentation:




Classic


Typed




Stash


In classic actors you 
mixin 
with Stash
extend AbstractActorWithStash
 to gain access to stashing of messages. In Typed you have access to similar capabilities via 
Behaviors.withStash
.


Links to reference documentation:




Classic


Typed




PersistentActor


The correspondence of the classic 
PersistentActor
 is 
akka.persistence.typed.scaladsl.EventSourcedBehavior
akka.persistence.typed.javadsl.EventSourcedBehavior
.


The Typed API is much more guided to facilitate Event Sourcing best practices. It also has tighter integration with Cluster Sharding.


Links to reference documentation:




Classic


Typed




Asynchronous Testing


The Test Kits for asynchronous testing are rather similar.


Links to reference documentation:




Classic


Typed




Synchronous Testing


Classic and typed have different Test Kits for synchronous testing.


Behaviors in Typed can be tested in isolation without having to be packaged into an actor. As a consequence, tests can run fully synchronously without having to worry about timeouts and spurious failures.


The 
BehaviorTestKit
 provides a nice way of unit testing a 
Behavior
 in a deterministic way, but it has some limitations to be aware of. Similar limitations exists for synchronous testing of classic actors.


Links to reference documentation:




Classic


Typed
















 
Style guide






Cluster 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/may-change.html
Modules marked May Change • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Modules marked “May Change”


To be able to introduce new modules and APIs without freezing them the moment they are released because of our 
binary compatibility guarantees
 we have introduced the term 
may change
.


Concretely 
may change
 means that an API or module is in early access mode and that it:




is not covered by Lightbend’s commercial support (unless specifically stated otherwise)


is not guaranteed to be binary compatible in minor releases


may have its API change in breaking ways in minor releases


may be entirely dropped from Akka in a minor release




Complete modules can be marked as 
may change
, which will be stated in the module’s description and in the docs.


Individual public APIs can be annotated with 
ApiMayChange
 to signal that it has less guarantees than the rest of the module it lives in. For example, when while introducing “new” Java 8 APIs into existing stable modules, these APIs may be marked with this annotation to signal that they are not frozen yet. Please use such methods and classes with care, however if you see such APIs that is the best point in time to try them out and provide feedback (e.g. using the pull request comments or issues) before they are frozen as fully stable API.


Best effort migration guides may be provided, but this is decided on a case-by-case basis for 
may change
 modules.


The purpose of this is to be able to release features early and make them easily available and improve based on feedback, or even discover that the module or API wasn’t useful.


These are the current complete modules marked as 
may change
:




Multi Node Testing


Reliable Delivery
















 
Downstream upgrade strategy






IDE Tips 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/issue-tracking.html
Issue Tracking • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking




Browsing


Creating tickets


Submitting Pull Requests




Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking




Browsing


Creating tickets


Submitting Pull Requests




Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Issue Tracking


Akka is using GitHub Issues as its issue tracking system.


Browsing


Tickets


Before filing a ticket, please check the existing 
Akka tickets
 for earlier reports of the same problem. You are very welcome to comment on existing tickets, especially if you have reproducible test cases that you can share.


Creating tickets


Please include the versions of Scala and Akka and relevant configuration files.


You can create a 
new ticket
 if you have registered a GitHub user account.


Thanks a lot for reporting bugs and suggesting features!


Submitting Pull Requests
Note


A pull request is worth a thousand +1’s.
 – Old Klangian Proverb


Pull Requests fixing issues or adding functionality are very welcome. Please read 
CONTRIBUTING.md
 for more information about contributing to Akka.














 
Rolling Updates and Versions






Licenses 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/index.html
General Concepts • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















General Concepts






Terminology, Concepts




Concurrency vs. Parallelism


Asynchronous vs. Synchronous


Non-blocking vs. Blocking


Deadlock vs. Starvation vs. Live-lock


Race Condition


Non-blocking Guarantees (Progress Conditions)


Recommended literature




Actor Systems




Hierarchical Structure


Configuration Container


Actor Best Practices


What you should not concern yourself with


Terminating ActorSystem




What is an Actor?




Actor Reference


State


Behavior


Mailbox


Child Actors


Supervisor Strategy


When an Actor Terminates




Supervision and Monitoring




What Supervision Means


The Top-Level actors


What Restarting Means


What Lifecycle Monitoring Means


Actors and exceptions




Actor References, Paths and Addresses




What is an Actor Reference


What is an Actor Path?


How are Actor References obtained?


Actor Reference and Path Equality


Reusing Actor Paths


What is the Address part used for?


Top-Level Scopes for Actor Paths




Location Transparency




Distributed by Default


Ways in which Transparency is Broken


Peer-to-Peer vs. Client-Server


Marking Points for Scaling Up with Routers




Akka and the Java Memory Model




The Java Memory Model


Actors and the Java Memory Model


Futures and the Java Memory Model


Actors and shared mutable state




Message Delivery Reliability




The General Rules


The Rules for In-JVM (Local) Message Sends


Higher-level abstractions


Dead Letters




Configuration




Where configuration is read from


License key


When using JarJar, OneJar, Assembly or any jar-bundler


Custom application.conf


Including files


Logging of Configuration


A Word About ClassLoaders


Application specific settings


Configuring multiple ActorSystem


Reading configuration from a custom location


Listing of the Reference Configuration




Default configuration




akka-actor


akka-actor-typed


akka-cluster-typed


akka-cluster


akka-discovery


akka-coordination


akka-multi-node-testkit


akka-persistence-typed


akka-persistence


akka-persistence-query


akka-persistence-testkit


akka-remote artery


akka-testkit


akka-cluster-metrics


akka-cluster-tools


akka-cluster-sharding-typed


akka-cluster-sharding


akka-distributed-data


akka-stream


akka-stream-testkit




















 
Part 5: Querying Device Groups






Terminology, Concepts 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#migrating-from-akka-management-discovery-before-1-0-0-
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-projection/current/
Akka Projection







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Projection





Version 1.6.8





Java
Scala










Overview


Use Cases


Getting Started Guide


Events from Akka Persistence


Changes from Durable State


Messages from and to Kafka


Offset in a relational DB with R2DBC


Offset in Cassandra


Offset in a relational DB with JDBC


Offset in a relational DB with Slick


Offset in DynamoDB


Running a Projection


Processing with Actor


Processing with Akka Streams


Error handling


Projection Settings


Akka Projection gRPC


Akka Projection gRPC with producer push


Akka Replicated Event Sourcing over gRPC


Management of a Projection


Testing


Akka Classic


Snapshots


















Akka Projection





Version 1.6.8





Java
Scala












Overview


Use Cases


Getting Started Guide


Events from Akka Persistence


Changes from Durable State


Messages from and to Kafka


Offset in a relational DB with R2DBC


Offset in Cassandra


Offset in a relational DB with JDBC


Offset in a relational DB with Slick


Offset in DynamoDB


Running a Projection


Processing with Actor


Processing with Akka Streams


Error handling


Projection Settings


Akka Projection gRPC


Akka Projection gRPC with producer push


Akka Replicated Event Sourcing over gRPC


Management of a Projection


Testing


Akka Classic


Snapshots




















Akka Projection


.






Overview




Dependencies


Contributing




Use Cases




Command Query Responsibility Segregation (CQRS)


Service to service communication




Getting Started Guide




Video Introduction


Setup your application


Choosing a Source Provider


Build a Projection handler


Writing tests for a Projection


Running the Projection


Running the Projection in Akka Cluster




Events from Akka Persistence




Dependencies


SourceProvider for eventsByTag


SourceProvider for eventsBySlices


SourceProvider for eventsBySlicesStartingFromSnapshots


Many Projections




Changes from Durable State




Dependencies


SourceProvider for changesByTag


SourceProvider for changesBySlices


SourceProvider for eventsBySlices




Messages from and to Kafka




Dependencies


KafkaSourceProvider


Committing offset outside Kafka


Committing offset in Kafka


Sending to Kafka


Mergeable Offset


Configuration




Offset in a relational DB with R2DBC




Dependencies


Schema


Configuration


Running with Sharded Daemon Process


Slices


exactly-once


at-least-once


groupedWithin


Handler


Offset types


Publish events for lower latency


Multiple plugins


Custom Connection Factory




Offset in Cassandra




Dependencies


at-least-once


at-most-once


groupedWithin


Handler


Schema


Offset types


Configuration




Offset in a relational DB with JDBC




Dependencies


Required configuration settings


Defining a JdbcSession


Blocking JDBC Dispatcher


exactly-once


at-least-once


groupedWithin


Handler


Schema


Offset types


Configuration




Offset in a relational DB with Slick




Dependencies


exactly-once


at-least-once


groupedWithin


Handler


Schema


Offset types


Configuration




Offset in DynamoDB




Dependencies


Tables


Configuration


Running with Sharded Daemon Process


Slices


exactly-once


at-least-once


exactly-once (grouped)


at-least-once (grouped)


Handler


Publish events for lower latency


Multiple plugins


Time to Live (TTL)




Running a Projection




Dependencies


Running with Sharded Daemon Process


Running with local Actor


Running in Cluster Singleton




Processing with Actor


Processing with Akka Streams


Error handling




Handler recovery


Projection restart




Projection Settings




Configuration




Akka Projection gRPC




Overview


Dependencies


Consumer


Producer


Filters


Sample projects


Security


Access control


Performance considerations


Configuration




Akka Projection gRPC with producer push




Overview


Dependencies


Consumer set up


Producer set up




Akka Replicated Event Sourcing over gRPC




Overview


Dependencies


API and setup


Serialization of events


Filters


Sample projects


Security


Access control


Migrating from non-replicated




Management of a Projection




Offset management


Pause and resume


Status tracking




Testing




Dependencies


Initializing the Projection TestKit


Testing with an assert function


Testing with a TestSink probe


Testing with mocked Projection and SourceProvider




Akka Classic




Actor System


PersistentActor


Running




Snapshots




Configure repository


Documentation
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka Projections is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial_2.html
Part 2: Creating the First Actor • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor




Introduction


What’s next?




Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor




Introduction


What’s next?




Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Part 2: Creating the First Actor


Introduction


With an understanding of actor hierarchy and behavior, the remaining question is how to map the top-level components of our IoT system to actors. The 
user guardian
 can be an actor that represents the whole application. In other words, we will have a single top-level actor in our IoT system. The components that create and manage devices and dashboards will be children of this actor. This allows us to refactor the example use case architecture diagram into a tree of actors:




We can define the first actor, the IotSupervisor, with a few lines of code. To start your tutorial application:




Create a new 
IotSupervisor
 source file in the 
com.example
 package.


Paste the following code into the new file to define the IotSupervisor.






Scala




copy
source
package com.example

import akka.actor.typed.Behavior
import akka.actor.typed.PostStop
import akka.actor.typed.Signal
import akka.actor.typed.scaladsl.AbstractBehavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object IotSupervisor {
  def apply(): Behavior[Nothing] =
    Behaviors.setup[Nothing](context => new IotSupervisor(context))
}

class IotSupervisor(context: ActorContext[Nothing]) extends AbstractBehavior[Nothing](context) {
  context.log.info("IoT Application started")

  override def onMessage(msg: Nothing): Behavior[Nothing] = {
    // No need to handle any messages
    Behaviors.unhandled
  }

  override def onSignal: PartialFunction[Signal, Behavior[Nothing]] = {
    case PostStop =>
      context.log.info("IoT Application stopped")
      this
  }
}


Java




copy
source
package com.example;

import akka.actor.typed.Behavior;
import akka.actor.typed.PostStop;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;

public class IotSupervisor extends AbstractBehavior<Void> {

  public static Behavior<Void> create() {
    return Behaviors.setup(IotSupervisor::new);
  }

  private IotSupervisor(ActorContext<Void> context) {
    super(context);
    context.getLog().info("IoT Application started");
  }

  // No need to handle any messages
  @Override
  public Receive<Void> createReceive() {
    return newReceiveBuilder().onSignal(PostStop.class, signal -> onPostStop()).build();
  }

  private IotSupervisor onPostStop() {
    getContext().getLog().info("IoT Application stopped");
    return this;
  }
}




The code is similar to the actor examples we used in the previous experiments, but notice that instead of 
println()
 we use Akka’s built in logging facility via 
context.log
context.getLog()
.


To provide the 
main
 entry point that creates the actor system, add the following code to the new 
IotApp
 object
 
IotMain
 class
.




Scala




copy
source
package com.example

import akka.actor.typed.ActorSystem

object IotApp {

  def main(args: Array[String]): Unit = {
    // Create ActorSystem and top level supervisor
    ActorSystem[Nothing](IotSupervisor(), "iot-system")
  }

}


Java




copy
source
package com.example;

import akka.actor.typed.ActorSystem;

public class IotMain {

  public static void main(String[] args) {
    // Create ActorSystem and top level supervisor
    ActorSystem.create(IotSupervisor.create(), "iot-system");
  }
}




The application does little, other than log that it is started. But, we have the first actor in place and we are ready to add other actors.


What’s next?


In the following chapters we will grow the application gradually, by:




Creating the representation for a device.


Creating the device management component.


Adding query capabilities to device groups.
















 
Part 1: Actor Architecture






Part 3: Working with Device Actors 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/coordinated-shutdown.html
Coordinated Shutdown • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Coordinated Shutdown


Under normal conditions, when an 
ActorSystem
 is terminated or the JVM process is shut down, certain actors and services will be stopped in a specific order. 


The 
CoordinatedShutdown
CoordinatedShutdown
 extension registers internal and user-defined tasks to be executed during the shutdown process. The tasks are grouped in configuration-defined “phases” which define the shutdown order.


Especially the phases 
before-service-unbind
, 
before-cluster-shutdown
 and 
before-actor-system-terminate
 are intended for application specific phases or tasks.


The order of the shutdown phases is defined in configuration 
akka.coordinated-shutdown.phases
. See the default phases in the 
reference.conf
 tab:




Most relevant default phases










Phase 


Description 










before-service-unbind 


The first pre-defined phase during shutdown. 






before-cluster-shutdown 


Phase for custom application tasks that are to be run after service shutdown and before cluster shutdown. 






before-actor-system-terminate 


Phase for custom application tasks that are to be run after cluster shutdown and before 
ActorSystem
 termination. 








reference.conf (HOCON)
  




copy
source
# CoordinatedShutdown is enabled by default and will run the tasks that
# are added to these phases by individual Akka modules and user logic.
#
# The phases are ordered as a DAG by defining the dependencies between the phases
# to make sure shutdown tasks are run in the right order.
#
# In general user tasks belong in the first few phases, but there may be use
# cases where you would want to hook in new phases or register tasks later in
# the DAG.
#
# Each phase is defined as a named config section with the
# following optional properties:
# - timeout=15s: Override the default-phase-timeout for this phase.
# - recover=off: If the phase fails the shutdown is aborted
#                and depending phases will not be executed.
# - enabled=off: Skip all tasks registered in this phase. DO NOT use
#                this to disable phases unless you are absolutely sure what the
#                consequences are. Many of the built in tasks depend on other tasks
#                having been executed in earlier phases and may break if those are disabled.
# depends-on=[]: Run the phase after the given phases
phases {

  # The first pre-defined phase that applications can add tasks to.
  # Note that more phases can be added in the application's
  # configuration by overriding this phase with an additional
  # depends-on.
  before-service-unbind {
  }

  # Stop accepting new incoming connections.
  # This is where you can register tasks that makes a server stop accepting new connections. Already
  # established connections should be allowed to continue and complete if possible.
  service-unbind {
    depends-on = [before-service-unbind]
  }

  # Wait for requests that are in progress to be completed.
  # This is where you register tasks that will wait for already established connections to complete, potentially
  # also first telling them that it is time to close down.
  service-requests-done {
    depends-on = [service-unbind]
  }

  # Final shutdown of service endpoints.
  # This is where you would add tasks that forcefully kill connections that are still around.
  service-stop {
    depends-on = [service-requests-done]
  }

  # Phase for custom application tasks that are to be run
  # after service shutdown and before cluster shutdown.
  before-cluster-shutdown {
    depends-on = [service-stop]
  }

  # Graceful shutdown of the Cluster Sharding regions.
  # This phase is not meant for users to add tasks to.
  cluster-sharding-shutdown-region {
    timeout = 10 s
    depends-on = [before-cluster-shutdown]
  }

  # Emit the leave command for the node that is shutting down.
  # This phase is not meant for users to add tasks to.
  cluster-leave {
    depends-on = [cluster-sharding-shutdown-region]
  }

  # Shutdown cluster singletons
  # This is done as late as possible to allow the shard region shutdown triggered in
  # the "cluster-sharding-shutdown-region" phase to complete before the shard coordinator is shut down.
  # This phase is not meant for users to add tasks to.
  cluster-exiting {
    timeout = 10 s
    depends-on = [cluster-leave]
  }

  # Wait until exiting has been completed
  # This phase is not meant for users to add tasks to.
  cluster-exiting-done {
    depends-on = [cluster-exiting]
  }

  # Shutdown the cluster extension
  # This phase is not meant for users to add tasks to.
  cluster-shutdown {
    depends-on = [cluster-exiting-done]
  }

  # Phase for custom application tasks that are to be run
  # after cluster shutdown and before ActorSystem termination.
  before-actor-system-terminate {
    depends-on = [cluster-shutdown]
  }

  # Last phase. See terminate-actor-system and exit-jvm above.
  # Don't add phases that depends on this phase because the
  # dispatcher and scheduler of the ActorSystem have been shutdown.
  # This phase is not meant for users to add tasks to.
  actor-system-terminate {
    timeout = 10 s
    depends-on = [before-actor-system-terminate]
  }
}




More phases can be added in the application’s 
application.conf
 if needed by overriding a phase with an additional 
depends-on
.


The default phases are defined in a single linear order, but the phases can be ordered as a directed acyclic graph (DAG) by defining the dependencies between the phases. The phases are ordered with 
topological
 sort of the DAG.


Tasks can be added to a phase like in this example which allows a certain actor to react before termination starts:




Scala




copy
source
object MyActor {

  trait Messages
  case class Stop(replyTo: ActorRef[Done]) extends Messages

  def behavior: Behavior[Messages] =
    Behaviors.receiveMessage {
      // ...
      case Stop(replyTo) =>
        // shut down the actor internals
        // ..
        replyTo.tell(Done)
        Behaviors.stopped
    }
}

  CoordinatedShutdown(context.system).addTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, "someTaskName") { () =>
    implicit val timeout: Timeout = 5.seconds
    myActor.ask(MyActor.Stop(_))
  }


Java




copy
source
import static akka.actor.typed.javadsl.AskPattern.ask;

public static class MyActor extends AbstractBehavior<MyActor.Messages> {
  interface Messages {}

  // ...

  static final class Stop implements Messages {
    final ActorRef<Done> replyTo;

    Stop(ActorRef<Done> replyTo) {
      this.replyTo = replyTo;
    }
  }
  @Override
  public Receive<Messages> createReceive() {
    return newReceiveBuilder().onMessage(Stop.class, this::stop).build();
  }

  private Behavior<Messages> stop(Stop stop) {
    // shut down the actor internal
    // ...
    stop.replyTo.tell(Done.done());
    return Behaviors.stopped();
  }
}

          CoordinatedShutdown.get(system)
              .addTask(
                  CoordinatedShutdown.PhaseBeforeServiceUnbind(),
                  "someTaskName",
                  () ->
                      ask(myActor, MyActor.Stop::new, Duration.ofSeconds(5), system.scheduler()));




The returned 
Future[Done]
 
CompletionStage<Done>
 should be completed when the task is completed. The task name parameter is only used for debugging/logging.


Tasks added to the same phase are executed in parallel without any ordering assumptions. Next phase will not start until all tasks of previous phase have been completed.


If tasks are not completed within a configured timeout (see 
reference.conf
) the next phase will be started anyway. It is possible to configure 
recover=off
 for a phase to abort the rest of the shutdown process if a task fails or is not completed within the timeout.


If cancellation of previously added tasks is required:




Scala




copy
source
val c: Cancellable =
  CoordinatedShutdown(system).addCancellableTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, "cleanup") { () =>
    Future {
      cleanup()
      Done
    }
  }

// much later...
c.cancel()


Java




copy
source
Cancellable cancellable =
    CoordinatedShutdown.get(system)
        .addCancellableTask(
            CoordinatedShutdown.PhaseBeforeServiceUnbind(), "someTaskCleanup", () -> cleanup());
// much later...
cancellable.cancel();




In the above example, it may be more convenient to simply stop the actor when it’s done shutting down, rather than send back a done message, and for the shutdown task to not complete until the actor is terminated. A convenience method is provided that adds a task that sends a message to the actor and then watches its termination (there is currently no corresponding functionality for the new actors API 
see #29056
):




Scala




copy
source
CoordinatedShutdown(system).addActorTerminationTask(
  CoordinatedShutdown.PhaseBeforeServiceUnbind,
  "someTaskName",
  someActor,
  Some("stop"))


Java




copy
source
CoordinatedShutdown.get(system)
    .addActorTerminationTask(
        CoordinatedShutdown.PhaseBeforeServiceUnbind(),
        "someTaskName",
        someActor,
        Optional.of("stop"));




Tasks should typically be registered as early as possible after system startup. When running the coordinated shutdown tasks that have been registered will be performed but tasks that are added too late will not be run.


To start the coordinated shutdown process you can either invoke 
terminate()
 on the 
ActorSystem
, or 
run
 
runAll
 on the 
CoordinatedShutdown
 extension and pass it a class implementing 
CoordinatedShutdown.Reason
CoordinatedShutdown.Reason
 for informational purposes:




Scala




copy
source
// shut down with `ActorSystemTerminateReason`
system.terminate()

// or define a specific reason
case object UserInitiatedShutdown extends CoordinatedShutdown.Reason

val done: Future[Done] = CoordinatedShutdown(system).run(UserInitiatedShutdown)


Java




copy
source
// shut down with `ActorSystemTerminateReason`
system.terminate();

// or define a specific reason
class UserInitiatedShutdown implements CoordinatedShutdown.Reason {
  @Override
  public String toString() {
    return "UserInitiatedShutdown";
  }
}

CompletionStage<Done> done =
    CoordinatedShutdown.get(system).runAll(new UserInitiatedShutdown());




It’s safe to call the 
run
 
runAll
 method multiple times. It will only run once.


That also means that the 
ActorSystem
 will be terminated in the last phase. By default, the JVM is not forcefully stopped (it will be stopped if all non-daemon threads have been terminated). To enable a hard 
System.exit
 as a final action you can configure:


akka.coordinated-shutdown.exit-jvm = on



The coordinated shutdown process is also started once the actor system’s root actor is stopped.


When using 
Akka Cluster
 the 
CoordinatedShutdown
 will automatically run when the cluster node sees itself as 
Exiting
, i.e. leaving from another node will trigger the shutdown process on the leaving node. Tasks for graceful leaving of cluster including graceful shutdown of Cluster Singletons and Cluster Sharding are added automatically when Akka Cluster is used, i.e. running the shutdown process will also trigger the graceful leaving if it’s not already in progress.


By default, the 
CoordinatedShutdown
 will be run when the JVM process exits, e.g. via 
kill SIGTERM
 signal (
SIGINT
 ctrl-c doesn’t work). This behavior can be disabled with:


akka.coordinated-shutdown.run-by-jvm-shutdown-hook=off



Note that if running in Kubernetes, a SIGKILL will be issued after a set amount of time has passed since SIGTERM. By default this time is 30 seconds (
terminationGracePeriodSeconds
): it may be worth adjusting the Kubernetes configuration or the phase timeouts to make 
CoordinatedShutdown
 more likely to completely exectue before SIGKILL is received.


If you have application specific JVM shutdown hooks it’s recommended that you register them via the 
CoordinatedShutdown
 so that they are running before Akka internal shutdown hooks, e.g. those shutting down Akka Remoting (Artery).




Scala




copy
source
CoordinatedShutdown(system).addJvmShutdownHook {
  println("custom JVM shutdown hook...")
}


Java




copy
source
CoordinatedShutdown.get(system)
    .addJvmShutdownHook(() -> System.out.println("custom JVM shutdown hook..."));




For some tests it might be undesired to terminate the 
ActorSystem
 via 
CoordinatedShutdown
. You can disable that by adding the following to the configuration of the 
ActorSystem
 that is used in the test:


# Don't terminate ActorSystem via CoordinatedShutdown in tests
akka.coordinated-shutdown.terminate-actor-system = off
akka.coordinated-shutdown.run-by-actor-system-terminate = off
akka.coordinated-shutdown.run-by-jvm-shutdown-hook = off
akka.cluster.run-coordinated-shutdown-when-down = off















 
Behaviors as finite state machines






Dispatchers 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/interaction-patterns.html
Interaction Patterns • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns




Dependency


Introduction


Fire and Forget


Request-Response


Adapted Response


Request-Response with ask between two actors


Request-Response with ask from outside an Actor


Generic response wrapper


Ignoring replies


Send Future result to self


Per session child Actor


General purpose response aggregator


Latency tail chopping


Scheduling messages to self


Responding to a sharded actor




Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns




Dependency


Introduction


Fire and Forget


Request-Response


Adapted Response


Request-Response with ask between two actors


Request-Response with ask from outside an Actor


Generic response wrapper


Ignoring replies


Send Future result to self


Per session child Actor


General purpose response aggregator


Latency tail chopping


Scheduling messages to self


Responding to a sharded actor




Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Interaction Patterns


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Actors
.


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


Interacting with an Actor in Akka is done through an 
ActorRef[T]
ActorRef<T>
 where 
T
 is the type of messages the actor accepts, also known as the “protocol”. This ensures that only the right kind of messages can be sent to an actor and also that no one else but the Actor itself can access the Actor instance internals.


Message exchange with Actors follow a few common patterns, let’s go through each one of them. 


Fire and Forget


The fundamental way to interact with an actor is through 
“tell”, which is so common that it has a special symbolic method name: 
actorRef
 
!
 
message
actorRef.tell(message)
. Sending a message with tell can safely be done from any thread.


Tell is asynchronous which means that the method returns right away. After the statement is executed there is no guarantee that the message has been processed by the recipient yet. It also means there is no way to know if the message was received, the processing succeeded or failed.


Example:




With the given protocol and actor behavior:




Scala




copy
source
object Printer {

  case class PrintMe(message: String)

  def apply(): Behavior[PrintMe] =
    Behaviors.receive {
      case (context, PrintMe(message)) =>
        context.log.info(message)
        Behaviors.same
    }
}


Java




copy
source
public class Printer {
  public static class PrintMe {
    public final String message;

    public PrintMe(String message) {
      this.message = message;
    }
  }

  public static Behavior<PrintMe> create() {
    return Behaviors.setup(
        context ->
            Behaviors.receive(PrintMe.class)
                .onMessage(
                    PrintMe.class,
                    printMe -> {
                      context.getLog().info(printMe.message);
                      return Behaviors.same();
                    })
                .build());
  }
}




Fire and forget looks like this:




Scala




copy
source
val system = ActorSystem(Printer(), "fire-and-forget-sample")

// note how the system is also the top level actor ref
val printer: ActorRef[Printer.PrintMe] = system

// these are all fire and forget
printer ! Printer.PrintMe("message 1")
printer ! Printer.PrintMe("not message 2")


Java




copy
source
final ActorSystem<Printer.PrintMe> system =
    ActorSystem.create(Printer.create(), "printer-sample-system");

// note that system is also the ActorRef to the guardian actor
final ActorRef<Printer.PrintMe> ref = system;

// these are all fire and forget
ref.tell(new Printer.PrintMe("message 1"));
ref.tell(new Printer.PrintMe("message 2"));




Useful when:




It is not critical to be sure that the message was processed


There is no way to act on non successful delivery or processing


We want to minimize the number of messages created to get higher throughput (sending a response would require creating twice the number of messages)




Problems:




If the inflow of messages is higher than the actor can process the inbox will fill up and can in the worst case cause the JVM crash with an 
OutOfMemoryError


If the message gets lost, the sender will not know




Request-Response


Many interactions between actors require one or more response message being sent back from the receiving actor. A response message can be a result of a query, some form of acknowledgment that the message was received and processed or events that the request subscribed to.


In Akka the recipient of responses has to be encoded as a field in the message itself, which the recipient can then use to send (tell) a response back.


Example:




With the following protocol:




Scala




copy
source
case class Request(query: String, replyTo: ActorRef[Response])
case class Response(result: String)


Java




copy
source
public static class Request {
  public final String query;
  public final ActorRef<Response> replyTo;

  public Request(String query, ActorRef<Response> replyTo) {
    this.query = query;
    this.replyTo = replyTo;
  }
}

public static class Response {
  public final String result;

  public Response(String result) {
    this.result = result;
  }
}




The sender would use its own 
ActorRef[Response]
ActorRef<Response>
, which it can access through 
ActorContext.self
ActorContext.getSelf()
, for the 
replyTo
. 




Scala




copy
source
cookieFabric ! CookieFabric.Request("give me cookies", context.self)


Java




copy
source
cookieFabric.tell(new CookieFabric.Request("give me cookies", context.getSelf()));




On the receiving side the 
ActorRef[Response]
ActorRef<Response>
 can then be used to send one or more responses back:




Scala




copy
source
def apply(): Behaviors.Receive[Request] =
  Behaviors.receiveMessage[Request] {
    case Request(query, replyTo) =>
      // ... process query ...
      replyTo ! Response(s"Here are the cookies for [$query]!")
      Behaviors.same
  }


Java




copy
source
// actor behavior
public static Behavior<Request> create() {
  return Behaviors.receive(Request.class)
      .onMessage(Request.class, CookieFabric::onRequest)
      .build();
}

private static Behavior<Request> onRequest(Request request) {
  // ... process request ...
  request.replyTo.tell(new Response("Here are the cookies for " + request.query));
  return Behaviors.same();
}




Useful when:




Subscribing to an actor that will send many response messages back




Problems:




Actors seldom have a response message from another actor as a part of their protocol (see 
adapted response
)


It is hard to detect that a message request was not delivered or processed (see 
ask
)


Unless the protocol already includes a way to provide context, for example a request id that is also sent in the  response, it is not possible to tie an interaction to some specific context without introducing a new,  separate, actor (see 
ask
 or 
per session child actor
)




Request response with Scala 3


Scala 3 introduces union types, allowing for ad hoc combinations of types, this can be leveraged for response message types instead of the message adapters. The behavior is internally declared as union of its own protocol and any response messages it may accept.


The public protocol that the actor accepts by returning 
Behavior[Command]
 stays the same by use of 
.narrow
:




Scala




copy
source
object CookieMonster {
  sealed trait Command
  case object Munch extends Command

  def apply(cookieFabric: ActorRef[CookieFabric.Request]): Behavior[Command] =
    Behaviors
      .setup[Command | CookieFabric.Response] { context =>
        Behaviors.receiveMessage {
          case Munch =>
            cookieFabric ! CookieFabric.Request("Give me cookies", context.self)
            Behaviors.same

          case CookieFabric.Response(response) =>
            context.log.info("nonomnom got cookies: {}", response)
            cookieFabric ! CookieFabric.Request("Give me more cookies", context.self)
            Behaviors.same
        }
      }
      .narrow
}




Adapted Response


Most often the sending actor does not, and should not, support receiving the response messages of another actor. In such cases we need to provide an 
ActorRef
ActorRef
 of the right type and adapt the response message to a type that the sending actor can handle.


Example:






Scala




copy
source
object Backend {
  sealed trait Request
  final case class StartTranslationJob(taskId: Int, site: URI, replyTo: ActorRef[Response]) extends Request

  sealed trait Response
  final case class JobStarted(taskId: Int) extends Response
  final case class JobProgress(taskId: Int, progress: Double) extends Response
  final case class JobCompleted(taskId: Int, result: URI) extends Response
}

object Frontend {

  sealed trait Command
  final case class Translate(site: URI, replyTo: ActorRef[URI]) extends Command
  private final case class WrappedBackendResponse(response: Backend.Response) extends Command

  def apply(backend: ActorRef[Backend.Request]): Behavior[Command] =
    Behaviors.setup[Command] { context =>
      val backendResponseMapper: ActorRef[Backend.Response] =
        context.messageAdapter(rsp => WrappedBackendResponse(rsp))

      def active(inProgress: Map[Int, ActorRef[URI]], count: Int): Behavior[Command] = {
        Behaviors.receiveMessage[Command] {
          case Translate(site, replyTo) =>
            val taskId = count + 1
            backend ! Backend.StartTranslationJob(taskId, site, backendResponseMapper)
            active(inProgress.updated(taskId, replyTo), taskId)

          case wrapped: WrappedBackendResponse =>
            wrapped.response match {
              case Backend.JobStarted(taskId) =>
                context.log.info("Started {}", taskId)
                Behaviors.same
              case Backend.JobProgress(taskId, progress) =>
                context.log.info("Progress {}: {}", taskId, progress)
                Behaviors.same
              case Backend.JobCompleted(taskId, result) =>
                context.log.info("Completed {}: {}", taskId, result)
                inProgress(taskId) ! result
                active(inProgress - taskId, count)
            }
        }
      }

      active(inProgress = Map.empty, count = 0)
    }
}


Java




copy
source
public class Backend {
  public interface Request {}

  public static class StartTranslationJob implements Request {
    public final int taskId;
    public final URI site;
    public final ActorRef<Response> replyTo;

    public StartTranslationJob(int taskId, URI site, ActorRef<Response> replyTo) {
      this.taskId = taskId;
      this.site = site;
      this.replyTo = replyTo;
    }
  }

  public interface Response {}

  public static class JobStarted implements Response {
    public final int taskId;

    public JobStarted(int taskId) {
      this.taskId = taskId;
    }
  }

  public static class JobProgress implements Response {
    public final int taskId;
    public final double progress;

    public JobProgress(int taskId, double progress) {
      this.taskId = taskId;
      this.progress = progress;
    }
  }

  public static class JobCompleted implements Response {
    public final int taskId;
    public final URI result;

    public JobCompleted(int taskId, URI result) {
      this.taskId = taskId;
      this.result = result;
    }
  }
}

public class Frontend {

  public interface Command {}

  public static class Translate implements Command {
    public final URI site;
    public final ActorRef<URI> replyTo;

    public Translate(URI site, ActorRef<URI> replyTo) {
      this.site = site;
      this.replyTo = replyTo;
    }
  }

  private static class WrappedBackendResponse implements Command {
    final Backend.Response response;

    public WrappedBackendResponse(Backend.Response response) {
      this.response = response;
    }
  }

  public static class Translator extends AbstractBehavior<Command> {
    private final ActorRef<Backend.Request> backend;
    private final ActorRef<Backend.Response> backendResponseAdapter;

    private int taskIdCounter = 0;
    private Map<Integer, ActorRef<URI>> inProgress = new HashMap<>();

    public Translator(ActorContext<Command> context, ActorRef<Backend.Request> backend) {
      super(context);
      this.backend = backend;
      this.backendResponseAdapter =
          context.messageAdapter(Backend.Response.class, WrappedBackendResponse::new);
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(Translate.class, this::onTranslate)
          .onMessage(WrappedBackendResponse.class, this::onWrappedBackendResponse)
          .build();
    }

    private Behavior<Command> onTranslate(Translate cmd) {
      taskIdCounter += 1;
      inProgress.put(taskIdCounter, cmd.replyTo);
      backend.tell(
          new Backend.StartTranslationJob(taskIdCounter, cmd.site, backendResponseAdapter));
      return this;
    }

    private Behavior<Command> onWrappedBackendResponse(WrappedBackendResponse wrapped) {
      Backend.Response response = wrapped.response;
      if (response instanceof Backend.JobStarted) {
        Backend.JobStarted rsp = (Backend.JobStarted) response;
        getContext().getLog().info("Started {}", rsp.taskId);
      } else if (response instanceof Backend.JobProgress) {
        Backend.JobProgress rsp = (Backend.JobProgress) response;
        getContext().getLog().info("Progress {}", rsp.taskId);
      } else if (response instanceof Backend.JobCompleted) {
        Backend.JobCompleted rsp = (Backend.JobCompleted) response;
        getContext().getLog().info("Completed {}", rsp.taskId);
        inProgress.get(rsp.taskId).tell(rsp.result);
        inProgress.remove(rsp.taskId);
      } else {
        return Behaviors.unhandled();
      }

      return this;
    }
  }
}




You can register several message adapters for different message classes. It’s only possible to have one message adapter per message class to make sure that the number of adapters are not growing unbounded if registered repeatedly. That also means that a registered adapter will replace an existing adapter for the same message class.


A message adapter will be used if the message class matches the given class or is a subclass thereof. The registered adapters are tried in reverse order of their registration order, i.e. the last registered first.


A message adapter (and the returned 
ActorRef
ActorRef
) has the same lifecycle as the receiving actor. It’s recommended to register the adapters in a top level 
Behaviors.setup
Behaviors.setup
 or constructor of 
AbstractBehavior
AbstractBehavior
 but it’s possible to register them later if needed.


The adapter function is running in the receiving actor and can safely access its state, but if it throws an exception the actor is stopped.


Useful when:




Translating between different actor message protocols


Subscribing to an actor that will send many response messages back




Problems:




It is hard to detect that a message request was not delivered or processed (see 
ask
)


Only one adaption can be made per response message type, if a new one is registered the old one is replaced,  for example different target actors can’t have different adaption if they use the same response types, unless some  correlation is encoded in the messages


Unless the protocol already includes a way to provide context, for example a request id that is also sent in the  response, it is not possible to tie an interaction to some specific context without introducing a new,  separate, actor




Responses with Scala 3


Scala 3 introduces union types, allowing for ad hoc combinations of types, this can be leveraged for response message types instead of the message adapters. The behavior is internally declared as union of its own protocol and any response messages it may accept.


The public protocol that the actor accepts by returning 
Behavior[Command]
 stays the same by use of 
.narrow
:




Scala




copy
source
object Frontend {

  sealed trait Command

  final case class Translate(site: URI, replyTo: ActorRef[URI]) extends Command

  private type CommandOrResponse = Command | Backend.Response

  def apply(backend: ActorRef[Backend.Request]): Behavior[Command] =
    Behaviors
      .setup[CommandOrResponse] { context =>
        def active(inProgress: Map[Int, ActorRef[URI]], count: Int): Behavior[CommandOrResponse] = {
          Behaviors.receiveMessage[CommandOrResponse] {
            case Translate(site, replyTo) =>
              val taskId = count + 1
              backend ! Backend.StartTranslationJob(taskId, site, context.self)
              active(inProgress.updated(taskId, replyTo), taskId)

            case Backend.JobStarted(taskId) =>
              context.log.info("Started {}", taskId)
              Behaviors.same
            case Backend.JobProgress(taskId, progress) =>
              context.log.info("Progress {}: {}", taskId, progress)
              Behaviors.same
            case Backend.JobCompleted(taskId, result) =>
              context.log.info("Completed {}: {}", taskId, result)
              inProgress(taskId) ! result
              active(inProgress - taskId, count)
          }
        }

        active(inProgress = Map.empty, count = 0)
      }
      .narrow
}




Request-Response with ask between two actors


In an interaction where there is a 1:1 mapping between a request and a response we can use 
ask
 on the 
ActorContext
ActorContext
 to interact with another actor.


The interaction has two steps, first we need to construct the outgoing message, to do that we need an 
ActorRef[Response]
ActorRef<Response>
 to put as recipient in the outgoing message. The second step is to transform the successful 
Response
 or failure into a message that is part of the protocol of the sending actor. See also the 
Generic response wrapper
 for replies that are either a success or an error.


Example:






Scala




copy
source
object Hal {
  sealed trait Command
  case class OpenThePodBayDoorsPlease(replyTo: ActorRef[Response]) extends Command
  case class Response(message: String)

  def apply(): Behaviors.Receive[Hal.Command] =
    Behaviors.receiveMessage[Command] {
      case OpenThePodBayDoorsPlease(replyTo) =>
        replyTo ! Response("I'm sorry, Dave. I'm afraid I can't do that.")
        Behaviors.same
    }
}

object Dave {

  sealed trait Command
  // this is a part of the protocol that is internal to the actor itself
  private case class AdaptedResponse(message: String) extends Command

  def apply(hal: ActorRef[Hal.Command]): Behavior[Dave.Command] =
    Behaviors.setup[Command] { context =>
      // asking someone requires a timeout, if the timeout hits without response
      // the ask is failed with a TimeoutException
      implicit val timeout: Timeout = 3.seconds

      // Note: The second parameter list takes a function `ActorRef[T] => Message`,
      // as OpenThePodBayDoorsPlease is a case class it has a factory apply method
      // that is what we are passing as the second parameter here it could also be written
      // as `ref => OpenThePodBayDoorsPlease(ref)`
      context.ask(hal, Hal.OpenThePodBayDoorsPlease.apply) {
        case Success(Hal.Response(message)) => AdaptedResponse(message)
        case Failure(_)                     => AdaptedResponse("Request failed")
      }

      // we can also tie in request context into an interaction, it is safe to look at
      // actor internal state from the transformation function, but remember that it may have
      // changed at the time the response arrives and the transformation is done, best is to
      // use immutable state we have closed over like here.
      val requestId = 1
      context.ask(hal, Hal.OpenThePodBayDoorsPlease.apply) {
        case Success(Hal.Response(message)) => AdaptedResponse(s"$requestId: $message")
        case Failure(_)                     => AdaptedResponse(s"$requestId: Request failed")
      }

      Behaviors.receiveMessage {
        // the adapted message ends up being processed like any other
        // message sent to the actor
        case AdaptedResponse(message) =>
          context.log.info("Got response from hal: {}", message)
          Behaviors.same
      }
    }
}


Java




copy
source
public class Hal extends AbstractBehavior<Hal.Command> {

  public static Behavior<Hal.Command> create() {
    return Behaviors.setup(Hal::new);
  }

  private Hal(ActorContext<Command> context) {
    super(context);
  }

  public interface Command {}

  public static final class OpenThePodBayDoorsPlease implements Command {
    public final ActorRef<HalResponse> respondTo;

    public OpenThePodBayDoorsPlease(ActorRef<HalResponse> respondTo) {
      this.respondTo = respondTo;
    }
  }

  public static final class HalResponse {
    public final String message;

    public HalResponse(String message) {
      this.message = message;
    }
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(OpenThePodBayDoorsPlease.class, this::onOpenThePodBayDoorsPlease)
        .build();
  }

  private Behavior<Command> onOpenThePodBayDoorsPlease(OpenThePodBayDoorsPlease message) {
    message.respondTo.tell(new HalResponse("I'm sorry, Dave. I'm afraid I can't do that."));
    return this;
  }
}

public class Dave extends AbstractBehavior<Dave.Command> {

  public interface Command {}

  // this is a part of the protocol that is internal to the actor itself
  private static final class AdaptedResponse implements Command {
    public final String message;

    public AdaptedResponse(String message) {
      this.message = message;
    }
  }

  public static Behavior<Command> create(ActorRef<Hal.Command> hal) {
    return Behaviors.setup(context -> new Dave(context, hal));
  }

  private Dave(ActorContext<Command> context, ActorRef<Hal.Command> hal) {
    super(context);

    // asking someone requires a timeout, if the timeout hits without response
    // the ask is failed with a TimeoutException
    final Duration timeout = Duration.ofSeconds(3);

    context.ask(
        Hal.HalResponse.class,
        hal,
        timeout,
        // construct the outgoing message
        (ActorRef<Hal.HalResponse> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),
        // adapt the response (or failure to respond)
        (response, throwable) -> {
          if (response != null) {
            return new AdaptedResponse(response.message);
          } else {
            return new AdaptedResponse("Request failed");
          }
        });

    // we can also tie in request context into an interaction, it is safe to look at
    // actor internal state from the transformation function, but remember that it may have
    // changed at the time the response arrives and the transformation is done, best is to
    // use immutable state we have closed over like here.
    final int requestId = 1;
    context.ask(
        Hal.HalResponse.class,
        hal,
        timeout,
        // construct the outgoing message
        (ActorRef<Hal.HalResponse> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),
        // adapt the response (or failure to respond)
        (response, throwable) -> {
          if (response != null) {
            return new AdaptedResponse(requestId + ": " + response.message);
          } else {
            return new AdaptedResponse(requestId + ": Request failed");
          }
        });
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        // the adapted message ends up being processed like any other
        // message sent to the actor
        .onMessage(AdaptedResponse.class, this::onAdaptedResponse)
        .build();
  }

  private Behavior<Command> onAdaptedResponse(AdaptedResponse response) {
    getContext().getLog().info("Got response from HAL: {}", response.message);
    return this;
  }
}




The response adapting function is running in the receiving actor and can safely access its state, but if it throws an exception the actor is stopped.


Useful when:




Single response queries


An actor needs to know that the message was processed before continuing


To allow an actor to resend if a timely response is not produced


To keep track of outstanding requests and not overwhelm a recipient with messages (“backpressure”)


Context should be attached to the interaction but the protocol does not support that (request id, what query the response was for)




Problems:




There can only be a single response to one 
ask
 (see 
per session child Actor
)


When 
ask
 times out, the receiving actor does not know and may still process it to completion, or even start processing it after the fact


Finding a good value for the timeout, especially when 
ask
 triggers chained 
ask
s in the receiving actor. You want a short timeout to be responsive and answer back to the requester, but at the same time you do not want to have many false positives






Request-Response with ask from outside an Actor


Sometimes you need to interact with actors from the outside of the actor system, this can be done with fire-and-forget as described above or through another version of 
ask
 that returns a 
Future[Response]
CompletionStage<Response>
 that is either completed with a successful response or failed with a 
TimeoutException
 if there was no response within the specified timeout.


To do this we use 
ask
 (or the symbolic 
?
) implicitly added to 
ActorRef
 by 
akka.actor.typed.scaladsl.AskPattern._
 to send a message to an actor and get a 
Future[Response]
 back. 
ask
 takes implicit 
Timeout
 and 
ActorSystem
 parameters.
 
To do this we use 
akka.actor.typed.javadsl.AskPattern.ask
 to send a message to an actor and get a 
CompletionState[Response]
 back.


Example:






Scala




copy
source
object CookieFabric {
  sealed trait Command
  case class GiveMeCookies(count: Int, replyTo: ActorRef[Reply]) extends Command

  sealed trait Reply
  case class Cookies(count: Int) extends Reply
  case class InvalidRequest(reason: String) extends Reply

  def apply(): Behaviors.Receive[CookieFabric.GiveMeCookies] =
    Behaviors.receiveMessage { message =>
      if (message.count >= 5)
        message.replyTo ! InvalidRequest("Too many cookies.")
      else
        message.replyTo ! Cookies(message.count)
      Behaviors.same
    }
}

import akka.actor.typed.scaladsl.AskPattern._
import akka.util.Timeout

// asking someone requires a timeout if the timeout hits without response
// the ask is failed with a TimeoutException
implicit val timeout: Timeout = 3.seconds
// implicit ActorSystem in scope
implicit val system: ActorSystem[_] = theSystem

val result: Future[CookieFabric.Reply] = cookieFabric.ask(ref => CookieFabric.GiveMeCookies(3, ref))

// the response callback will be executed on this execution context
implicit val ec = system.executionContext

result.onComplete {
  case Success(CookieFabric.Cookies(count))         => println(s"Yay, $count cookies!")
  case Success(CookieFabric.InvalidRequest(reason)) => println(s"No cookies for me. $reason")
  case Failure(ex)                                  => println(s"Boo! didn't get cookies: ${ex.getMessage}")
}


Java




copy
source
public class CookieFabric extends AbstractBehavior<CookieFabric.Command> {

  interface Command {}

  public static class GiveMeCookies implements Command {
    public final int count;
    public final ActorRef<Reply> replyTo;

    public GiveMeCookies(int count, ActorRef<Reply> replyTo) {
      this.count = count;
      this.replyTo = replyTo;
    }
  }

  interface Reply {}

  public static class Cookies implements Reply {
    public final int count;

    public Cookies(int count) {
      this.count = count;
    }
  }

  public static class InvalidRequest implements Reply {
    public final String reason;

    public InvalidRequest(String reason) {
      this.reason = reason;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(CookieFabric::new);
  }

  private CookieFabric(ActorContext<Command> context) {
    super(context);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder().onMessage(GiveMeCookies.class, this::onGiveMeCookies).build();
  }

  private Behavior<Command> onGiveMeCookies(GiveMeCookies request) {
    if (request.count >= 5) request.replyTo.tell(new InvalidRequest("Too many cookies."));
    else request.replyTo.tell(new Cookies(request.count));

    return this;
  }
}

  public void askAndPrint(
      ActorSystem<Void> system, ActorRef<CookieFabric.Command> cookieFabric) {
    CompletionStage<CookieFabric.Reply> result =
        AskPattern.ask(
            cookieFabric,
            replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),
            // asking someone requires a timeout and a scheduler, if the timeout hits without
            // response the ask is failed with a TimeoutException
            Duration.ofSeconds(3),
            system.scheduler());

    result.whenComplete(
        (reply, failure) -> {
          if (reply instanceof CookieFabric.Cookies)
            System.out.println("Yay, " + ((CookieFabric.Cookies) reply).count + " cookies!");
          else if (reply instanceof CookieFabric.InvalidRequest)
            System.out.println(
                "No cookies for me. " + ((CookieFabric.InvalidRequest) reply).reason);
          else System.out.println("Boo! didn't get cookies in time. " + failure);
        });
  }




Note that validation errors are also explicit in the message protocol. The 
GiveMeCookies
 request can reply with 
Cookies
 or 
InvalidRequest
. The requestor has to decide how to handle an 
InvalidRequest
 reply. Sometimes it should be treated as a failed 
Future
CompletionStage
 and for that the reply can be mapped on the requestor side. See also the 
Generic response wrapper
 for replies that are either a success or an error.




Scala




copy
source
val cookies: Future[CookieFabric.Cookies] =
  cookieFabric.ask[CookieFabric.Reply](ref => CookieFabric.GiveMeCookies(3, ref)).flatMap {
    case c: CookieFabric.Cookies             => Future.successful(c)
    case CookieFabric.InvalidRequest(reason) => Future.failed(new IllegalArgumentException(reason))
  }

cookies.onComplete {
  case Success(CookieFabric.Cookies(count)) => println(s"Yay, $count cookies!")
  case Failure(ex)                          => println(s"Boo! didn't get cookies: ${ex.getMessage}")
}


Java




copy
source
CompletionStage<CookieFabric.Reply> result =
    AskPattern.ask(
        cookieFabric,
        replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),
        Duration.ofSeconds(3),
        system.scheduler());

CompletionStage<CookieFabric.Cookies> cookies =
    result.thenCompose(
        (CookieFabric.Reply reply) -> {
          if (reply instanceof CookieFabric.Cookies) {
            return CompletableFuture.completedFuture((CookieFabric.Cookies) reply);
          } else if (reply instanceof CookieFabric.InvalidRequest) {
            CompletableFuture<CookieFabric.Cookies> failed = new CompletableFuture<>();
            failed.completeExceptionally(
                new IllegalArgumentException(((CookieFabric.InvalidRequest) reply).reason));
            return failed;
          } else {
            throw new IllegalStateException("Unexpected reply: " + reply.getClass());
          }
        });

cookies.whenComplete(
    (cookiesReply, failure) -> {
      if (cookiesReply != null)
        System.out.println("Yay, " + cookiesReply.count + " cookies!");
      else System.out.println("Boo! didn't get cookies in time. " + failure);
    });




Useful when:




Querying an actor from outside of the actor system




Problems:




It is easy to accidentally close over and unsafely mutable state with the callbacks on the returned 
Future
CompletionStage
 as those will be executed on a different thread


There can only be a single response to one 
ask
 (see 
per session child Actor
)


When 
ask
 times out, the receiving actor does not know and may still process it to completion, or even start processing it after the fact




Generic response wrapper


In many cases the response can either be a successful result or an error (a validation error that the command was invalid for example). Having to define two response classes and a shared supertype for every request type can be repetitive, especially in a cluster context where you also have to make sure the messages can be serialized to be sent over the network.


To help with this a generic status-response type is included in Akka: 
StatusReply
StatusReply
, everywhere where 
ask
 can be used there is also a second method 
askWithStatus
askWithStatus
 which, given that the response is a 
StatusReply
 will unwrap successful responses and help with handling validation errors. Akka includes pre-built serializers for the type, so in the normal use case a clustered application only needs to provide a serializer for the successful result.


For the case where the successful reply does not contain an actual value but is more of an acknowledgment there is a pre defined 
StatusReply.Ack
StatusReply.ack()
 of type 
StatusReply[Done]
StatusReply<Done>
.


Errors are preferably sent as a text describing what is wrong, but using exceptions to attach a type is also possible.


Example actor to actor ask:




Scala




copy
source
object Hal {
  sealed trait Command
  case class OpenThePodBayDoorsPlease(replyTo: ActorRef[StatusReply[String]]) extends Command

  def apply(): Behaviors.Receive[Hal.Command] =
    Behaviors.receiveMessage[Command] {
      case OpenThePodBayDoorsPlease(replyTo) =>
        // reply with a validation error description
        replyTo ! StatusReply.Error("I'm sorry, Dave. I'm afraid I can't do that.")
        Behaviors.same
    }
}

object Dave {

  sealed trait Command
  // this is a part of the protocol that is internal to the actor itself
  private case class AdaptedResponse(message: String) extends Command

  def apply(hal: ActorRef[Hal.Command]): Behavior[Dave.Command] =
    Behaviors.setup[Command] { context =>
      // asking someone requires a timeout, if the timeout hits without response
      // the ask is failed with a TimeoutException
      implicit val timeout: Timeout = 3.seconds

      // A StatusReply.Success(m) ends up as a Success(m) here, while a
      // StatusReply.Error(text) becomes a Failure(ErrorMessage(text))
      context.askWithStatus(hal, Hal.OpenThePodBayDoorsPlease.apply) {
        case Success(message)                        => AdaptedResponse(message)
        case Failure(StatusReply.ErrorMessage(text)) => AdaptedResponse(s"Request denied: $text")
        case Failure(_)                              => AdaptedResponse("Request failed")
      }

      Behaviors.receiveMessage {
        // the adapted message ends up being processed like any other
        // message sent to the actor
        case AdaptedResponse(message) =>
          context.log.info("Got response from hal: {}", message)
          Behaviors.same
      }
    }
}


Java




copy
source
import akka.pattern.StatusReply;

public class Hal extends AbstractBehavior<Hal.Command> {

  public static Behavior<Hal.Command> create() {
    return Behaviors.setup(Hal::new);
  }

  private Hal(ActorContext<Hal.Command> context) {
    super(context);
  }

  public interface Command {}

  public static final class OpenThePodBayDoorsPlease implements Hal.Command {
    public final ActorRef<StatusReply<String>> respondTo;

    public OpenThePodBayDoorsPlease(ActorRef<StatusReply<String>> respondTo) {
      this.respondTo = respondTo;
    }
  }

  @Override
  public Receive<Hal.Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Hal.OpenThePodBayDoorsPlease.class, this::onOpenThePodBayDoorsPlease)
        .build();
  }

  private Behavior<Hal.Command> onOpenThePodBayDoorsPlease(
      Hal.OpenThePodBayDoorsPlease message) {
    message.respondTo.tell(StatusReply.error("I'm sorry, Dave. I'm afraid I can't do that."));
    return this;
  }
}

public class Dave extends AbstractBehavior<Dave.Command> {

  public interface Command {}

  // this is a part of the protocol that is internal to the actor itself
  private static final class AdaptedResponse implements Dave.Command {
    public final String message;

    public AdaptedResponse(String message) {
      this.message = message;
    }
  }

  public static Behavior<Dave.Command> create(ActorRef<Hal.Command> hal) {
    return Behaviors.setup(context -> new Dave(context, hal));
  }

  private Dave(ActorContext<Dave.Command> context, ActorRef<Hal.Command> hal) {
    super(context);

    // asking someone requires a timeout, if the timeout hits without response
    // the ask is failed with a TimeoutException
    final Duration timeout = Duration.ofSeconds(3);

    context.askWithStatus(
        String.class,
        hal,
        timeout,
        // construct the outgoing message
        (ActorRef<StatusReply<String>> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),
        // adapt the response (or failure to respond)
        (response, throwable) -> {
          if (response != null) {
            // a ReponseWithStatus.success(m) is unwrapped and passed as response
            return new Dave.AdaptedResponse(response);
          } else {
            // a ResponseWithStatus.error will end up as a StatusReply.ErrorMessage()
            // exception here
            return new Dave.AdaptedResponse("Request failed: " + throwable.getMessage());
          }
        });
  }

  @Override
  public Receive<Dave.Command> createReceive() {
    return newReceiveBuilder()
        // the adapted message ends up being processed like any other
        // message sent to the actor
        .onMessage(Dave.AdaptedResponse.class, this::onAdaptedResponse)
        .build();
  }

  private Behavior<Dave.Command> onAdaptedResponse(Dave.AdaptedResponse response) {
    getContext().getLog().info("Got response from HAL: {}", response.message);
    return this;
  }
}




A validation error is turned into a 
Failure
 for the message adapter. In this case we are explicitly handling the validation error separately from other ask failures.


Example ask from the outside:




Scala




copy
source
object CookieFabric {
  sealed trait Command
  case class GiveMeCookies(count: Int, replyTo: ActorRef[StatusReply[Cookies]]) extends Command
  case class Cookies(count: Int)

  def apply(): Behaviors.Receive[CookieFabric.GiveMeCookies] =
    Behaviors.receiveMessage { message =>
      if (message.count >= 5)
        message.replyTo ! StatusReply.Error("Too many cookies.")
      else
        message.replyTo ! StatusReply.Success(Cookies(message.count))
      Behaviors.same
    }
}

import akka.actor.typed.scaladsl.AskPattern._
import akka.util.Timeout

// asking someone requires a timeout if the timeout hits without response
// the ask is failed with a TimeoutException
implicit val timeout: Timeout = 3.seconds
// implicit ActorSystem in scope
implicit val system: ActorSystem[_] = theSystem

val result: Future[CookieFabric.Cookies] = cookieFabric.askWithStatus(ref => CookieFabric.GiveMeCookies(3, ref))

// the response callback will be executed on this execution context
implicit val ec = system.executionContext

result.onComplete {
  case Success(CookieFabric.Cookies(count))      => println(s"Yay, $count cookies!")
  case Failure(StatusReply.ErrorMessage(reason)) => println(s"No cookies for me. $reason")
  case Failure(ex)                               => println(s"Boo! didn't get cookies: ${ex.getMessage}")
}


Java




copy
source
public class CookieFabric extends AbstractBehavior<CookieFabric.Command> {

  interface Command {}

  public static class GiveMeCookies implements CookieFabric.Command {
    public final int count;
    public final ActorRef<StatusReply<CookieFabric.Cookies>> replyTo;

    public GiveMeCookies(int count, ActorRef<StatusReply<CookieFabric.Cookies>> replyTo) {
      this.count = count;
      this.replyTo = replyTo;
    }
  }

  public static class Cookies {
    public final int count;

    public Cookies(int count) {
      this.count = count;
    }
  }

  public static Behavior<CookieFabric.Command> create() {
    return Behaviors.setup(CookieFabric::new);
  }

  private CookieFabric(ActorContext<CookieFabric.Command> context) {
    super(context);
  }

  @Override
  public Receive<CookieFabric.Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(CookieFabric.GiveMeCookies.class, this::onGiveMeCookies)
        .build();
  }

  private Behavior<CookieFabric.Command> onGiveMeCookies(CookieFabric.GiveMeCookies request) {
    if (request.count >= 5) request.replyTo.tell(StatusReply.error("Too many cookies."));
    else request.replyTo.tell(StatusReply.success(new CookieFabric.Cookies(request.count)));

    return this;
  }
}

  public void askAndPrint(
      ActorSystem<Void> system, ActorRef<CookieFabric.Command> cookieFabric) {
    CompletionStage<CookieFabric.Cookies> result =
        AskPattern.askWithStatus(
            cookieFabric,
            replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),
            // asking someone requires a timeout and a scheduler, if the timeout hits without
            // response the ask is failed with a TimeoutException
            Duration.ofSeconds(3),
            system.scheduler());

    result.whenComplete(
        (reply, failure) -> {
          if (reply != null) System.out.println("Yay, " + reply.count + " cookies!");
          else if (failure instanceof StatusReply.ErrorMessage)
            System.out.println("No cookies for me. " + failure.getMessage());
          else System.out.println("Boo! didn't get cookies in time. " + failure);
        });
  }




Note that validation errors are also explicit in the message protocol, but encoded as the wrapper type, constructed using 
StatusReply.Error(text)
StatusReply.error(text)
:




Scala




copy
source
val cookies: Future[CookieFabric.Cookies] =
  cookieFabric.askWithStatus[CookieFabric.Cookies](ref => CookieFabric.GiveMeCookies(3, ref)).flatMap {
    case c: CookieFabric.Cookies => Future.successful(c)
  }

cookies.onComplete {
  case Success(CookieFabric.Cookies(count)) => println(s"Yay, $count cookies!")
  case Failure(ex)                          => println(s"Boo! didn't get cookies: ${ex.getMessage}")
}


Java




copy
source
CompletionStage<CookieFabric.Cookies> cookies =
    AskPattern.askWithStatus(
        cookieFabric,
        replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),
        Duration.ofSeconds(3),
        system.scheduler());

cookies.whenComplete(
    (cookiesReply, failure) -> {
      if (cookiesReply != null)
        System.out.println("Yay, " + cookiesReply.count + " cookies!");
      else System.out.println("Boo! didn't get cookies in time. " + failure);
    });




Ignoring replies


In some situations an actor has a response for a particular request message but you are not interested in the response. In this case you can pass 
system.ignoreRef
system.ignoreRef()
 turning the request-response into a fire-and-forget.


system.ignoreRef
system.ignoreRef()
, as the name indicates, returns an 
ActorRef
ActorRef
 that ignores any message sent to it.


With the same protocol as the 
request response
 above, if the sender would prefer to ignore the reply it could pass 
system.ignoreRef
system.ignoreRef()
 for the 
replyTo
, which it can access through 
ActorContext.system.ignoreRef
ActorContext.getSystem().ignoreRef()
. 




Scala




copy
source
cookieFabric ! CookieFabric.Request("don't send cookies back", context.system.ignoreRef)


Java




copy
source
cookieFabric.tell(
    new CookieFabric.Request("don't send cookies back", context.getSystem().ignoreRef()));




Useful when:




Sending a message for which the protocol defines a reply, but you are not interested in getting the reply




Problems:


The returned 
ActorRef
ActorRef
 ignores all messages sent to it, therefore it should be used carefully.




Passing it around inadvertently as if it was a normal 
ActorRef
 may result in broken actor-to-actor interactions.


Using it when performing an 
ask
 from outside the Actor System will cause the 
Future
CompletionStage
 returned by the 
ask
 to timeout since it will never complete.


Finally, it’s legal to 
watch
watch
 it, but since it’s of a special kind, it never terminates and therefore you will never receive a 
Terminated
Terminated
 signal from it.




Send Future result to self


When using an API that returns a 
Future
CompletionStage
 from an actor it’s common that you would like to use the value of the response in the actor when the 
Future
CompletionStage
 is completed. For this purpose the 
ActorContext
 provides a 
pipeToSelf
pipeToSelf
 method.


Example:




An actor, 
CustomerRepository
, is invoking a method on 
CustomerDataAccess
 that returns a 
Future
CompletionStage
.




Scala




copy
source
trait CustomerDataAccess {
  def update(value: Customer): Future[Done]
}

final case class Customer(id: String, version: Long, name: String, address: String)

object CustomerRepository {
  sealed trait Command

  final case class Update(value: Customer, replyTo: ActorRef[UpdateResult]) extends Command
  sealed trait UpdateResult
  final case class UpdateSuccess(id: String) extends UpdateResult
  final case class UpdateFailure(id: String, reason: String) extends UpdateResult
  private final case class WrappedUpdateResult(result: UpdateResult, replyTo: ActorRef[UpdateResult])
      extends Command

  private val MaxOperationsInProgress = 10

  def apply(dataAccess: CustomerDataAccess): Behavior[Command] = {
    next(dataAccess, operationsInProgress = 0)
  }

  private def next(dataAccess: CustomerDataAccess, operationsInProgress: Int): Behavior[Command] = {
    Behaviors.receive { (context, command) =>
      command match {
        case Update(value, replyTo) =>
          if (operationsInProgress == MaxOperationsInProgress) {
            replyTo ! UpdateFailure(value.id, s"Max $MaxOperationsInProgress concurrent operations supported")
            Behaviors.same
          } else {
            val futureResult = dataAccess.update(value)
            context.pipeToSelf(futureResult) {
              // map the Future value to a message, handled by this actor
              case Success(_) => WrappedUpdateResult(UpdateSuccess(value.id), replyTo)
              case Failure(e) => WrappedUpdateResult(UpdateFailure(value.id, e.getMessage), replyTo)
            }
            // increase operationsInProgress counter
            next(dataAccess, operationsInProgress + 1)
          }

        case WrappedUpdateResult(result, replyTo) =>
          // send result to original requestor
          replyTo ! result
          // decrease operationsInProgress counter
          next(dataAccess, operationsInProgress - 1)
      }
    }
  }
}


Java




copy
source
public interface CustomerDataAccess {
  CompletionStage<Done> update(Customer customer);
}

public class Customer {
  public final String id;
  public final long version;
  public final String name;
  public final String address;

  public Customer(String id, long version, String name, String address) {
    this.id = id;
    this.version = version;
    this.name = name;
    this.address = address;
  }
}

public class CustomerRepository extends AbstractBehavior<CustomerRepository.Command> {

  private static final int MAX_OPERATIONS_IN_PROGRESS = 10;

  interface Command {}

  public static class Update implements Command {
    public final Customer customer;
    public final ActorRef<OperationResult> replyTo;

    public Update(Customer customer, ActorRef<OperationResult> replyTo) {
      this.customer = customer;
      this.replyTo = replyTo;
    }
  }

  interface OperationResult {}

  public static class UpdateSuccess implements OperationResult {
    public final String id;

    public UpdateSuccess(String id) {
      this.id = id;
    }
  }

  public static class UpdateFailure implements OperationResult {
    public final String id;
    public final String reason;

    public UpdateFailure(String id, String reason) {
      this.id = id;
      this.reason = reason;
    }
  }

  private static class WrappedUpdateResult implements Command {
    public final OperationResult result;
    public final ActorRef<OperationResult> replyTo;

    private WrappedUpdateResult(OperationResult result, ActorRef<OperationResult> replyTo) {
      this.result = result;
      this.replyTo = replyTo;
    }
  }

  public static Behavior<Command> create(CustomerDataAccess dataAccess) {
    return Behaviors.setup(context -> new CustomerRepository(context, dataAccess));
  }

  private final CustomerDataAccess dataAccess;
  private int operationsInProgress = 0;

  private CustomerRepository(ActorContext<Command> context, CustomerDataAccess dataAccess) {
    super(context);
    this.dataAccess = dataAccess;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Update.class, this::onUpdate)
        .onMessage(WrappedUpdateResult.class, this::onUpdateResult)
        .build();
  }

  private Behavior<Command> onUpdate(Update command) {
    if (operationsInProgress == MAX_OPERATIONS_IN_PROGRESS) {
      command.replyTo.tell(
          new UpdateFailure(
              command.customer.id,
              "Max " + MAX_OPERATIONS_IN_PROGRESS + " concurrent operations supported"));
    } else {
      // increase operationsInProgress counter
      operationsInProgress++;
      CompletionStage<Done> futureResult = dataAccess.update(command.customer);
      getContext()
          .pipeToSelf(
              futureResult,
              (ok, exc) -> {
                if (exc == null)
                  return new WrappedUpdateResult(
                      new UpdateSuccess(command.customer.id), command.replyTo);
                else
                  return new WrappedUpdateResult(
                      new UpdateFailure(command.customer.id, exc.getMessage()),
                      command.replyTo);
              });
    }
    return this;
  }

  private Behavior<Command> onUpdateResult(WrappedUpdateResult wrapped) {
    // decrease operationsInProgress counter
    operationsInProgress--;
    // send result to original requestor
    wrapped.replyTo.tell(wrapped.result);
    return this;
  }
}




It could be tempting to just use 
onComplete on the Future
a callback on the CompletionStage
, but that introduces the risk of accessing internal state of the actor that is not thread-safe from an external thread. For example, the 
numberOfPendingOperations
 counter in above example can’t be accessed from such callback. Therefore it is better to map the result to a message and perform further processing when receiving that message.


Useful when:




Accessing APIs that are returning 
Future
CompletionStage
 from an actor, such as a database or  an external service


The actor needs to continue processing when the 
Future
CompletionStage
 has completed


Keep context from the original request and use that when the 
Future
CompletionStage
 has completed,  for example an 
replyTo
 actor reference




Problems:




Boilerplate of adding wrapper messages for the results




Per session child Actor


In some cases a complete response to a request can only be created and sent back after collecting multiple answers from other actors. For these kinds of interaction it can be good to delegate the work to a per “session” child actor. The child could also contain arbitrary logic to implement retrying, failing on timeout, tail chopping, progress inspection etc.


Note that this is essentially how 
ask
 is implemented, if all you need is a single response with a timeout it is better to use 
ask
.


The child is created with the context it needs to do the work, including an 
ActorRef
ActorRef
 that it can respond to. When the complete result is there the child responds with the result and stops itself.


As the protocol of the session actor is not a public API but rather an implementation detail of the parent actor, it may not always make sense to have an explicit protocol and adapt the messages of the actors that the session actor interacts with. For this use case it is possible to express that the actor can receive any message (
Any
Object
).


Example:






Scala




copy
source
// dummy data types just for this sample
case class Keys()

case class Wallet()


  object Home {
    sealed trait Command
    case class LeaveHome(who: String, replyTo: ActorRef[ReadyToLeaveHome]) extends Command
    case class ReadyToLeaveHome(who: String, keys: Keys, wallet: Wallet)

    def apply(): Behavior[Command] = {
      Behaviors.setup[Command] { context =>
        val keyCabinet: ActorRef[KeyCabinet.GetKeys] = context.spawn(KeyCabinet(), "key-cabinet")
        val drawer: ActorRef[Drawer.GetWallet] = context.spawn(Drawer(), "drawer")

        Behaviors.receiveMessage[Command] {
          case LeaveHome(who, replyTo) =>
            context.spawn(prepareToLeaveHome(who, replyTo, keyCabinet, drawer), s"leaving-$who")
            Behaviors.same
        }
      }
    }

    // per session actor behavior
    def prepareToLeaveHome(
        whoIsLeaving: String,
        replyTo: ActorRef[ReadyToLeaveHome],
        keyCabinet: ActorRef[KeyCabinet.GetKeys],
        drawer: ActorRef[Drawer.GetWallet]): Behavior[NotUsed] = {
      // we don't _really_ care about the actor protocol here as nobody will send us
      // messages except for responses to our queries, so we just accept any kind of message
      // but narrow that to more limited types when we interact
      Behaviors
        .setup[AnyRef] { context =>
          var wallet: Option[Wallet] = None
          var keys: Option[Keys] = None

          // we narrow the ActorRef type to any subtype of the actual type we accept
          keyCabinet ! KeyCabinet.GetKeys(whoIsLeaving, context.self.narrow[Keys])
          drawer ! Drawer.GetWallet(whoIsLeaving, context.self.narrow[Wallet])

          def nextBehavior(): Behavior[AnyRef] =
            (keys, wallet) match {
              case (Some(w), Some(k)) =>
                // we got both, "session" is completed!
                replyTo ! ReadyToLeaveHome(whoIsLeaving, w, k)
                Behaviors.stopped

              case _ =>
                Behaviors.same
            }

          Behaviors.receiveMessage {
            case w: Wallet =>
              wallet = Some(w)
              nextBehavior()
            case k: Keys =>
              keys = Some(k)
              nextBehavior()
            case _ =>
              Behaviors.unhandled
          }
        }
        .narrow[NotUsed] // we don't let anyone else know we accept anything
    }
  }


Java




copy
source
// dummy data types just for this sample
public class Keys {}

public class Wallet {}

public class KeyCabinet {
  public static class GetKeys {
    public final String whoseKeys;
    public final ActorRef<Keys> replyTo;

    public GetKeys(String whoseKeys, ActorRef<Keys> respondTo) {
      this.whoseKeys = whoseKeys;
      this.replyTo = respondTo;
    }
  }

  public static Behavior<GetKeys> create() {
    return Behaviors.receiveMessage(KeyCabinet::onGetKeys);
  }

  private static Behavior<GetKeys> onGetKeys(GetKeys message) {
    message.replyTo.tell(new Keys());
    return Behaviors.same();
  }
}

public class Drawer {

  public static class GetWallet {
    public final String whoseWallet;
    public final ActorRef<Wallet> replyTo;

    public GetWallet(String whoseWallet, ActorRef<Wallet> replyTo) {
      this.whoseWallet = whoseWallet;
      this.replyTo = replyTo;
    }
  }

  public static Behavior<GetWallet> create() {
    return Behaviors.receiveMessage(Drawer::onGetWallet);
  }

  private static Behavior<GetWallet> onGetWallet(GetWallet message) {
    message.replyTo.tell(new Wallet());
    return Behaviors.same();
  }
}

public class Home {

  public interface Command {}

  public static class LeaveHome implements Command {
    public final String who;
    public final ActorRef<ReadyToLeaveHome> respondTo;

    public LeaveHome(String who, ActorRef<ReadyToLeaveHome> respondTo) {
      this.who = who;
      this.respondTo = respondTo;
    }
  }

  public static class ReadyToLeaveHome {
    public final String who;
    public final Keys keys;
    public final Wallet wallet;

    public ReadyToLeaveHome(String who, Keys keys, Wallet wallet) {
      this.who = who;
      this.keys = keys;
      this.wallet = wallet;
    }
  }

  private final ActorContext<Command> context;

  private final ActorRef<KeyCabinet.GetKeys> keyCabinet;
  private final ActorRef<Drawer.GetWallet> drawer;

  private Home(ActorContext<Command> context) {
    this.context = context;
    this.keyCabinet = context.spawn(KeyCabinet.create(), "key-cabinet");
    this.drawer = context.spawn(Drawer.create(), "drawer");
  }

  private Behavior<Command> behavior() {
    return Behaviors.receive(Command.class)
        .onMessage(LeaveHome.class, this::onLeaveHome)
        .build();
  }

  private Behavior<Command> onLeaveHome(LeaveHome message) {
    context.spawn(
        PrepareToLeaveHome.create(message.who, message.respondTo, keyCabinet, drawer),
        "leaving" + message.who);
    return Behaviors.same();
  }

  // actor behavior
  public static Behavior<Command> create() {
    return Behaviors.setup(context -> new Home(context).behavior());
  }
}

// per session actor behavior
class PrepareToLeaveHome extends AbstractBehavior<Object> {
  static Behavior<Object> create(
      String whoIsLeaving,
      ActorRef<Home.ReadyToLeaveHome> replyTo,
      ActorRef<KeyCabinet.GetKeys> keyCabinet,
      ActorRef<Drawer.GetWallet> drawer) {
    return Behaviors.setup(
        context -> new PrepareToLeaveHome(context, whoIsLeaving, replyTo, keyCabinet, drawer));
  }

  private final String whoIsLeaving;
  private final ActorRef<Home.ReadyToLeaveHome> replyTo;
  private final ActorRef<KeyCabinet.GetKeys> keyCabinet;
  private final ActorRef<Drawer.GetWallet> drawer;
  private Optional<Wallet> wallet = Optional.empty();
  private Optional<Keys> keys = Optional.empty();

  private PrepareToLeaveHome(
      ActorContext<Object> context,
      String whoIsLeaving,
      ActorRef<Home.ReadyToLeaveHome> replyTo,
      ActorRef<KeyCabinet.GetKeys> keyCabinet,
      ActorRef<Drawer.GetWallet> drawer) {
    super(context);
    this.whoIsLeaving = whoIsLeaving;
    this.replyTo = replyTo;
    this.keyCabinet = keyCabinet;
    this.drawer = drawer;
  }

  @Override
  public Receive<Object> createReceive() {
    return newReceiveBuilder()
        .onMessage(Wallet.class, this::onWallet)
        .onMessage(Keys.class, this::onKeys)
        .build();
  }

  private Behavior<Object> onWallet(Wallet wallet) {
    this.wallet = Optional.of(wallet);
    return completeOrContinue();
  }

  private Behavior<Object> onKeys(Keys keys) {
    this.keys = Optional.of(keys);
    return completeOrContinue();
  }

  private Behavior<Object> completeOrContinue() {
    if (wallet.isPresent() && keys.isPresent()) {
      replyTo.tell(new Home.ReadyToLeaveHome(whoIsLeaving, keys.get(), wallet.get()));
      return Behaviors.stopped();
    } else {
      return this;
    }
  }
}




In an actual session child you would likely want to include some form of timeout as well (see 
scheduling messages to self
).


Useful when:




A single incoming request should result in multiple interactions with other actors before a result can be built,  for example aggregation of several results


You need to handle acknowledgement and retry messages for at-least-once delivery




Problems:




Children have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped


It increases complexity, since each such child can execute concurrently with other children and the parent




General purpose response aggregator


This is similar to above 
Per session child Actor
 pattern. Sometimes you might end up repeating the same way of aggregating replies and want to extract that to a reusable actor.


There are many variations of this pattern and that is the reason this is provided as a documentation example rather than a built in 
Behavior
Behavior
 in Akka. It is intended to be adjusted to your specific needs.


Example:




This example is an aggregator of expected number of replies. Requests for quotes are sent with the given 
sendRequests
 function to the two hotel actors, which both speak different protocols. When both expected replies have been collected they are aggregated with the given 
aggregateReplies
 function and sent back to the 
replyTo
. If replies don’t arrive within the 
timeout
 the replies so far are aggregated and sent back to the 
replyTo
.




Scala




copy
source
object Hotel1 {
  final case class RequestQuote(replyTo: ActorRef[Quote])
  final case class Quote(hotel: String, price: BigDecimal)
}
object Hotel2 {
  final case class RequestPrice(replyTo: ActorRef[Price])
  final case class Price(hotel: String, price: BigDecimal)
}

// Any since no common type between Hotel1 and Hotel2
type Reply = Any

object HotelCustomer {
  sealed trait Command
  final case class Quote(hotel: String, price: BigDecimal)
  final case class AggregatedQuotes(quotes: List[Quote]) extends Command

  def apply(hotel1: ActorRef[Hotel1.RequestQuote], hotel2: ActorRef[Hotel2.RequestPrice]): Behavior[Command] = {

    Behaviors.setup[Command] { context =>
      context.spawnAnonymous(
        Aggregator[Reply, AggregatedQuotes](
          sendRequests = { replyTo =>
            hotel1 ! Hotel1.RequestQuote(replyTo)
            hotel2 ! Hotel2.RequestPrice(replyTo)
          },
          expectedReplies = 2,
          context.self,
          aggregateReplies = replies =>
            // The hotels have different protocols with different replies,
            // convert them to `HotelCustomer.Quote` that this actor understands.
            AggregatedQuotes(
              replies
                .map {
                  case Hotel1.Quote(hotel, price) => Quote(hotel, price)
                  case Hotel2.Price(hotel, price) => Quote(hotel, price)
                  case unknown                    => throw new RuntimeException(s"Unknown reply $unknown")
                }
                .sortBy(_.price)
                .toList),
          timeout = 5.seconds))

      Behaviors.receiveMessage {
        case AggregatedQuotes(quotes) =>
          context.log.info("Best {}", quotes.headOption.getOrElse("Quote N/A"))
          Behaviors.same
      }
    }
  }
}


Java




copy
source
public class Hotel1 {
  public static class RequestQuote {
    public final ActorRef<Quote> replyTo;

    public RequestQuote(ActorRef<Quote> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Quote {
    public final String hotel;
    public final BigDecimal price;

    public Quote(String hotel, BigDecimal price) {
      this.hotel = hotel;
      this.price = price;
    }
  }
}

public class Hotel2 {
  public static class RequestPrice {
    public final ActorRef<Price> replyTo;

    public RequestPrice(ActorRef<Price> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Price {
    public final String hotel;
    public final BigDecimal price;

    public Price(String hotel, BigDecimal price) {
      this.hotel = hotel;
      this.price = price;
    }
  }
}

public class HotelCustomer extends AbstractBehavior<HotelCustomer.Command> {

  interface Command {}

  public static class Quote {
    public final String hotel;
    public final BigDecimal price;

    public Quote(String hotel, BigDecimal price) {
      this.hotel = hotel;
      this.price = price;
    }
  }

  public static class AggregatedQuotes implements Command {
    public final List<Quote> quotes;

    public AggregatedQuotes(List<Quote> quotes) {
      this.quotes = quotes;
    }
  }

  public static Behavior<Command> create(
      ActorRef<Hotel1.RequestQuote> hotel1, ActorRef<Hotel2.RequestPrice> hotel2) {
    return Behaviors.setup(context -> new HotelCustomer(context, hotel1, hotel2));
  }

  public HotelCustomer(
      ActorContext<Command> context,
      ActorRef<Hotel1.RequestQuote> hotel1,
      ActorRef<Hotel2.RequestPrice> hotel2) {
    super(context);

    Consumer<ActorRef<Object>> sendRequests =
        replyTo -> {
          hotel1.tell(new Hotel1.RequestQuote(replyTo.narrow()));
          hotel2.tell(new Hotel2.RequestPrice(replyTo.narrow()));
        };

    int expectedReplies = 2;
    // Object since no common type between Hotel1 and Hotel2
    context.spawnAnonymous(
        Aggregator.create(
            Object.class,
            sendRequests,
            expectedReplies,
            context.getSelf(),
            this::aggregateReplies,
            Duration.ofSeconds(5)));
  }

  private AggregatedQuotes aggregateReplies(List<Object> replies) {
    List<Quote> quotes =
        replies.stream()
            .map(
                r -> {
                  // The hotels have different protocols with different replies,
                  // convert them to `HotelCustomer.Quote` that this actor understands.
                  if (r instanceof Hotel1.Quote) {
                    Hotel1.Quote q = (Hotel1.Quote) r;
                    return new Quote(q.hotel, q.price);
                  } else if (r instanceof Hotel2.Price) {
                    Hotel2.Price p = (Hotel2.Price) r;
                    return new Quote(p.hotel, p.price);
                  } else {
                    throw new IllegalArgumentException("Unknown reply " + r);
                  }
                })
            .sorted((a, b) -> a.price.compareTo(b.price))
            .collect(Collectors.toList());

    return new AggregatedQuotes(quotes);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(AggregatedQuotes.class, this::onAggregatedQuotes)
        .build();
  }

  private Behavior<Command> onAggregatedQuotes(AggregatedQuotes aggregated) {
    if (aggregated.quotes.isEmpty()) getContext().getLog().info("Best Quote N/A");
    else getContext().getLog().info("Best {}", aggregated.quotes.get(0));
    return this;
  }
}




The implementation of the 
Aggregator
:




Scala




copy
source
import scala.collection.immutable
import scala.concurrent.duration.FiniteDuration
import scala.reflect.ClassTag

import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors

object Aggregator {

  sealed trait Command
  private case object ReceiveTimeout extends Command
  private case class WrappedReply[R](reply: R) extends Command

  def apply[Reply: ClassTag, Aggregate](
      sendRequests: ActorRef[Reply] => Unit,
      expectedReplies: Int,
      replyTo: ActorRef[Aggregate],
      aggregateReplies: immutable.IndexedSeq[Reply] => Aggregate,
      timeout: FiniteDuration): Behavior[Command] = {
    Behaviors.setup { context =>
      context.setReceiveTimeout(timeout, ReceiveTimeout)
      val replyAdapter = context.messageAdapter[Reply](WrappedReply(_))
      sendRequests(replyAdapter)

      def collecting(replies: immutable.IndexedSeq[Reply]): Behavior[Command] = {
        Behaviors.receiveMessage {
          case WrappedReply(reply) =>
            val newReplies = replies :+ reply.asInstanceOf[Reply]
            if (newReplies.size == expectedReplies) {
              val result = aggregateReplies(newReplies)
              replyTo ! result
              Behaviors.stopped
            } else
              collecting(newReplies)

          case ReceiveTimeout =>
            val aggregate = aggregateReplies(replies)
            replyTo ! aggregate
            Behaviors.stopped
        }
      }

      collecting(Vector.empty)
    }
  }

}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.time.Duration;
import java.util.ArrayList;
import java.util.List;
import java.util.function.Consumer;
import java.util.function.Function;

public class Aggregator<Reply, Aggregate> extends AbstractBehavior<Aggregator.Command> {

  interface Command {}

  private enum ReceiveTimeout implements Command {
    INSTANCE
  }

  private class WrappedReply implements Command {
    final Reply reply;

    private WrappedReply(Reply reply) {
      this.reply = reply;
    }
  }

  public static <R, A> Behavior<Command> create(
      Class<R> replyClass,
      Consumer<ActorRef<R>> sendRequests,
      int expectedReplies,
      ActorRef<A> replyTo,
      Function<List<R>, A> aggregateReplies,
      Duration timeout) {
    return Behaviors.setup(
        context ->
            new Aggregator<R, A>(
                replyClass,
                context,
                sendRequests,
                expectedReplies,
                replyTo,
                aggregateReplies,
                timeout));
  }

  private final int expectedReplies;
  private final ActorRef<Aggregate> replyTo;
  private final Function<List<Reply>, Aggregate> aggregateReplies;
  private final List<Reply> replies = new ArrayList<>();

  private Aggregator(
      Class<Reply> replyClass,
      ActorContext<Command> context,
      Consumer<ActorRef<Reply>> sendRequests,
      int expectedReplies,
      ActorRef<Aggregate> replyTo,
      Function<List<Reply>, Aggregate> aggregateReplies,
      Duration timeout) {
    super(context);
    this.expectedReplies = expectedReplies;
    this.replyTo = replyTo;
    this.aggregateReplies = aggregateReplies;

    context.setReceiveTimeout(timeout, ReceiveTimeout.INSTANCE);

    ActorRef<Reply> replyAdapter = context.messageAdapter(replyClass, WrappedReply::new);
    sendRequests.accept(replyAdapter);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(WrappedReply.class, this::onReply)
        .onMessage(ReceiveTimeout.class, notUsed -> onReceiveTimeout())
        .build();
  }

  private Behavior<Command> onReply(WrappedReply wrappedReply) {
    Reply reply = wrappedReply.reply;
    replies.add(reply);
    if (replies.size() == expectedReplies) {
      Aggregate result = aggregateReplies.apply(replies);
      replyTo.tell(result);
      return Behaviors.stopped();
    } else {
      return this;
    }
  }

  private Behavior<Command> onReceiveTimeout() {
    Aggregate result = aggregateReplies.apply(replies);
    replyTo.tell(result);
    return Behaviors.stopped();
  }
}




Useful when:




Aggregating replies are performed in the same way at multiple places and should be extracted to a more general  purpose actor.


A single incoming request should result in multiple interactions with other actors before a result can be built,  for example aggregation of several results


You need to handle acknowledgement and retry messages for at-least-once delivery




Problems:




Message protocols with generic types are difficult since the generic types are erased in runtime


Children have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped


It increases complexity, since each such child can execute concurrently with other children and the parent




Latency tail chopping


This is a variation of above 
General purpose response aggregator
 pattern.


The goal of this algorithm is to decrease tail latencies (“chop off the tail latency”) in situations where multiple destination actors can perform the same piece of work, and where an actor may occasionally respond more slowly than expected. In this case, sending the same work request (also known as a “backup request”) to another actor results in decreased response time - because it’s less probable that multiple actors are under heavy load simultaneously. This technique is explained in depth in Jeff Dean’s presentation on 
Achieving Rapid Response Times in Large Online Services
.


There are many variations of this pattern and that is the reason this is provided as a documentation example rather than a built in 
Behavior
Behavior
 in Akka. It is intended to be adjusted to your specific needs.


Example:






Scala




copy
source
import scala.concurrent.duration.FiniteDuration
import scala.reflect.ClassTag

import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors

object TailChopping {

  sealed trait Command
  private case object RequestTimeout extends Command
  private case object FinalTimeout extends Command
  private case class WrappedReply[R](reply: R) extends Command

  def apply[Reply: ClassTag](
      sendRequest: (Int, ActorRef[Reply]) => Boolean,
      nextRequestAfter: FiniteDuration,
      replyTo: ActorRef[Reply],
      finalTimeout: FiniteDuration,
      timeoutReply: Reply): Behavior[Command] = {
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        val replyAdapter = context.messageAdapter[Reply](WrappedReply(_))

        def waiting(requestCount: Int): Behavior[Command] = {
          Behaviors.receiveMessage {
            case WrappedReply(reply) =>
              replyTo ! reply.asInstanceOf[Reply]
              Behaviors.stopped

            case RequestTimeout =>
              sendNextRequest(requestCount + 1)

            case FinalTimeout =>
              replyTo ! timeoutReply
              Behaviors.stopped
          }
        }

        def sendNextRequest(requestCount: Int): Behavior[Command] = {
          if (sendRequest(requestCount, replyAdapter)) {
            timers.startSingleTimer(RequestTimeout, nextRequestAfter)
          } else {
            timers.startSingleTimer(FinalTimeout, finalTimeout)
          }
          waiting(requestCount)
        }

        sendNextRequest(1)
      }
    }
  }

}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import akka.actor.typed.javadsl.TimerScheduler;
import java.time.Duration;
import java.util.function.BiFunction;

public class TailChopping<Reply> extends AbstractBehavior<TailChopping.Command> {

  interface Command {}

  private enum RequestTimeout implements Command {
    INSTANCE
  }

  private enum FinalTimeout implements Command {
    INSTANCE
  }

  private class WrappedReply implements Command {
    final Reply reply;

    private WrappedReply(Reply reply) {
      this.reply = reply;
    }
  }

  public static <R> Behavior<Command> create(
      Class<R> replyClass,
      BiFunction<Integer, ActorRef<R>, Boolean> sendRequest,
      Duration nextRequestAfter,
      ActorRef<R> replyTo,
      Duration finalTimeout,
      R timeoutReply) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(
                timers ->
                    new TailChopping<R>(
                        replyClass,
                        context,
                        timers,
                        sendRequest,
                        nextRequestAfter,
                        replyTo,
                        finalTimeout,
                        timeoutReply)));
  }

  private final TimerScheduler<Command> timers;
  private final BiFunction<Integer, ActorRef<Reply>, Boolean> sendRequest;
  private final Duration nextRequestAfter;
  private final ActorRef<Reply> replyTo;
  private final Duration finalTimeout;
  private final Reply timeoutReply;
  private final ActorRef<Reply> replyAdapter;

  private int requestCount = 0;

  private TailChopping(
      Class<Reply> replyClass,
      ActorContext<Command> context,
      TimerScheduler<Command> timers,
      BiFunction<Integer, ActorRef<Reply>, Boolean> sendRequest,
      Duration nextRequestAfter,
      ActorRef<Reply> replyTo,
      Duration finalTimeout,
      Reply timeoutReply) {
    super(context);
    this.timers = timers;
    this.sendRequest = sendRequest;
    this.nextRequestAfter = nextRequestAfter;
    this.replyTo = replyTo;
    this.finalTimeout = finalTimeout;
    this.timeoutReply = timeoutReply;

    replyAdapter = context.messageAdapter(replyClass, WrappedReply::new);

    sendNextRequest();
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(WrappedReply.class, this::onReply)
        .onMessage(RequestTimeout.class, notUsed -> onRequestTimeout())
        .onMessage(FinalTimeout.class, notUsed -> onFinalTimeout())
        .build();
  }

  private Behavior<Command> onReply(WrappedReply wrappedReply) {
    Reply reply = wrappedReply.reply;
    replyTo.tell(reply);
    return Behaviors.stopped();
  }

  private Behavior<Command> onRequestTimeout() {
    sendNextRequest();
    return this;
  }

  private Behavior<Command> onFinalTimeout() {
    replyTo.tell(timeoutReply);
    return Behaviors.stopped();
  }

  private void sendNextRequest() {
    requestCount++;
    if (sendRequest.apply(requestCount, replyAdapter)) {
      timers.startSingleTimer(RequestTimeout.INSTANCE, RequestTimeout.INSTANCE, nextRequestAfter);
    } else {
      timers.startSingleTimer(FinalTimeout.INSTANCE, FinalTimeout.INSTANCE, finalTimeout);
    }
  }
}




Useful when:




Reducing higher latency percentiles and variations of latency are important


The “work” can be done more than once with the same result, e.g. a request to retrieve information




Problems:




Increased load since more messages are sent and “work” is performed more than once


Can’t be used when the “work” is not idempotent and must only be performed once


Message protocols with generic types are difficult since the generic types are erased in runtime


Children have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped






Scheduling messages to self


The following example demonstrates how to use timers to schedule messages to an actor. 


Example:




The 
Buncher
 actor buffers a burst of incoming messages and delivers them as a batch after a timeout or when the number of batched messages exceeds a maximum size.




Scala




copy
source
object Buncher {

  sealed trait Command
  final case class ExcitingMessage(message: String) extends Command
  final case class Batch(messages: Vector[Command])
  private case object Timeout extends Command
  private case object TimerKey

  def apply(target: ActorRef[Batch], after: FiniteDuration, maxSize: Int): Behavior[Command] = {
    Behaviors.withTimers(timers => new Buncher(timers, target, after, maxSize).idle())
  }
}

class Buncher(
    timers: TimerScheduler[Buncher.Command],
    target: ActorRef[Buncher.Batch],
    after: FiniteDuration,
    maxSize: Int) {
  import Buncher._

  private def idle(): Behavior[Command] = {
    Behaviors.receiveMessage[Command] { message =>
      timers.startSingleTimer(TimerKey, Timeout, after)
      active(Vector(message))
    }
  }

  def active(buffer: Vector[Command]): Behavior[Command] = {
    Behaviors.receiveMessage[Command] {
      case Timeout =>
        target ! Batch(buffer)
        idle()
      case m =>
        val newBuffer = buffer :+ m
        if (newBuffer.size == maxSize) {
          timers.cancel(TimerKey)
          target ! Batch(newBuffer)
          idle()
        } else
          active(newBuffer)
    }
  }
}


Java




copy
source
public class Buncher {

  public interface Command {}

  public static final class Batch {
    private final List<Command> messages;

    public Batch(List<Command> messages) {
      this.messages = Collections.unmodifiableList(messages);
    }

    public List<Command> getMessages() {
      return messages;
    }
  }

  public static final class ExcitingMessage implements Command {
    public final String message;

    public ExcitingMessage(String message) {
      this.message = message;
    }
  }

  private static final Object TIMER_KEY = new Object();

  private enum Timeout implements Command {
    INSTANCE
  }

  public static Behavior<Command> create(ActorRef<Batch> target, Duration after, int maxSize) {
    return Behaviors.withTimers(timers -> new Buncher(timers, target, after, maxSize).idle());
  }

  private final TimerScheduler<Command> timers;
  private final ActorRef<Batch> target;
  private final Duration after;
  private final int maxSize;

  private Buncher(
      TimerScheduler<Command> timers, ActorRef<Batch> target, Duration after, int maxSize) {
    this.timers = timers;
    this.target = target;
    this.after = after;
    this.maxSize = maxSize;
  }

  private Behavior<Command> idle() {
    return Behaviors.receive(Command.class)
        .onMessage(Command.class, this::onIdleCommand)
        .build();
  }

  private Behavior<Command> onIdleCommand(Command message) {
    timers.startSingleTimer(TIMER_KEY, Timeout.INSTANCE, after);
    return Behaviors.setup(context -> new Active(context, message));
  }

  private class Active extends AbstractBehavior<Command> {

    private final List<Command> buffer = new ArrayList<>();

    Active(ActorContext<Command> context, Command firstCommand) {
      super(context);
      buffer.add(firstCommand);
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(Timeout.class, message -> onTimeout())
          .onMessage(Command.class, this::onCommand)
          .build();
    }

    private Behavior<Command> onTimeout() {
      target.tell(new Batch(buffer));
      return idle(); // switch to idle
    }

    private Behavior<Command> onCommand(Command message) {
      buffer.add(message);
      if (buffer.size() == maxSize) {
        timers.cancel(TIMER_KEY);
        target.tell(new Batch(buffer));
        return idle(); // switch to idle
      } else {
        return this; // stay Active
      }
    }
  }
}




There are a few things worth noting here:




To get access to the timers you start with 
Behaviors.withTimers
Behaviors.withTimers
 that will pass a 
TimerScheduler
TimerScheduler
 instance to the function. This can be used with any type of 
Behavior
Behavior
, including 
receive
receive
, 
receiveMessage
receiveMessage
, but also 
setup
setup
 or any other behavior.


Each timer has a key and if a new timer with the same key is started, the previous is cancelled. It is guaranteed that a message from the previous timer is not received, even if it was already enqueued in the mailbox when the new timer was started.


Both periodic and single message timers are supported.


The 
TimerScheduler
 is mutable in itself, because it performs and manages the side effects of registering the scheduled tasks.


The 
TimerScheduler
 is bound to the lifecycle of the actor that owns it and is cancelled automatically when the actor is stopped.


Behaviors.withTimers
 can also be used inside 
Behaviors.supervise
Behaviors.supervise
 and it will automatically cancel the started timers correctly when the actor is restarted, so that the new incarnation will not receive scheduled messages from a previous incarnation.




Schedule periodically


Scheduling of recurring messages can have two different characteristics:




fixed-delay - The delay between sending subsequent messages will always be (at least) the given 
delay
.  Use 
startTimerWithFixedDelay
startTimerWithFixedDelay
.


fixed-rate - The frequency of execution over time will meet the given 
interval
. Use 
startTimerAtFixedRate
startTimerAtFixedRate
.




If you are uncertain of which one to use you should pick 
startTimerWithFixedDelay
.


When using 
fixed-delay
 it will not compensate the delay between messages if the scheduling is delayed longer than specified for some reason. The delay between sending subsequent messages will always be (at least) the given 
delay
. In the long run, the frequency of messages will generally be slightly lower than the reciprocal of the specified 
delay
.


Fixed-delay execution is appropriate for recurring activities that require “smoothness.” In other words, it is appropriate for activities where it is more important to keep the frequency accurate in the short run than in the long run.


When using 
fixed-rate
 it will compensate the delay for a subsequent task if the previous messages were delayed too long. In such cases, the actual sending interval will differ from the interval passed to the 
scheduleAtFixedRate
 method.


If the tasks are delayed longer than the 
interval
, the subsequent message will be sent immediately after the prior one. This also has the consequence that after long garbage collection pauses or other reasons when the JVM was suspended all “missed” tasks will execute when the process wakes up again. For example, 
scheduleAtFixedRate
 with an interval of 1 second and the process is suspended for 30 seconds will result in 30 messages being sent in rapid succession to catch up. In the long run, the frequency of execution will be exactly the reciprocal of the specified 
interval
.


Fixed-rate execution is appropriate for recurring activities that are sensitive to absolute time or where the total time to perform a fixed number of executions is important, such as a countdown timer that ticks once every second for ten seconds.
Warning


scheduleAtFixedRate
 can result in bursts of scheduled messages after long garbage collection pauses, which may in worst case cause undesired load on the system. 
scheduleWithFixedDelay
 is often preferred.


Responding to a sharded actor


When 
Akka Cluster
 is used to 
shard actors
 you need to take into account that an actor may move or get passivated.


The normal pattern for expecting a reply is to include an 
ActorRef
ActorRef
 in the message, typically a message adapter. This can be used for a sharded actor but if 
ctx.self
ctx.getSelf()
 is sent and the sharded actor is moved or passivated then the reply will sent to dead letters.


An alternative is to send the 
entityId
 in the message and have the reply sent via sharding.


Example:






Scala




copy
source
// a sharded actor that needs counter updates
object CounterConsumer {
  sealed trait Command
  final case class NewCount(count: Long) extends Command
  val TypeKey: EntityTypeKey[Command] = EntityTypeKey[Command]("example-sharded-response")
}

// a sharded counter that sends responses to another sharded actor
object Counter {
  trait Command
  case object Increment extends Command
  final case class GetValue(replyToEntityId: String) extends Command
  val TypeKey: EntityTypeKey[Command] = EntityTypeKey[Command]("example-sharded-counter")

  private def apply(): Behavior[Command] =
    Behaviors.setup { context =>
      counter(ClusterSharding(context.system), 0)
    }

  private def counter(sharding: ClusterSharding, value: Long): Behavior[Command] =
    Behaviors.receiveMessage {
      case Increment =>
        counter(sharding, value + 1)
      case GetValue(replyToEntityId) =>
        val replyToEntityRef = sharding.entityRefFor(CounterConsumer.TypeKey, replyToEntityId)
        replyToEntityRef ! CounterConsumer.NewCount(value)
        Behaviors.same
    }

}


Java




copy
source
// a sharded actor that needs counter updates
public class CounterConsumer {
  public static EntityTypeKey<Command> typeKey =
      EntityTypeKey.create(Command.class, "example-sharded-response");

  public interface Command {}

  public static class NewCount implements Command {
    public final long value;

    public NewCount(long value) {
      this.value = value;
    }
  }
}

// a sharded counter that sends responses to another sharded actor
public class Counter extends AbstractBehavior<Counter.Command> {
  public static EntityTypeKey<Command> typeKey =
      EntityTypeKey.create(Command.class, "example-sharded-counter");

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    public final String replyToEntityId;

    public GetValue(String replyToEntityId) {
      this.replyToEntityId = replyToEntityId;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(Counter::new);
  }

  private final ClusterSharding sharding;
  private int value = 0;

  private Counter(ActorContext<Command> context) {
    super(context);
    this.sharding = ClusterSharding.get(context.getSystem());
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, msg -> onIncrement())
        .onMessage(GetValue.class, this::onGetValue)
        .build();
  }

  private Behavior<Command> onIncrement() {
    value++;
    return this;
  }

  private Behavior<Command> onGetValue(GetValue msg) {
    EntityRef<CounterConsumer.Command> entityRef =
        sharding.entityRefFor(CounterConsumer.typeKey, msg.replyToEntityId);
    entityRef.tell(new CounterConsumer.NewCount(value));
    return this;
  }
}




A disadvantage is that a message adapter can’t be used so the response has to be in the protocol of the actor being responded to. Additionally the 
EntityTypeKey
EntityTypeKey
 could be included in the message if it is not known statically.


As an “alternative to the alternative”, an 
EntityRef
EntityRef
 can be included in the messages. The 
EntityRef
 transparently wraps messages in a 
ShardingEnvelope
ShardingEnvelope
 and sends them via sharding. If the target sharded entity has been passivated, it will be delivered to a new incarnation of that entity; if the target sharded entity has been moved to a different cluster node, it will be routed to that new node. If using this approach, be aware that at this time, 
a custom serializer is required
.


As with directly including the 
entityId
 and 
EntityTypeKey
 in the message, 
EntityRef
s do not support message adaptation: the response has to be in the protocol of the entity being responded to.


In some cases, it may be useful to define messages with a 
RecipientRef
RecipientRef
 which is a common supertype of 
ActorRef
ActorRef
 and 
EntityRef
. At this time, serializing a 
RecipientRef
 requires a custom serializer.














 
Actor lifecycle






Fault Tolerance 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/faq.html
Frequently Asked Questions • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions




Akka Project


Resources with Explicit Lifecycle


Actors


Cluster


Debugging


Other questions?




Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions




Akka Project


Resources with Explicit Lifecycle


Actors


Cluster


Debugging


Other questions?




Books and Videos


Example projects


Project




Akka Classic




















Frequently Asked Questions


Akka Project


Where does the name Akka come from?


It is the name of a beautiful Swedish 
mountain
 up in the northern part of Sweden called Laponia. The mountain is also sometimes called ‘The Queen of Laponia’.


Akka is also the name of a goddess in the SÃ¡mi (the native Swedish population) mythology. She is the goddess that stands for all the beauty and good in the world. The mountain can be seen as the symbol of this goddess.


Also, the name AKKA is a palindrome of the letters A and K as in Actor Kernel.


Akka is also:




the name of the goose that Nils traveled across Sweden on in 
The Wonderful Adventures of Nils
 by the Swedish writer Selma LagerlÃ¶f.


the Finnish word for ‘nasty elderly woman’ and the word for ‘elder sister’ in the Indian languages Tamil, Telugu, Kannada and Marathi.


a 
font


a town in Morocco


a near-earth asteroid




Resources with Explicit Lifecycle


Actors, ActorSystems, Materializers (for streams), all these types of objects bind resources that must be released explicitly. The reason is that Actors are meant to have a life of their own, existing independently of whether messages are currently en route to them. Therefore you should always make sure that for every creation of such an object you have a matching 
stop
, 
terminate
, or 
shutdown
 call implemented.


In particular you typically want to bind such values to immutable references, i.e. 
final ActorSystem system
 in Java or 
val system: ActorSystem
 in Scala.


JVM application or Scala REPL âhangingâ


Due to an ActorSystemâs explicit lifecycle the JVM will not exit until it is stopped. Therefore it is necessary to shutdown all ActorSystems within a running application or Scala REPL session in order to allow these processes to terminate.


Shutting down an ActorSystem will properly terminate all Actors and Materializers that were created within it.


Actors


Why OutOfMemoryError?


It can be many reasons for OutOfMemoryError. For example, in a pure push based system with message consumers that are potentially slower than corresponding message producers you must add some kind of message flow control. Otherwise messages will be queued in the consumers’ mailboxes and thereby filling up the heap memory.


Cluster


How reliable is the message delivery?


The general rule is 
at-most-once delivery
, i.e. no guaranteed delivery. Stronger reliability can be built on top, and Akka provides tools to do so.


Read more in 
Message Delivery Reliability
.


Debugging


How do I turn on debug logging?


To turn on debug logging in your actor system add the following to your configuration:


akka.loglevel = DEBUG



Read more about it in the docs for 
Logging
.


Other questions?


Do you have a question not covered here? Find out how to 
get involved in the community
 or 
set up a time
 to discuss enterprise-grade expert support from 
Akka
.














 
Licenses






Books and Videos 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/extending.html
Extending Akka • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Building an extension


Loading from configuration






Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Building an extension


Loading from configuration






Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Extending Akka


Akka extensions can be used for almost anything, they provide a way to create an instance of a class only once for the whole ActorSystem and be able to access it from anywhere. Akka features such as Cluster, Serialization and Sharding are all Akka extensions. Below is the use-case of managing an expensive database connection pool and accessing it from various places in your application.


You can choose to have your Extension loaded on-demand or at 
ActorSystem
 creation time through the Akka configuration. Details on how to make that happens are below, in the 
Loading from Configuration
 section.
Warning


Since an extension is a way to hook into Akka itself, the implementor of the extension needs to ensure the thread safety and that it is non-blocking.


Building an extension


Let’s build an extension to manage a shared database connection pool.




Scala




copy
source
class ExpensiveDatabaseConnection {
  def executeQuery(query: String): Future[Any] = ???
}


Java




copy
source
public class ExpensiveDatabaseConnection {
  public CompletionStage<Object> executeQuery(String query) {
    throw new RuntimeException("I should do a database query");
  }
  // ...
}




First create an 
Extension
Extension
, this will be created only once per ActorSystem:




Scala




copy
source
class DatabasePool(system: ActorSystem[_]) extends Extension {
  // database configuration can be loaded from config
  // from the actor system
  private val _connection = new ExpensiveDatabaseConnection()

  def connection(): ExpensiveDatabaseConnection = _connection
}


Java




copy
source
public class DatabaseConnectionPool implements Extension {

  private final ExpensiveDatabaseConnection _connection;

  private DatabaseConnectionPool(ActorSystem<?> system) {
    // database configuration can be loaded from config
    // from the actor system
    _connection = new ExpensiveDatabaseConnection();
  }

  public ExpensiveDatabaseConnection connection() {
    return _connection;
  }
}




This is the public API of your extension. Internally in this example we instantiate our expensive database connection. 


Then create an 
ExtensionId
ExtensionId
 to identify the extension. 
A good convention is to let the companion object of the 
Extension
 be the 
ExtensionId
.
A good convention is to define the 
ExtensionId
 as a static inner class of the 
Extension
.




Scala




copy
source
object DatabasePool extends ExtensionId[DatabasePool] {
  // will only be called once
  def createExtension(system: ActorSystem[_]): DatabasePool = new DatabasePool(system)

  // Java API
  def get(system: ActorSystem[_]): DatabasePool = apply(system)
}


Java




copy
source
public static class Id extends ExtensionId<DatabaseConnectionPool> {

  private static final Id instance = new Id();

  private Id() {}

  // called once per ActorSystem
  @Override
  public DatabaseConnectionPool createExtension(ActorSystem<?> system) {
    return new DatabaseConnectionPool(system);
  }

  public static DatabaseConnectionPool get(ActorSystem<?> system) {
    return instance.apply(system);
  }
}




Then finally to use the extension it can be looked up:




Scala




copy
source
Behaviors.setup[Any] { ctx =>
  DatabasePool(ctx.system).connection().executeQuery("insert into...")
  initialBehavior
}


Java




copy
source
Behaviors.setup(
    (context) -> {
      DatabaseConnectionPool.Id.get(context.getSystem())
          .connection()
          .executeQuery("insert into...");
      return initialBehavior();
    });




The 
DatabaseConnectionPool
 can be looked up in this way any number of times and it will return the same instance.




Loading from configuration


Loading an extension from configuration is optional. It is an optimization and can be used to eagerly load the extension when the ActorSystem is started. If not done from configuration, the extension is instantiated and registered the first time it is accessed.


To be able to load extensions from your Akka configuration you must add FQCNs of implementations of the 
ExtensionId
 in the 
akka.actor.typed.extensions
 section of the config you provide to your 
ActorSystem
.




Scala




copy
source
akka.actor.typed.extensions = ["docs.akka.extensions.DatabasePool"]


Java


ruby
   akka.actor.typed {
 extensions = ["jdocs.akka.extensions.ExtensionDocTest$DatabaseConnectionPool$Id"]
   }

















 
Futures patterns






Other Akka modules 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-dependencies/current/
Akka module versions 







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka module versions 





Version 24.10.6





sbt
Maven
Gradle










Repository


Akka (core) 2.10.0


Akka gRPC 2.5.0


Akka HTTP 10.7.0


Akka Persistence plugin for Cassandra 1.3.0


Akka Persistence plugin for DynamoDB 2.0.5


Akka Persistence plugin for JDBC 5.5.0


Akka Persistence plugin for R2DBC 1.3.1


Akka Projections 1.6.7


Akka Management 1.6.0


Akka Diagnostics 2.2.0


Alpakka 9.0.0


Alpakka Kafka 7.0.0


Akka Edge Rust 0.8.0


Akka Insights


Architecture


Certified Java versions


Support terminology


















Akka module versions 





Version 24.10.6





sbt
Maven
Gradle












Repository


Akka (core) 2.10.0


Akka gRPC 2.5.0


Akka HTTP 10.7.0


Akka Persistence plugin for Cassandra 1.3.0


Akka Persistence plugin for DynamoDB 2.0.5


Akka Persistence plugin for JDBC 5.5.0


Akka Persistence plugin for R2DBC 1.3.1


Akka Projections 1.6.7


Akka Management 1.6.0


Akka Diagnostics 2.2.0


Alpakka 9.0.0


Alpakka Kafka 7.0.0


Akka Edge Rust 0.8.0


Akka Insights


Architecture


Certified Java versions


Support terminology




















Akka module versions 24.10


This table lists all Akka modules that are part of the Akka 24.10 release along with their current versions.


Akka 24.10.6 is cross-built for Scala 2.13 and Scala 3.3. Akka is certified for use with certain Java versions, see 
Java Versions
.


Some modules in Akka do not live up to the high standards we require to fully support them for Akka Licensees. Modules are marked with their 
readiness level
 in the “project info” section of their documentation.


Akka is licensed under the Business Source License 1.1, please see 
Akka License FAQ
.








Module 


Current version 


Documentation 










Akka Dependencies BOM 


24.10.6 


 






Akka (core) 


2.10.0 


Documentation
 
JavaDoc
 
ScalaDoc
 






Akka Actors 


2.10.0 


Documentation
 
JavaDoc
 
ScalaDoc
 






Akka Cluster 


2.10.0 


Documentation
 
JavaDoc
 
ScalaDoc
 






Akka Event Sourcing 


2.10.0 


Documentation
 
JavaDoc
 
ScalaDoc
 






Akka Durable State 


2.10.0 


Documentation
 
JavaDoc
 
ScalaDoc
 






Akka Streams 


2.10.0 


Documentation
 
Operators
 
JavaDoc
 
ScalaDoc
 






Akka gRPC 


2.5.0 


Documentation
 
ScalaDoc
 






Akka HTTP 


10.7.0 


Documentation
 
Predefined Directives
 
JavaDoc
 
ScalaDoc
 






Akka Persistence plugin for Cassandra 


1.3.0 


Documentation
 






Akka Persistence plugin for JDBC 


5.5.0 


Documentation
 






Akka Persistence plugin for R2DBC 


1.3.1 


Documentation
 






Akka Persistence plugin for DynamoDB 


2.0.5 


Documentation
 






Akka Projections 


1.6.7 


Documentation
 






Akka Management 


1.6.0 


Documentation
 






Akka Diagnostics 


2.2.0 


Documentation
 






Alpakka 


9.0.0 


Documentation
 






Alpakka Kafka 


7.0.0 


Documentation
 






Akka Edge Rust 


0.8.0 


Guide
 
API
 








Repository


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Akka (core) 2.10.0


At the core of Akka: A model for concurrency and distribution without all the pain of threading primitives.


Documentation
 
JavaDoc
 
ScalaDoc
 
Migration guides


Complete dependency listing of Akka core modules


Check the 
documentation
 to learn which dependencies you require. 
Maven
<properties>
  <akka.version>2.10.0</akka.version>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-tools_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-distributed-data_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-multi-node-testkit_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
    <scope>test</scope>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-tck_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-query_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-protobuf-v3_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-remote_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-testkit-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
    <scope>test</scope>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-slf4j_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
    <scope>test</scope>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-testkit_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-testkit_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-coordination_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-metrics_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding-typed_${scala.binary.version}</artifactId>
    <version>${akka.version}</version>
  </dependency>
</dependencies>
sbt
val AkkaVersion = "2.10.0"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster-tools" % AkkaVersion,
  "com.typesafe.akka" %% "akka-discovery" % AkkaVersion,
  "com.typesafe.akka" %% "akka-distributed-data" % AkkaVersion,
  "com.typesafe.akka" %% "akka-multi-node-testkit" % AkkaVersion % Test,
  "com.typesafe.akka" %% "akka-persistence" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-tck" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-query" % AkkaVersion,
  "com.typesafe.akka" %% "akka-protobuf-v3" % AkkaVersion,
  "com.typesafe.akka" %% "akka-remote" % AkkaVersion,
  "com.typesafe.akka" %% "akka-actor-testkit-typed" % AkkaVersion % Test,
  "com.typesafe.akka" %% "akka-slf4j" % AkkaVersion,
  "com.typesafe.akka" %% "akka-stream" % AkkaVersion % Test,
  "com.typesafe.akka" %% "akka-stream-testkit" % AkkaVersion,
  "com.typesafe.akka" %% "akka-stream-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-testkit" % AkkaVersion,
  "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-coordination" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster-metrics" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster-sharding" % AkkaVersion,
  "com.typesafe.akka" %% "akka-cluster-sharding-typed" % AkkaVersion
)
Gradle
def versions = [
  AkkaVersion: "2.10.0",
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.typesafe.akka:akka-actor_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster-tools_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-distributed-data_${versions.ScalaBinary}:${versions.AkkaVersion}"
  testImplementation "com.typesafe.akka:akka-multi-node-testkit_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-persistence_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-persistence-tck_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-persistence-query_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-protobuf-v3_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-remote_${versions.ScalaBinary}:${versions.AkkaVersion}"
  testImplementation "com.typesafe.akka:akka-actor-testkit-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-slf4j_${versions.ScalaBinary}:${versions.AkkaVersion}"
  testImplementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-stream-testkit_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-stream-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-testkit_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-coordination_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster-metrics_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster-sharding_${versions.ScalaBinary}:${versions.AkkaVersion}"
  implementation "com.typesafe.akka:akka-cluster-sharding-typed_${versions.ScalaBinary}:${versions.AkkaVersion}"
}


Akka Actors


The Actor Model provides a higher level of abstraction for writing concurrent and distributed systems. It alleviates the developer from having to deal with explicit locking and thread management, making it easier to write correct concurrent and parallel systems.


Documentation
 
JavaDoc
 
ScalaDoc


Akka Cluster


Akka Cluster provides a fault-tolerant decentralized peer-to-peer based Cluster Membership Service with no single point of failure or single point of bottleneck. It does this using gossip protocols and an automatic failure detector.


Documentation
 
JavaDoc
 
ScalaDoc


Akka Event Sourcing


Akka Persistence enables stateful actors to persist their state so that it can be recovered when an actor is either restarted, such as after a JVM crash, by a supervisor or a manual stop-start, or migrated within a cluster.


Documentation
 
JavaDoc
 
ScalaDoc


Akka Durable State


Enables stateful actors to persist their latest state, so that it can be recovered when an actor is restarted.


Documentation
 
JavaDoc
 
ScalaDoc


Akka Streams


An intuitive and safe way to do asynchronous, non-blocking backpressured stream processing.


Documentation
 
Operators
 


JavaDoc
 
ScalaDoc


Akka gRPC 2.5.0


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams and Akka HTTP.


Documentation
 
ScalaDoc




Maven




<project>
  <modelVersion>4.0.0</modelVersion>
  <name>Project name</name>
  <groupId>com.example</groupId>
  <artifactId>my-grpc-app</artifactId>
  <version>0.1-SNAPSHOT</version>
  <properties>
    <akka.grpc.version>2.5.0</akka.grpc.version>
    <project.encoding>UTF-8</project.encoding>
  </properties>
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
  <pluginRepositories>
    <pluginRepository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </pluginRepository>
  </pluginRepositories>
  <dependencies>
    <dependency>
      <groupId>com.lightbend.akka.grpc</groupId>
      <artifactId>akka-grpc-runtime_2.13</artifactId>
      <version>${akka.grpc.version}</version>
    </dependency>
  </dependencies>
  <build>
    <plugins>
      <plugin>
        <groupId>com.lightbend.akka.grpc</groupId>
        <artifactId>akka-grpc-maven-plugin</artifactId>
        <version>${akka.grpc.version}</version>
        <executions>
          <execution>
            <goals>
              <goal>generate</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
</project>



sbt




// in project/plugins.sbt:
resolvers += "Akka library repository".at("https://repo.akka.io/maven")
addSbtPlugin("com.lightbend.akka.grpc" % "sbt-akka-grpc" % "2.5.0")
//
// in build.sbt:
resolvers += "Akka library repository".at("https://repo.akka.io/maven")
enablePlugins(AkkaGrpcPlugin)



Gradle




buildscript {
  repositories {
    gradlePluginPortal()
    maven {
      url "https://repo.akka.io/maven"
    }
  }
}
plugins {
  id 'java'
  id 'application'
  id 'com.lightbend.akka.grpc.gradle' version '2.5.0'
}
repositories {
  mavenCentral()
  maven {
    url "https://repo.akka.io/maven"
  }
}





Akka HTTP 10.7.0


The Akka HTTP modules implement a full server- and client-side HTTP stack on top of Akka Actors and Akka Streams.


Documentation
 
Predefined Directives
 
Migration guides


JavaDoc
 
ScalaDoc


Complete dependency listing of Akka HTTP modules


Check the 
documentation
 to learn which dependencies you require.
Maven
<properties>
  <akka.http.version>10.7.0</akka.http.version>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http-core_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http-jackson_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http-spray-json_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http-testkit_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
    <scope>test</scope>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-http-xml_${scala.binary.version}</artifactId>
    <version>${akka.http.version}</version>
  </dependency>
</dependencies>
sbt
val AkkaHttpVersion = "10.7.0"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-http" % AkkaHttpVersion,
  "com.typesafe.akka" %% "akka-http-core" % AkkaHttpVersion,
  "com.typesafe.akka" %% "akka-http-jackson" % AkkaHttpVersion,
  "com.typesafe.akka" %% "akka-http-spray-json" % AkkaHttpVersion,
  "com.typesafe.akka" %% "akka-http-testkit" % AkkaHttpVersion % Test,
  "com.typesafe.akka" %% "akka-http-xml" % AkkaHttpVersion
)
Gradle
def versions = [
  AkkaHttpVersion: "10.7.0",
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.typesafe.akka:akka-http_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
  implementation "com.typesafe.akka:akka-http-core_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
  implementation "com.typesafe.akka:akka-http-jackson_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
  implementation "com.typesafe.akka:akka-http-spray-json_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
  testImplementation "com.typesafe.akka:akka-http-testkit_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
  implementation "com.typesafe.akka:akka-http-xml_${versions.ScalaBinary}:${versions.AkkaHttpVersion}"
}


Akka Persistence plugin for Cassandra 1.3.0


The Akka Persistence Cassandra plugin allows for using Apache Cassandra as a backend for 
Akka Persistence
 and 
Akka Persistence Query
.


Documentation
 
Migration guides
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-cassandra_${scala.binary.version}</artifactId>
    <version>1.3.0</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.typesafe.akka" %% "akka-persistence-cassandra" % "1.3.0"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.typesafe.akka:akka-persistence-cassandra_${versions.ScalaBinary}:1.3.0"
}


Akka Persistence plugin for DynamoDB 2.0.5


The Akka Persistence DynamoDB plugin allows for using AWS DynamoDB as a backend for 
Akka Persistence
.


Documentation
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka</groupId>
    <artifactId>akka-persistence-dynamodb_${scala.binary.version}</artifactId>
    <version>2.0.5</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.lightbend.akka" %% "akka-persistence-dynamodb" % "2.0.5"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka:akka-persistence-dynamodb_${versions.ScalaBinary}:2.0.5"
}


Akka Persistence plugin for JDBC 5.5.0


The Akka Persistence JDBC plugin allows for using JDBC-compliant databases as backend for 
Akka Persistence
 and 
Akka Persistence Query
.


Documentation
 
Migration guides
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka</groupId>
    <artifactId>akka-persistence-jdbc_${scala.binary.version}</artifactId>
    <version>5.5.0</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.lightbend.akka" %% "akka-persistence-jdbc" % "5.5.0"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka:akka-persistence-jdbc_${versions.ScalaBinary}:5.5.0"
}


Akka Persistence plugin for R2DBC 1.3.1


The Akka Persistence R2DBC plugin allows for using SQL database with R2DBC as a backend for 
Akka Persistence
 and 
Akka Persistence Query
.


Documentation
 
Migration guides
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka</groupId>
    <artifactId>akka-persistence-r2dbc_${scala.binary.version}</artifactId>
    <version>1.3.1</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.lightbend.akka" %% "akka-persistence-r2dbc" % "1.3.1"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka:akka-persistence-r2dbc_${versions.ScalaBinary}:1.3.1"
}


Akka Projections 1.6.7


Akka Projections is intended for implementing Command Query Responsibility Segregation (CQRS) and Service to service communication.


Documentation
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka</groupId>
    <artifactId>akka-projection-core_${scala.binary.version}</artifactId>
    <version>1.6.7</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.lightbend.akka" %% "akka-projection-core" % "1.6.7"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka:akka-projection-core_${versions.ScalaBinary}:1.6.7"
}


Akka Management 1.6.0


Akka Management is a suite of tools for operating Akka Clusters.


Documentation
 
Migration guides


Complete dependency listing of Akka Management modules


Check the 
documentation
 to learn which dependencies you require.
Maven
<properties>
  <akka.management.version>1.6.0</akka.management.version>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka.management</groupId>
    <artifactId>akka-management_${scala.binary.version}</artifactId>
    <version>${akka.management.version}</version>
  </dependency>
  <dependency>
    <groupId>com.lightbend.akka.management</groupId>
    <artifactId>akka-management-cluster-http_${scala.binary.version}</artifactId>
    <version>${akka.management.version}</version>
  </dependency>
  <dependency>
    <groupId>com.lightbend.akka.management</groupId>
    <artifactId>akka-management-cluster-bootstrap_${scala.binary.version}</artifactId>
    <version>${akka.management.version}</version>
  </dependency>
  <dependency>
    <groupId>com.lightbend.akka.management</groupId>
    <artifactId>akka-rolling-update-kubernetes_${scala.binary.version}</artifactId>
    <version>${akka.management.version}</version>
  </dependency>
  <dependency>
    <groupId>com.lightbend.akka.discovery</groupId>
    <artifactId>akka-discovery-kubernetes-api_${scala.binary.version}</artifactId>
    <version>${akka.management.version}</version>
  </dependency>
</dependencies>
sbt
val AkkaManagementVersion = "1.6.0"
libraryDependencies ++= Seq(
  "com.lightbend.akka.management" %% "akka-management" % AkkaManagementVersion,
  "com.lightbend.akka.management" %% "akka-management-cluster-http" % AkkaManagementVersion,
  "com.lightbend.akka.management" %% "akka-management-cluster-bootstrap" % AkkaManagementVersion,
  "com.lightbend.akka.management" %% "akka-rolling-update-kubernetes" % AkkaManagementVersion,
  "com.lightbend.akka.discovery" %% "akka-discovery-kubernetes-api" % AkkaManagementVersion
)
Gradle
def versions = [
  AkkaManagementVersion: "1.6.0",
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka.management:akka-management_${versions.ScalaBinary}:${versions.AkkaManagementVersion}"
  implementation "com.lightbend.akka.management:akka-management-cluster-http_${versions.ScalaBinary}:${versions.AkkaManagementVersion}"
  implementation "com.lightbend.akka.management:akka-management-cluster-bootstrap_${versions.ScalaBinary}:${versions.AkkaManagementVersion}"
  implementation "com.lightbend.akka.management:akka-rolling-update-kubernetes_${versions.ScalaBinary}:${versions.AkkaManagementVersion}"
  implementation "com.lightbend.akka.discovery:akka-discovery-kubernetes-api_${versions.ScalaBinary}:${versions.AkkaManagementVersion}"
}


Akka Diagnostics 2.2.0


The Akka Thread Starvation Detector is a diagnostic tool that monitors the dispatcher of an ActorSystem and will log a warning if the dispatcher becomes unresponsive. The Config Checker tries to help you by finding potential configuration issues.


Documentation
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.lightbend.akka</groupId>
    <artifactId>akka-diagnostics_${scala.binary.version}</artifactId>
    <version>2.2.0</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.lightbend.akka" %% "akka-diagnostics" % "2.2.0"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.lightbend.akka:akka-diagnostics_${versions.ScalaBinary}:2.2.0"
}


Alpakka 9.0.0


The Alpakka project is an open source initiative to implement stream-aware and reactive integration pipelines for Java and Scala. It is built on top of Akka Streams.


Documentation


Alpakka Cassandra


Alpakka Cassandra offers an Akka Streams API on top of a 
CqlSession
 from the Datastax Java Driver version 4.0+.


Documentation


Alpakka Comma-separated files (CSV)


Comma-Separated Values are used as interchange format for tabular data of text.


Documentation


Alpakka Kafka 7.0.0


Alpakka Kafka lets you connect Apache Kafka to Akka Streams.


Documentation
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream-kafka_${scala.binary.version}</artifactId>
    <version>7.0.0</version>
  </dependency>
</dependencies>
sbt
libraryDependencies += "com.typesafe.akka" %% "akka-stream-kafka" % "7.0.0"
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation "com.typesafe.akka:akka-stream-kafka_${versions.ScalaBinary}:7.0.0"
}


Akka Edge Rust 0.8.0


Incubating


Extending the power of Akkaâs event-driven model to resource-constrained devices.


Akka Edge support in Rust is designed to empower Akka developers at the edge. Recognizing the resource constraints present at the edge, Akka Edge Rust enables event-driven solutions for developers familiar with Akka Persistence and Projections.


Guide


API


Akka Insights


Akka Insights
 provides a view into the distributed applications at runtime. This view allows developers and operations to respond quickly to problems, track down unexpected behavior and tune the system.








Akka 


Compatible Akka Insights 


 










Akka 24.10 


Akka Insights version 2.21 


Documentation
 






Akka 24.05 


Lightbend Telemetry version 2.20 


Documentation
 






Akka 23.10 


Lightbend Telemetry version 2.19 


Documentation
 






Akka 23.05 


Lightbend Telemetry version 2.18 


Documentation
 






Akka 22.10 (and older) 


Lightbend Telemetry version 2.17 


Documentation
 








Architecture


Microservices


Tutorial bringing together essential parts of Akka to build resilient and scalable microservices. 
Akka Guide


Security: assuming Zero Trust


Learn how the Zero Trust approach helps you to build secure systems with Akka. 
Akka Guide


Distributed Cluster


Connect Akka services across geographical locations for lower latency and higher availability. 
Akka Distributed Cluster


Edge


Move your endpoints to the edge of the cloud for lower latency and higher availability. 
Akka Edge Rust
 extends the power of Akkaâs event-driven model to resource-constrained devices. 
Akka Edge


















Certified Java versions 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.




















© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Listing
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-cluster.html
Classic Clustering • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering




Classic Cluster Usage


Classic Cluster Aware Routers


Classic Cluster Singleton


Classic Distributed Publish Subscribe in Cluster


Classic Cluster Sharding


Classic Cluster Metrics Extension


Classic Distributed Data


Classic Serialization




Classic Networking


Classic Utilities




















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering




Classic Cluster Usage


Classic Cluster Aware Routers


Classic Cluster Singleton


Classic Distributed Publish Subscribe in Cluster


Classic Cluster Sharding


Classic Cluster Metrics Extension


Classic Distributed Data


Classic Serialization




Classic Networking


Classic Utilities






















Classic Clustering
Note


Akka Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Akka Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see 
coexistence
. For new projects we recommend using 
the new Actor API
.


For the new API see 
Cluster
. 






Classic Cluster Usage




Module info


When and where to use Akka Cluster


Cluster API Extension


Cluster Membership API


Leaving


Downing


Subscribe to Cluster Events


Node Roles


How To Startup when Cluster Size Reached


How To Startup when Member is Up


How To Cleanup when Member is Removed


Higher level Cluster tools


Failure Detector


How to Test


Management


Configuration




Classic Cluster Aware Routers




Dependency


Router with Group of Routees


Router with Pool of Remote Deployed Routees




Classic Cluster Singleton




Module info


Introduction


An Example


Configuration


Supervision


Lease




Classic Distributed Publish Subscribe in Cluster




Module info


Introduction


Publish


Send


DistributedPubSub Extension


Delivery Guarantee




Classic Cluster Sharding




Module info


Introduction


Basic example


How it works


Sharding State Store Mode


Proxy Only Mode


Passivation


Remembering Entities


Supervision


Graceful Shutdown


Removal of Internal Cluster Sharding Data


Inspecting cluster sharding state


Lease


Configuration




Classic Cluster Metrics Extension




Module info


Introduction


Metrics Collector


Metrics Events


Hyperic Sigar Provisioning


Adaptive Load Balancing


Subscribe to Metrics Events


Custom Metrics Collector


Configuration




Classic Distributed Data




Dependency


Introduction


Using the Replicator


Replicated data types


Durable Storage


Limitations


Learn More about CRDTs


Configuration




Classic Serialization




Dependency


Serializing ActorRefs




















 
Testing Classic Actors






Classic Cluster Usage 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/distributed-pub-sub.html
Distributed Publish Subscribe in Cluster • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster




Module info


The Topic Registry


The Topic Actor


Pub Sub Scalability


Delivery Guarantee




Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster




Module info


The Topic Registry


The Topic Actor


Pub Sub Scalability


Delivery Guarantee




Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Distributed Publish Subscribe in Cluster


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Distributed Publish Subscribe
.


Module info


The distributed publish subscribe topic API is available and usable with the core 
akka-actor-typed
 module, however it will only be distributed when used in a clustered application.


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}


The Topic Registry


Distributed publish subscribe is achieved by representing each pub sub topic with an actor, 
akka.actor.typed.pubsub.Topic
akka.actor.typed.pubsub.Topic
.


The topic actor needs to run on each node where subscribers will live or that wants to publish messages to the topic. 


Topics can be looked up in the 
PubSub
PubSub
 registry, this way the same topic will be represented by the same actor for a whole actor system. If the topic has not yet been started it is started and returned, if it already is running, the existing 
ActorRef
 is returned.




Scala




copy
source
import akka.actor.typed.pubsub.Topic
import akka.actor.typed.pubsub.PubSub

Behaviors.setup { context =>
  val pubSub = PubSub(context.system)

  val topic: ActorRef[Topic.Command[Message]] = pubSub.topic[Message]("my-topic")


Java




copy
source
import akka.actor.typed.pubsub.PubSub;
import akka.actor.typed.pubsub.Topic;

import java.time.Duration;

Behaviors.setup(
    context -> {
      PubSub pubSub = PubSub.get(context.getSystem());

      ActorRef<Topic.Command<Message>> topic =
          pubSub.topic(Message.class, "my-topic");




The identity of the topic is the topic name and if it already has been started with a different type of message this will lead to an exception.


Local actors can then subscribe to the topic (and unsubscribe from it) via messages defined in 
Topic
Topic
:




Scala




copy
source
topic ! Topic.Subscribe(subscriberActor)

topic ! Topic.Unsubscribe(subscriberActor)


Java




copy
source
topic.tell(Topic.subscribe(subscriberActor));

topic.tell(Topic.unsubscribe(subscriberActor));




And publish messages to the topic:




Scala




copy
source
topic ! Topic.Publish(Message("Hello Subscribers!"))


Java




copy
source
topic.tell(Topic.publish(new Message("Hello Subscribers!")));




Messages published only be delivered to other nodes if the topic is started and there are any local subscribers registered with the topic there. The message is deduplicated so that even if there are multiple subscribers on a node, the message is only passed over the network to that node once.


It is possible to define a TTL (time to live) for the local topic actor, if it has no local subscribers or messages passing through for the given time period it stopped and removed from the registry:




Scala




copy
source
val topicWithTtl = pubSub.topic[Message]("my-topic", 3.minutes)


Java




copy
source
ActorRef<Topic.Command<Message>> topicWithTtl =
    pubSub.topic(Message.class, "my-ttl-topic", Duration.ofMinutes(3));




The Topic Actor


The topic actor can also be started and managed manually. This means that multiple actors for the same topic can be started on the same node. Messages published to a topic on other cluster nodes will be sent between the nodes once per active topic actor that has any local subscribers:




Scala




copy
source
import akka.actor.typed.pubsub.Topic

Behaviors.setup { context =>
  val topic = context.spawn(Topic[Message]("my-topic"), "MyTopic")


Java




copy
source
import akka.actor.typed.pubsub.Topic;

import java.time.Duration;

Behaviors.setup(
    context -> {
      ActorRef<Topic.Command<Message>> topic =
          context.spawn(Topic.create(Message.class, "my-topic"), "MyTopic");




Pub Sub Scalability


Each topic is represented by one 
Receptionist
 service key meaning that the number of topics will scale to thousands or tens of thousands but for higher numbers of topics will require custom solutions. It also means that a very high turnaround of unique topics will not work well and for such use cases a custom solution is advised.


The topic actor acts as a proxy and delegates to the local subscribers handling deduplication so that a published message is only sent once to a node regardless of how many subscribers there are to the topic on that node.


When a topic actor has no subscribers for a topic it will deregister itself from the receptionist meaning published messages for the topic will not be sent to it.


Delivery Guarantee


As in 
Message Delivery Reliability
 of Akka, message delivery guarantee in distributed pub sub modes is 
at-most-once delivery
. In other words, messages can be lost over the wire. In addition to that the registry of nodes which have subscribers is eventually consistent meaning that subscribing an actor on one node will have a short delay before it is known on other nodes and published to.


If you are looking for at-least-once delivery guarantee, we recommend 
Alpakka Kafka
.














 
Sharded Daemon Process






Reliable delivery 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-utilities.html
Utilities • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Utilities






EventStream




Dependency


Introduction


How to use


EventBus


Classifiers




Logging




Dependency


Introduction


How to log


MDC


SLF4J API compatibility


SLF4J backend


Internal logging by Akka


Logging in tests




Circuit Breaker




Why are they used?


What do they do?


Examples




Futures patterns




Dependency


After


Retry




Extending Akka




Building an extension


Loading from configuration




















 
Discovery






EventStream 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/mailboxes.html
Mailboxes • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes




Dependency


Introduction


Selecting what mailbox is used


Mailbox Implementations


Custom Mailbox type




Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes




Dependency


Introduction


Selecting what mailbox is used


Mailbox Implementations


Custom Mailbox type




Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Mailboxes


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Mailboxes
.


Dependency


Mailboxes are part of core Akka, which means that they are part of the 
akka-actor
 dependency. This page describes how to use mailboxes with 
akka-actor-typed
.


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


Each actor in Akka has a 
Mailbox
, this is where the messages are enqueued before being processed by the actor.


By default an unbounded mailbox is used, this means any number of messages can be enqueued into the mailbox. 


The unbounded mailbox is a convenient default but in a scenario where messages are added to the mailbox faster than the actor can process them, this can lead to the application running out of memory. For this reason a bounded mailbox can be specified, the bounded mailbox will pass new messages to 
deadletters
 when the mailbox is full.


For advanced use cases it is also possible to defer mailbox selection to config by pointing to a config path.


Selecting what mailbox is used


Selecting a Mailbox Type for an Actor


To select a specific mailbox for an actor use 
MailboxSelector
MailboxSelector
 to create a 
Props
Props
 instance for spawning your actor:




Scala




copy
source
context.spawn(childBehavior, "bounded-mailbox-child", MailboxSelector.bounded(100))

val props = MailboxSelector.fromConfig("my-app.my-special-mailbox")
context.spawn(childBehavior, "from-config-mailbox-child", props)


Java




copy
source
context.spawn(childBehavior, "bounded-mailbox-child", MailboxSelector.bounded(100));

context.spawn(
    childBehavior,
    "from-config-mailbox-child",
    MailboxSelector.fromConfig("my-app.my-special-mailbox"));




fromConfig
fromConfig
 takes an absolute config path to a block defining the dispatcher in the config file:


copy
source
my-app {
  my-special-mailbox {
    mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
  }
}


Default Mailbox


The default mailbox is used when the mailbox is not specified and is the 
SingleConsumerOnlyUnboundedMailbox
SingleConsumerOnlyUnboundedMailbox


Which Configuration is passed to the Mailbox Type


Each mailbox type is implemented by a class which extends 
MailboxType
MailboxType
 and takes two constructor arguments: a 
ActorSystem.Settings
ActorSystem.Settings
 object and a 
Config
 section. The latter is computed by obtaining the named configuration section from the 
ActorSystem
ActorSystem
 configuration, overriding its 
id
 key with the configuration path of the mailbox type and adding a fall-back to the default mailbox configuration section.


Mailbox Implementations


Akka ships with a number of mailbox implementations:




 
SingleConsumerOnlyUnboundedMailbox
SingleConsumerOnlyUnboundedMailbox
 (default)
    


This is the default


Backed by a Multiple-Producer Single-Consumer queue, cannot be used with 
BalancingDispatcher


Blocking: No


Bounded: No


Configuration name: 
"akka.dispatch.SingleConsumerOnlyUnboundedMailbox"






 
UnboundedMailbox
UnboundedMailbox




Backed by a 
java.util.concurrent.ConcurrentLinkedQueue


Blocking: No


Bounded: No


Configuration name: 
"unbounded"
 or 
"akka.dispatch.UnboundedMailbox"








NonBlockingBoundedMailbox
NonBlockingBoundedMailbox




Backed by a very efficient Multiple-Producer Single-Consumer queue


Blocking: No (discards overflowing messages into deadLetters)


Bounded: Yes


Configuration name: 
"akka.dispatch.NonBlockingBoundedMailbox"






 
UnboundedControlAwareMailbox
UnboundedControlAwareMailbox




Delivers messages that extend 
akka.dispatch.ControlMessage
akka.dispatch.ControlMessage
 with higher priority


Backed by two 
java.util.concurrent.ConcurrentLinkedQueue


Blocking: No


Bounded: No


Configuration name: 
"akka.dispatch.UnboundedControlAwareMailbox"






 
UnboundedPriorityMailbox
UnboundedPriorityMailbox




Backed by a 
java.util.concurrent.PriorityBlockingQueue


Delivery order for messages of equal priority is undefined - contrast with the 
UnboundedStablePriorityMailbox


Blocking: No


Bounded: No


Configuration name: 
"akka.dispatch.UnboundedPriorityMailbox"






 
UnboundedStablePriorityMailbox
UnboundedStablePriorityMailbox




Backed by a 
java.util.concurrent.PriorityBlockingQueue
 wrapped in an 
akka.util.PriorityQueueStabilizer
akka.util.PriorityQueueStabilizer


FIFO order is preserved for messages of equal priority - contrast with the 
UnboundedPriorityMailbox


Blocking: No


Bounded: No


Configuration name: 
"akka.dispatch.UnboundedStablePriorityMailbox"








Other bounded mailbox implementations which will block the sender if the capacity is reached and configured with non-zero 
mailbox-push-timeout-time
. 
Note


The following mailboxes should only be used with zero 
mailbox-push-timeout-time
.




BoundedMailbox
BoundedMailbox




Backed by a 
java.util.concurrent.LinkedBlockingQueue


Blocking: Yes if used with non-zero 
mailbox-push-timeout-time
, otherwise No


Bounded: Yes


Configuration name: 
"bounded"
 or 
"akka.dispatch.BoundedMailbox"






BoundedPriorityMailbox
BoundedPriorityMailbox




Backed by a 
java.util.PriorityQueue
 wrapped in an 
akka.util.BoundedBlockingQueue
akka.util.BoundedBlockingQueue


Delivery order for messages of equal priority is undefined - contrast with the 
BoundedStablePriorityMailbox


Blocking: Yes if used with non-zero 
mailbox-push-timeout-time
, otherwise No


Bounded: Yes


Configuration name: 
"akka.dispatch.BoundedPriorityMailbox"






BoundedStablePriorityMailbox
BoundedStablePriorityMailbox




Backed by a 
java.util.PriorityQueue
 wrapped in an 
akka.util.PriorityQueueStabilizer
akka.util.PriorityQueueStabilizer
 and an 
akka.util.BoundedBlockingQueue
akka.util.BoundedBlockingQueue


FIFO order is preserved for messages of equal priority - contrast with the BoundedPriorityMailbox


Blocking: Yes if used with non-zero 
mailbox-push-timeout-time
, otherwise No


Bounded: Yes


Configuration name: 
"akka.dispatch.BoundedStablePriorityMailbox"






BoundedControlAwareMailbox
BoundedControlAwareMailbox




Delivers messages that extend 
akka.dispatch.ControlMessage
akka.dispatch.ControlMessage
 with higher priority


Backed by two 
java.util.concurrent.ConcurrentLinkedQueue
 and blocking on enqueue if capacity has been reached


Blocking: Yes if used with non-zero 
mailbox-push-timeout-time
, otherwise No


Bounded: Yes


Configuration name: 
"akka.dispatch.BoundedControlAwareMailbox"








Custom Mailbox type


The best way to show how to create your own Mailbox type is by example:




Scala




copy
source
// Marker trait used for mailbox requirements mapping
trait MyUnboundedMessageQueueSemantics


Java




copy
source
// Marker interface used for mailbox requirements mapping
public interface MyUnboundedMessageQueueSemantics {}






Scala




copy
source
import akka.actor.ActorRef
import akka.actor.ActorSystem
import akka.dispatch.Envelope
import akka.dispatch.MailboxType
import akka.dispatch.MessageQueue
import akka.dispatch.ProducesMessageQueue
import com.typesafe.config.Config
import java.util.concurrent.ConcurrentLinkedQueue
import scala.Option

object MyUnboundedMailbox {
  // This is the MessageQueue implementation
  class MyMessageQueue extends MessageQueue with MyUnboundedMessageQueueSemantics {

    private final val queue = new ConcurrentLinkedQueue[Envelope]()

    // these should be implemented; queue used as example
    def enqueue(receiver: ActorRef, handle: Envelope): Unit =
      queue.offer(handle)
    def dequeue(): Envelope = queue.poll()
    def numberOfMessages: Int = queue.size
    def hasMessages: Boolean = !queue.isEmpty
    def cleanUp(owner: ActorRef, deadLetters: MessageQueue): Unit = {
      while (hasMessages) {
        deadLetters.enqueue(owner, dequeue())
      }
    }
  }
}

// This is the Mailbox implementation
class MyUnboundedMailbox extends MailboxType with ProducesMessageQueue[MyUnboundedMailbox.MyMessageQueue] {

  import MyUnboundedMailbox._

  // This constructor signature must exist, it will be called by Akka
  def this(settings: ActorSystem.Settings, config: Config) = {
    // put your initialization code here
    this()
  }

  // The create method is called to create the MessageQueue
  final override def create(owner: Option[ActorRef], system: Option[ActorSystem]): MessageQueue =
    new MyMessageQueue()
}


Java




copy
source
import akka.actor.ActorRef;
import akka.actor.ActorSystem;
import akka.dispatch.Envelope;
import akka.dispatch.MailboxType;
import akka.dispatch.MessageQueue;
import akka.dispatch.ProducesMessageQueue;
import com.typesafe.config.Config;
import java.util.Queue;
import java.util.concurrent.ConcurrentLinkedQueue;
import scala.Option;

public class MyUnboundedMailbox
    implements MailboxType, ProducesMessageQueue<MyUnboundedMailbox.MyMessageQueue> {

  // This is the MessageQueue implementation
  public static class MyMessageQueue implements MessageQueue, MyUnboundedMessageQueueSemantics {
    private final Queue<Envelope> queue = new ConcurrentLinkedQueue<Envelope>();

    // these must be implemented; queue used as example
    public void enqueue(ActorRef receiver, Envelope handle) {
      queue.offer(handle);
    }

    public Envelope dequeue() {
      return queue.poll();
    }

    public int numberOfMessages() {
      return queue.size();
    }

    public boolean hasMessages() {
      return !queue.isEmpty();
    }

    public void cleanUp(ActorRef owner, MessageQueue deadLetters) {
      while (!queue.isEmpty()) {
        deadLetters.enqueue(owner, dequeue());
      }
    }
  }

  // This constructor signature must exist, it will be called by Akka
  public MyUnboundedMailbox(ActorSystem.Settings settings, Config config) {
    // put your initialization code here
  }

  // The create method is called to create the MessageQueue
  public MessageQueue create(Option<ActorRef> owner, Option<ActorSystem> system) {
    return new MyMessageQueue();
  }
}




And then you specify the FQCN of your MailboxType as the value of the “mailbox-type” in the dispatcher configuration, or the mailbox configuration.
Note


Make sure to include a constructor which takes 
akka.actor.ActorSystem.Settings
akka.actor.ActorSystem.Settings
 and 
com.typesafe.config.Config
 arguments, as this constructor is invoked reflectively to construct your mailbox type. The config passed in as second argument is that section from the configuration which describes the dispatcher or mailbox setting using this mailbox type; the mailbox type will be instantiated once for each dispatcher or mailbox setting using it.














 
Dispatchers






Testing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index.html#akka-documentation
Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Akka Documentation






Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities
























Security Announcements 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/event-stream.html
EventStream • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream




Dependency


Introduction


How to use


EventBus


Classifiers




Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream




Dependency


Introduction


How to use


EventBus


Classifiers




Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















EventStream


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Event Stream
.


Dependency




The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}




Introduction


The event stream is the main 
Event Bus
 of each actor system: it is used for carrying 
log messages
 and 
Dead Letters
 and may be used by the user code for other purposes as well. 


It uses 
Subchannel Classification
 which enables registering to related sets of channels.


How to use


The following example demonstrates how a subscription works. Given an actor:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.eventstream.EventStream.Subscribe
import akka.actor.typed.scaladsl.Behaviors

object DeadLetterListener {

  def apply(): Behavior[String] = Behaviors.setup { context =>
    // subscribe DeadLetter at startup.
    val adapter = context.messageAdapter[DeadLetter](d => d.message.toString)
    context.system.eventStream ! Subscribe(adapter)

    Behaviors.receiveMessage {
      case msg: String =>
        println(msg)
        Behaviors.same
    }
  }
}
  ActorSystem(Behaviors.setup[Void] { context =>
    context.spawn(DeadLetterListener(), "DeadLetterListener", Props.empty)
    Behaviors.empty
  }, "System")


Java




copy
source
import akka.actor.DeadLetter;
import akka.actor.typed.ActorRef;
import akka.actor.typed.ActorSystem;




the actor definition like this:


copy
source
static class DeadLetterActor extends AbstractBehavior<String> {

    public static Behavior<String> create() {
        return Behaviors.setup(DeadLetterActor::new);
    }

    public DeadLetterActor(ActorContext<String> context) {
        super(context);
        ActorRef<DeadLetter> messageAdapter = context.messageAdapter(
            DeadLetter.class,
            d -> d.message().toString()
        );
        context.getSystem().eventStream()
            .tell(new Subscribe<>(DeadLetter.class, messageAdapter));
    }

    @Override
    public Receive<String> createReceive() {
        return newReceiveBuilder().onMessage(String.class, msg -> {
            System.out.println(msg);
            return Behaviors.same();
        }).build();
    }
}


it can be subscribed like this:


copy
source
ActorSystem<DeadLetter> system = ActorSystem.create(Behaviors.empty(), "DeadLetters");
system.eventStream().tell(new Subscribe<>(DeadLetter.class, system));


It is also worth pointing out that thanks to the way the subchannel classification is implemented in the event stream, it is possible to subscribe to a group of events, by subscribing to their common superclass as demonstrated in the following example:




Scala




copy
source
object ListenerActor {
  abstract class AllKindsOfMusic { def artist: String }
  case class Jazz(artist: String) extends AllKindsOfMusic
  case class Electronic(artist: String) extends AllKindsOfMusic

  def apply(): Behavior[ListenerActor.AllKindsOfMusic] = Behaviors.receive { (context, msg) =>
    msg match {
      case m: Jazz =>
        println(s"${context.self.path.name} is listening to: ${m.artist}")
        Behaviors.same
      case m: Electronic =>
        println(s"${context.self.path.name} is listening to: ${m.artist}")
        Behaviors.same
      case _ =>
        Behaviors.same
    }
  }
}

  implicit val system: ActorSystem[SpawnProtocol.Command] = ActorSystem(SpawnProtocol(), "SpawnProtocol")
  implicit val ec: ExecutionContext = system.executionContext

  val jazzListener: Future[ActorRef[Jazz]] =
    system.ask(Spawn(behavior = ListenerActor(), name = "jazz", props = Props.empty, _))
  val musicListener: Future[ActorRef[AllKindsOfMusic]] =
    system.ask(Spawn(behavior = ListenerActor(), name = "music", props = Props.empty, _))

  for (jazzListenerRef <- jazzListener; musicListenerRef <- musicListener) {
    system.eventStream ! Subscribe(jazzListenerRef)
    system.eventStream ! Subscribe(musicListenerRef)
  }

  // only musicListener gets this message, since it listens to *all* kinds of music:
  system.eventStream ! Publish(Electronic("Parov Stelar"))

  // jazzListener and musicListener will be notified about Jazz:
  system.eventStream ! Publish(Jazz("Sonny Rollins"))


Java




copy
source
interface AllKindsOfMusic {

}

class Jazz implements AllKindsOfMusic {

    public final String artist;

    public Jazz(String artist) {
        this.artist = artist;
    }
}

class Electronic implements AllKindsOfMusic {

    public final String artist;

    public Electronic(String artist) {
        this.artist = artist;
    }
}

static class Listener extends AbstractBehavior<AllKindsOfMusic> {

    public static Behavior<AllKindsOfMusic> create() {
        return Behaviors.setup(Listener::new);
    }

    public Listener(ActorContext<AllKindsOfMusic> context) {
        super(context);
    }


    @Override
    public Receive<AllKindsOfMusic> createReceive() {
        return newReceiveBuilder()
            .onMessage(Jazz.class, msg -> {
                System.out.printf("%s is listening to: %s%n",
                    getContext().getSelf().path().name(),
                    msg);
                return Behaviors.same();
            })
            .onMessage(Electronic.class, msg -> {
                System.out.printf("%s is listening to: %s%n",
                    getContext().getSelf().path().name(),
                    msg);
                return Behaviors.same();
            }).build();
    }
}
    ActorSystem<SpawnProtocol.Command> system = ActorSystem.create(SpawnProtocol.create(),
        "Subclassification");
    final Duration timeout = Duration.ofSeconds(3);

    CompletionStage<ActorRef<Jazz>> jazzListener = AskPattern.ask(
        system,
        replyTo -> new Spawn<>(Listener.create().narrow(), "jazzListener", Props.empty(), replyTo),
        timeout,
        system.scheduler()
    );

    CompletionStage<ActorRef<AllKindsOfMusic>> musicListener = AskPattern.ask(
        system,
        replyTo -> new Spawn<>(Listener.create(), "musicListener", Props.empty(), replyTo),
        timeout,
        system.scheduler()
    );

    ActorRef<Jazz> jazzListenerActorRef = jazzListener.toCompletableFuture().join();
    ActorRef<AllKindsOfMusic> musicListenerActorRef = musicListener.toCompletableFuture()
        .join();

    system.eventStream().tell(new Subscribe<>(Jazz.class, jazzListenerActorRef));
    system.eventStream().tell(new Subscribe<>(AllKindsOfMusic.class, musicListenerActorRef));
    // only musicListener gets this message, since it listens to *all* kinds of music:
    system.eventStream().tell(new Publish<>(new Electronic("Parov Stelar")));

    // jazzListener and musicListener will be notified about Jazz:
    system.eventStream().tell(new Publish<>(new Jazz("Sonny Rollins")));





Similarly to 
Actor Classification
, 
EventStream
EventStream
 will automatically remove subscribers when they terminate.
Note


The event stream is a 
local facility
, meaning that it will 
not
 distribute events to other nodes in a clustered environment (unless you subscribe a Remote Actor to the stream explicitly). If you need to broadcast events in an Akka cluster, 
without
 knowing your recipients explicitly (i.e. obtaining their ActorRefs), you may want to look into: 
The Receptionist
, 
Group Routers
 or 
Distributed Publish Subscribe in Cluster
.


Dead Letters


As described at 
Stopping actors
, messages queued when an actor terminates or sent after its death are re-routed to the dead letter mailbox, which by default will publish the messages wrapped in 
DeadLetter
DeadLetter
. This wrapper holds the original sender, receiver and message of the envelope which was redirected.


Some internal messages (marked with the 
DeadLetterSuppression
DeadLetterSuppression
 
trait
interface
) will not end up as dead letters like normal messages. These are by design safe and expected to sometimes arrive at a terminated actor and since they are nothing to worry about, they are suppressed from the default dead letters logging mechanism.


However, in case you find yourself in need of debugging these kinds of low level suppressed dead letters, it’s still possible to subscribe to them explicitly:




Scala




copy
source
import akka.actor.SuppressedDeadLetter
system.eventStream ! Subscribe[SuppressedDeadLetter](listener)


Java




copy
source
system.eventStream().tell(new Subscribe<>(SuppressedDeadLetter.class, actor));




or all dead letters (including the suppressed ones):




Scala




copy
source
import akka.actor.AllDeadLetters
system.eventStream ! Subscribe[AllDeadLetters](listener)


Java




copy
source
system.eventStream().tell(new Subscribe<>(AllDeadLetters.class, actor));




Other Uses


The event stream is always there and ready to be used, you can publish your own events (it accepts 
AnyRef
Object
) and subscribe listeners to the corresponding JVM classes.


EventBus




Originally conceived as a way to send messages to groups of actors, the 
EventBus
EventBus
 has been generalized into a set of 
composable traits
 
abstract base classes
 implementing a simple interface: 




Scala




copy
source
/**
 * Attempts to register the subscriber to the specified Classifier
 * @return true if successful and false if not (because it was already
 *   subscribed to that Classifier, or otherwise)
 */
def subscribe(subscriber: Subscriber, to: Classifier): Boolean

/**
 * Attempts to deregister the subscriber from the specified Classifier
 * @return true if successful and false if not (because it wasn't subscribed
 *   to that Classifier, or otherwise)
 */
def unsubscribe(subscriber: Subscriber, from: Classifier): Boolean

/**
 * Attempts to deregister the subscriber from all Classifiers it may be subscribed to
 */
def unsubscribe(subscriber: Subscriber): Unit

/**
 * Publishes the specified Event to this bus
 */
def publish(event: Event): Unit


Java




copy
source
/**
 * Attempts to register the subscriber to the specified Classifier
 *
 * @return true if successful and false if not (because it was already subscribed to that
 *     Classifier, or otherwise)
 */
public boolean subscribe(Subscriber subscriber, Classifier to);

/**
 * Attempts to deregister the subscriber from the specified Classifier
 *
 * @return true if successful and false if not (because it wasn't subscribed to that Classifier,
 *     or otherwise)
 */
public boolean unsubscribe(Subscriber subscriber, Classifier from);

/** Attempts to deregister the subscriber from all Classifiers it may be subscribed to */
public void unsubscribe(Subscriber subscriber);

/** Publishes the specified Event to this bus */
public void publish(Event event);





Note


Please note that the EventBus does not preserve the sender of the published messages. If you need a reference to the original sender you have to provide it inside the message.


This mechanism is used in different places within Akka, e.g. the EventStream. Implementations can make use of the specific building blocks presented below.


An event bus must define the following three 
abstract types
type parameters
:




Event
 is the type of all events published on that bus


Subscriber
 is the type of subscribers allowed to register on that event bus


Classifier
 defines the classifier to be used in selecting  subscribers for dispatching events




The traits below are still generic in these types, but they need to be defined for any concrete implementation. 


Classifiers




The classifiers presented here are part of the Akka distribution, but rolling your own in case you do not find a perfect match is not difficult, check the implementation of the existing ones on 
github
 


Lookup Classification




The simplest classification is just to extract an arbitrary classifier from each event and maintaining a set of subscribers for each possible classifier. This can be compared to tuning in on a radio station. The 
trait 
LookupClassification
abstract class 
LookupEventBus
 is still generic in that it abstracts over how to compare subscribers and how exactly to classify them.


The necessary methods to be implemented are illustrated with the following example: 




Scala




copy
source
import akka.event.EventBus
import akka.event.LookupClassification

final case class MsgEnvelope(topic: String, payload: Any)

/**
 * Publishes the payload of the MsgEnvelope when the topic of the
 * MsgEnvelope equals the String specified when subscribing.
 */
class LookupBusImpl extends EventBus with LookupClassification {
  type Event = MsgEnvelope
  type Classifier = String
  type Subscriber = ActorRef

  // is used for extracting the classifier from the incoming events
  override protected def classify(event: Event): Classifier = event.topic

  // will be invoked for each event for all subscribers which registered themselves
  // for the eventâs classifier
  override protected def publish(event: Event, subscriber: Subscriber): Unit = {
    subscriber ! event.payload
  }

  // must define a full order over the subscribers, expressed as expected from
  // `java.lang.Comparable.compare`
  override protected def compareSubscribers(a: Subscriber, b: Subscriber): Int =
    a.compareTo(b)

  // determines the initial size of the index data structure
  // used internally (i.e. the expected number of different classifiers)
  override protected def mapSize(): Int = 128

}



Java




copy
source
import akka.event.japi.LookupEventBus;

static class MsgEnvelope {
  public final String topic;
  public final Object payload;

  public MsgEnvelope(String topic, Object payload) {
    this.topic = topic;
    this.payload = payload;
  }
}

/**
 * Publishes the payload of the MsgEnvelope when the topic of the MsgEnvelope equals the String
 * specified when subscribing.
 */
static class LookupBusImpl extends LookupEventBus<MsgEnvelope, ActorRef, String> {

  // is used for extracting the classifier from the incoming events
  @Override
  public String classify(MsgEnvelope event) {
    return event.topic;
  }

  // will be invoked for each event for all subscribers which registered themselves
  // for the eventâs classifier
  @Override
  public void publish(MsgEnvelope event, ActorRef subscriber) {
    subscriber.tell(event.payload, ActorRef.noSender());
  }

  // must define a full order over the subscribers, expressed as expected from
  // `java.lang.Comparable.compare`
  @Override
  public int compareSubscribers(ActorRef a, ActorRef b) {
    return a.compareTo(b);
  }

  // determines the initial size of the index data structure
  // used internally (i.e. the expected number of different classifiers)
  @Override
  public int mapSize() {
    return 128;
  }
}




A test for this implementation may look like this:




Scala




copy
source
val lookupBus = new LookupBusImpl
lookupBus.subscribe(testActor, "greetings")
lookupBus.publish(MsgEnvelope("time", System.currentTimeMillis()))
lookupBus.publish(MsgEnvelope("greetings", "hello"))
expectMsg("hello")


Java




copy
source
LookupBusImpl lookupBus = new LookupBusImpl();
lookupBus.subscribe(getTestActor(), "greetings");
lookupBus.publish(new MsgEnvelope("time", System.currentTimeMillis()));
lookupBus.publish(new MsgEnvelope("greetings", "hello"));
expectMsgEquals("hello");






This classifier is efficient in case no subscribers exist for a particular event. 


Subchannel Classification




If classifiers form a hierarchy and it is desired that subscription be possible not only at the leaf nodes, this classification may be just the right one. It can be compared to tuning in on (possibly multiple) radio channels by genre. This classification has been developed for the case where the classifier is just the JVM class of the event and subscribers may be interested in subscribing to all subclasses of a certain class, but it may be used with any classifier hierarchy.


The necessary methods to be implemented are illustrated with the following example: 




Scala




copy
source
import akka.util.Subclassification

class StartsWithSubclassification extends Subclassification[String] {
  override def isEqual(x: String, y: String): Boolean =
    x == y

  override def isSubclass(x: String, y: String): Boolean =
    x.startsWith(y)
}

import akka.event.SubchannelClassification

/**
 * Publishes the payload of the MsgEnvelope when the topic of the
 * MsgEnvelope starts with the String specified when subscribing.
 */
class SubchannelBusImpl extends EventBus with SubchannelClassification {
  type Event = MsgEnvelope
  type Classifier = String
  type Subscriber = ActorRef

  // Subclassification is an object providing `isEqual` and `isSubclass`
  // to be consumed by the other methods of this classifier
  override protected val subclassification: Subclassification[Classifier] =
    new StartsWithSubclassification

  // is used for extracting the classifier from the incoming events
  override protected def classify(event: Event): Classifier = event.topic

  // will be invoked for each event for all subscribers which registered
  // themselves for the eventâs classifier
  override protected def publish(event: Event, subscriber: Subscriber): Unit = {
    subscriber ! event.payload
  }
}


Java




copy
source
import akka.event.japi.SubchannelEventBus;

static class StartsWithSubclassification implements Subclassification<String> {
  @Override
  public boolean isEqual(String x, String y) {
    return x.equals(y);
  }

  @Override
  public boolean isSubclass(String x, String y) {
    return x.startsWith(y);
  }
}

/**
 * Publishes the payload of the MsgEnvelope when the topic of the MsgEnvelope starts with the
 * String specified when subscribing.
 */
static class SubchannelBusImpl extends SubchannelEventBus<MsgEnvelope, ActorRef, String> {

  // Subclassification is an object providing `isEqual` and `isSubclass`
  // to be consumed by the other methods of this classifier
  @Override
  public Subclassification<String> subclassification() {
    return new StartsWithSubclassification();
  }

  // is used for extracting the classifier from the incoming events
  @Override
  public String classify(MsgEnvelope event) {
    return event.topic;
  }

  // will be invoked for each event for all subscribers which registered themselves
  // for the eventâs classifier
  @Override
  public void publish(MsgEnvelope event, ActorRef subscriber) {
    subscriber.tell(event.payload, ActorRef.noSender());
  }
}




A test for this implementation may look like this:




Scala




copy
source
val subchannelBus = new SubchannelBusImpl
subchannelBus.subscribe(testActor, "abc")
subchannelBus.publish(MsgEnvelope("xyzabc", "x"))
subchannelBus.publish(MsgEnvelope("bcdef", "b"))
subchannelBus.publish(MsgEnvelope("abc", "c"))
expectMsg("c")
subchannelBus.publish(MsgEnvelope("abcdef", "d"))
expectMsg("d")


Java




copy
source
SubchannelBusImpl subchannelBus = new SubchannelBusImpl();
subchannelBus.subscribe(getTestActor(), "abc");
subchannelBus.publish(new MsgEnvelope("xyzabc", "x"));
subchannelBus.publish(new MsgEnvelope("bcdef", "b"));
subchannelBus.publish(new MsgEnvelope("abc", "c"));
expectMsgEquals("c");
subchannelBus.publish(new MsgEnvelope("abcdef", "d"));
expectMsgEquals("d");






This classifier is also efficient in case no subscribers are found for an event, but it uses conventional locking to synchronize an internal classifier cache, hence it is not well-suited to use cases in which subscriptions change with very high frequency (keep in mind that âopeningâ a classifier by sending the first message will also have to re-check all previous subscriptions). 


Scanning Classification




The previous classifier was built for multi-classifier subscriptions which are strictly hierarchical, this classifier is useful if there are overlapping classifiers which cover various parts of the event space without forming a hierarchy. It can be compared to tuning in on (possibly multiple) radio stations by geographical reachability (for old-school radio-wave transmission).


The necessary methods to be implemented are illustrated with the following example: 




Scala




copy
source
import akka.event.ScanningClassification

/**
 * Publishes String messages with length less than or equal to the length
 * specified when subscribing.
 */
class ScanningBusImpl extends EventBus with ScanningClassification {
  type Event = String
  type Classifier = Int
  type Subscriber = ActorRef

  // is needed for determining matching classifiers and storing them in an
  // ordered collection
  override protected def compareClassifiers(a: Classifier, b: Classifier): Int =
    if (a < b) -1 else if (a == b) 0 else 1

  // is needed for storing subscribers in an ordered collection
  override protected def compareSubscribers(a: Subscriber, b: Subscriber): Int =
    a.compareTo(b)

  // determines whether a given classifier shall match a given event; it is invoked
  // for each subscription for all received events, hence the name of the classifier
  override protected def matches(classifier: Classifier, event: Event): Boolean =
    event.length <= classifier

  // will be invoked for each event for all subscribers which registered themselves
  // for a classifier matching this event
  override protected def publish(event: Event, subscriber: Subscriber): Unit = {
    subscriber ! event
  }
}


Java




copy
source
import akka.event.japi.ScanningEventBus;

/**
 * Publishes String messages with length less than or equal to the length specified when
 * subscribing.
 */
static class ScanningBusImpl extends ScanningEventBus<String, ActorRef, Integer> {

  // is needed for determining matching classifiers and storing them in an
  // ordered collection
  @Override
  public int compareClassifiers(Integer a, Integer b) {
    return a.compareTo(b);
  }

  // is needed for storing subscribers in an ordered collection
  @Override
  public int compareSubscribers(ActorRef a, ActorRef b) {
    return a.compareTo(b);
  }

  // determines whether a given classifier shall match a given event; it is invoked
  // for each subscription for all received events, hence the name of the classifier
  @Override
  public boolean matches(Integer classifier, String event) {
    return event.length() <= classifier;
  }

  // will be invoked for each event for all subscribers which registered themselves
  // for the eventâs classifier
  @Override
  public void publish(String event, ActorRef subscriber) {
    subscriber.tell(event, ActorRef.noSender());
  }
}




A test for this implementation may look like this:




Scala




copy
source
val scanningBus = new ScanningBusImpl
scanningBus.subscribe(testActor, 3)
scanningBus.publish("xyzabc")
scanningBus.publish("ab")
expectMsg("ab")
scanningBus.publish("abc")
expectMsg("abc")


Java




copy
source
ScanningBusImpl scanningBus = new ScanningBusImpl();
scanningBus.subscribe(getTestActor(), 3);
scanningBus.publish("xyzabc");
scanningBus.publish("ab");
expectMsgEquals("ab");
scanningBus.publish("abc");
expectMsgEquals("abc");






This classifier takes always a time which is proportional to the number of subscriptions, independent of how many actually match. 


Actor Classification


This classification was originally developed specifically for implementing 
DeathWatch
: subscribers as well as classifiers are of type 
ActorRef
ActorRef
.




This classification requires an 
ActorSystem
ActorSystem
 in order to perform book-keeping operations related to the subscribers being Actors, which can terminate without first unsubscribing from the EventBus. ManagedActorClassification maintains a system Actor which takes care of unsubscribing terminated actors automatically.


The necessary methods to be implemented are illustrated with the following example: 




Scala




copy
source
import akka.event.ActorEventBus
import akka.event.ManagedActorClassification
import akka.event.ActorClassifier

final case class Notification(ref: ActorRef, id: Int)

class ActorBusImpl(val system: ActorSystem)
    extends ActorEventBus
    with ActorClassifier
    with ManagedActorClassification {
  type Event = Notification

  // is used for extracting the classifier from the incoming events
  override protected def classify(event: Event): ActorRef = event.ref

  // determines the initial size of the index data structure
  // used internally (i.e. the expected number of different classifiers)
  override protected def mapSize: Int = 128
}


Java




copy
source
import akka.event.japi.ManagedActorEventBus;

static class Notification {
  public final ActorRef ref;
  public final int id;

  public Notification(ActorRef ref, int id) {
    this.ref = ref;
    this.id = id;
  }
}

static class ActorBusImpl extends ManagedActorEventBus<Notification> {

  // the ActorSystem will be used for book-keeping operations, such as subscribers terminating
  public ActorBusImpl(ActorSystem system) {
    super(system);
  }

  // is used for extracting the classifier from the incoming events
  @Override
  public ActorRef classify(Notification event) {
    return event.ref;
  }

  // determines the initial size of the index data structure
  // used internally (i.e. the expected number of different classifiers)
  @Override
  public int mapSize() {
    return 128;
  }
}




A test for this implementation may look like this:




Scala




copy
source
val observer1 = TestProbe().ref
val observer2 = TestProbe().ref
val probe1 = TestProbe()
val probe2 = TestProbe()
val subscriber1 = probe1.ref
val subscriber2 = probe2.ref
val actorBus = new ActorBusImpl(system)
actorBus.subscribe(subscriber1, observer1)
actorBus.subscribe(subscriber2, observer1)
actorBus.subscribe(subscriber2, observer2)
actorBus.publish(Notification(observer1, 100))
probe1.expectMsg(Notification(observer1, 100))
probe2.expectMsg(Notification(observer1, 100))
actorBus.publish(Notification(observer2, 101))
probe2.expectMsg(Notification(observer2, 101))
probe1.expectNoMessage(500.millis)


Java




copy
source
ActorRef observer1 = new TestKit(system).getRef();
ActorRef observer2 = new TestKit(system).getRef();
TestKit probe1 = new TestKit(system);
TestKit probe2 = new TestKit(system);
ActorRef subscriber1 = probe1.getRef();
ActorRef subscriber2 = probe2.getRef();
ActorBusImpl actorBus = new ActorBusImpl(system);
actorBus.subscribe(subscriber1, observer1);
actorBus.subscribe(subscriber2, observer1);
actorBus.subscribe(subscriber2, observer2);
Notification n1 = new Notification(observer1, 100);
actorBus.publish(n1);
probe1.expectMsgEquals(n1);
probe2.expectMsgEquals(n1);
Notification n2 = new Notification(observer2, 101);
actorBus.publish(n2);
probe2.expectMsgEquals(n2);
probe1.expectNoMessage(Duration.ofMillis(500));






This classifier is still is generic in the event type, and it is efficient for all use cases. 














 
Utilities






Logging 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-composition.html
Modularity, Composition and Hierarchy • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy




Dependency


Introduction


Basics of composition and modularity


Composing complex systems


Materialized values


Attributes




Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy




Dependency


Introduction


Basics of composition and modularity


Composing complex systems


Materialized values


Attributes




Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Modularity, Composition and Hierarchy


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


Akka Streams provide a uniform model of stream processing graphs, which allows flexible composition of reusable components. In this chapter we show how these look like from the conceptual and API perspective, demonstrating the modularity aspects of the library.


Basics of composition and modularity


Every operator used in Akka Streams can be imagined as a “box” with input and output ports where elements to be processed arrive and leave the operator. In this view, a 
Source
 is nothing else than a “box” with a single output port, or, a 
BidiFlow
 is a “box” with exactly two input and two output ports. In the figure below we illustrate the most commonly used operators viewed as “boxes”.




The 
linear
 operators are 
Source
, 
Sink
 and 
Flow
, as these can be used to compose strict chains of operators. Fan-in and Fan-out operators have usually multiple input or multiple output ports, therefore they allow to build more complex graph layouts, not only chains. 
BidiFlow
 operators are usually useful in IO related tasks, where there are input and output channels to be handled. Due to the specific shape of 
BidiFlow
 it is easy to stack them on top of each other to build a layered protocol for example. The 
TLS
 support in Akka is for example implemented as a 
BidiFlow
.


These reusable components already allow the creation of complex processing networks. What we have seen so far does not implement modularity though. It is desirable for example to package up a larger graph entity into a reusable component which hides its internals only exposing the ports that are meant to the users of the module to interact with. One good example is the 
Http
 server component, which is encoded internally as a 
BidiFlow
 which interfaces with the client TCP connection using an input-output port pair accepting and sending 
ByteString
 s, while its upper ports emit and receive 
HttpRequest
 and 
HttpResponse
 instances.


The following figure demonstrates various composite operators, that contain various other type of operators internally, but hiding them behind a 
shape
 that looks like a 
Source
, 
Flow
, etc.




One interesting example above is a 
Flow
 which is composed of a disconnected 
Sink
 and 
Source
. This can be achieved by using the 
fromSinkAndSource()
 constructor method on 
Flow
 which takes the two parts as parameters.


Please note that when combining a 
Flow
 using that method, the termination signals are not carried “through” as the 
Sink
 and 
Source
 are assumed to be fully independent. If however you want to construct a 
Flow
 like this but need the termination events to trigger “the other side” of the composite flow, you can use 
Flow.fromSinkAndSourceCoupled
 or 
Flow.fromSinkAndSourceCoupledMat
 which does just that. For example the cancellation of the composite flows source-side will then lead to completion of its sink-side. Read 
Flow
Flow
’s API documentation for a detailed explanation how this works.


The example 
BidiFlow
 demonstrates that internally a module can be of arbitrary complexity, and the exposed ports can be wired in flexible ways. The only constraint is that all the ports of enclosed modules must be either connected to each other, or exposed as interface ports, and the number of such ports needs to match the requirement of the shape, for example a 
Source
 allows only one exposed output port, the rest of the internal ports must be properly connected.


These mechanics allow arbitrary nesting of modules. For example the following figure demonstrates a 
RunnableGraph
 that is built from a composite 
Source
 and a composite 
Sink
 (which in turn contains a composite 
Flow
).




The above diagram contains one more shape that we have not seen yet, which is called 
RunnableGraph
. It turns out, that if we wire all exposed ports together, so that no more open ports remain, we get a module that is 
closed
. This is what the 
RunnableGraph
 class represents. This is the shape that a 
Materializer
 can take and turn into a network of running entities that perform the task described. In fact, a 
RunnableGraph
 is a module itself, and (maybe somewhat surprisingly) it can be used as part of larger graphs. It is rarely useful to embed a closed graph shape in a larger graph (since it becomes an isolated island as there are no open port for communication with the rest of the graph), but this demonstrates the uniform underlying model.


If we try to build a code snippet that corresponds to the above diagram, our first try might look like this:




Scala




copy
source
Source.single(0).map(_ + 1).filter(_ != 0).map(_ - 2).to(Sink.fold(0)(_ + _))

// ... where is the nesting?


Java




copy
source
Source.single(0)
    .map(i -> i + 1)
    .filter(i -> i != 0)
    .map(i -> i - 2)
    .to(Sink.fold(0, (acc, i) -> acc + i));

// ... where is the nesting?




It is clear however that there is no nesting present in our first attempt. Since the library cannot figure out where we intended to put composite module boundaries, it is our responsibility to do that. If we are using the DSL provided by the 
Flow
, 
Source
, 
Sink
 classes then nesting can be achieved by calling one of the methods 
withAttributes()
 or 
named()
 (where the latter is a shorthand for adding a name attribute).


The following code demonstrates how to achieve the desired nesting:




Scala




copy
source
val nestedSource =
  Source
    .single(0) // An atomic source
    .map(_ + 1) // an atomic processing stage
    .named("nestedSource") // wraps up the current Source and gives it a name

val nestedFlow =
  Flow[Int]
    .filter(_ != 0) // an atomic processing stage
    .map(_ - 2) // another atomic processing stage
    .named("nestedFlow") // wraps up the Flow, and gives it a name

val nestedSink =
  nestedFlow
    .to(Sink.fold(0)(_ + _)) // wire an atomic sink to the nestedFlow
    .named("nestedSink") // wrap it up

// Create a RunnableGraph
val runnableGraph = nestedSource.to(nestedSink)


Java




copy
source
final Source<Integer, NotUsed> nestedSource =
    Source.single(0) // An atomic source
        .map(i -> i + 1) // an atomic processing stage
        .named("nestedSource"); // wraps up the current Source and gives it a name

final Flow<Integer, Integer, NotUsed> nestedFlow =
    Flow.of(Integer.class)
        .filter(i -> i != 0) // an atomic processing stage
        .map(i -> i - 2) // another atomic processing stage
        .named("nestedFlow"); // wraps up the Flow, and gives it a name

final Sink<Integer, NotUsed> nestedSink =
    nestedFlow
        .to(Sink.fold(0, (acc, i) -> acc + i)) // wire an atomic sink to the nestedFlow
        .named("nestedSink"); // wrap it up

// Create a RunnableGraph
final RunnableGraph<NotUsed> runnableGraph = nestedSource.to(nestedSink);




Once we have hidden the internals of our components, they act like any other built-in component of similar shape. If we hide some of the internals of our composites, the result looks just like if any other predefine component has been used:




If we look at usage of built-in components, and our custom components, there is no difference in usage as the code snippet below demonstrates.




Scala




copy
source
// Create a RunnableGraph from our components
val runnableGraph = nestedSource.to(nestedSink)

// Usage is uniform, no matter if modules are composite or atomic
val runnableGraph2 = Source.single(0).to(Sink.fold(0)(_ + _))


Java




copy
source
// Create a RunnableGraph from our components
final RunnableGraph<NotUsed> runnableGraph = nestedSource.to(nestedSink);

// Usage is uniform, no matter if modules are composite or atomic
final RunnableGraph<NotUsed> runnableGraph2 =
    Source.single(0).to(Sink.fold(0, (acc, i) -> acc + i));




Composing complex systems


In the previous section we explored the possibility of composition, and hierarchy, but we stayed away from non-linear, generalized operators. There is nothing in Akka Streams though that enforces that stream processing layouts can only be linear. The DSL for 
Source
 and friends is optimized for creating such linear chains, as they are the most common in practice. There is a more advanced DSL for building complex graphs, that can be used if more flexibility is needed. We will see that the difference between the two DSLs is only on the surface: the concepts they operate on are uniform across all DSLs and fit together nicely.


As a first example, let’s look at a more complex layout:




The diagram shows a 
RunnableGraph
 (remember, if there are no unwired ports, the graph is closed, and therefore can be materialized) that encapsulates a non-trivial stream processing network. It contains fan-in, fan-out operators, directed and non-directed cycles. The 
runnable()
 method of the 
GraphDSL
 object allows the creation of a general, closed, and runnable graph. For example the network on the diagram can be realized like this:




Scala




copy
source
import GraphDSL.Implicits._
RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>
  val A: Outlet[Int]                  = builder.add(Source.single(0)).out
  val B: UniformFanOutShape[Int, Int] = builder.add(Broadcast[Int](2))
  val C: UniformFanInShape[Int, Int]  = builder.add(Merge[Int](2))
  val D: FlowShape[Int, Int]          = builder.add(Flow[Int].map(_ + 1))
  val E: UniformFanOutShape[Int, Int] = builder.add(Balance[Int](2))
  val F: UniformFanInShape[Int, Int]  = builder.add(Merge[Int](2))
  val G: Inlet[Any]                   = builder.add(Sink.foreach(println)).in

                C     <~      F
  A  ~>  B  ~>  C     ~>      F
         B  ~>  D  ~>  E  ~>  F
                       E  ~>  G

  ClosedShape
})


Java




copy
source
RunnableGraph.fromGraph(
    GraphDSL.create(
        builder -> {
          final Outlet<Integer> A = builder.add(Source.single(0)).out();
          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));
          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));
          final FlowShape<Integer, Integer> D =
              builder.add(Flow.of(Integer.class).map(i -> i + 1));
          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));
          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));
          final Inlet<Integer> G = builder.add(Sink.<Integer>foreach(System.out::println)).in();

          builder.from(F).toFanIn(C);
          builder.from(A).viaFanOut(B).viaFanIn(C).toFanIn(F);
          builder.from(B).via(D).viaFanOut(E).toFanIn(F);
          builder.from(E).toInlet(G);
          return ClosedShape.getInstance();
        }));




In the code above we used the implicit port numbering feature (to make the graph more readable and similar to the diagram) and we imported 
Source
 s, 
Sink
 s and 
Flow
 s explicitly. It is possible to refer to the ports explicitly, and it is not necessary to import our linear operators via 
add()
, so another version might look like this:




Scala




copy
source
import GraphDSL.Implicits._
RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>
  val B = builder.add(Broadcast[Int](2))
  val C = builder.add(Merge[Int](2))
  val E = builder.add(Balance[Int](2))
  val F = builder.add(Merge[Int](2))

  Source.single(0) ~> B.in; B.out(0) ~> C.in(1); C.out ~> F.in(0)
  C.in(0) <~ F.out

  B.out(1).map(_ + 1) ~> E.in; E.out(0) ~> F.in(1)
  E.out(1) ~> Sink.foreach(println)
  ClosedShape
})


Java




copy
source
RunnableGraph.fromGraph(
    GraphDSL.create(
        builder -> {
          final SourceShape<Integer> A = builder.add(Source.single(0));
          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));
          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));
          final FlowShape<Integer, Integer> D =
              builder.add(Flow.of(Integer.class).map(i -> i + 1));
          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));
          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));
          final SinkShape<Integer> G = builder.add(Sink.foreach(System.out::println));

          builder.from(F.out()).toInlet(C.in(0));
          builder.from(A).toInlet(B.in());
          builder.from(B.out(0)).toInlet(C.in(1));
          builder.from(C.out()).toInlet(F.in(0));
          builder.from(B.out(1)).via(D).toInlet(E.in());
          builder.from(E.out(0)).toInlet(F.in(1));
          builder.from(E.out(1)).to(G);
          return ClosedShape.getInstance();
        }));




Similar to the case in the first section, so far we have not considered modularity. We created a complex graph, but the layout is flat, not modularized. We will modify our example, and create a reusable component with the graph DSL. The way to do it is to use the 
create()
 factory method on 
GraphDSL
. If we remove the sources and sinks from the previous example, what remains is a partial graph:




We can recreate a similar graph in code, using the DSL in a similar way than before:




Scala




copy
source
import GraphDSL.Implicits._
val partial = GraphDSL.create() { implicit builder =>
  val B = builder.add(Broadcast[Int](2))
  val C = builder.add(Merge[Int](2))
  val E = builder.add(Balance[Int](2))
  val F = builder.add(Merge[Int](2))

                                   C  <~  F
  B  ~>                            C  ~>  F
  B  ~>  Flow[Int].map(_ + 1)  ~>  E  ~>  F
  FlowShape(B.in, E.out(1))
}.named("partial")


Java




copy
source
final Graph<FlowShape<Integer, Integer>, NotUsed> partial =
    GraphDSL.create(
        builder -> {
          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));
          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));
          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));
          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));

          builder.from(F.out()).toInlet(C.in(0));
          builder.from(B).viaFanIn(C).toFanIn(F);
          builder
              .from(B)
              .via(builder.add(Flow.of(Integer.class).map(i -> i + 1)))
              .viaFanOut(E)
              .toFanIn(F);

          return new FlowShape<Integer, Integer>(B.in(), E.out(1));
        });





The only new addition is the return value of the builder block, which is a 
Shape
. All operators (including 
Source
, 
BidiFlow
, etc.) have a shape, which encodes the 
typed
 ports of the module. In our example there is exactly one input and output port left, so we can declare it to have a 
FlowShape
 by returning an instance of it. While it is possible to create new 
Shape
 types, it is usually recommended to use one of the matching built-in ones.


The resulting graph is already a properly wrapped module, so there is no need to call 
named()
 to encapsulate the graph, but it is a good practice to give names to modules to help debugging.




Since our partial graph has the right shape, it can be already used in the simpler, linear DSL:




Scala




copy
source
Source.single(0).via(partial).to(Sink.ignore)


Java




copy
source
Source.single(0).via(partial).to(Sink.ignore());




It is not possible to use it as a 
Flow
 yet, though (i.e. we cannot call 
.filter()
 on it), but 
Flow
 has a 
fromGraph()
 method that adds the DSL to a 
FlowShape
. There are similar methods on 
Source
, 
Sink
 and 
BidiShape
, so it is easy to get back to the simpler DSL if an operator has the right shape. For convenience, it is also possible to skip the partial graph creation, and use one of the convenience creator methods. To demonstrate this, we will create the following graph:




The code version of the above closed graph might look like this:




Scala




copy
source
// Convert the partial graph of FlowShape to a Flow to get
// access to the fluid DSL (for example to be able to call .filter())
val flow = Flow.fromGraph(partial)

// Simple way to create a graph backed Source
val source = Source.fromGraph( GraphDSL.create() { implicit builder =>
  val merge = builder.add(Merge[Int](2))
  Source.single(0)      ~> merge
  Source(List(2, 3, 4)) ~> merge

  // Exposing exactly one output port
  SourceShape(merge.out)
})

// Building a Sink with a nested Flow, using the fluid DSL
val sink = {
  val nestedFlow = Flow[Int].map(_ * 2).drop(10).named("nestedFlow")
  nestedFlow.to(Sink.head)
}

// Putting all together
val closed = source.via(flow.filter(_ > 1)).to(sink)


Java




copy
source
// Convert the partial graph of FlowShape to a Flow to get
// access to the fluid DSL (for example to be able to call .filter())
final Flow<Integer, Integer, NotUsed> flow = Flow.fromGraph(partial);

// Simple way to create a graph backed Source
final Source<Integer, NotUsed> source =
    Source.fromGraph(
        GraphDSL.create(
            builder -> {
              final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));
              builder.from(builder.add(Source.single(0))).toFanIn(merge);
              builder.from(builder.add(Source.from(Arrays.asList(2, 3, 4)))).toFanIn(merge);
              // Exposing exactly one output port
              return new SourceShape<Integer>(merge.out());
            }));

// Building a Sink with a nested Flow, using the fluid DSL
final Sink<Integer, NotUsed> sink =
    Flow.of(Integer.class).map(i -> i * 2).drop(10).named("nestedFlow").to(Sink.head());

// Putting all together
final RunnableGraph<NotUsed> closed = source.via(flow.filter(i -> i > 1)).to(sink);


Note


All graph builder sections check if the resulting graph has all ports connected except the exposed ones and will throw an exception if this is violated.


We are still in debt of demonstrating that 
RunnableGraph
 is a component like any other, which can be embedded in graphs. In the following snippet we embed one closed graph in another:




Scala




copy
source
val closed1 = Source.single(0).to(Sink.foreach(println))
val closed2 = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>
  val embeddedClosed: ClosedShape = builder.add(closed1)
  // â¦
  embeddedClosed
})


Java




copy
source
final RunnableGraph<NotUsed> closed1 = Source.single(0).to(Sink.foreach(System.out::println));
final RunnableGraph<NotUsed> closed2 =
    RunnableGraph.fromGraph(
        GraphDSL.create(
            builder -> {
              final ClosedShape embeddedClosed = builder.add(closed1);
              return embeddedClosed; // Could return ClosedShape.getInstance()
            }));




The type of the imported module indicates that the imported module has a 
ClosedShape
, and so we are not able to wire it to anything else inside the enclosing closed graph. Nevertheless, this “island” is embedded properly, and will be materialized just like any other module that is part of the graph.


As we have demonstrated, the two DSLs are fully interoperable, as they encode a similar nested structure of “boxes with ports”, it is only the DSLs that differ to be as much powerful as possible on the given abstraction level. It is possible to embed complex graphs in the fluid DSL, and it is just as easy to import and embed a 
Flow
, etc, in a larger, complex structure.


We have also seen, that every module has a 
Shape
 (for example a 
Sink
 has a 
SinkShape
) independently which DSL was used to create it. This uniform representation enables the rich composability of various stream processing entities in a convenient way.


Materialized values


After realizing that 
RunnableGraph
 is nothing more than a module with no unused ports (it is an island), it becomes clear that after materialization the only way to communicate with the running stream processing logic is via some side-channel. This side channel is represented as a 
materialized value
. The situation is similar to 
Actor
 s, where the 
Props
 instance describes the actor logic, but it is the call to 
actorOf()
 that creates an actually running actor, and returns an 
ActorRef
 that can be used to communicate with the running actor itself. Since the 
Props
 can be reused, each call will return a different reference.


When it comes to streams, each materialization creates a new running network corresponding to the blueprint that was encoded in the provided 
RunnableGraph
. To be able to interact with the running network, each materialization needs to return a different object that provides the necessary interaction capabilities. In other words, the 
RunnableGraph
 can be seen as a factory, which creates:




a network of running processing entities, inaccessible from the outside


a materialized value, optionally providing a controlled interaction capability with the network




Unlike actors though, each of the operators might provide a materialized value, so when we compose multiple operators or modules, we need to combine the materialized value as well (there are default rules which make this easier, for example 
to()
 and 
via()
 takes care of the most common case of taking the materialized value to the left. See 
Combining materialized values
 for details). We demonstrate how this works by a code example and a diagram which graphically demonstrates what is happening.


The propagation of the individual materialized values from the enclosed modules towards the top will look like this:




To implement the above, first, we create a composite 
Source
, where the enclosed 
Source
 have a materialized type of 
Promise[[Option[Int]]
 
CompletableFuture<Optional<Integer>>>
. By using the combiner function 
Keep.left
, the resulting materialized type is of the nested module (indicated by the color 
red
 on the diagram):




Scala




copy
source
// Materializes to Promise[Option[Int]]                                   (red)
val source: Source[Int, Promise[Option[Int]]] = Source.maybe[Int]

// Materializes to NotUsed                                               (black)
val flow1: Flow[Int, Int, NotUsed] = Flow[Int].take(100)

// Materializes to Promise[Int]                                          (red)
val nestedSource: Source[Int, Promise[Option[Int]]] =
  source.viaMat(flow1)(Keep.left).named("nestedSource")


Java




copy
source
// Materializes to CompletableFuture<Optional<Integer>>                   (red)
final Source<Integer, CompletableFuture<Optional<Integer>>> source = Source.<Integer>maybe();

// Materializes to NotUsed                                                (black)
final Flow<Integer, Integer, NotUsed> flow1 = Flow.of(Integer.class).take(100);

// Materializes to CompletableFuture<Optional<Integer>>                  (red)
final Source<Integer, CompletableFuture<Optional<Integer>>> nestedSource =
    source.viaMat(flow1, Keep.left()).named("nestedSource");




Next, we create a composite 
Flow
 from two smaller components. Here, the second enclosed 
Flow
 has a materialized type of 
Future[OutgoingConnection]
 
CompletionStage<OutgoingConnection>
, and we propagate this to the parent by using 
Keep.right
 as the combiner function (indicated by the color 
yellow
 on the diagram):




Scala




copy
source
// Materializes to NotUsed                                                (orange)
val flow2: Flow[Int, ByteString, NotUsed] = Flow[Int].map { i =>
  ByteString(i.toString)
}

// Materializes to Future[OutgoingConnection]                             (yellow)
val flow3: Flow[ByteString, ByteString, Future[OutgoingConnection]] =
  Tcp(system).outgoingConnection("localhost", 8080)

// Materializes to Future[OutgoingConnection]                             (yellow)
val nestedFlow: Flow[Int, ByteString, Future[OutgoingConnection]] =
  flow2.viaMat(flow3)(Keep.right).named("nestedFlow")


Java




copy
source
// Materializes to NotUsed                                                (orange)
final Flow<Integer, ByteString, NotUsed> flow2 =
    Flow.of(Integer.class).map(i -> ByteString.fromString(i.toString()));

// Materializes to Future<OutgoingConnection>                             (yellow)
final Flow<ByteString, ByteString, CompletionStage<OutgoingConnection>> flow3 =
    Tcp.get(system).outgoingConnection("localhost", 8080);

// Materializes to Future<OutgoingConnection>                             (yellow)
final Flow<Integer, ByteString, CompletionStage<OutgoingConnection>> nestedFlow =
    flow2.viaMat(flow3, Keep.right()).named("nestedFlow");




As a third step, we create a composite 
Sink
, using our 
nestedFlow
 as a building block. In this snippet, both the enclosed 
Flow
 and the folding 
Sink
 has a materialized value that is interesting for us, so we use 
Keep.both
 to get a 
Pair
 of them as the materialized type of 
nestedSink
 (indicated by the color 
blue
 on the diagram)




Scala




copy
source
// Materializes to Future[String]                                         (green)
val sink: Sink[ByteString, Future[String]] = Sink.fold("")(_ + _.utf8String)

// Materializes to (Future[OutgoingConnection], Future[String])           (blue)
val nestedSink: Sink[Int, (Future[OutgoingConnection], Future[String])] =
  nestedFlow.toMat(sink)(Keep.both)


Java




copy
source
// Materializes to Future<String>                                         (green)
final Sink<ByteString, CompletionStage<String>> sink =
    Sink.<String, ByteString>fold("", (acc, i) -> acc + i.utf8String());

// Materializes to Pair<Future<OutgoingConnection>, Future<String>>       (blue)
final Sink<Integer, Pair<CompletionStage<OutgoingConnection>, CompletionStage<String>>>
    nestedSink = nestedFlow.toMat(sink, Keep.both());




As the last example, we wire together 
nestedSource
 and 
nestedSink
 and we use a custom combiner function to create a yet another materialized type of the resulting 
RunnableGraph
. This combiner function ignores the 
Future[String]
 
CompletionStage<String>
 part, and wraps the other two values in a custom case class 
MyClass
 (indicated by color 
purple
 on the diagram):




Scala




copy
source
case class MyClass(private val p: Promise[Option[Int]], conn: OutgoingConnection) {
  def close() = p.trySuccess(None)
}

def f(p: Promise[Option[Int]], rest: (Future[OutgoingConnection], Future[String])): Future[MyClass] = {

  val connFuture = rest._1
  connFuture.map(MyClass(p, _))
}

// Materializes to Future[MyClass]                                        (purple)
val runnableGraph: RunnableGraph[Future[MyClass]] =
  nestedSource.toMat(nestedSink)(f)


Java




copy
source
static class MyClass {
  private CompletableFuture<Optional<Integer>> p;
  private OutgoingConnection conn;

  public MyClass(CompletableFuture<Optional<Integer>> p, OutgoingConnection conn) {
    this.p = p;
    this.conn = conn;
  }

  public void close() {
    p.complete(Optional.empty());
  }
}

static class Combiner {
  static CompletionStage<MyClass> f(
      CompletableFuture<Optional<Integer>> p,
      Pair<CompletionStage<OutgoingConnection>, CompletionStage<String>> rest) {
    return rest.first().thenApply(c -> new MyClass(p, c));
  }
}


copy
source
// Materializes to Future<MyClass>                                        (purple)
final RunnableGraph<CompletionStage<MyClass>> runnableGraph =
    nestedSource.toMat(nestedSink, Combiner::f);




Note


The nested structure in the above example is not necessary for combining the materialized values, it demonstrates how the two features work together. See 
Combining materialized values
 for further examples of combining materialized values without nesting and hierarchy involved.


Attributes


We have seen that we can use 
named()
 to introduce a nesting level in the fluid DSL and also explicit nesting by using 
create()
 from 
GraphDSL
. Apart from having the effect of adding a nesting level, 
named()
 is actually a shorthand for calling 
addAttributes(Attributes.name("someName"))
, adding the 
name
 attribute to the graph. 


Attributes provide a way to fine-tune certain aspects of the materialized running entity. Attributes are inherited by nested modules, unless they override them with a custom value. This means the attribute specified closest to the operator in the graph will be the one that is in effect for that operator. 


Another example of an attribute is the 
inputBuffer
 attribute which has the main purpose to provide control over buffer sizes for asynchronous boundaries (see 
Buffers for asynchronous operators
). 


The code below, a modification of an earlier example sets the 
inputBuffer
 attribute on certain modules, but not on others. 
Note
 that this is only to show how attributes inheritance works, the actual 
inputBuffer
 attribute does not have any specific effect when running these streams:




Scala




copy
source
import Attributes._
val nestedSource =
  Source.single(0).map(_ + 1).named("nestedSource") // Wrap, no inputBuffer set

val nestedFlow =
  Flow[Int]
    .filter(_ != 0)
    .via(Flow[Int].map(_ - 2).withAttributes(inputBuffer(4, 4))) // override
    .named("nestedFlow") // Wrap, no inputBuffer set

val nestedSink =
  nestedFlow
    .to(Sink.fold(0)(_ + _)) // wire an atomic sink to the nestedFlow
    .withAttributes(name("nestedSink") and inputBuffer(3, 3)) // override


Java




copy
source
final Source<Integer, NotUsed> nestedSource =
    Source.single(0).map(i -> i + 1).named("nestedSource"); // Wrap, no inputBuffer set

final Flow<Integer, Integer, NotUsed> nestedFlow =
    Flow.of(Integer.class)
        .filter(i -> i != 0)
        .via(
            Flow.of(Integer.class)
                .map(i -> i - 2)
                .withAttributes(Attributes.inputBuffer(4, 4))) // override
        .named("nestedFlow"); // Wrap, no inputBuffer set

final Sink<Integer, NotUsed> nestedSink =
    nestedFlow
        .to(Sink.fold(0, (acc, i) -> acc + i)) // wire an atomic sink to the nestedFlow
        .withAttributes(
            Attributes.name("nestedSink").and(Attributes.inputBuffer(3, 3))); // override




The effect is, that each module inherits the 
inputBuffer
 attribute from its enclosing parent, unless it has the same attribute explicitly set. 
nestedSource
 gets the default attributes from the materializer itself. 
nestedSink
 on the other hand has this attribute set, so it will be used by all nested modules. 
nestedFlow
 will inherit from 
nestedSink
 except the 
map
 operator which has again an explicitly provided attribute overriding the inherited one.




This diagram illustrates the inheritance process for the example code (representing the materializer default attributes as the color 
red
, the attributes set on 
nestedSink
 as 
blue
 and the attributes set on 
nestedFlow
 as 
green
).














 
Working with Graphs






Buffers and working with rate 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-cookbook.html
Streams Cookbook • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook




Dependency


Introduction


Working with Flows


Working with Operators


Working with rate


Working with IO




Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook




Dependency


Introduction


Working with Flows


Working with Operators


Working with rate


Working with IO




Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Streams Cookbook


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


This is a collection of patterns to demonstrate various usage of the Akka Streams API by solving small targeted problems in the format of “recipes”. The purpose of this page is to give inspiration and ideas how to approach various small tasks involving streams. The recipes in this page can be used directly as-is, but they are most powerful as starting points: customization of the code snippets is warmly encouraged. The recipes can be extended or can provide a basis for the implementation of other 
patterns
 involving 
Alpakka
.


This part also serves as supplementary material for the main body of documentation. It is a good idea to have this page open while reading the manual and look for examples demonstrating various streaming concepts as they appear in the main body of documentation.


If you need a quick reference of the available operators used in the recipes see 
operator index
.


Working with Flows


In this collection we show simple recipes that involve linear flows. The recipes in this section are rather general, more targeted recipes are available as separate sections (
Buffers and working with rate
, 
Working with streaming IO
).


Logging in streams


Situation:
 During development it is sometimes helpful to see what happens in a particular section of a stream.


The simplest solution is to use a 
map
 operation and use 
println
 to print the elements received to the console. While this recipe is rather simplistic, it is often suitable for a quick debug session.




Scala




copy
source
val loggedSource = mySource.map { elem =>
  println(elem); elem
}


Java




copy
source
mySource.map(
    elem -> {
      System.out.println(elem);
      return elem;
    });




Another approach to logging is to use 
log()
 operation. This approach gives you more fine-grained control of logging levels for elements flowing through the stream, finish and failure of the stream.




Scala




copy
source
// customise log levels
mySource
  .log("before-map")
  .withAttributes(Attributes
    .logLevels(onElement = Logging.WarningLevel, onFinish = Logging.InfoLevel, onFailure = Logging.DebugLevel))
  .map(analyse)
// or provide custom logging adapter
implicit val adapter: LoggingAdapter = Logging(system, "customLogger")
mySource.log("custom")


Java




copy
source
// customise log levels
mySource
    .log("before-map")
    .withAttributes(
        Attributes.createLogLevels(
            Logging.WarningLevel(), // onElement
            Logging.InfoLevel(), // onFinish
            Logging.DebugLevel() // onFailure
            ))
    .map(i -> analyse(i));

// or provide custom logging adapter
final LoggingAdapter adapter = Logging.getLogger(system, "customLogger");
mySource.log("custom", adapter);




Creating a source that continuously evaluates a function


Situation:
 A source is required that continuously provides elements obtained by evaluating a given function, so long as there is demand.


The simplest implementation is to use a 
Source.repeat
 that produces some arbitrary element - e.g. 
NotUsed
 - and then map those elements to the function evaluation. E.g. if we have some 
builderFunction()
, we can use:




Scala




copy
source
val source = Source.repeat(NotUsed).map(_ => builderFunction())


Java




copy
source
final Source<String, NotUsed> source =
    Source.repeat(NotUsed.getInstance()).map(elem -> builderFunction());




Note: if the element-builder function touches mutable state, then a guaranteed single-threaded source should be used instead; e.g. 
Source.unfold
 or 
Source.unfoldResource
.


Flattening a stream of sequences


Situation:
 A stream is given as a stream of sequence of elements, but a stream of elements needed instead, streaming all the nested elements inside the sequences separately.


The 
mapConcat
 operation can be used to implement a one-to-many transformation of elements using a mapper function in the form of 
In => immutable.Seq[Out]
 
In -> List<Out>
. In this case we want to map a 
Seq
 
List
 of elements to the elements in the collection itself, so we can call 
mapConcat(identity)
 
mapConcat(l -> l)
.




Scala




copy
source
val myData: Source[List[Message], NotUsed] = someDataSource
val flattened: Source[Message, NotUsed] = myData.mapConcat(identity)


Java




copy
source
Source<List<Message>, NotUsed> myData = someDataSource;
Source<Message, NotUsed> flattened = myData.mapConcat(i -> i);




Draining a stream to a strict collection


Situation:
 A possibly unbounded sequence of elements is given as a stream, which needs to be collected into a Scala collection while ensuring boundedness


A common situation when working with streams is one where we need to collect incoming elements into a Scala collection. This operation is supported via 
Sink.seq
 which materializes into a 
Future[Seq[T]]
 
CompletionStage<List<T>>
.


The function 
limit
 or 
take
 should always be used in conjunction in order to guarantee stream boundedness, thus preventing the program from running out of memory.


For example, this is best avoided:




Scala




copy
source
// Dangerous: might produce a collection with 2 billion elements!
val f: Future[Seq[String]] = mySource.runWith(Sink.seq)


Java




copy
source
// Dangerous: might produce a collection with 2 billion elements!
final CompletionStage<List<String>> strings = mySource.runWith(Sink.seq(), system);




Rather, use 
limit
 or 
take
 to ensure that the resulting 
Seq
 
List
 will contain only up to 
max
 
MAX_ALLOWED_SIZE
 elements:




Scala




copy
source
val MAX_ALLOWED_SIZE = 100

// OK. Future will fail with a `StreamLimitReachedException`
// if the number of incoming elements is larger than max
val limited: Future[Seq[String]] =
  mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq)

// OK. Collect up until max-th elements only, then cancel upstream
val ignoreOverflow: Future[Seq[String]] =
  mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq)


Java




copy
source
final int MAX_ALLOWED_SIZE = 100;

// OK. Future will fail with a `StreamLimitReachedException`
// if the number of incoming elements is larger than max
final CompletionStage<List<String>> strings =
    mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq(), system);

// OK. Collect up until max-th elements only, then cancel upstream
final CompletionStage<List<String>> strings =
    mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq(), system);




Calculating the digest of a ByteString stream


Situation:
 A stream of bytes is given as a stream of 
ByteString
 s and we want to calculate the cryptographic digest of the stream.


This recipe uses a 
GraphStage
 to define a custom Akka Stream operator, to host a mutable 
MessageDigest
 class (part of the Java Cryptography API) and update it with the bytes arriving from the stream. When the stream starts, the 
onPull
 handler of the operator is called, which bubbles up the 
pull
 event to its upstream. As a response to this pull, a ByteString chunk will arrive (
onPush
) which we use to update the digest, then it will pull for the next chunk.


Eventually the stream of 
ByteString
 s depletes and we get a notification about this event via 
onUpstreamFinish
. At this point we want to emit the digest value, but we cannot do it with 
push
 in this handler directly since there may be no downstream demand. Instead we call 
emit
 which will temporarily replace the handlers, emit the provided value when demand comes in and then reset the operator state. It will then complete the operator.




Scala




copy
source
import java.security.MessageDigest

import akka.NotUsed
import akka.stream.{ Attributes, FlowShape, Inlet, Outlet }
import akka.stream.scaladsl.{ Sink, Source }
import akka.util.ByteString

import akka.stream.stage._

val data: Source[ByteString, NotUsed] = Source.single(ByteString("abc"))

class DigestCalculator(algorithm: String) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString]("DigestCalculator.in")
  val out = Outlet[ByteString]("DigestCalculator.out")
  override val shape = FlowShape(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private val digest = MessageDigest.getInstance(algorithm)

    setHandler(out, new OutHandler {
      override def onPull(): Unit = pull(in)
    })

    setHandler(in, new InHandler {
      override def onPush(): Unit = {
        val chunk = grab(in)
        digest.update(chunk.toArray)
        pull(in)
      }

      override def onUpstreamFinish(): Unit = {
        emit(out, ByteString(digest.digest()))
        completeStage()
      }
    })
  }
}

val digest: Source[ByteString, NotUsed] = data.via(new DigestCalculator("SHA-256"))


Java




copy
source
class DigestCalculator extends GraphStage<FlowShape<ByteString, ByteString>> {
  private final String algorithm;
  public Inlet<ByteString> in = Inlet.create("DigestCalculator.in");
  public Outlet<ByteString> out = Outlet.create("DigestCalculator.out");
  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);

  public DigestCalculator(String algorithm) {
    this.algorithm = algorithm;
  }

  @Override
  public FlowShape<ByteString, ByteString> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      final MessageDigest digest;

      {
        try {
          digest = MessageDigest.getInstance(algorithm);
        } catch (NoSuchAlgorithmException ex) {
          throw new RuntimeException(ex);
        }

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() {
                pull(in);
              }
            });

        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                ByteString chunk = grab(in);
                digest.update(chunk.toArray());
                pull(in);
              }

              @Override
              public void onUpstreamFinish() {
                // If the stream is finished, we need to emit the digest
                // before completing
                emit(out, ByteString.fromArray(digest.digest()));
                completeStage();
              }
            });
      }
    };
  }
}




copy
source
final Source<ByteString, NotUsed> digest = data.via(new DigestCalculator("SHA-256"));






Parsing lines from a stream of ByteStrings


Situation:
 A stream of bytes is given as a stream of 
ByteString
 s containing lines terminated by line ending characters (or, alternatively, containing binary frames delimited by a special delimiter byte sequence) which needs to be parsed.


The 
Framing
 helper 
object
 
class
 contains a convenience method to parse messages from a stream of 
ByteString
 s:




Scala




copy
source
import akka.stream.scaladsl.Framing
val linesStream = rawData
  .via(Framing.delimiter(ByteString("\r\n"), maximumFrameLength = 100, allowTruncation = true))
  .map(_.utf8String)


Java




copy
source
final Source<String, NotUsed> lines =
    rawData
        .via(Framing.delimiter(ByteString.fromString("\r\n"), 100, FramingTruncation.ALLOW))
        .map(b -> b.utf8String());




Dealing with compressed data streams


Situation:
 A gzipped stream of bytes is given as a stream of 
ByteString
 s, for example from a 
FileIO
 source.


The 
Compression
 helper 
object
 
class
 contains convenience methods for decompressing data streams compressed with Gzip or Deflate.




Scala




copy
source
import akka.stream.scaladsl.Compression
val uncompressed = compressed.via(Compression.gunzip()).map(_.utf8String)


Java




copy
source
final Source<ByteString, NotUsed> decompressedStream =
    compressedStream.via(Compression.gunzip(100));




Implementing a Splitter


Situation:
 Given a stream of messages, where each message is a composition of different elements, we want to split the message into a series of individual sub-messages, each of which may be processed in a different way.


The 
Splitter
 is an integration pattern as described in 
Enterprise Integration Patterns
. Let’s say that we have a stream containing strings. Each string contains a few numbers separated by “-”. We want to create out of this a stream that only contains the numbers.




Scala




copy
source
//Sample Source
val source: Source[String, NotUsed] = Source(List("1-2-3", "2-3", "3-4"))

val ret = source
  .map(s => s.split("-").toList)
  .mapConcat(identity)
  //Sub-streams logic
  .map(s => s.toInt)
  .runWith(Sink.seq)

//Verify results

ret.futureValue should be(Vector(1, 2, 3, 2, 3, 3, 4))


Java




copy
source
// Sample Source
Source<String, NotUsed> source = Source.from(Arrays.asList("1-2-3", "2-3", "3-4"));

CompletionStage<List<Integer>> ret =
    source
        .map(s -> Arrays.asList(s.split("-")))
        .mapConcat(f -> f)
        // Sub-streams logic
        .map(s -> Integer.valueOf(s))
        .runWith(Sink.seq(), system);

// Verify results
List<Integer> list = ret.toCompletableFuture().get();
assert list.equals(Arrays.asList(1, 2, 3, 2, 3, 3, 4));




Implementing a Splitter and Aggregator


Situation:
 Given a message, we want to split the message and aggregate its sub-messages into a new message


Sometimes it’s very useful to split a message and aggregate its sub-messages into a new message. This involves a combination of 
Splitter
 and 
Aggregator


Let’s say that now we want to create a new stream containing the sums of the numbers in each original string.




Scala




copy
source
//Sample Source
val source: Source[String, NotUsed] = Source(List("1-2-3", "2-3", "3-4"))

val result = source
  .map(s => s.split("-").toList)
  //split all messages into sub-streams
  .splitWhen(_ => true)
  //now split each collection
  .mapConcat(identity)
  //Sub-streams logic
  .map(s => s.toInt)
  //aggregate each sub-stream
  .reduce((a, b) => a + b)
  //and merge back the result into the original stream
  .mergeSubstreams
  .runWith(Sink.seq);

//Verify results
result.futureValue should be(Vector(6, 5, 7))


Java




copy
source
// Sample Source
Source<String, NotUsed> source = Source.from(Arrays.asList("1-2-3", "2-3", "3-4"));

CompletionStage<List<Integer>> ret =
    source
        .map(s -> Arrays.asList(s.split("-")))
        // split all messages into sub-streams
        .splitWhen(a -> true)
        // now split each collection
        .mapConcat(f -> f)
        // Sub-streams logic
        .map(s -> Integer.valueOf(s))
        // aggregate each sub-stream
        .reduce((a, b) -> a + b)
        // and merge back the result into the original stream
        .mergeSubstreams()
        .runWith(Sink.seq(), system);

// Verify results
List<Integer> list = ret.toCompletableFuture().get();
assert list.equals(Arrays.asList(6, 5, 7));




While in real life this solution is overkill for such a simple problem (you can just do everything in a map), more complex scenarios, involving in particular I/O, will benefit from the fact that you can parallelize sub-streams and get back-pressure for “free”.


Implementing reduce-by-key


Situation:
 Given a stream of elements, we want to calculate some aggregated value on different subgroups of the elements.


The “hello world” of reduce-by-key style operations is 
wordcount
 which we demonstrate below. Given a stream of words we first create a new stream that groups the words according to the 
identity
 
iÂ ->Â i
 function, i.e. now we have a stream of streams, where every substream will serve identical words.


To count the words, we need to process the stream of streams (the actual groups containing identical words). 
groupBy
 returns a 
SubFlow
 
SubSource
, which means that we transform the resulting substreams directly. In this case we use the 
reduce
 operator to aggregate the word itself and the number of its occurrences within a 
tuple 
(String, Integer)
 
Pair<String, Integer>
. Each substream will then emit one final valueâprecisely such a pairâwhen the overall input completes. As a last step we merge back these values from the substreams into one single output stream.


One noteworthy detail pertains to the 
MaximumDistinctWords
 
MAXIMUM_DISTINCT_WORDS
 parameter: this defines the breadth of the groupBy and merge operations. Akka Streams is focused on bounded resource consumption and the number of concurrently open inputs to the merge operator describes the amount of resources needed by the merge itself. Therefore only a finite number of substreams can be active at any given time. If the 
groupBy
 operator encounters more keys than this number then the stream cannot continue without violating its resource bound, in this case 
groupBy
 will terminate with a failure.




Scala




copy
source
val counts: Source[(String, Int), NotUsed] = words
// split the words into separate streams first
  .groupBy(MaximumDistinctWords, identity)
  //transform each element to pair with number of words in it
  .map(_ -> 1)
  // add counting logic to the streams
  .reduce((l, r) => (l._1, l._2 + r._2))
  // get a stream of word counts
  .mergeSubstreams


Java




copy
source
final int MAXIMUM_DISTINCT_WORDS = 1000;

final Source<Pair<String, Integer>, NotUsed> counts =
    words
        // split the words into separate streams first
        .groupBy(MAXIMUM_DISTINCT_WORDS, i -> i)
        // transform each element to pair with number of words in it
        .map(i -> new Pair<>(i, 1))
        // add counting logic to the streams
        .reduce((left, right) -> new Pair<>(left.first(), left.second() + right.second()))
        // get a stream of word counts
        .mergeSubstreams();




By extracting the parts specific to 
wordcount
 into




a 
groupKey
 function that defines the groups


a 
map
 map each element to value that is used by the reduce on the substream


a 
reduce
 function that does the actual reduction




we get a generalized version below:




Scala




copy
source
def reduceByKey[In, K, Out](maximumGroupSize: Int, groupKey: (In) => K, map: (In) => Out)(
    reduce: (Out, Out) => Out): Flow[In, (K, Out), NotUsed] = {

  Flow[In]
    .groupBy[K](maximumGroupSize, groupKey)
    .map(e => groupKey(e) -> map(e))
    .reduce((l, r) => l._1 -> reduce(l._2, r._2))
    .mergeSubstreams
}

val wordCounts = words.via(
  reduceByKey(MaximumDistinctWords, groupKey = (word: String) => word, map = (word: String) => 1)(
    (left: Int, right: Int) => left + right))


Java




copy
source
public static <In, K, Out> Flow<In, Pair<K, Out>, NotUsed> reduceByKey(
    int maximumGroupSize,
    Function<In, K> groupKey,
    Function<In, Out> map,
    Function2<Out, Out, Out> reduce) {

  return Flow.<In>create()
      .groupBy(maximumGroupSize, groupKey)
      .map(i -> new Pair<>(groupKey.apply(i), map.apply(i)))
      .reduce(
          (left, right) -> new Pair<>(left.first(), reduce.apply(left.second(), right.second())))
      .mergeSubstreams();
}




copy
source
final int MAXIMUM_DISTINCT_WORDS = 1000;

Source<Pair<String, Integer>, NotUsed> counts =
    words.via(
        reduceByKey(
            MAXIMUM_DISTINCT_WORDS,
            word -> word,
            word -> 1,
            (left, right) -> left + right));



Note


Please note that the reduce-by-key version we discussed above is sequential in reading the overall input stream, in other words it is 
NOT
 a parallelization pattern like MapReduce and similar frameworks.


Sorting elements to multiple groups with groupBy


Situation:
 The 
groupBy
 operation strictly partitions incoming elements, each element belongs to exactly one group. Sometimes we want to map elements into multiple groups simultaneously.


To achieve the desired result, we attack the problem in two steps:




first, using a function 
topicMapper
 that gives a list of topics (groups) a message belongs to, we transform our stream of 
Message
 to a stream of 
(Message, Topic)
 
Pair<Message, Topic>
 where for each topic the message belongs to a separate pair will be emitted. This is achieved by using 
mapConcat


Then we take this new stream of message topic pairs (containing a separate pair for each topic a given message belongs to) and feed it into groupBy, using the topic as the group key.






Scala




copy
source
val topicMapper: (Message) => immutable.Seq[Topic] = extractTopics

val messageAndTopic: Source[(Message, Topic), NotUsed] = elems.mapConcat { (msg: Message) =>
  val topicsForMessage = topicMapper(msg)
  // Create a (Msg, Topic) pair for each of the topics
  // the message belongs to
  topicsForMessage.map(msg -> _)
}

val multiGroups = messageAndTopic.groupBy(2, _._2).map {
  case (msg, topic) =>
    // do what needs to be done
}


Java




copy
source
final Function<Message, List<Topic>> topicMapper = m -> extractTopics(m);

final Source<Pair<Message, Topic>, NotUsed> messageAndTopic =
    elems.mapConcat(
        (Message msg) -> {
          List<Topic> topicsForMessage = topicMapper.apply(msg);
          // Create a (Msg, Topic) pair for each of the topics

          // the message belongs to
          return topicsForMessage.stream()
              .map(topic -> new Pair<Message, Topic>(msg, topic))
              .collect(toList());
        });

SubSource<Pair<Message, Topic>, NotUsed> multiGroups =
    messageAndTopic
        .groupBy(2, pair -> pair.second())
        .map(
            pair -> {
              Message message = pair.first();
              Topic topic = pair.second();

              // do what needs to be done
            });




Adhoc source


Situation:
 The idea is that you have a source which you don’t want to start until you have a demand. Also, you want to shut it down when there is no more demand, and start it up again there is new demand again.


You can achieve this behavior by combining 
lazySource
, 
backpressureTimeout
 and 
recoverWithRetries
 as follows:




Scala




copy
source
def adhocSource[T](source: Source[T, _], timeout: FiniteDuration, maxRetries: Int): Source[T, _] =
  Source.lazySource(
    () =>
      source
        .backpressureTimeout(timeout)
        .recoverWithRetries(maxRetries, {
          case _: TimeoutException =>
            Source.lazySource(() => source.backpressureTimeout(timeout)).mapMaterializedValue(_ => NotUsed)
        }))


Java




copy
source
public <T> Source<T, ?> adhocSource(Source<T, ?> source, Duration timeout, int maxRetries) {
  return Source.lazySource(
      () ->
          source
              .backpressureTimeout(timeout)
              .recoverWithRetries(
                  maxRetries,
                  new PFBuilder<Throwable, Source<T, NotUsed>>()
                      .match(
                          TimeoutException.class,
                          ex ->
                              Source.lazySource(() -> source.backpressureTimeout(timeout))
                                  .mapMaterializedValue(v -> NotUsed.getInstance()))
                      .build()));
}




Working with Operators


In this collection we show recipes that use stream operators to achieve various goals.


Triggering the flow of elements programmatically


Situation:
 Given a stream of elements we want to control the emission of those elements according to a trigger signal. In other words, even if the stream would be able to flow (not being backpressured) we want to hold back elements until a trigger signal arrives.


This recipe solves the problem by zipping the stream of 
Message
 elements with the stream of 
Trigger
 signals. Since 
Zip
 produces pairs, we map the output stream selecting the first element of the pair.




Scala




copy
source
val graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>
  import GraphDSL.Implicits._
  val zip = builder.add(Zip[Message, Trigger]())
  elements ~> zip.in0
  triggerSource ~> zip.in1
  zip.out ~> Flow[(Message, Trigger)].map { case (msg, _) => msg } ~> sink
  ClosedShape
})


Java




copy
source
final RunnableGraph<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>> g =
    RunnableGraph
        .<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>>fromGraph(
            GraphDSL.create(
                triggerSource,
                messageSink,
                (p, s) -> new Pair<>(p, s),
                (builder, source, sink) -> {
                  SourceShape<Message> elements =
                      builder.add(
                          Source.from(Arrays.asList("1", "2", "3", "4"))
                              .map(t -> new Message(t)));
                  FlowShape<Pair<Message, Trigger>, Message> takeMessage =
                      builder.add(
                          Flow.<Pair<Message, Trigger>>create().map(p -> p.first()));
                  final FanInShape2<Message, Trigger, Pair<Message, Trigger>> zip =
                      builder.add(Zip.create());
                  builder.from(elements).toInlet(zip.in0());
                  builder.from(source).toInlet(zip.in1());
                  builder.from(zip.out()).via(takeMessage).to(sink);
                  return ClosedShape.getInstance();
                }));




Alternatively, instead of using a 
Zip
, and then using 
map
 to get the first element of the pairs, we can avoid creating the pairs in the first place by using 
ZipWith
 which takes a two argument function to produce the output element. If this function would return a pair of the two argument it would be exactly the behavior of 
Zip
 so 
ZipWith
 is a generalization of zipping.




Scala




copy
source
val graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>
  import GraphDSL.Implicits._
  val zip = builder.add(ZipWith((msg: Message, _: Trigger) => msg))

  elements ~> zip.in0
  triggerSource ~> zip.in1
  zip.out ~> sink
  ClosedShape
})


Java




copy
source
final RunnableGraph<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>> g =
    RunnableGraph
        .<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>>fromGraph(
            GraphDSL.create(
                triggerSource,
                messageSink,
                (p, s) -> new Pair<>(p, s),
                (builder, source, sink) -> {
                  final SourceShape<Message> elements =
                      builder.add(
                          Source.from(Arrays.asList("1", "2", "3", "4"))
                              .map(t -> new Message(t)));
                  final FanInShape2<Message, Trigger, Message> zipWith =
                      builder.add(ZipWith.create((msg, trigger) -> msg));
                  builder.from(elements).toInlet(zipWith.in0());
                  builder.from(source).toInlet(zipWith.in1());
                  builder.from(zipWith.out()).to(sink);
                  return ClosedShape.getInstance();
                }));






Balancing jobs to a fixed pool of workers


Situation:
 Given a stream of jobs and a worker process expressed as a 
Flow
 create a pool of workers that automatically balances incoming jobs to available workers, then merges the results.


We will express our solution as a function that takes a worker flow and the number of workers to be allocated and gives a flow that internally contains a pool of these workers. To achieve the desired result we will create a 
Flow
 from an operator.


The operator consists of a 
Balance
 node which is a special fan-out operation that tries to route elements to available downstream consumers. In a 
for
 loop we wire all of our desired workers as outputs of this balancer element, then we wire the outputs of these workers to a 
Merge
 element that will collect the results from the workers.


To make the worker operators run in parallel we mark them as asynchronous with 
async
.




Scala




copy
source
def balancer[In, Out](worker: Flow[In, Out, Any], workerCount: Int): Flow[In, Out, NotUsed] = {
  import GraphDSL.Implicits._

  Flow.fromGraph(GraphDSL.create() { implicit b =>
    val balancer = b.add(Balance[In](workerCount, waitForAllDownstreams = true))
    val merge = b.add(Merge[Out](workerCount))

    for (_ <- 1 to workerCount) {
      // for each worker, add an edge from the balancer to the worker, then wire
      // it to the merge element
      balancer ~> worker.async ~> merge
    }

    FlowShape(balancer.in, merge.out)
  })
}

val processedJobs: Source[Result, NotUsed] = myJobs.via(balancer(worker, 3))


Java




copy
source
public static <In, Out> Flow<In, Out, NotUsed> balancer(
    Flow<In, Out, NotUsed> worker, int workerCount) {
  return Flow.fromGraph(
      GraphDSL.create(
          b -> {
            boolean waitForAllDownstreams = true;
            final UniformFanOutShape<In, In> balance =
                b.add(Balance.<In>create(workerCount, waitForAllDownstreams));
            final UniformFanInShape<Out, Out> merge = b.add(Merge.<Out>create(workerCount));

            for (int i = 0; i < workerCount; i++) {
              b.from(balance.out(i)).via(b.add(worker.async())).toInlet(merge.in(i));
            }

            return FlowShape.of(balance.in(), merge.out());
          }));
}




copy
source
Flow<Message, Message, NotUsed> balancer = balancer(worker, 3);
Source<Message, NotUsed> processedJobs = data.via(balancer);




Working with rate


This collection of recipes demonstrate various patterns where rate differences between upstream and downstream needs to be handled by other strategies than simple backpressure.


Dropping elements


Situation:
 Given a fast producer and a slow consumer, we want to drop elements if necessary to not slow down the producer too much.


This can be solved by using a versatile rate-transforming operation, 
conflate
. Conflate can be thought as a special 
reduce
 operation that collapses multiple upstream elements into one aggregate element if needed to keep the speed of the upstream unaffected by the downstream.


When the upstream is faster, the reducing process of the 
conflate
 starts. Our reducer function takes the freshest element. This in a simple dropping operation.




Scala




copy
source
val droppyStream: Flow[Message, Message, NotUsed] =
  Flow[Message].conflate((lastMessage, newMessage) => newMessage)


Java




copy
source
final Flow<Message, Message, NotUsed> droppyStream =
    Flow.of(Message.class).conflate((lastMessage, newMessage) -> newMessage);




There is a more general version of 
conflate
 named 
conflateWithSeed
 that allows to express more complex aggregations, more similar to a 
fold
.


Dropping broadcast


Situation:
 The default 
Broadcast
 operator is properly backpressured, but that means that a slow downstream consumer can hold back the other downstream consumers resulting in lowered throughput. In other words the rate of 
Broadcast
 is the rate of its slowest downstream consumer. In certain cases it is desirable to allow faster consumers to progress independently of their slower siblings by dropping elements if necessary.


One solution to this problem is to append a 
buffer
 element in front of all of the downstream consumers defining a dropping strategy instead of the default 
Backpressure
. This allows small temporary rate differences between the different consumers (the buffer smooths out small rate variances), but also allows faster consumers to progress by dropping from the buffer of the slow consumers if necessary.




Scala




copy
source
val graph = RunnableGraph.fromGraph(GraphDSL.createGraph(mySink1, mySink2, mySink3)((_, _, _)) {
  implicit b => (sink1, sink2, sink3) =>
    import GraphDSL.Implicits._

    val bcast = b.add(Broadcast[Int](3))
    myElements ~> bcast

    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink1
    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink2
    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink3
    ClosedShape
})


Java




copy
source
// Makes a sink drop elements if too slow
public <T> Sink<T, CompletionStage<Done>> droppySink(
    Sink<T, CompletionStage<Done>> sink, int size) {
  return Flow.<T>create().buffer(size, OverflowStrategy.dropHead()).toMat(sink, Keep.right());
}




copy
source
RunnableGraph.fromGraph(
    GraphDSL.create(
        builder -> {
          final int outputCount = 3;
          final UniformFanOutShape<Integer, Integer> bcast =
              builder.add(Broadcast.create(outputCount));
          builder.from(builder.add(myData)).toFanOut(bcast);
          builder.from(bcast).to(builder.add(droppySink(mySink1, 10)));
          builder.from(bcast).to(builder.add(droppySink(mySink2, 10)));
          builder.from(bcast).to(builder.add(droppySink(mySink3, 10)));
          return ClosedShape.getInstance();
        }));




Collecting missed ticks


Situation:
 Given a regular (stream) source of ticks, instead of trying to backpressure the producer of the ticks we want to keep a counter of the missed ticks instead and pass it down when possible.


We will use 
conflateWithSeed
 to solve the problem. The seed version of conflate takes two functions:




A seed function that produces the zero element for the folding process that happens when the upstream is faster than the downstream. In our case the seed function is a constant function that returns 0 since there were no missed ticks at that point.


A fold function that is invoked when multiple upstream messages needs to be collapsed to an aggregate value due to the insufficient processing rate of the downstream. Our folding function increments the currently stored count of the missed ticks so far.




As a result, we have a flow of 
Int
 where the number represents the missed ticks. A number 0 means that we were able to consume the tick fast enough (i.e. zero means: 1 non-missed tick + 0 missed ticks)




Scala




copy
source
val missedTicks: Flow[Tick, Int, NotUsed] =
  Flow[Tick].conflateWithSeed(seed = _ => 0)((missedTicks, _) => missedTicks + 1)


Java




copy
source
final Flow<Tick, Integer, NotUsed> missedTicks =
    Flow.of(Tick.class).conflateWithSeed(tick -> 0, (missed, tick) -> missed + 1);




Create a stream processor that repeats the last element seen


Situation:
 Given a producer and consumer, where the rate of neither is known in advance, we want to ensure that none of them is slowing down the other by dropping earlier unconsumed elements from the upstream if necessary, and repeating the last value for the downstream if necessary.


We have two options to implement this feature. In both cases we will use 
GraphStage
, to build our custom operator. In the first version we will use a provided initial value 
initial
 that will be used to feed the downstream if no upstream element is ready yet. In the 
onPush()
 handler we overwrite the 
currentValue
 variable and immediately relieve the upstream by calling 
pull()
. The downstream 
onPull
 handler is very similar, we immediately relieve the downstream by emitting 
currentValue
.




Scala




copy
source
import akka.stream._
import akka.stream.stage._
final class HoldWithInitial[T](initial: T) extends GraphStage[FlowShape[T, T]] {
  val in = Inlet[T]("HoldWithInitial.in")
  val out = Outlet[T]("HoldWithInitial.out")

  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var currentValue: T = initial

    setHandlers(in, out, new InHandler with OutHandler {
      override def onPush(): Unit = {
        currentValue = grab(in)
        pull(in)
      }

      override def onPull(): Unit = {
        push(out, currentValue)
      }
    })

    override def preStart(): Unit = {
      pull(in)
    }
  }

}


Java




copy
source
class HoldWithInitial<T> extends GraphStage<FlowShape<T, T>> {

  public Inlet<T> in = Inlet.<T>create("HoldWithInitial.in");
  public Outlet<T> out = Outlet.<T>create("HoldWithInitial.out");
  private FlowShape<T, T> shape = FlowShape.of(in, out);

  private final T initial;

  public HoldWithInitial(T initial) {
    this.initial = initial;
  }

  @Override
  public FlowShape<T, T> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private T currentValue = initial;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                currentValue = grab(in);
                pull(in);
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                push(out, currentValue);
              }
            });
      }

      @Override
      public void preStart() {
        pull(in);
      }
    };
  }
}




While it is relatively simple, the drawback of the first version is that it needs an arbitrary initial element which is not always possible to provide. Hence, we create a second version where the downstream might need to wait in one single case: if the very first element is not yet available.


We introduce a boolean variable 
waitingFirstValue
 to denote whether the first element has been provided or not (alternatively an 
Option
 
Optional
 can be used for 
currentValue
 or if the element type is a subclass of 
AnyRef
 
Object
 a null can be used with the same purpose). In the downstream 
onPull()
 handler the difference from the previous version is that we check if we have received the first value and only emit if we have. This leads to that when the first element comes in we must check if there possibly already was demand from downstream so that we in that case can push the element directly.




Scala




copy
source
import akka.stream._
import akka.stream.stage._
final class HoldWithWait[T] extends GraphStage[FlowShape[T, T]] {
  val in = Inlet[T]("HoldWithWait.in")
  val out = Outlet[T]("HoldWithWait.out")

  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var currentValue: T = _
    private var waitingFirstValue = true

    setHandlers(
      in,
      out,
      new InHandler with OutHandler {
        override def onPush(): Unit = {
          currentValue = grab(in)
          if (waitingFirstValue) {
            waitingFirstValue = false
            if (isAvailable(out)) push(out, currentValue)
          }
          pull(in)
        }

        override def onPull(): Unit = {
          if (!waitingFirstValue) push(out, currentValue)
        }
      })

    override def preStart(): Unit = {
      pull(in)
    }
  }
}


Java




copy
source
class HoldWithWait<T> extends GraphStage<FlowShape<T, T>> {
  public Inlet<T> in = Inlet.<T>create("HoldWithInitial.in");
  public Outlet<T> out = Outlet.<T>create("HoldWithInitial.out");
  private FlowShape<T, T> shape = FlowShape.of(in, out);

  @Override
  public FlowShape<T, T> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private T currentValue = null;
      private boolean waitingFirstValue = true;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                currentValue = grab(in);
                if (waitingFirstValue) {
                  waitingFirstValue = false;
                  if (isAvailable(out)) push(out, currentValue);
                }
                pull(in);
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                if (!waitingFirstValue) push(out, currentValue);
              }
            });
      }

      @Override
      public void preStart() {
        pull(in);
      }
    };
  }
}




Globally limiting the rate of a set of streams


Situation:
 Given a set of independent streams that we cannot merge, we want to globally limit the aggregate throughput of the set of streams.


One possible solution uses a shared actor as the global limiter combined with mapAsync to create a reusable 
Flow
 that can be plugged into a stream to limit its rate.


As the first step we define an actor that will do the accounting for the global rate limit. The actor maintains a timer, a counter for pending permit tokens and a queue for possibly waiting participants. The actor has an 
open
 and 
closed
 state. The actor is in the 
open
 state while it has still pending permits. Whenever a request for permit arrives as a 
WantToPass
 message to the actor the number of available permits is decremented and we notify the sender that it can pass by answering with a 
MayPass
 message. If the amount of permits reaches zero, the actor transitions to the 
closed
 state. In this state requests are not immediately answered, instead the reference of the sender is added to a queue. Once the timer for replenishing the pending permits fires by sending a 
ReplenishTokens
 message, we increment the pending permits counter and send a reply to each of the waiting senders. If there are more waiting senders than permits available we will stay in the 
closed
 state.




Scala




copy
source
object Limiter {
  case object WantToPass
  case object MayPass

  case object ReplenishTokens

  def props(maxAvailableTokens: Int, tokenRefreshPeriod: FiniteDuration, tokenRefreshAmount: Int): Props =
    Props(new Limiter(maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount))
}

class Limiter(val maxAvailableTokens: Int, val tokenRefreshPeriod: FiniteDuration, val tokenRefreshAmount: Int)
    extends Actor {
  import Limiter._
  import context.dispatcher
  import akka.actor.Status

  private var waitQueue = immutable.Queue.empty[ActorRef]
  private var permitTokens = maxAvailableTokens
  private val replenishTimer = system.scheduler.scheduleWithFixedDelay(
    initialDelay = tokenRefreshPeriod,
    delay = tokenRefreshPeriod,
    receiver = self,
    ReplenishTokens)

  override def receive: Receive = open

  val open: Receive = {
    case ReplenishTokens =>
      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)
    case WantToPass =>
      permitTokens -= 1
      sender() ! MayPass
      if (permitTokens == 0) context.become(closed)
  }

  val closed: Receive = {
    case ReplenishTokens =>
      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)
      releaseWaiting()
    case WantToPass =>
      waitQueue = waitQueue.enqueue(sender())
  }

  private def releaseWaiting(): Unit = {
    val (toBeReleased, remainingQueue) = waitQueue.splitAt(permitTokens)
    waitQueue = remainingQueue
    permitTokens -= toBeReleased.size
    toBeReleased.foreach(_ ! MayPass)
    if (permitTokens > 0) context.become(open)
  }

  override def postStop(): Unit = {
    replenishTimer.cancel()
    waitQueue.foreach(_ ! Status.Failure(new IllegalStateException("limiter stopped")))
  }
}


Java




copy
source
static class Limiter extends AbstractActor {

  public static class WantToPass {}

  public static final WantToPass WANT_TO_PASS = new WantToPass();

  public static class MayPass {}

  public static final MayPass MAY_PASS = new MayPass();

  public static class ReplenishTokens {}

  public static final ReplenishTokens REPLENISH_TOKENS = new ReplenishTokens();

  private final int maxAvailableTokens;
  private final Duration tokenRefreshPeriod;
  private final int tokenRefreshAmount;

  private final List<ActorRef> waitQueue = new ArrayList<>();
  private final Cancellable replenishTimer;

  private int permitTokens;

  public static Props props(
      int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {
    return Props.create(
        Limiter.class, maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount);
  }

  private Limiter(int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {
    this.maxAvailableTokens = maxAvailableTokens;
    this.tokenRefreshPeriod = tokenRefreshPeriod;
    this.tokenRefreshAmount = tokenRefreshAmount;
    this.permitTokens = maxAvailableTokens;

    this.replenishTimer =
        system
            .scheduler()
            .scheduleWithFixedDelay(
                this.tokenRefreshPeriod,
                this.tokenRefreshPeriod,
                getSelf(),
                REPLENISH_TOKENS,
                getContext().getSystem().dispatcher(),
                getSelf());
  }

  @Override
  public Receive createReceive() {
    return open();
  }

  private Receive open() {
    return receiveBuilder()
        .match(
            ReplenishTokens.class,
            rt -> {
              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);
            })
        .match(
            WantToPass.class,
            wtp -> {
              permitTokens -= 1;
              getSender().tell(MAY_PASS, getSelf());
              if (permitTokens == 0) {
                getContext().become(closed());
              }
            })
        .build();
  }

  private Receive closed() {
    return receiveBuilder()
        .match(
            ReplenishTokens.class,
            rt -> {
              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);
              releaseWaiting();
            })
        .match(
            WantToPass.class,
            wtp -> {
              waitQueue.add(getSender());
            })
        .build();
  }

  private void releaseWaiting() {
    final List<ActorRef> toBeReleased = new ArrayList<>(permitTokens);
    for (Iterator<ActorRef> it = waitQueue.iterator(); permitTokens > 0 && it.hasNext(); ) {
      toBeReleased.add(it.next());
      it.remove();
      permitTokens--;
    }

    toBeReleased.stream().forEach(ref -> ref.tell(MAY_PASS, getSelf()));
    if (permitTokens > 0) {
      getContext().become(open());
    }
  }

  @Override
  public void postStop() {
    replenishTimer.cancel();
    waitQueue.stream()
        .forEach(
            ref -> {
              ref.tell(
                  new Status.Failure(new IllegalStateException("limiter stopped")), getSelf());
            });
  }
}




To create a Flow that uses this global limiter actor we use the 
mapAsync
 function with the combination of the 
ask
 pattern. We also define a timeout, so if a reply is not received during the configured maximum wait period the returned future from 
ask
 will fail, which will fail the corresponding stream as well.




Scala




copy
source
def limitGlobal[T](limiter: ActorRef, maxAllowedWait: FiniteDuration): Flow[T, T, NotUsed] = {
  import akka.pattern.ask
  import akka.util.Timeout
  Flow[T].mapAsync(4)((element: T) => {
    import system.dispatcher
    implicit val triggerTimeout = Timeout(maxAllowedWait)
    val limiterTriggerFuture = limiter ? Limiter.WantToPass
    limiterTriggerFuture.map((_) => element)
  })

}


Java




copy
source
public <T> Flow<T, T, NotUsed> limitGlobal(ActorRef limiter, Duration maxAllowedWait) {
  final int parallelism = 4;
  final Flow<T, T, NotUsed> f = Flow.create();

  return f.mapAsync(
      parallelism,
      element -> {
        final CompletionStage<Object> limiterTriggerFuture =
            Patterns.ask(limiter, Limiter.WANT_TO_PASS, maxAllowedWait);
        return limiterTriggerFuture.thenApplyAsync(response -> element, system.dispatcher());
      });
}


Note


The global actor used for limiting introduces a global bottleneck. You might want to assign a dedicated dispatcher for this actor.


Working with IO


Chunking up a stream of ByteStrings into limited size ByteStrings


Situation:
 Given a stream of 
ByteString
 s we want to produce a stream of 
ByteString
 s containing the same bytes in the same sequence, but capping the size of 
ByteString
 s. In other words we want to slice up 
ByteString
 s into smaller chunks if they exceed a size threshold.


This can be achieved with a single 
GraphStage
 to define a custom Akka Stream operator. The main logic of our operator is in 
emitChunk()
 which implements the following logic:




if the buffer is empty, and upstream is not closed we pull for more bytes, if it is closed we complete


if the buffer is nonEmpty, we split it according to the 
chunkSize
. This will give a next chunk that we will emit, and an empty or nonempty remaining buffer.




Both 
onPush()
 and 
onPull()
 calls 
emitChunk()
 the only difference is that the push handler also stores the incoming chunk by appending to the end of the buffer.




Scala




copy
source
import akka.stream.stage._

class Chunker(val chunkSize: Int) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString]("Chunker.in")
  val out = Outlet[ByteString]("Chunker.out")
  override val shape = FlowShape.of(in, out)
  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var buffer = ByteString.empty

    setHandler(out, new OutHandler {
      override def onPull(): Unit = {
        emitChunk()
      }
    })
    setHandler(
      in,
      new InHandler {
        override def onPush(): Unit = {
          val elem = grab(in)
          buffer ++= elem
          emitChunk()
        }

        override def onUpstreamFinish(): Unit = {
          if (buffer.isEmpty) completeStage()
          else {
            // There are elements left in buffer, so
            // we keep accepting downstream pulls and push from buffer until emptied.
            //
            // It might be though, that the upstream finished while it was pulled, in which
            // case we will not get an onPull from the downstream, because we already had one.
            // In that case we need to emit from the buffer.
            if (isAvailable(out)) emitChunk()
          }
        }
      })

    private def emitChunk(): Unit = {
      if (buffer.isEmpty) {
        if (isClosed(in)) completeStage()
        else pull(in)
      } else {
        val (chunk, nextBuffer) = buffer.splitAt(chunkSize)
        buffer = nextBuffer
        push(out, chunk)
      }
    }
  }
}

val chunksStream = rawBytes.via(new Chunker(ChunkLimit))


Java




copy
source
class Chunker extends GraphStage<FlowShape<ByteString, ByteString>> {

  private final int chunkSize;

  public Inlet<ByteString> in = Inlet.<ByteString>create("Chunker.in");
  public Outlet<ByteString> out = Outlet.<ByteString>create("Chunker.out");
  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);

  public Chunker(int chunkSize) {
    this.chunkSize = chunkSize;
  }

  @Override
  public FlowShape<ByteString, ByteString> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private ByteString buffer = emptyByteString();

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                emitChunk();
              }
            });

        setHandler(
            in,
            new AbstractInHandler() {

              @Override
              public void onPush() throws Exception {
                ByteString elem = grab(in);
                buffer = buffer.concat(elem);
                emitChunk();
              }

              @Override
              public void onUpstreamFinish() throws Exception {
                if (buffer.isEmpty()) completeStage();
                else {
                  // There are elements left in buffer, so
                  // we keep accepting downstream pulls and push from buffer until emptied.
                  //
                  // It might be though, that the upstream finished while it was pulled, in
                  // which
                  // case we will not get an onPull from the downstream, because we already
                  // had one.
                  // In that case we need to emit from the buffer.
                  if (isAvailable(out)) emitChunk();
                }
              }
            });
      }

      private void emitChunk() {
        if (buffer.isEmpty()) {
          if (isClosed(in)) completeStage();
          else pull(in);
        } else {
          Tuple2<ByteString, ByteString> split = buffer.splitAt(chunkSize);
          ByteString chunk = split._1();
          buffer = split._2();
          push(out, chunk);
        }
      }
    };
  }
}




copy
source
Source<ByteString, NotUsed> chunksStream = rawBytes.via(new Chunker(CHUNK_LIMIT));




Limit the number of bytes passing through a stream of ByteStrings


Situation:
 Given a stream of 
ByteString
 s we want to fail the stream if more than a given maximum of bytes has been consumed.


This recipe uses a 
GraphStage
 to implement the desired feature. In the only handler we override, 
onPush()
 we update a counter and see if it gets larger than 
maximumBytes
. If a violation happens we signal failure, otherwise we forward the chunk we have received.




Scala




copy
source
import akka.stream.stage._
class ByteLimiter(val maximumBytes: Long) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString]("ByteLimiter.in")
  val out = Outlet[ByteString]("ByteLimiter.out")
  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var count = 0

    setHandlers(in, out, new InHandler with OutHandler {

      override def onPull(): Unit = {
        pull(in)
      }

      override def onPush(): Unit = {
        val chunk = grab(in)
        count += chunk.size
        if (count > maximumBytes) failStage(new IllegalStateException("Too much bytes"))
        else push(out, chunk)
      }
    })
  }
}

val limiter = Flow[ByteString].via(new ByteLimiter(SizeLimit))


Java




copy
source
class ByteLimiter extends GraphStage<FlowShape<ByteString, ByteString>> {

  final long maximumBytes;

  public Inlet<ByteString> in = Inlet.<ByteString>create("ByteLimiter.in");
  public Outlet<ByteString> out = Outlet.<ByteString>create("ByteLimiter.out");
  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);

  public ByteLimiter(long maximumBytes) {
    this.maximumBytes = maximumBytes;
  }

  @Override
  public FlowShape<ByteString, ByteString> shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private int count = 0;

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                ByteString chunk = grab(in);
                count += chunk.size();
                if (count > maximumBytes) {
                  failStage(new IllegalStateException("Too much bytes"));
                } else {
                  push(out, chunk);
                }
              }
            });
      }
    };
  }
}


copy
source
Flow<ByteString, ByteString, NotUsed> limiter =
    Flow.of(ByteString.class).via(new ByteLimiter(SIZE_LIMIT));




Compact ByteStrings in a stream of ByteStrings


Situation:
 After a long stream of transformations, due to their immutable, structural sharing nature 
ByteString
 s may refer to multiple original ByteString instances unnecessarily retaining memory. As the final step of a transformation chain we want to have clean copies that are no longer referencing the original 
ByteString
 s.


The recipe is a simple use of map, calling the 
compact()
 method of the 
ByteString
 elements. This does copying of the underlying arrays, so this should be the last element of a long chain if used.




Scala




copy
source
val compacted: Source[ByteString, NotUsed] = data.map(_.compact)


Java




copy
source
Source<ByteString, NotUsed> compacted = rawBytes.map(ByteString::compact);




Injecting keep-alive messages into a stream of ByteStrings


Situation:
 Given a communication channel expressed as a stream of 
ByteString
 s we want to inject keep-alive messages but only if this does not interfere with normal traffic.


There is a built-in operation that allows to do this directly:




Scala




copy
source
import scala.concurrent.duration._
val injectKeepAlive: Flow[ByteString, ByteString, NotUsed] =
  Flow[ByteString].keepAlive(1.second, () => keepaliveMessage)


Java




copy
source
Flow<ByteString, ByteString, NotUsed> keepAliveInject =
    Flow.of(ByteString.class).keepAlive(Duration.ofSeconds(1), () -> keepAliveMessage);
















 
Substreams






Configuration 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io
Build and run apps that react to change






















































































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



















































































        Build and run apps that react to change
      








Responsive by design, Akka apps are elastic, agile, and resilient.
Simple dev. Automated ops.










GET STARTED










































WHAT IS AKKA










































99.9999%




MULTI-REGION AVAILABILITY






99.9999% multi-region availability




only with Akka!












































Responsive by Design






Akka is relied upon when application responsiveness must be guaranteed. Engineering teams use a simple SDK and powerful libraries to build transactional, durable, and real-time services that distribute logic and data together. Operations are fully automated in serverless and BYOC environments, and a self-hosted option is available. 


























































What


is Akka






A platform to build and run apps that are elastic, agile, and resilient.












EXPLORE




















































Why


choose Akka






To guarantee responsiveness with apps that maintain responsibility for their own outcomes.












ADAPT CONTINUOUSLY




















































How


Akka works






By distributing logic and data together in movable apps that act as their own in-memory, durable database.












LOOK INSIDE
















































































It's not what we say...

























                  HPE redefines customer experiences processing petabytes of data
                










CASE STUDY

























































                  Verizon drives 235% sales growth and cuts its
hardware by half
                










CASE STUDY

























































                  Akka powers CERN’s NXCALS system for groundbreaking physics
                










CASE STUDY

























































                  Dream11 cuts cloud
costs by 30%
                










CASE STUDY

























































                  Tubi boots ad revenue with unique hyper-personalized experiences
                










CASE STUDY

























































                  Norwegian sets sail for growth with agility and stability boosts
                










CASE STUDY








































































































...it's the apps you build...






Use Akka to build transactional systems, AI inference, real-time data, edge and IoT, workflows, and globally distributed apps.  




























Transactional




Embed in-memory databases










KNOW MORE






























































Event-Sourced 




Track and replay state










KNOW MORE






























































AI




Inference for AI models










KNOW MORE






























































Digital Twin




Converge replicated data










KNOW MORE






























































Durable Execution




Orchestrate persisted processes










KNOW MORE






























































Analytics




Analyze real-time data points










KNOW MORE






























































Streaming




Process continuous data










KNOW MORE






























































Edge




Daisy chain device-to-cloud data










KNOW MORE






































































































...and it's guaranteed.




























The Akka Resilience Guarantee


Akka will indemnify against losses caused by an Akka App becoming unreliable.  Psst, it’s never happened.
















Akka IP Protections


Akka indemnifies our IP and dependencies when open source cannot.
















Akka Data Protections


Akka is audited annually against the AICPA SOC2 standard.
















Akka Cybersecurity Protections


We develop, maintain, and support Akka according to detailed cybersecurity processes, including NIST.










































Stay Responsive 
to Change.










GET STARTED










































REQUEST A DEMO








































































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/2017-08-09-camel.html
Camel Dependency, Fixed in Akka 2.5.4 • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4




Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements




Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4




Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements




Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Camel Dependency, Fixed in Akka 2.5.4


Date


9 August 2017


Description of Vulnerability


Apache Camel’s Validation Component is vulnerable against SSRF via remote DTDs and XXE, as described in 
CVE-2017-5643


To protect against such attacks the system should be updated to Akka 
2.4.20
, 
2.5.4
 or later. Dependencies to Camel libraries should be updated to version 2.17.7.


Severity


The 
CVSS
 score of this vulnerability is 7.4 (High), according to 
CVE-2017-5643
.


Affected Versions




Akka 
2.4.19
 and prior


Akka 
2.5.3
 and prior




Fixed Versions


We have prepared patches for the affected versions, and have released the following versions which resolve the issue: 




Akka 
2.4.20
 (Scala 2.11, 2.12)


Akka 
2.5.4
 (Scala 2.11, 2.12)




Acknowledgements


We would like to thank Thomas Szymanski for bringing this issue to our attention.














 
Java Serialization, Fixed in Akka 2.4.17






Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/index.html#fixed-security-vulnerabilities
Security Announcements • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Security Announcements
Note


Security announcements has moved to a shared page for all Akka projects and can now be found at 
akka.io/security


Receiving Security Advisories


The best way to receive any and all security announcements is to subscribe to the 
Akka security list
.


The mailing list is very low traffic, and receives notifications only after security reports have been managed by the core team and fixes are publicly available.


Reporting Vulnerabilities


We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.


Following best practice, we strongly encourage anyone to report potential security vulnerabilities to 
[email protected]
 before disclosing them in a public forum like the mailing list or as a GitHub issue.


Reports to this email address will be handled by our security team, who will work together with you to ensure that a fix can be provided without delay.


Security Related Documentation




Java Serialization


Remote deployment allow list


Remote Security




Fixed Security Vulnerabilities






Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


















 
Akka Documentation






Java Serialization, Fixed in Akka 2.4.17 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-core/current/stream/
Streams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Streams






Introduction




Motivation


How to read these docs


Module info




Streams Quickstart Guide




Dependency


First steps


Reusable Pieces


Time-Based Processing


Reactive Tweets




Design Principles behind Akka Streams




What shall users of Akka Streams expect?


Interoperation with other Reactive Streams implementations


What shall users of streaming libraries expect?


The difference between Error and Failure




Basics and working with Flows




Dependency


Introduction


Core concepts


Defining and running streams


Back-pressure explained


Stream Materialization


Stream ordering


Actor Materializer Lifecycle




Working with Graphs




Dependency


Introduction


Constructing Graphs


Constructing and combining Partial Graphs


Constructing Sources, Sinks and Flows from Partial Graphs


Combining Sources and Sinks with simplified API


Building reusable Graph components


Predefined shapes


Bidirectional Flows


Accessing the materialized value inside the Graph


Graph cycles, liveness and deadlocks




Modularity, Composition and Hierarchy




Dependency


Introduction


Basics of composition and modularity


Composing complex systems


Materialized values


Attributes




Buffers and working with rate




Dependency


Introduction


Buffers for asynchronous operators


Buffers in Akka Streams


Rate transformation




Context Propagation




Restrictions


Creation


Composition




Dynamic stream handling




Dependency


Introduction


Controlling stream completion with KillSwitch


Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub




Custom stream processing




Dependency


Introduction


Custom processing with GraphStage


Thread safety of custom operators


Resources and the operator lifecycle


Extending Flow Operators with Custom Operators




Futures interop




Dependency


Overview




Actors interop




Dependency


Overview




Reactive Streams Interop




Dependency


Overview


Other implementations




Error Handling in Streams




Dependency


Introduction


Logging errors


Recover


Recover with retries


Delayed restarts with a backoff operator


Supervision Strategies




Working with streaming IO




Dependency


Introduction


Streaming TCP


Streaming File IO




StreamRefs - Reactive Streams over the network




Dependency


Introduction


Stream References


Bulk Stream References


Serialization of SourceRef and SinkRef


Configuration




Pipelining and Parallelism




Dependency


Introduction


Pipelining


Parallel processing


Combining pipelining and parallel processing




Testing streams




Dependency


Introduction


Built-in sources, sinks and operators


TestKit


Streams TestKit


Fuzzing Mode




Substreams




Dependency


Introduction


Nesting operators


Flattening operators




Streams Cookbook




Dependency


Introduction


Working with Flows


Working with Operators


Working with rate


Working with IO




Configuration


Operators




Source operators


Sink operators


Additional Sink and Source converters


File IO Sinks and Sources


Simple operators


Flow operators composed of Sinks and Sources


Asynchronous operators


Timer driven operators


Backpressure aware operators


Nesting and flattening operators


Time aware operators


Fan-in operators


Fan-out operators


Watching status operators


Actor interop operators


Compression operators


Error handling


Source.actorRef


Sink.actorRef


ActorSource.actorRef


ActorSink.actorRef


Source.actorRefWithBackpressure


Sink.actorRefWithBackpressure


ActorSource.actorRefWithBackpressure


ActorSink.actorRefWithBackpressure


aggregateWithBoundary


alsoTo


alsoToAll


Flow.asFlowWithContext


StreamConverters.asInputStream


StreamConverters.asJavaStream


ask


ActorFlow.ask


ActorFlow.askWithContext


ActorFlow.askWithStatus


ActorFlow.askWithContext


StreamConverters.asOutputStream


Sink.asPublisher


Source.asSourceWithContext


Source.asSubscriber


backpressureTimeout


Balance


batch


batchWeighted


Broadcast


buffer


Sink.cancelled


collect


Sink.collect


Sink.collection


collectType


Source.combine


Sink.combine


Source.completionStage


Flow.completionStageFlow


Sink.completionStageSink


Source.completionStageSource


completionTimeout


concat


concatAllLazy


concatLazy


conflate


conflateWithSeed


contramap


Source.cycle


Compression.deflate


delay


delayWith


detach


divertTo


drop


dropWhile


dropWithin


Source.empty


expand


extrapolate


Source.failed


filter


filterNot


flatMapConcat


flatMapMerge


flatMapPrefix


Flow.flattenOptional


fold


Sink.fold


foldAsync


Sink.foreach


Sink.foreachAsync


Source.apply
Source.from


Source.fromCompletionStage


FileIO.fromFile


Source.fromFuture


Source.fromFutureSource


StreamConverters.fromInputStream


Source.fromIterator


fromJavaStream


StreamConverters.fromJavaStream


fromMaterializer


Sink.fromMaterializer


StreamConverters.fromOutputStream


FileIO.fromPath


Source.fromPublisher


Flow.fromSinkAndSource


Flow.fromSinkAndSourceCoupled


Source.fromSourceCompletionStage


Sink.fromSubscriber


Source.future


Flow.futureFlow


Sink.futureSink


Source.futureSource


groupBy


grouped


groupedWeighted


groupedWeightedWithin


groupedWithin


Compression.gunzip


Compression.gzip


Sink.head


Sink.headOption


idleTimeout


Sink.ignore


Compression.inflate


initialDelay


initialTimeout


interleave


interleaveAll


intersperse


StreamConverters.javaCollector


StreamConverters.javaCollectorParallelUnordered


keepAlive


Sink.last


Sink.lastOption


Source.lazily


Source.lazilyAsync


Source.lazyCompletionStage


Flow.lazyCompletionStageFlow


Sink.lazyCompletionStageSink


Source.lazyCompletionStageSource


Flow.lazyFlow


Source.lazyFuture


Flow.lazyFutureFlow


Sink.lazyFutureSink


Source.lazyFutureSource


Flow.lazyInitAsync


Sink.lazyInitAsync


Source.lazySingle


Sink.lazySink


Source.lazySource


limit


limitWeighted


log


logWithMarker


map


mapAsync


mapAsyncPartitioned


mapAsyncUnordered


mapConcat


mapError


mapWithResource


Source.maybe


merge


mergeAll


mergeLatest


mergePreferred


mergePrioritized


mergePrioritizedN


MergeSequence


mergeSorted


monitor


never


Sink.never


Sink.onComplete


onErrorComplete


RestartSource.onFailuresWithBackoff


RestartFlow.onFailuresWithBackoff


orElse


Partition


prefixAndTail


preMaterialize


Sink.preMaterialize


prepend


prependLazy


Source.queue


Sink.queue


Source.range


recover


recoverWith


recoverWithRetries


reduce


Sink.reduce


Source.repeat


scan


scanAsync


Sink.seq


setup


Sink.setup


Source.single


PubSub.sink


sliding


PubSub.source


splitAfter


splitWhen


statefulMap


statefulMapConcat


take


Sink.takeLast


takeWhile


takeWithin


throttle


Source.tick


FileIO.toFile


FileIO.toPath


Source.unfold


Source.unfoldAsync


Source.unfoldResource


Source.unfoldResourceAsync


Unzip


UnzipWith


watch


watchTermination


wireTap


RestartSource.withBackoff


RestartFlow.withBackoff


RestartSink.withBackoff


RetryFlow.withBackoff


RetryFlow.withBackoffAndContext


zip


zipAll


zipLatest


zipLatestWith


Source.zipN


zipWith


zipWithIndex


Source.zipWithN




















 
Building a storage backend for Durable State






Introduction 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial_4.html
Part 4: Working with Device Groups • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups




Introduction


Device manager hierarchy


The Registration Protocol


Adding registration support to device group actors


Creating device manager actors


What’s next?




Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups




Introduction


Device manager hierarchy


The Registration Protocol


Adding registration support to device group actors


Creating device manager actors


What’s next?




Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Part 4: Working with Device Groups


Introduction


Let’s take a closer look at the main functionality required by our use case. In a complete IoT system for monitoring home temperatures, the steps for connecting a device sensor to our system might look like this:




A sensor device in the home connects through some protocol.


The component managing network connections accepts the connection.


The sensor provides its group and device ID to register with the device manager component of our system.


The device manager component handles registration by looking up or creating the actor responsible for keeping sensor state.


The actor responds with an acknowledgement, exposing its 
ActorRef
ActorRef
.


The networking component now uses the 
ActorRef
 for communication between the sensor and device actor without going through the device manager.




Steps 1 and 2 take place outside the boundaries of our tutorial system. In this chapter, we will start addressing steps 3-6 and create a way for sensors to register with our system and to communicate with actors. But first, we have another architectural decision — how many levels of actors should we use to represent device groups and device sensors?


One of the main design challenges for Akka programmers is choosing the best granularity for actors. In practice, depending on the characteristics of the interactions between actors, there are usually several valid ways to organize a system. In our use case, for example, it would be possible to have a single actor maintain all the groups and devices — perhaps using hash maps. It would also be reasonable to have an actor for each group that tracks the state of all devices in the same home.


The following guidelines help us choose the most appropriate actor hierarchy:




In general, prefer larger granularity. Introducing more fine-grained actors than needed causes more problems than it solves.


Add finer granularity when the system requires:
    


Higher concurrency.


Complex conversations between actors that have many states. We will see a very good example for this in the next chapter.


Sufficient state that it makes sense to divide into smaller actors.


Multiple unrelated responsibilities. Using separate actors allows individuals to fail and be restored with little impact on others.








Device manager hierarchy


Considering the principles outlined in the previous section, we will model the device manager component as an actor tree with three levels:




The top level supervisor actor represents the system component for devices. It is also the entry point to look up and create device group and device actors.


At the next level, group actors each supervise the device actors for one group id (e.g. one home). They also provide services, such as querying temperature readings from all of the available devices in their group.


Device actors manage all the interactions with the actual device sensors, such as storing temperature readings.






We chose this three-layered architecture for these reasons:






Having groups of individual actors:




Isolates failures that occur in a group. If a single actor managed all device groups, an error in one group that causes a restart would wipe out the state of groups that are otherwise non-faulty.


Simplifies the problem of querying all the devices belonging to a group. Each group actor only contains state related to its group.


Increases parallelism in the system. Since each group has a dedicated actor, they run concurrently and we can query multiple groups concurrently.








Having sensors modeled as individual device actors:




Isolates failures of one device actor from the rest of the devices in the group.


Increases the parallelism of collecting temperature readings. Network connections from different sensors communicate with their individual device actors directly, reducing contention points.








With the architecture defined, we can start working on the protocol for registering sensors.


The Registration Protocol


As the first step, we need to design the protocol both for registering a device and for creating the group and device actors that will be responsible for it. This protocol will be provided by the 
DeviceManager
 component itself because that is the only actor that is known and available up front: device groups and device actors are created on-demand.


Looking at registration in more detail, we can outline the necessary functionality:




When a 
DeviceManager
 receives a request with a group and device id:
    


If the manager already has an actor for the device group, it forwards the request to it.


Otherwise, it creates a new device group actor and then forwards the request.






The 
DeviceGroup
 actor receives the request to register an actor for the given device:
    


If the group already has an actor for the device it replies with the 
ActorRef
ActorRef
 of the existing device actor.


Otherwise, the 
DeviceGroup
 actor first creates a device actor and replies with the 
ActorRef
 of the newly created device actor.






The sensor will now have the 
ActorRef
 of the device actor to send messages directly to it.




The messages that we will use to communicate registration requests and their acknowledgement have the definition:




Scala




copy
source
final case class RequestTrackDevice(groupId: String, deviceId: String, replyTo: ActorRef[DeviceRegistered])
    extends DeviceManager.Command
    with DeviceGroup.Command

final case class DeviceRegistered(device: ActorRef[Device.Command])


Java




copy
source
public class DeviceManager extends AbstractBehavior<DeviceManager.Command> {

  public interface Command {}

  public static final class RequestTrackDevice
      implements DeviceManager.Command, DeviceGroup.Command {
    public final String groupId;
    public final String deviceId;
    public final ActorRef<DeviceRegistered> replyTo;

    public RequestTrackDevice(String groupId, String deviceId, ActorRef<DeviceRegistered> replyTo) {
      this.groupId = groupId;
      this.deviceId = deviceId;
      this.replyTo = replyTo;
    }
  }

  public static final class DeviceRegistered {
    public final ActorRef<Device.Command> device;

    public DeviceRegistered(ActorRef<Device.Command> device) {
      this.device = device;
    }
  }
}




In this case we have not included a request ID field in the messages. Since registration happens once, when the component connects the system to some network protocol, the ID is not important. However, it is usually a best practice to include a request ID.


Now, we’ll start implementing the protocol from the bottom up. In practice, both a top-down and bottom-up approach can work, but in our case, we benefit from the bottom-up approach as it allows us to immediately write tests for the new features without mocking out parts that we will need to build later.


Adding registration support to device group actors


A group actor has some work to do when it comes to registrations, including:




Handling the registration request for existing device actor or by creating a new actor.


Tracking which device actors exist in the group and removing them from the group when they are stopped.




Handling the registration request


A device group actor must either reply to the request with the 
ActorRef
ActorRef
 of an existing child, or it should create one. To look up child actors by their device IDs we will use a 
Map
.


Add the following to your source file:




Scala




copy
source
object DeviceGroup {
  def apply(groupId: String): Behavior[Command] =
    Behaviors.setup(context => new DeviceGroup(context, groupId))

  trait Command

  private final case class DeviceTerminated(device: ActorRef[Device.Command], groupId: String, deviceId: String)
      extends Command

}

class DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)
    extends AbstractBehavior[DeviceGroup.Command](context) {
  import DeviceGroup._
  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }

  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]

  context.log.info("DeviceGroup {} started", groupId)

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>
        deviceIdToActor.get(deviceId) match {
          case Some(deviceActor) =>
            replyTo ! DeviceRegistered(deviceActor)
          case None =>
            context.log.info("Creating device actor for {}", trackMsg.deviceId)
            val deviceActor = context.spawn(Device(groupId, deviceId), s"device-$deviceId")
            deviceIdToActor += deviceId -> deviceActor
            replyTo ! DeviceRegistered(deviceActor)
        }
        this

      case RequestTrackDevice(gId, _, _) =>
        context.log.warn("Ignoring TrackDevice request for {}. This actor is responsible for {}.", gId, groupId)
        this
    }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("DeviceGroup {} stopped", groupId)
      this
  }
}


Java




copy
source
public class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {

  public interface Command {}

  private class DeviceTerminated implements Command {
    public final ActorRef<Device.Command> device;
    public final String groupId;
    public final String deviceId;

    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {
      this.device = device;
      this.groupId = groupId;
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(String groupId) {
    return Behaviors.setup(context -> new DeviceGroup(context, groupId));
  }

  private final String groupId;
  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();

  private DeviceGroup(ActorContext<Command> context, String groupId) {
    super(context);
    this.groupId = groupId;
    context.getLog().info("DeviceGroup {} started", groupId);
  }

  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {
    if (this.groupId.equals(trackMsg.groupId)) {
      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);
      if (deviceActor != null) {
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      } else {
        getContext().getLog().info("Creating device actor for {}", trackMsg.deviceId);
        deviceActor =
            getContext()
                .spawn(Device.create(groupId, trackMsg.deviceId), "device-" + trackMsg.deviceId);
        deviceIdToActor.put(trackMsg.deviceId, deviceActor);
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      }
    } else {
      getContext()
          .getLog()
          .warn(
              "Ignoring TrackDevice request for {}. This actor is responsible for {}.",
              groupId,
              this.groupId);
    }
    return this;
  }


  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)
        .build();
  }

  private DeviceGroup onPostStop() {
    getContext().getLog().info("DeviceGroup {} stopped", groupId);
    return this;
  }
}




Just as we did with the device, we test this new functionality. We also test that the actors returned for the two different IDs are actually different, and we also attempt to record a temperature reading for each of the devices to see if the actors are responding.




Scala




copy
source
"be able to register a device actor" in {
  val probe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("group", "device1", probe.ref)
  val registered1 = probe.receiveMessage()
  val deviceActor1 = registered1.device

  // another deviceId
  groupActor ! RequestTrackDevice("group", "device2", probe.ref)
  val registered2 = probe.receiveMessage()
  val deviceActor2 = registered2.device
  deviceActor1 should !==(deviceActor2)

  // Check that the device actors are working
  val recordProbe = createTestProbe[TemperatureRecorded]()
  deviceActor1 ! RecordTemperature(requestId = 0, 1.0, recordProbe.ref)
  recordProbe.expectMessage(TemperatureRecorded(requestId = 0))
  deviceActor2 ! Device.RecordTemperature(requestId = 1, 2.0, recordProbe.ref)
  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 1))
}

"ignore requests for wrong groupId" in {
  val probe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("wrongGroup", "device1", probe.ref)
  probe.expectNoMessage(500.milliseconds)
}


Java




copy
source
@Test
public void testReplyToRegistrationRequests() {
  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));

  groupActor.tell(new RequestTrackDevice("group", "device", probe.getRef()));
  DeviceRegistered registered1 = probe.receiveMessage();

  // another deviceId
  groupActor.tell(new RequestTrackDevice("group", "device3", probe.getRef()));
  DeviceRegistered registered2 = probe.receiveMessage();
  assertNotEquals(registered1.device, registered2.device);

  // Check that the device actors are working
  TestProbe<Device.TemperatureRecorded> recordProbe =
      testKit.createTestProbe(Device.TemperatureRecorded.class);
  registered1.device.tell(new Device.RecordTemperature(0L, 1.0, recordProbe.getRef()));
  assertEquals(0L, recordProbe.receiveMessage().requestId);
  registered2.device.tell(new Device.RecordTemperature(1L, 2.0, recordProbe.getRef()));
  assertEquals(1L, recordProbe.receiveMessage().requestId);
}

@Test
public void testIgnoreWrongRegistrationRequests() {
  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));
  groupActor.tell(new RequestTrackDevice("wrongGroup", "device1", probe.getRef()));
  probe.expectNoMessage();
}




If a device actor already exists for the registration request, we would like to use the existing actor instead of a new one. We have not tested this yet, so we need to fix this:




Scala




copy
source
"return same actor for same deviceId" in {
  val probe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("group", "device1", probe.ref)
  val registered1 = probe.receiveMessage()

  // registering same again should be idempotent
  groupActor ! RequestTrackDevice("group", "device1", probe.ref)
  val registered2 = probe.receiveMessage()

  registered1.device should ===(registered2.device)
}


Java




copy
source
@Test
public void testReturnSameActorForSameDeviceId() {
  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));

  groupActor.tell(new RequestTrackDevice("group", "device", probe.getRef()));
  DeviceRegistered registered1 = probe.receiveMessage();

  // registering same again should be idempotent
  groupActor.tell(new RequestTrackDevice("group", "device", probe.getRef()));
  DeviceRegistered registered2 = probe.receiveMessage();
  assertEquals(registered1.device, registered2.device);
}




Keeping track of the device actors in the group


So far, we have implemented logic for registering device actors in the group. Devices come and go, however, so we will need a way to remove device actors from the 
Map[String, ActorRef[DeviceMessage]]
Map<String, ActorRef<DeviceMessage>>
. We will assume that when a device is removed, its corresponding device actor is stopped. Supervision, as we discussed earlier, only handles error scenarios — not graceful stopping. So we need to notify the parent when one of the device actors is stopped.


Akka provides a 
Death Watch
 feature that allows an actor to 
watch
 another actor and be notified if the other actor is stopped. Unlike supervision, watching is not limited to parent-child relationships, any actor can watch any other actor as long as it knows the 
ActorRef
ActorRef
. After a watched actor stops, the watcher receives a 
Terminated(actorRef)
Terminated(actorRef)
 signal which also contains the reference to the watched actor. The watcher can either handle this message explicitly or will fail with a 
DeathPactException
DeathPactException
. This latter is useful if the actor can no longer perform its own duties after the watched actor has been stopped. In our case, the group should still function after one device have been stopped, so we need to handle the 
Terminated(actorRef)
 signal.


Our device group actor needs to include functionality that:




Starts watching new device actors when they are created.


Removes a device actor from the 
Map[String, ActorRef[DeviceMessage]]
Map<String, ActorRef<DeviceMessage>>
 — which maps devices to device actors — when the notification indicates it has stopped.




Unfortunately, the 
Terminated
 signal only contains the 
ActorRef
 of the child actor. We need the actor’s ID to remove it from the map of existing device to device actor mappings. An alternative to the 
Terminated
 signal is to define a custom message that will be sent when the watched actor is stopped. We will use that here because it gives us the possibility to carry the device ID in that message.


Adding the functionality to identify the actor results in this:




Scala




copy
source
class DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)
    extends AbstractBehavior[DeviceGroup.Command](context) {
  import DeviceGroup._
  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }

  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]

  context.log.info("DeviceGroup {} started", groupId)

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>
        deviceIdToActor.get(deviceId) match {
          case Some(deviceActor) =>
            replyTo ! DeviceRegistered(deviceActor)
          case None =>
            context.log.info("Creating device actor for {}", trackMsg.deviceId)
            val deviceActor = context.spawn(Device(groupId, deviceId), s"device-$deviceId")
            context.watchWith(deviceActor, DeviceTerminated(deviceActor, groupId, deviceId))
            deviceIdToActor += deviceId -> deviceActor
            replyTo ! DeviceRegistered(deviceActor)
        }
        this

      case RequestTrackDevice(gId, _, _) =>
        context.log.warn("Ignoring TrackDevice request for {}. This actor is responsible for {}.", gId, groupId)
        this

      case DeviceTerminated(_, _, deviceId) =>
        context.log.info("Device actor for {} has been terminated", deviceId)
        deviceIdToActor -= deviceId
        this

    }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("DeviceGroup {} stopped", groupId)
      this
  }
}


Java




copy
source
public class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {

  public interface Command {}

  private class DeviceTerminated implements Command {
    public final ActorRef<Device.Command> device;
    public final String groupId;
    public final String deviceId;

    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {
      this.device = device;
      this.groupId = groupId;
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(String groupId) {
    return Behaviors.setup(context -> new DeviceGroup(context, groupId));
  }

  private final String groupId;
  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();

  private DeviceGroup(ActorContext<Command> context, String groupId) {
    super(context);
    this.groupId = groupId;
    context.getLog().info("DeviceGroup {} started", groupId);
  }

  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {
    if (this.groupId.equals(trackMsg.groupId)) {
      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);
      if (deviceActor != null) {
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      } else {
        getContext().getLog().info("Creating device actor for {}", trackMsg.deviceId);
        deviceActor =
            getContext()
                .spawn(Device.create(groupId, trackMsg.deviceId), "device-" + trackMsg.deviceId);
        getContext()
            .watchWith(deviceActor, new DeviceTerminated(deviceActor, groupId, trackMsg.deviceId));
        deviceIdToActor.put(trackMsg.deviceId, deviceActor);
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      }
    } else {
      getContext()
          .getLog()
          .warn(
              "Ignoring TrackDevice request for {}. This actor is responsible for {}.",
              groupId,
              this.groupId);
    }
    return this;
  }


  private DeviceGroup onTerminated(DeviceTerminated t) {
    getContext().getLog().info("Device actor for {} has been terminated", t.deviceId);
    deviceIdToActor.remove(t.deviceId);
    return this;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)
        .onMessage(DeviceTerminated.class, this::onTerminated)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private DeviceGroup onPostStop() {
    getContext().getLog().info("DeviceGroup {} stopped", groupId);
    return this;
  }
}




So far we have no means to get which devices the group device actor keeps track of and, therefore, we cannot test our new functionality yet. To make it testable, we add a new query capability (message 
RequestDeviceList
) that lists the currently active device IDs:




Scala




copy
source
final case class RequestDeviceList(requestId: Long, groupId: String, replyTo: ActorRef[ReplyDeviceList])
    extends DeviceManager.Command
    with DeviceGroup.Command

final case class ReplyDeviceList(requestId: Long, ids: Set[String])


Java




copy
source
public static final class RequestDeviceList
    implements DeviceManager.Command, DeviceGroup.Command {
  final long requestId;
  final String groupId;
  final ActorRef<ReplyDeviceList> replyTo;

  public RequestDeviceList(long requestId, String groupId, ActorRef<ReplyDeviceList> replyTo) {
    this.requestId = requestId;
    this.groupId = groupId;
    this.replyTo = replyTo;
  }
}

public static final class ReplyDeviceList {
  final long requestId;
  final Set<String> ids;

  public ReplyDeviceList(long requestId, Set<String> ids) {
    this.requestId = requestId;
    this.ids = ids;
  }
}






Scala




copy
source
object DeviceGroup {
  def apply(groupId: String): Behavior[Command] =
    Behaviors.setup(context => new DeviceGroup(context, groupId))

  trait Command

  private final case class DeviceTerminated(device: ActorRef[Device.Command], groupId: String, deviceId: String)
      extends Command

}

class DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)
    extends AbstractBehavior[DeviceGroup.Command](context) {
  import DeviceGroup._
  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }

  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]

  context.log.info("DeviceGroup {} started", groupId)

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>
        deviceIdToActor.get(deviceId) match {
          case Some(deviceActor) =>
            replyTo ! DeviceRegistered(deviceActor)
          case None =>
            context.log.info("Creating device actor for {}", trackMsg.deviceId)
            val deviceActor = context.spawn(Device(groupId, deviceId), s"device-$deviceId")
            context.watchWith(deviceActor, DeviceTerminated(deviceActor, groupId, deviceId))
            deviceIdToActor += deviceId -> deviceActor
            replyTo ! DeviceRegistered(deviceActor)
        }
        this

      case RequestTrackDevice(gId, _, _) =>
        context.log.warn("Ignoring TrackDevice request for {}. This actor is responsible for {}.", gId, groupId)
        this

      case RequestDeviceList(requestId, gId, replyTo) =>
        if (gId == groupId) {
          replyTo ! ReplyDeviceList(requestId, deviceIdToActor.keySet)
          this
        } else
          Behaviors.unhandled

      case DeviceTerminated(_, _, deviceId) =>
        context.log.info("Device actor for {} has been terminated", deviceId)
        deviceIdToActor -= deviceId
        this

    }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("DeviceGroup {} stopped", groupId)
      this
  }
}


Java




copy
source
public class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {

  public interface Command {}

  private class DeviceTerminated implements Command {
    public final ActorRef<Device.Command> device;
    public final String groupId;
    public final String deviceId;

    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {
      this.device = device;
      this.groupId = groupId;
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(String groupId) {
    return Behaviors.setup(context -> new DeviceGroup(context, groupId));
  }

  private final String groupId;
  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();

  private DeviceGroup(ActorContext<Command> context, String groupId) {
    super(context);
    this.groupId = groupId;
    context.getLog().info("DeviceGroup {} started", groupId);
  }

  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {
    if (this.groupId.equals(trackMsg.groupId)) {
      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);
      if (deviceActor != null) {
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      } else {
        getContext().getLog().info("Creating device actor for {}", trackMsg.deviceId);
        deviceActor =
            getContext()
                .spawn(Device.create(groupId, trackMsg.deviceId), "device-" + trackMsg.deviceId);
        getContext()
            .watchWith(deviceActor, new DeviceTerminated(deviceActor, groupId, trackMsg.deviceId));
        deviceIdToActor.put(trackMsg.deviceId, deviceActor);
        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));
      }
    } else {
      getContext()
          .getLog()
          .warn(
              "Ignoring TrackDevice request for {}. This actor is responsible for {}.",
              groupId,
              this.groupId);
    }
    return this;
  }


  private DeviceGroup onDeviceList(DeviceManager.RequestDeviceList r) {
    r.replyTo.tell(new DeviceManager.ReplyDeviceList(r.requestId, deviceIdToActor.keySet()));
    return this;
  }

  private DeviceGroup onTerminated(DeviceTerminated t) {
    getContext().getLog().info("Device actor for {} has been terminated", t.deviceId);
    deviceIdToActor.remove(t.deviceId);
    return this;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)
        .onMessage(
            DeviceManager.RequestDeviceList.class,
            r -> r.groupId.equals(groupId),
            this::onDeviceList)
        .onMessage(DeviceTerminated.class, this::onTerminated)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private DeviceGroup onPostStop() {
    getContext().getLog().info("DeviceGroup {} stopped", groupId);
    return this;
  }
}




We are almost ready to test the removal of devices. But, we still need the following capabilities:




To stop a device actor from our test case, from the outside, we must send a message to it. We add a 
Passivate
 message which instructs the actor to stop.


To be notified once the device actor is stopped. We can use the 
Death Watch
 facility for this purpose, too.






Scala




copy
source
case object Passivate extends Command


Java




copy
source
static enum Passivate implements Command {
  INSTANCE
}






Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.PostStop
import akka.actor.typed.Signal
import akka.actor.typed.scaladsl.AbstractBehavior
import akka.actor.typed.scaladsl.ActorContext
import akka.actor.typed.scaladsl.Behaviors

object Device {
  def apply(groupId: String, deviceId: String): Behavior[Command] =
    Behaviors.setup(context => new Device(context, groupId, deviceId))

  sealed trait Command

  final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command
  final case class RespondTemperature(requestId: Long, value: Option[Double])

  final case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])
      extends Command
  final case class TemperatureRecorded(requestId: Long)

  case object Passivate extends Command
}

class Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)
    extends AbstractBehavior[Device.Command](context) {
  import Device._

  var lastTemperatureReading: Option[Double] = None

  context.log.info("Device actor {}-{} started", groupId, deviceId)

  override def onMessage(msg: Command): Behavior[Command] = {
    msg match {
      case RecordTemperature(id, value, replyTo) =>
        context.log.info("Recorded temperature reading {} with {}", value, id)
        lastTemperatureReading = Some(value)
        replyTo ! TemperatureRecorded(id)
        this

      case ReadTemperature(id, replyTo) =>
        replyTo ! RespondTemperature(id, lastTemperatureReading)
        this

      case Passivate =>
        Behaviors.stopped
    }
  }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("Device actor {}-{} stopped", groupId, deviceId)
      this
  }

}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.PostStop;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import java.util.Optional;

public class Device extends AbstractBehavior<Device.Command> {

  public interface Command {}

  public static final class RecordTemperature implements Command {
    final long requestId;
    final double value;
    final ActorRef<TemperatureRecorded> replyTo;

    public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {
      this.requestId = requestId;
      this.value = value;
      this.replyTo = replyTo;
    }
  }

  public static final class TemperatureRecorded {
    final long requestId;

    public TemperatureRecorded(long requestId) {
      this.requestId = requestId;
    }
  }

  public static final class ReadTemperature implements Command {
    final long requestId;
    final ActorRef<RespondTemperature> replyTo;

    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {
      this.requestId = requestId;
      this.replyTo = replyTo;
    }
  }

  public static final class RespondTemperature {
    final long requestId;
    final Optional<Double> value;

    public RespondTemperature(long requestId, Optional<Double> value) {
      this.requestId = requestId;
      this.value = value;
    }
  }

  static enum Passivate implements Command {
    INSTANCE
  }

  public static Behavior<Command> create(String groupId, String deviceId) {
    return Behaviors.setup(context -> new Device(context, groupId, deviceId));
  }

  private final String groupId;
  private final String deviceId;

  private Optional<Double> lastTemperatureReading = Optional.empty();

  private Device(ActorContext<Command> context, String groupId, String deviceId) {
    super(context);
    this.groupId = groupId;
    this.deviceId = deviceId;

    context.getLog().info("Device actor {}-{} started", groupId, deviceId);
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(RecordTemperature.class, this::onRecordTemperature)
        .onMessage(ReadTemperature.class, this::onReadTemperature)
        .onMessage(Passivate.class, m -> Behaviors.stopped())
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private Behavior<Command> onRecordTemperature(RecordTemperature r) {
    getContext().getLog().info("Recorded temperature reading {} with {}", r.value, r.requestId);
    lastTemperatureReading = Optional.of(r.value);
    r.replyTo.tell(new TemperatureRecorded(r.requestId));
    return this;
  }

  private Behavior<Command> onReadTemperature(ReadTemperature r) {
    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));
    return this;
  }

  private Behavior<Command> onPostStop() {
    getContext().getLog().info("Device actor {}-{} stopped", groupId, deviceId);
    return Behaviors.stopped();
  }
}




We add two more test cases now. In the first, we test that we get back the list of proper IDs once we have added a few devices. The second test case makes sure that the device ID is properly removed after the device actor has been stopped. The 
TestProbe
TestProbe
 has a 
expectTerminated
 method that we can easily use to assert that the device actor has been terminated.




Scala




copy
source
"be able to list active devices" in {
  val registeredProbe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("group", "device1", registeredProbe.ref)
  registeredProbe.receiveMessage()

  groupActor ! RequestTrackDevice("group", "device2", registeredProbe.ref)
  registeredProbe.receiveMessage()

  val deviceListProbe = createTestProbe[ReplyDeviceList]()
  groupActor ! RequestDeviceList(requestId = 0, groupId = "group", deviceListProbe.ref)
  deviceListProbe.expectMessage(ReplyDeviceList(requestId = 0, Set("device1", "device2")))
}

"be able to list active devices after one shuts down" in {
  val registeredProbe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("group", "device1", registeredProbe.ref)
  val registered1 = registeredProbe.receiveMessage()
  val toShutDown = registered1.device

  groupActor ! RequestTrackDevice("group", "device2", registeredProbe.ref)
  registeredProbe.receiveMessage()

  val deviceListProbe = createTestProbe[ReplyDeviceList]()
  groupActor ! RequestDeviceList(requestId = 0, groupId = "group", deviceListProbe.ref)
  deviceListProbe.expectMessage(ReplyDeviceList(requestId = 0, Set("device1", "device2")))

  toShutDown ! Passivate
  registeredProbe.expectTerminated(toShutDown, registeredProbe.remainingOrDefault)

  // using awaitAssert to retry because it might take longer for the groupActor
  // to see the Terminated, that order is undefined
  registeredProbe.awaitAssert {
    groupActor ! RequestDeviceList(requestId = 1, groupId = "group", deviceListProbe.ref)
    deviceListProbe.expectMessage(ReplyDeviceList(requestId = 1, Set("device2")))
  }
}


Java




copy
source
@Test
public void testListActiveDevices() {
  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));

  groupActor.tell(new RequestTrackDevice("group", "device1", registeredProbe.getRef()));
  registeredProbe.receiveMessage();

  groupActor.tell(new RequestTrackDevice("group", "device2", registeredProbe.getRef()));
  registeredProbe.receiveMessage();

  TestProbe<ReplyDeviceList> deviceListProbe = testKit.createTestProbe(ReplyDeviceList.class);

  groupActor.tell(new RequestDeviceList(0L, "group", deviceListProbe.getRef()));
  ReplyDeviceList reply = deviceListProbe.receiveMessage();
  assertEquals(0L, reply.requestId);
  assertEquals(Stream.of("device1", "device2").collect(Collectors.toSet()), reply.ids);
}

@Test
public void testListActiveDevicesAfterOneShutsDown() {
  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));

  groupActor.tell(new RequestTrackDevice("group", "device1", registeredProbe.getRef()));
  DeviceRegistered registered1 = registeredProbe.receiveMessage();

  groupActor.tell(new RequestTrackDevice("group", "device2", registeredProbe.getRef()));
  DeviceRegistered registered2 = registeredProbe.receiveMessage();

  ActorRef<Device.Command> toShutDown = registered1.device;

  TestProbe<ReplyDeviceList> deviceListProbe = testKit.createTestProbe(ReplyDeviceList.class);

  groupActor.tell(new RequestDeviceList(0L, "group", deviceListProbe.getRef()));
  ReplyDeviceList reply = deviceListProbe.receiveMessage();
  assertEquals(0L, reply.requestId);
  assertEquals(Stream.of("device1", "device2").collect(Collectors.toSet()), reply.ids);

  toShutDown.tell(Device.Passivate.INSTANCE);
  registeredProbe.expectTerminated(toShutDown, registeredProbe.getRemainingOrDefault());

  // using awaitAssert to retry because it might take longer for the groupActor
  // to see the Terminated, that order is undefined
  registeredProbe.awaitAssert(
      () -> {
        groupActor.tell(new RequestDeviceList(1L, "group", deviceListProbe.getRef()));
        ReplyDeviceList r = deviceListProbe.receiveMessage();
        assertEquals(1L, r.requestId);
        assertEquals(Stream.of("device2").collect(Collectors.toSet()), r.ids);
        return null;
      });
}




Creating device manager actors


Going up to the next level in our hierarchy, we need to create the entry point for our device manager component in the 
DeviceManager
 source file. This actor is very similar to the device group actor, but creates device group actors instead of device actors:




Scala




copy
source
object DeviceManager {
  def apply(): Behavior[Command] =
    Behaviors.setup(context => new DeviceManager(context))


  sealed trait Command

  final case class RequestTrackDevice(groupId: String, deviceId: String, replyTo: ActorRef[DeviceRegistered])
      extends DeviceManager.Command
      with DeviceGroup.Command

  final case class DeviceRegistered(device: ActorRef[Device.Command])

  final case class RequestDeviceList(requestId: Long, groupId: String, replyTo: ActorRef[ReplyDeviceList])
      extends DeviceManager.Command
      with DeviceGroup.Command

  final case class ReplyDeviceList(requestId: Long, ids: Set[String])

  private final case class DeviceGroupTerminated(groupId: String) extends DeviceManager.Command
}

class DeviceManager(context: ActorContext[DeviceManager.Command])
    extends AbstractBehavior[DeviceManager.Command](context) {
  import DeviceManager._

  var groupIdToActor = Map.empty[String, ActorRef[DeviceGroup.Command]]

  context.log.info("DeviceManager started")

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      case trackMsg @ RequestTrackDevice(groupId, _, replyTo) =>
        groupIdToActor.get(groupId) match {
          case Some(ref) =>
            ref ! trackMsg
          case None =>
            context.log.info("Creating device group actor for {}", groupId)
            val groupActor = context.spawn(DeviceGroup(groupId), "group-" + groupId)
            context.watchWith(groupActor, DeviceGroupTerminated(groupId))
            groupActor ! trackMsg
            groupIdToActor += groupId -> groupActor
        }
        this

      case req @ RequestDeviceList(requestId, groupId, replyTo) =>
        groupIdToActor.get(groupId) match {
          case Some(ref) =>
            ref ! req
          case None =>
            replyTo ! ReplyDeviceList(requestId, Set.empty)
        }
        this

      case DeviceGroupTerminated(groupId) =>
        context.log.info("Device group actor for {} has been terminated", groupId)
        groupIdToActor -= groupId
        this
    }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("DeviceManager stopped")
      this
  }

}


Java




copy
source
public class DeviceManager extends AbstractBehavior<DeviceManager.Command> {

  public interface Command {}

  public static final class RequestTrackDevice
      implements DeviceManager.Command, DeviceGroup.Command {
    public final String groupId;
    public final String deviceId;
    public final ActorRef<DeviceRegistered> replyTo;

    public RequestTrackDevice(String groupId, String deviceId, ActorRef<DeviceRegistered> replyTo) {
      this.groupId = groupId;
      this.deviceId = deviceId;
      this.replyTo = replyTo;
    }
  }

  public static final class DeviceRegistered {
    public final ActorRef<Device.Command> device;

    public DeviceRegistered(ActorRef<Device.Command> device) {
      this.device = device;
    }
  }

  public static final class RequestDeviceList
      implements DeviceManager.Command, DeviceGroup.Command {
    final long requestId;
    final String groupId;
    final ActorRef<ReplyDeviceList> replyTo;

    public RequestDeviceList(long requestId, String groupId, ActorRef<ReplyDeviceList> replyTo) {
      this.requestId = requestId;
      this.groupId = groupId;
      this.replyTo = replyTo;
    }
  }

  public static final class ReplyDeviceList {
    final long requestId;
    final Set<String> ids;

    public ReplyDeviceList(long requestId, Set<String> ids) {
      this.requestId = requestId;
      this.ids = ids;
    }
  }

  private static class DeviceGroupTerminated implements DeviceManager.Command {
    public final String groupId;

    DeviceGroupTerminated(String groupId) {
      this.groupId = groupId;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.setup(DeviceManager::new);
  }

  private final Map<String, ActorRef<DeviceGroup.Command>> groupIdToActor = new HashMap<>();

  private DeviceManager(ActorContext<Command> context) {
    super(context);
    context.getLog().info("DeviceManager started");
  }

  private DeviceManager onTrackDevice(RequestTrackDevice trackMsg) {
    String groupId = trackMsg.groupId;
    ActorRef<DeviceGroup.Command> ref = groupIdToActor.get(groupId);
    if (ref != null) {
      ref.tell(trackMsg);
    } else {
      getContext().getLog().info("Creating device group actor for {}", groupId);
      ActorRef<DeviceGroup.Command> groupActor =
          getContext().spawn(DeviceGroup.create(groupId), "group-" + groupId);
      getContext().watchWith(groupActor, new DeviceGroupTerminated(groupId));
      groupActor.tell(trackMsg);
      groupIdToActor.put(groupId, groupActor);
    }
    return this;
  }

  private DeviceManager onRequestDeviceList(RequestDeviceList request) {
    ActorRef<DeviceGroup.Command> ref = groupIdToActor.get(request.groupId);
    if (ref != null) {
      ref.tell(request);
    } else {
      request.replyTo.tell(new ReplyDeviceList(request.requestId, Collections.emptySet()));
    }
    return this;
  }

  private DeviceManager onTerminated(DeviceGroupTerminated t) {
    getContext().getLog().info("Device group actor for {} has been terminated", t.groupId);
    groupIdToActor.remove(t.groupId);
    return this;
  }

  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(RequestTrackDevice.class, this::onTrackDevice)
        .onMessage(RequestDeviceList.class, this::onRequestDeviceList)
        .onMessage(DeviceGroupTerminated.class, this::onTerminated)
        .onSignal(PostStop.class, signal -> onPostStop())
        .build();
  }

  private DeviceManager onPostStop() {
    getContext().getLog().info("DeviceManager stopped");
    return this;
  }
}




We leave tests of the device manager as an exercise for you since it is very similar to the tests we have already written for the group actor.


What’s next?


We have now a hierarchical component for registering and tracking devices and recording measurements. We have seen how to implement different types of conversation patterns, such as:




Request-respond (for temperature recordings)


Create-on-demand (for registration of devices)


Create-watch-terminate (for creating the group and device actor as children)




In the next chapter, we will introduce group query capabilities, which will establish a new conversation pattern of scatter-gather. In particular, we will implement the functionality that allows users to query the status of all the devices belonging to a group.














 
Part 3: Working with Device Actors






Part 5: Querying Device Groups 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/dispatchers.html
Dispatchers • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers




Dependency


Introduction


Default dispatcher


Internal dispatcher


Looking up a Dispatcher


Selecting a dispatcher


Types of dispatchers


Dispatcher aliases


Blocking Needs Careful Management


More dispatcher configuration examples




Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers




Dependency


Introduction


Default dispatcher


Internal dispatcher


Looking up a Dispatcher


Selecting a dispatcher


Types of dispatchers


Dispatcher aliases


Blocking Needs Careful Management


More dispatcher configuration examples




Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Dispatchers


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Dispatchers
.


Dependency


Dispatchers are part of core Akka, which means that they are part of the 
akka-actor
 dependency. This page describes how to use dispatchers with 
akka-actor-typed
.


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


An Akka 
MessageDispatcher
 is what makes Akka Actors “tick”, it is the engine of the machine so to speak. All 
MessageDispatcher
 implementations are also an 
ExecutionContext
Executor
, which means that they can be used to execute arbitrary code, for instance 
Future
s
CompletableFuture
s
.


Default dispatcher


Every 
ActorSystem
 will have a default dispatcher that will be used in case nothing else is configured for an 
Actor
. The default dispatcher can be configured, and is by default a 
Dispatcher
 with the configured 
akka.actor.default-dispatcher.executor
. If no executor is selected a “fork-join-executor” is selected, which gives excellent performance in most cases.


Internal dispatcher


To protect the internal Actors that are spawned by the various Akka modules, a separate internal dispatcher is used by default. The internal dispatcher can be tuned in a fine-grained way with the setting 
akka.actor.internal-dispatcher
, it can also be replaced by another dispatcher by making 
akka.actor.internal-dispatcher
 an 
alias
.




Looking up a Dispatcher


Dispatchers implement the 
ExecutionContext
Executor
 interface and can thus be used to run 
Future
CompletableFuture
 invocations etc.




Scala




copy
source
// for use with Futures, Scheduler, etc.
import akka.actor.typed.DispatcherSelector
implicit val executionContext = context.system.dispatchers.lookup(DispatcherSelector.fromConfig("my-dispatcher"))


Java




copy
source
// this is scala.concurrent.ExecutionContextExecutor, which implements
// both scala.concurrent.ExecutionContext (for use with Futures, Scheduler, etc.)
// and java.util.concurrent.Executor (for use with CompletableFuture etc.)
final ExecutionContextExecutor ex =
    system.dispatchers().lookup(DispatcherSelector.fromConfig("my-dispatcher"));




Selecting a dispatcher


A default dispatcher is used for all actors that are spawned without specifying a custom dispatcher. This is suitable for all actors that don’t block. Blocking in actors needs to be carefully managed, more details 
here
.


To select a dispatcher use 
DispatcherSelector
 to create a 
Props
 instance for spawning your actor:




Scala




copy
source
import akka.actor.typed.DispatcherSelector

context.spawn(yourBehavior, "DefaultDispatcher")
context.spawn(yourBehavior, "ExplicitDefaultDispatcher", DispatcherSelector.default())
context.spawn(yourBehavior, "BlockingDispatcher", DispatcherSelector.blocking())
context.spawn(yourBehavior, "ParentDispatcher", DispatcherSelector.sameAsParent())
context.spawn(yourBehavior, "DispatcherFromConfig", DispatcherSelector.fromConfig("your-dispatcher"))


Java




copy
source
context.spawn(behavior, "DefaultDispatcher");
context.spawn(behavior, "ExplicitDefaultDispatcher", DispatcherSelector.defaultDispatcher());
context.spawn(behavior, "BlockingDispatcher", DispatcherSelector.blocking());
context.spawn(behavior, "ParentDispatcher", DispatcherSelector.sameAsParent());
context.spawn(
    behavior, "DispatcherFromConfig", DispatcherSelector.fromConfig("your-dispatcher"));




DispatcherSelector
 has a few convenience methods:




DispatcherSelector.default
DispatcherSelector.defaultDispatcher
 to look up the default dispatcher


DispatcherSelector.blocking
 can be used to execute actors that block e.g. a legacy database API that does not support 
Future
CompletionStage
s


DispatcherSelector.sameAsParent
 to use the same dispatcher as the parent actor




The final example shows how to load a custom dispatcher from configuration and relies on this being in your 
application.conf
:




copy
source
your-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 32
  }
  throughput = 1
}


Types of dispatchers


There are 2 different types of message dispatchers:






Dispatcher


This is an event-based dispatcher that binds a set of Actors to a thread pool. The default dispatcher is used if no other is specified.




Shareability: Unlimited


Mailboxes: Any, creates one per Actor


Use cases: Default dispatcher, Bulkheading


Driven by: 
java.util.concurrent.ExecutorService
.  Specify using “executor” using “fork-join-executor”, “thread-pool-executor” or the fully-qualified  class name of an 
akka.dispatcher.ExecutorServiceConfigurator
 implementation.








PinnedDispatcher


This dispatcher dedicates a unique thread for each actor using it; i.e. each actor will have its own thread pool with only one thread in the pool.




Shareability: None


Mailboxes: Any, creates one per Actor


Use cases: Bulkheading


Driven by: Any 
akka.dispatch.ThreadPoolExecutorConfigurator
.  By default a “thread-pool-executor”.








Here is an example configuration of a Fork Join Pool dispatcher:




copy
source
my-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}


For more configuration options, see the 
More dispatcher configuration examples
 section and the 
default-dispatcher
 section of the 
configuration
.
Note


The 
parallelism-max
 for the 
fork-join-executor
 does not set the upper bound on the total number of threads allocated by the ForkJoinPool. It is a setting specifically talking about the number of 
hot
 threads the pool will keep running in order to reduce the latency of handling a new incoming task. You can read more about parallelism in the JDK’s 
ForkJoinPool documentation
.
Note


The 
thread-pool-executor
 dispatcher is implemented using by a 
java.util.concurrent.ThreadPoolExecutor
. You can read more about it in the JDK’s 
ThreadPoolExecutor documentation
.


Dispatcher aliases


When a dispatcher is looked up, and the given setting contains a string rather than a dispatcher config block, the lookup will treat it as an alias, and follow that string to an alternate location for a dispatcher config. If the dispatcher config is referenced both through an alias and through the absolute path only one dispatcher will be used and shared among the two ids.


Example: configuring 
internal-dispatcher
 to be an alias for 
default-dispatcher
:


akka.actor.internal-dispatcher = akka.actor.default-dispatcher





Blocking Needs Careful Management


In some cases it is unavoidable to do blocking operations, i.e. to put a thread to sleep for an indeterminate time, waiting for an external event to occur. Examples are legacy RDBMS drivers or messaging APIs, and the underlying reason is typically that (network) I/O occurs under the covers.


The 
Managing Blocking in Akka video
 explains why it is bad to block inside an actor, and how you can use custom dispatchers to manage blocking when you cannot avoid it.


Problem: Blocking on default dispatcher


Simply adding blocking calls to your actor message processing like this is problematic:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors

object BlockingActor {
  def apply(): Behavior[Int] =
    Behaviors.receiveMessage { i =>
      // DO NOT DO THIS HERE: this is an example of incorrect code,
      // better alternatives are described further on.

      //block for 5 seconds, representing blocking I/O, etc
      Thread.sleep(5000)
      println(s"Blocking operation finished: $i")
      Behaviors.same
    }
}


Java




copy
source
import akka.actor.typed.*;
import akka.actor.typed.javadsl.*;

public class BlockingActor extends AbstractBehavior<Integer> {
  public static Behavior<Integer> create() {
    return Behaviors.setup(BlockingActor::new);
  }

  private BlockingActor(ActorContext<Integer> context) {
    super(context);
  }

  @Override
  public Receive<Integer> createReceive() {
    return newReceiveBuilder()
        .onMessage(
            Integer.class,
            i -> {
              // DO NOT DO THIS HERE: this is an example of incorrect code,
              // better alternatives are described further on.

              // block for 5 seconds, representing blocking I/O, etc
              Thread.sleep(5000);
              System.out.println("Blocking operation finished: " + i);
              return Behaviors.same();
            })
        .build();
  }
}




Without any further configuration the default dispatcher runs this actor along with all other actors. This is very efficient when all actor message processing is non-blocking. When all of the available threads are blocked, however, then all the actors on the same dispatcher will starve for threads and will not be able to process incoming messages.
Note


Blocking APIs should also be avoided if possible. Try to find or build Reactive APIs, such that blocking is minimised, or moved over to dedicated dispatchers.


Often when integrating with existing libraries or systems it is not possible to avoid blocking APIs. The following solution explains how to handle blocking operations properly.


Note that the same hints apply to managing blocking operations anywhere in Akka, including Streams, HTTP and other reactive libraries built on top of it.


To demonstrate this problem, let’s set up an application with the above 
BlockingActor
 and the following 
PrintActor
:




Scala




copy
source
object PrintActor {
  def apply(): Behavior[Integer] =
    Behaviors.receiveMessage { i =>
      println(s"PrintActor: $i")
      Behaviors.same
    }
}


Java




copy
source
class PrintActor extends AbstractBehavior<Integer> {

  public static Behavior<Integer> create() {
    return Behaviors.setup(PrintActor::new);
  }

  private PrintActor(ActorContext<Integer> context) {
    super(context);
  }

  @Override
  public Receive<Integer> createReceive() {
    return newReceiveBuilder()
        .onMessage(
            Integer.class,
            i -> {
              System.out.println("PrintActor: " + i);
              return Behaviors.same();
            })
        .build();
  }
}






Scala




copy
source
val root = Behaviors.setup[Nothing] { context =>
  for (i <- 1 to 100) {
    context.spawn(BlockingFutureActor(), s"futureActor-$i") ! i
    context.spawn(PrintActor(), s"printActor-$i") ! i
  }
  Behaviors.empty
}
val system = ActorSystem[Nothing](root, "BlockingDispatcherSample")


Java




copy
source
Behavior<Void> root =
    Behaviors.setup(
        context -> {
          for (int i = 0; i < 100; i++) {
            context.spawn(BlockingActor.create(), "BlockingActor-" + i).tell(i);
            context.spawn(PrintActor.create(), "PrintActor-" + i).tell(i);
          }
          return Behaviors.ignore();
        });




Here the app is sending 100 messages to 
BlockingActor
s and 
PrintActor
s and large numbers of 
akka.actor.default-dispatcher
 threads are handling requests. When you run the above code, you will likely to see the entire application gets stuck somewhere like this:


>ãPrintActor: 44
>ãPrintActor: 45



PrintActor
 is considered non-blocking, however it is not able to proceed with handling the remaining messages, since all the threads are occupied and blocked by the other blocking actors - thus leading to thread starvation.


In the thread state diagrams below the colours have the following meaning:




Turquoise - Sleeping state


Orange - Waiting state


Green - Runnable state




The thread information was recorded using the YourKit profiler, however any good JVM profiler has this feature (including the free and bundled with the Oracle JDK 
VisualVM
, as well as 
Java Mission Control
).


The orange portion of the thread shows that it is idle. Idle threads are fine - they’re ready to accept new work. However, a large number of turquoise (blocked, or sleeping as in our example) threads leads to thread starvation.
Note


Thread Starvation Detector
 will issue warning log statements if it detects any of your dispatchers suffering from starvation and other. It is a helpful first step to identify the problem is occurring in a production system, and then you can apply the proposed solutions as explained below.




In the above example we put the code under load by sending hundreds of messages to blocking actors which causes threads of the default dispatcher to be blocked. The fork join pool based dispatcher in Akka then attempts to compensate for this blocking by adding more threads to the pool (
default-akka.actor.default-dispatcher 18,19,20,...
). This however is not able to help if those too will immediately get blocked, and eventually the blocking operations will dominate the entire dispatcher.


In essence, the 
Thread.sleep
 operation has dominated all threads and caused anything executing on the default dispatcher to starve for resources (including any actor that you have not configured an explicit dispatcher for).


Non-solution: Wrapping in a Future




When facing this, you may be tempted to wrap the blocking call inside a 
Future
 and work with that instead, but this strategy is too simplistic: you are quite likely to find bottlenecks or run out of memory or threads when the application runs under increased load.




Scala




copy
source
object BlockingFutureActor {
  def apply(): Behavior[Int] =
    Behaviors.setup { context =>
      implicit val executionContext: ExecutionContext = context.executionContext

      Behaviors.receiveMessage { i =>
        triggerFutureBlockingOperation(i)
        Behaviors.same
      }
    }

  def triggerFutureBlockingOperation(i: Int)(implicit ec: ExecutionContext): Future[Unit] = {
    println(s"Calling blocking Future: $i")
    Future {
      Thread.sleep(5000) //block for 5 seconds
      println(s"Blocking future finished $i")
    }
  }
}




The key problematic line here is this:


implicit val executionContext: ExecutionContext = context.executionContext



Using 
context.executionContext
 as the dispatcher on which the blocking 
Future
 executes can still be a problem, since this dispatcher is by default used for all other actor processing unless you 
set up a separate dispatcher for the actor
.


Solution: Dedicated dispatcher for blocking operations


An efficient method of isolating the blocking behavior, such that it does not impact the rest of the system, is to prepare and use a dedicated dispatcher for all those blocking operations. This technique is often referred to as “bulk-heading” or simply “isolating blocking”.


In 
application.conf
, the dispatcher dedicated to blocking behavior should be configured as follows:




copy
source
my-blocking-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 16
  }
  throughput = 1
}


A 
thread-pool-executor
 based dispatcher allows us to limit the number of threads it will host, and this way we gain tight control over the maximum number of blocked threads the system may use.


The exact size should be fine tuned depending on the workload you’re expecting to run on this dispatcher.


Whenever blocking has to be done, use the above configured dispatcher instead of the default one:




Scala




copy
source
object SeparateDispatcherFutureActor {
  def apply(): Behavior[Int] =
    Behaviors.setup { context =>
      implicit val executionContext: ExecutionContext =
        context.system.dispatchers.lookup(DispatcherSelector.fromConfig("my-blocking-dispatcher"))

      Behaviors.receiveMessage { i =>
        triggerFutureBlockingOperation(i)
        Behaviors.same
      }
    }

  def triggerFutureBlockingOperation(i: Int)(implicit ec: ExecutionContext): Future[Unit] = {
    println(s"Calling blocking Future: $i")
    Future {
      Thread.sleep(5000) //block for 5 seconds
      println(s"Blocking future finished $i")
    }
  }
}


Java




copy
source
class SeparateDispatcherFutureActor extends AbstractBehavior<Integer> {
  private final Executor ec;

  public static Behavior<Integer> create() {
    return Behaviors.setup(SeparateDispatcherFutureActor::new);
  }

  private SeparateDispatcherFutureActor(ActorContext<Integer> context) {
    super(context);
    ec =
        context
            .getSystem()
            .dispatchers()
            .lookup(DispatcherSelector.fromConfig("my-blocking-dispatcher"));
  }

  @Override
  public Receive<Integer> createReceive() {
    return newReceiveBuilder()
        .onMessage(
            Integer.class,
            i -> {
              triggerFutureBlockingOperation(i, ec);
              return Behaviors.same();
            })
        .build();
  }

  private static void triggerFutureBlockingOperation(Integer i, Executor ec) {
    System.out.println("Calling blocking Future on separate dispatcher: " + i);
    CompletableFuture<Integer> f =
        CompletableFuture.supplyAsync(
            () -> {
              try {
                Thread.sleep(5000);
                System.out.println("Blocking future finished: " + i);
                return i;
              } catch (InterruptedException e) {
                return -1;
              }
            },
            ec);
  }
}




The thread pool behavior is shown in the below diagram.




Messages sent to 
SeparateDispatcherFutureActor
SeparateDispatcherCompletionStageActor
 and 
PrintActor
 are handled by the default dispatcher - the green lines, which represent the actual execution.


When blocking operations are run on the 
my-blocking-dispatcher
, it uses the threads (up to the configured limit) to handle these operations. The sleeping in this case is nicely isolated to just this dispatcher, and the default one remains unaffected, allowing the rest of the application to proceed as if nothing bad was happening. After a certain period of idleness, threads started by this dispatcher will be shut down.


In this case, the throughput of other actors was not impacted - they were still served on the default dispatcher.


This is the recommended way of dealing with any kind of blocking in reactive applications.


For a similar discussion specifically about Akka HTTP, refer to 
Handling blocking operations in Akka HTTP
.


Available solutions to blocking operations


The non-exhaustive list of adequate solutions to the âblocking problemâ includes the following suggestions:




Do the blocking call within a 
Future
CompletionStage
, ensuring an upper bound on the number of such calls at any point in time (submitting an unbounded number of tasks of this nature will exhaust your memory or thread limits).


Do the blocking call within a 
Future
, providing a thread pool with an upper limit on the number of threads which is appropriate for the hardware on which the application runs, as explained in detail in this section.


Dedicate a single thread to manage a set of blocking resources (e.g. a NIO selector driving multiple channels) and dispatch events as they occur as actor messages.


Do the blocking call within an actor (or a set of actors) managed by a 
router
, making sure to configure a thread pool which is either dedicated for this purpose or sufficiently sized.




The last possibility is especially well-suited for resources which are single-threaded in nature, like database handles which traditionally can only execute one outstanding query at a time and use internal synchronization to ensure this. A common pattern is to create a router for N actors, each of which wraps a single DB connection and handles queries as sent to the router. The number N must then be tuned for maximum throughput, which will vary depending on which DBMS is deployed on what hardware.
Note


Configuring thread pools is a task best delegated to Akka, configure it in 
application.conf
 and instantiate through an 
ActorSystem


More dispatcher configuration examples


Fixed pool size


Configuring a dispatcher with fixed thread pool size, e.g. for actors that perform blocking IO:


copy
source
blocking-io-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 32
  }
  throughput = 1
}


Cores


Another example that uses the thread pool based on the number of cores (e.g. for CPU bound tasks)




copy
source
my-thread-pool-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "thread-pool-executor"
  # Configuration for the thread pool
  thread-pool-executor {
    # minimum number of threads to cap factor-based core number to
    core-pool-size-min = 2
    # No of core threads ... ceil(available processors * factor)
    core-pool-size-factor = 2.0
    # maximum number of threads to cap factor-based number to
    core-pool-size-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}


Pinned


A separate thread is dedicated for each actor that is configured to use the pinned dispatcher. 


Configuring a 
PinnedDispatcher
:




copy
source
my-pinned-dispatcher {
  executor = "thread-pool-executor"
  type = PinnedDispatcher
}


Note that 
thread-pool-executor
 configuration as per the above 
my-thread-pool-dispatcher
 example is NOT applicable. This is because every actor will have its own thread pool when using 
PinnedDispatcher
, and that pool will have only one thread.


Note that it’s not guaranteed that the 
same
 thread is used over time, since the core pool timeout is used for 
PinnedDispatcher
 to keep resource usage down in case of idle actors. To use the same thread all the time you need to add 
thread-pool-executor.allow-core-timeout=off
 to the configuration of the 
PinnedDispatcher
.


Thread shutdown timeout


Both the 
fork-join-executor
 and 
thread-pool-executor
 may shutdown threads when they are not used. If it’s desired to keep the threads alive longer there are some timeout settings that can be adjusted.




copy
source
my-dispatcher-with-timeouts {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 16
    # Keep alive time for threads
    keep-alive-time = 60s
    # Allow core threads to time out
    allow-core-timeout = off
  }
  # How long time the dispatcher will wait for new actors until it shuts down
  shutdown-timeout = 60s
}


When using the dispatcher as an 
ExecutionContext
 without assigning actors to it the 
shutdown-timeout
 should typically be increased, since the default of 1 second may cause too frequent shutdown of the entire thread pool.














 
Coordinated Shutdown






Mailboxes 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/serialization.html
Serialization • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization




Dependency


Introduction


Usage


Customization


Serialization of Akka’s messages


Java serialization


Rolling updates




Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization




Dependency


Introduction


Usage


Customization


Serialization of Akka’s messages


Java serialization


Rolling updates




Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Serialization


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Serialization, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor_${versions.ScalaBinary}"
}


Introduction


The messages that Akka actors send to each other are JVM objects 
(e.g. instances of Scala case classes)
. Message passing between actors that live on the same JVM is straightforward. It is done via reference passing. However, messages that have to escape the JVM to reach an actor running on a different host have to undergo some form of serialization (i.e. the objects have to be converted to and from byte arrays).


The serialization mechanism in Akka allows you to write custom serializers and to define which serializer to use for what.


Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference.


Google Protocol Buffers
 is good if you want more control over the schema evolution of your messages, but it requires more work to develop and maintain the mapping between serialized representation and domain representation.


Akka itself uses Protocol Buffers to serialize internal messages (for example cluster gossip messages).


Usage


Configuration


For Akka to know which 
Serializer
 to use for what, you need to edit your configuration: in the 
akka.actor.serializers
-section, you bind names to implementations of the 
akka.serialization.Serializer
akka.serialization.Serializer
 you wish to use, like this:


copy
source
akka {
  actor {
    serializers {
      jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      myown = "docs.serialization.MyOwnSerializer"
    }
  }
}


After you’ve bound names to different implementations of 
Serializer
 you need to wire which classes should be serialized using which 
Serializer
, this is done in the 
akka.actor.serialization-bindings
-section:


copy
source
akka {
  actor {
    serializers {
      jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      myown = "docs.serialization.MyOwnSerializer"
    }

    serialization-bindings {
      "com.google.protobuf.Message" = proto
      "docs.serialization.MyOwnSerializable" = myown
    }
  }
}


You only need to specify the name of 
a trait
an interface
 or abstract base class of the messages. In case of ambiguity, i.e. the message implements several of the configured classes, the most specific configured class will be used, i.e. the one of which all other candidates are superclasses. If this condition cannot be met, because e.g. two marker 
traits
interfaces
 that have been configured for serialization both apply and neither is a subtype of the other, a warning will be issued.
Note


If 
you are using Scala for your message protocol and
 your messages are contained inside of a Scala object, then in order to reference those messages, you will need to use the fully qualified Java class name. For a message named 
Message
 contained inside the 
Scala
 object named 
Wrapper
 you would need to reference it as 
Wrapper$Message
 instead of 
Wrapper.Message
.


Akka provides serializers for several primitive types and 
protobuf
 
com.google.protobuf.GeneratedMessage
 (protobuf2) and 
com.google.protobuf.GeneratedMessageV3
 (protobuf3) by default (the latter only if depending on the akka-remote module), so normally you don’t need to add configuration for that if you send raw protobuf messages as actor messages.


Programmatic


If you want to programmatically serialize/deserialize using Akka Serialization, here are some examples:




Scala




copy
source
import akka.actor._
import akka.actor.typed.scaladsl.Behaviors
import akka.cluster.Cluster
import akka.serialization._



Java




copy
source
import akka.actor.*;
import akka.serialization.*;

import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;







Scala




copy
source
val system = ActorSystem("example")

// Get the Serialization Extension
val serialization = SerializationExtension(system)

// Have something to serialize
val original = "woohoo"

// Turn it into bytes, and retrieve the serializerId and manifest, which are needed for deserialization
val bytes = serialization.serialize(original).get
val serializerId = serialization.findSerializerFor(original).identifier
val manifest = Serializers.manifestFor(serialization.findSerializerFor(original), original)

// Turn it back into an object
val back = serialization.deserialize(bytes, serializerId, manifest).get


Java




copy
source
ActorSystem system = ActorSystem.create("example");

// Get the Serialization Extension
Serialization serialization = SerializationExtension.get(system);

// Have something to serialize
String original = "woohoo";

// Turn it into bytes, and retrieve the serializerId and manifest, which are needed for
// deserialization
byte[] bytes = serialization.serialize(original).get();
int serializerId = serialization.findSerializerFor(original).identifier();
String manifest = Serializers.manifestFor(serialization.findSerializerFor(original), original);

// Turn it back into an object
String back = (String) serialization.deserialize(bytes, serializerId, manifest).get();




The manifest is a type hint so that the same serializer can be used for different classes.


Note that when deserializing from bytes the manifest and the identifier of the serializer are needed. It is important to use the serializer identifier in this way to support rolling updates, where the 
serialization-bindings
 for a class may have changed from one serializer to another. Therefore the three parts consisting of the bytes, the serializer id, and the manifest should always be transferred or stored together so that they can be deserialized with different 
serialization-bindings
 configuration.


The 
SerializationExtension
SerializationExtension
 is a Classic 
Extension
Extension
, but it can be used with an 
akka.actor.typed.ActorSystem
akka.actor.typed.ActorSystem
 like this:




Scala




copy
source
import akka.actor.typed.ActorSystem

val system = ActorSystem(Behaviors.empty, "example")

// Get the Serialization Extension
val serialization = SerializationExtension(system)


Java




copy
source
akka.actor.typed.ActorSystem<Void> system =
    akka.actor.typed.ActorSystem.create(Behaviors.empty(), "example");

// Get the Serialization Extension
Serialization serialization = SerializationExtension.get(system);




Customization


The first code snippet on this page contains a configuration file that references a custom serializer 
docs.serialization.MyOwnSerializer
. How would we go about creating such a custom serializer?


Creating new Serializers


A custom 
Serializer
 has to inherit from 
akka.serialization.Serializer
akka.serialization.Serializer
akka.serialization.JSerializer
akka.serialization.JSerializer
 and can be defined like the following:




Scala




copy
source
import akka.actor._
import akka.actor.typed.scaladsl.Behaviors
import akka.cluster.Cluster
import akka.serialization._



Java




copy
source
import akka.actor.*;
import akka.serialization.*;

import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;







Scala




copy
source
class MyOwnSerializer extends Serializer {

  // If you need logging here, introduce a constructor that takes an ExtendedActorSystem.
  // class MyOwnSerializer(actorSystem: ExtendedActorSystem) extends Serializer
  // Get a logger using:
  // private val logger = Logging(actorSystem, this)

  // This is whether "fromBinary" requires a "clazz" or not
  def includeManifest: Boolean = true

  // Pick a unique identifier for your Serializer,
  // you've got a couple of billions to choose from,
  // 0 - 40 is reserved by Akka itself
  def identifier = 1234567

  // "toBinary" serializes the given object to an Array of Bytes
  def toBinary(obj: AnyRef): Array[Byte] = {
    // Put the code that serializes the object here
    //#...
    Array[Byte]()
    //#...
  }

  // "fromBinary" deserializes the given array,
  // using the type hint (if any, see "includeManifest" above)
  def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
    // Put your code that deserializes here
    //#...
    null
    //#...
  }
}


Java




copy
source
static class MyOwnSerializer extends JSerializer {

  // If you need logging here, introduce a constructor that takes an ExtendedActorSystem.
  // public MyOwnSerializer(ExtendedActorSystem actorSystem)
  // Get a logger using:
  // private final LoggingAdapter logger = Logging.getLogger(actorSystem, this);

  // This is whether "fromBinary" requires a "clazz" or not
  @Override
  public boolean includeManifest() {
    return false;
  }

  // Pick a unique identifier for your Serializer,
  // you've got a couple of billions to choose from,
  // 0 - 40 is reserved by Akka itself
  @Override
  public int identifier() {
    return 1234567;
  }

  // "toBinary" serializes the given object to an Array of Bytes
  @Override
  public byte[] toBinary(Object obj) {
    // Put the code that serializes the object here
    // #...
    return new byte[0];
    // #...
  }

  // "fromBinary" deserializes the given array,
  // using the type hint (if any, see "includeManifest" above)
  @Override
  public Object fromBinaryJava(byte[] bytes, Class<?> clazz) {
    // Put your code that deserializes here
    // #...
    return null;
    // #...
  }
}




The 
identifier
 must be unique. The identifier is used when selecting which serializer to use for deserialization. If you have accidentally configured several serializers with the same identifier that will be detected and prevent the 
ActorSystem
ActorSystem
 from being started. It can be a hardcoded value because it must remain the same value to support rolling updates. 


If you prefer to define the identifier in cofiguration that is supported by the 
BaseSerializer
BaseSerializer
 trait, which implements the 
def identifier
 by reading it from configuration based on the serializer’s class name:




Scala




copy
source
akka {
  actor {
    serialization-identifiers {
      "docs.serialization.MyOwnSerializer" = 1234567
    }
  }
}




The manifest is a type hint so that the same serializer can be used for different classes. The manifest parameter in 
fromBinary
fromBinaryJava
 is the class of the object that was serialized. In 
fromBinary
fromBinaryJava
 you can match on the class and deserialize the bytes to different objects.


Then you only need to fill in the blanks, bind it to a name in your configuration and list which classes should be deserialized with it.


The serializers are initialized eagerly by the 
SerializationExtension
SerializationExtension
 when the 
ActorSystem
ActorSystem
 is started and therefore a serializer itself must not access the 
SerializationExtension
 from its constructor. Instead, it should access the 
SerializationExtension
 lazily.




Serializer with String Manifest


The 
Serializer
 illustrated above supports a class based manifest (type hint). For serialization of data that need to evolve over time the 
SerializerWithStringManifest
SerializerWithStringManifest
 is recommended instead of 
Serializer
 because the manifest (type hint) is a 
String
 instead of a 
Class
. That means that the class can be moved/removed and the serializer can still deserialize old data by matching on the 
String
. This is especially useful for 
Persistence
.


The manifest string can also encode a version number that can be used in 
fromBinary
fromBinaryJava
 to deserialize in different ways to migrate old data to new domain objects.


If the data was originally serialized with 
Serializer
 and in a later version of the system you change to 
SerializerWithStringManifest
 then the manifest string will be the full class name if you used 
includeManifest=true
, otherwise it will be the empty string.


This is how a 
SerializerWithStringManifest
 looks like:




Scala




copy
source
class MyOwnSerializer2 extends SerializerWithStringManifest {

  val CustomerManifest = "customer"
  val UserManifest = "user"
  val UTF_8 = StandardCharsets.UTF_8.name()

  // Pick a unique identifier for your Serializer,
  // you've got a couple of billions to choose from,
  // 0 - 40 is reserved by Akka itself
  def identifier = 1234567

  // The manifest (type hint) that will be provided in the fromBinary method
  // Use `""` if manifest is not needed.
  def manifest(obj: AnyRef): String =
    obj match {
      case _: Customer => CustomerManifest
      case _: User     => UserManifest
    }

  // "toBinary" serializes the given object to an Array of Bytes
  def toBinary(obj: AnyRef): Array[Byte] = {
    // Put the real code that serializes the object here
    obj match {
      case Customer(name) => name.getBytes(UTF_8)
      case User(name)     => name.getBytes(UTF_8)
    }
  }

  // "fromBinary" deserializes the given array,
  // using the type hint
  def fromBinary(bytes: Array[Byte], manifest: String): AnyRef = {
    // Put the real code that deserializes here
    manifest match {
      case CustomerManifest =>
        Customer(new String(bytes, UTF_8))
      case UserManifest =>
        User(new String(bytes, UTF_8))
    }
  }
}


Java




copy
source
static class MyOwnSerializer2 extends SerializerWithStringManifest {

  private static final String CUSTOMER_MANIFEST = "customer";
  private static final String USER_MANIFEST = "user";
  private static final Charset UTF_8 = StandardCharsets.UTF_8;

  // Pick a unique identifier for your Serializer,
  // you've got a couple of billions to choose from,
  // 0 - 40 is reserved by Akka itself
  @Override
  public int identifier() {
    return 1234567;
  }

  @Override
  public String manifest(Object obj) {
    if (obj instanceof Customer) return CUSTOMER_MANIFEST;
    else if (obj instanceof User) return USER_MANIFEST;
    else throw new IllegalArgumentException("Unknown type: " + obj);
  }

  // "toBinary" serializes the given object to an Array of Bytes
  @Override
  public byte[] toBinary(Object obj) {
    // Put the real code that serializes the object here
    if (obj instanceof Customer) return ((Customer) obj).name.getBytes(UTF_8);
    else if (obj instanceof User) return ((User) obj).name.getBytes(UTF_8);
    else throw new IllegalArgumentException("Unknown type: " + obj);
  }

  // "fromBinary" deserializes the given array,
  // using the type hint
  @Override
  public Object fromBinary(byte[] bytes, String manifest) {
    // Put the real code that deserializes here
    if (manifest.equals(CUSTOMER_MANIFEST)) return new Customer(new String(bytes, UTF_8));
    else if (manifest.equals(USER_MANIFEST)) return new User(new String(bytes, UTF_8));
    else throw new IllegalArgumentException("Unknown manifest: " + manifest);
  }
}




You must also bind it to a name in your configuration and then list which classes should be serialized by it.


It’s recommended to throw 
IllegalArgumentException
 or 
NotSerializableException
 in 
fromBinary
 if the manifest is unknown. This makes it possible to introduce new message types and send them to nodes that don’t know about them. This is typically needed when performing rolling updates, i.e. running a cluster with mixed versions for a while. Those exceptions are treated as a transient problem in the classic remoting layer. The problem will be logged and the message dropped. Other exceptions will tear down the TCP connection because it can be an indication of corrupt bytes from the underlying transport. Artery TCP handles all deserialization exceptions as transient problems.


Serializing ActorRefs


Actor references are typically included in the messages. All ActorRefs are serializable when using 
Serialization with Jackson
, but in case you are writing your own serializer, you might want to know how to serialize and deserialize them properly.


To serialize actor references to/from string representation you would use the 
ActorRefResolver
ActorRefResolver
.


For example here’s how a serializer could look for 
Ping
 and 
Pong
 messages:




Scala




copy
source
class PingSerializer(system: ExtendedActorSystem) extends SerializerWithStringManifest {
  private val actorRefResolver = ActorRefResolver(system.toTyped)

  private val PingManifest = "a"
  private val PongManifest = "b"

  override def identifier = 41

  override def manifest(msg: AnyRef) = msg match {
    case _: PingService.Ping => PingManifest
    case PingService.Pong    => PongManifest
    case _ =>
      throw new IllegalArgumentException(s"Can't serialize object of type ${msg.getClass} in [${getClass.getName}]")
  }

  override def toBinary(msg: AnyRef) = msg match {
    case PingService.Ping(who) =>
      actorRefResolver.toSerializationFormat(who).getBytes(StandardCharsets.UTF_8)
    case PingService.Pong =>
      Array.emptyByteArray
    case _ =>
      throw new IllegalArgumentException(s"Can't serialize object of type ${msg.getClass} in [${getClass.getName}]")
  }

  override def fromBinary(bytes: Array[Byte], manifest: String) = {
    manifest match {
      case PingManifest =>
        val str = new String(bytes, StandardCharsets.UTF_8)
        val ref = actorRefResolver.resolveActorRef[PingService.Pong.type](str)
        PingService.Ping(ref)
      case PongManifest =>
        PingService.Pong
      case _ =>
        throw new IllegalArgumentException(s"Unknown manifest [$manifest]")
    }
  }
}


Java




copy
source
public class PingSerializer extends SerializerWithStringManifest {

  final ExtendedActorSystem system;
  final ActorRefResolver actorRefResolver;

  static final String PING_MANIFEST = "a";
  static final String PONG_MANIFEST = "b";

  PingSerializer(ExtendedActorSystem system) {
    this.system = system;
    actorRefResolver = ActorRefResolver.get(Adapter.toTyped(system));
  }

  @Override
  public int identifier() {
    return 97876;
  }

  @Override
  public String manifest(Object obj) {
    if (obj instanceof Ping) return PING_MANIFEST;
    else if (obj instanceof Pong) return PONG_MANIFEST;
    else
      throw new IllegalArgumentException(
          "Can't serialize object of type "
              + obj.getClass()
              + " in ["
              + getClass().getName()
              + "]");
  }

  @Override
  public byte[] toBinary(Object obj) {
    if (obj instanceof Ping)
      return actorRefResolver
          .toSerializationFormat(((Ping) obj).replyTo)
          .getBytes(StandardCharsets.UTF_8);
    else if (obj instanceof Pong) return new byte[0];
    else
      throw new IllegalArgumentException(
          "Can't serialize object of type "
              + obj.getClass()
              + " in ["
              + getClass().getName()
              + "]");
  }

  @Override
  public Object fromBinary(byte[] bytes, String manifest) {
    if (PING_MANIFEST.equals(manifest)) {
      String str = new String(bytes, StandardCharsets.UTF_8);
      ActorRef<Pong> ref = actorRefResolver.resolveActorRef(str);
      return new Ping(ref);
    } else if (PONG_MANIFEST.equals(manifest)) {
      return new Pong();
    } else {
      throw new IllegalArgumentException("Unable to handle manifest: " + manifest);
    }
  }
}




Serialization of Classic 
ActorRef
ActorRef
 is described in 
Classic Serialization
. Classic and Typed actor references have the same serialization format so they can be interchanged.


Deep serialization of Actors


The recommended approach to do deep serialization of internal actor state is to use Akka 
Persistence
.


Serialization of Akka’s messages


Akka is using a Protobuf 3 for serialization of messages defined by Akka. This dependency is shaded in the 
akka-protobuf-v3
 artifact so that applications can use another version of Protobuf.


Applications should use standard Protobuf dependency and not 
akka-protobuf-v3
.


Java serialization


Java serialization is known to be slow and 
prone to attacks
 of various kinds - it never was designed for high throughput messaging after all. One may think that network bandwidth and latency limit the performance of remote messaging, but serialization is a more typical bottleneck.
Note


Akka serialization with Java serialization is disabled by default and Akka itself doesn’t use Java serialization for any of its internal messages. It is highly discouraged to enable Java serialization in production.


The log messages emitted by the disabled Java serializer in production SHOULD be treated as potential attacks which the serializer prevented, as they MAY indicate an external operator attempting to send malicious messages intending to use java serialization as attack vector. The attempts are logged with the SECURITY marker.


However, for early prototyping it is very convenient to use. For that reason and for compatibility with older systems that rely on Java serialization it can be enabled with the following configuration:


akka.actor.allow-java-serialization = on



Akka will still log warning when Java serialization is used and to silent that you may add:


akka.actor.warn-about-java-serializer-usage = off



Java serialization compatibility


It is not safe to mix major Scala versions when using the Java serialization as Scala does not guarantee compatibility and this could lead to very surprising errors.


Rolling updates


A serialized remote message (or persistent event) consists of serializer-id, the manifest, and the binary payload. When deserializing it is only looking at the serializer-id to pick which 
Serializer
JSerializer
 to use for 
fromBinary
fromBinaryJava
. The message class (the bindings) is not used for deserialization. The manifest is only used within the 
Serializer
JSerializer
 to decide how to deserialize the payload, so one 
Serializer
JSerializer
 can handle many classes.


That means that it is possible to change serialization for a message by performing two rolling update steps to switch to the new serializer.






Add the 
Serializer
JSerializer
 class and define it in 
akka.actor.serializers
 config section, but not in  
akka.actor.serialization-bindings
. Perform a rolling update for this change. This means that the  serializer class exists on all nodes and is registered, but it is still not used for serializing any  messages. That is important because during the rolling update the old nodes still don’t know about  the new serializer and would not be able to deserialize messages with that format.




The second change is to register that the serializer is to be used for certain classes by defining  those in the 
akka.actor.serialization-bindings
 config section. Perform a rolling update for this  change. This means that new nodes will use the new serializer when sending messages and old nodes will  be able to deserialize the new format. Old nodes will continue to use the old serializer when sending  messages and new nodes will be able to deserialize the old format.




As an optional third step the old serializer can be completely removed if it was not used for persistent events. It must still be possible to deserialize the events that were stored with the old serializer.


Verification


Normally, messages sent between local actors (i.e. same JVM) do not undergo serialization. For testing, sometimes, it may be desirable to force serialization on all messages (both remote and local). If you want to do this in order to verify that your messages are serializable you can enable the following config option:


copy
source
akka {
  actor {
    serialize-messages = on
  }
}


Certain messages can be excluded from verification by extending the marker 
trait
interface
 
akka.actor.NoSerializationVerificationNeeded
akka.actor.NoSerializationVerificationNeeded
 or define a class name prefix in configuration 
akka.actor.no-serialization-verification-needed-class-prefix
.


If you want to verify that your 
Props
Props
 are serializable you can enable the following config option:


copy
source
akka {
  actor {
    serialize-creators = on
  }
}
Warning


We recommend having these config options turned on 
only
 when you’re running tests. Turning these options on in production is pointless, as it would negatively impact the performance of local message passing without giving any gain.














 
Reliable delivery






Serialization with Jackson 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/coordination.html
Coordination • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination




Module info


Lease


Using a lease


Usages in other Akka modules


Lease implementations


Implementing a lease




Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination




Module info


Lease


Using a lease


Usages in other Akka modules


Lease implementations


Implementing a lease




Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Coordination


Akka Coordination is a set of tools for distributed coordination.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-coordination" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-coordination_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-coordination_${versions.ScalaBinary}"
}




Project Info: Akka Coordination


Artifact
com.typesafe.akka


akka-coordination


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.coordination


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.22, 2019-04-03




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Lease


The lease is a pluggable API for a distributed lock. 


Using a lease


Leases are loaded with:




Lease name


Config location to indicate which implementation should be loaded


Owner name




Any lease implementation should provide the following guarantees:




A lease with the same name loaded multiple times, even on different nodes, is the same lease


Only one owner can acquire the lease at a time




To acquire a lease:




Scala




copy
source
val lease = LeaseProvider(system).getLease("<name of the lease>", "docs-lease", "owner")
val acquired: Future[Boolean] = lease.acquire()
val stillAcquired: Boolean = lease.checkLease()
val released: Future[Boolean] = lease.release()


Java




copy
source
Lease lease =
    LeaseProvider.get(system).getLease("<name of the lease>", "jdocs-lease", "<owner name>");
CompletionStage<Boolean> acquired = lease.acquire();
boolean stillAcquired = lease.checkLease();
CompletionStage<Boolean> released = lease.release();




Acquiring a lease returns a 
Future
CompletionStage
 as lease implementations typically are implemented via a third party system such as the Kubernetes API server or Zookeeper.


Once a lease is acquired, 
checkLease
 can be called to ensure that the lease is still acquired. As lease implementations are based on other distributed systems, a lease can be lost due to a timeout with the third party system. This operation is not asynchronous, so it can be called before performing any action for which having the lease is important.


A lease has an owner. If the same owner tries to acquire the lease multiple times, it will succeed i.e. leases are reentrant. 


It is important to pick a lease name that will be unique for your use case. If a lease needs to be unique for each node in a Cluster the cluster host port can be used:




Scala




copy
source
val owner = Cluster(system).selfAddress.hostPort


Java




copy
source
// String owner = Cluster.get(system).selfAddress().hostPort();




For use cases where multiple different leases on the same node then something unique must be added to the name. For example a lease can be used with Cluster Sharding and in this case the shard Id is included in the lease name for each shard.


Setting a lease heartbeat


If a node with a lease crashes or is unresponsive the 
heartbeat-timeout
 is how long before other nodes can acquire the lease. Without this timeout operator intervention would be needed to release a lease in the case of a node crash. This is the safest option but not practical in all cases.


The value should be greater than the max expected JVM pause e.g. garbage collection, otherwise a lease can be acquired by another node and then when the original node becomes responsive again there will be a short time before the original lease owner can take action e.g. shutdown shards or singletons.


Usages in other Akka modules


Leases can be used for 
Split Brain Resolver
, 
Cluster Singleton
, and 
Cluster Sharding
. 


Lease implementations




Kubernetes API




Implementing a lease


Implementations should extend the 
akka.coordination.lease.scaladsl.Lease
akka.coordination.lease.javadsl.Lease
 




Scala




copy
source
class SampleLease(settings: LeaseSettings) extends Lease(settings) {

  override def acquire(): Future[Boolean] = {
    Future.successful(true)
  }

  override def acquire(leaseLostCallback: Option[Throwable] => Unit): Future[Boolean] = {
    Future.successful(true)
  }

  override def release(): Future[Boolean] = {
    Future.successful(true)
  }

  override def checkLease(): Boolean = {
    true
  }
}


Java




copy
source
static class SampleLease extends Lease {

  private LeaseSettings settings;

  public SampleLease(LeaseSettings settings) {
    this.settings = settings;
  }

  @Override
  public LeaseSettings getSettings() {
    return settings;
  }

  @Override
  public CompletionStage<Boolean> acquire() {
    return CompletableFuture.completedFuture(true);
  }

  @Override
  public CompletionStage<Boolean> acquire(Consumer<Optional<Throwable>> leaseLostCallback) {
    return CompletableFuture.completedFuture(true);
  }

  @Override
  public CompletionStage<Boolean> release() {
    return CompletableFuture.completedFuture(true);
  }

  @Override
  public boolean checkLease() {
    return true;
  }
}




The methods should provide the following guarantees:




acquire
 should complete with: 
true
 if the lease has been acquired, 
false
 if the lease is taken by another owner, or fail if it can’t communicate with the third party system implementing the lease.


release
 should complete with: 
true
 if the lease has definitely been released, 
false
 if the lease has definitely not been released, or fail if it is unknown if the lease has been released.


checkLease
 should return 
true
 if the lease has been acquired, should return 
false
 until an 
acquire
 
Future
CompletionStage
 has completed, and should return 
false
 if the lease is lost due to an error communicating with the third party. Check lease should not block.


The 
acquire
 lease lost callback should only be called after an 
acquire
 
Future
CompletionStage
 has completed and should be called if the lease is lost e.g. due to losing communication with the third party system.




In addition, it is expected that a lease implementation will include a time to live mechanism meaning that a lease won’t be held for ever in case the node crashes. If a user prefers to have outside intervention in this case for maximum safety then the time to live can be set to infinite.


The configuration must define the 
lease-class
 property for the FQCN of the lease implementation.


The lease implementation should have support for the following properties where the defaults come from 
akka.coordination.lease
:


copy
source
# if the node that acquired the leases crashes, how long should the lease be held before another owner can get it
heartbeat-timeout = 120s

# interval for communicating with the third party to confirm the lease is still held
heartbeat-interval = 12s

# lease implementations are expected to time out acquire and release calls or document
# that they do not implement an operation timeout
lease-operation-timeout = 5s



This configuration location is passed into 
getLease
.




Scala




copy
source
akka.actor.provider = cluster
docs-lease {
  lease-class = "docs.coordination.SampleLease"
  heartbeat-timeout = 100s
  heartbeat-interval = 1s
  lease-operation-timeout = 1s
  # Any lease specific configuration
}


Java




copy
source
akka.actor.provider = cluster
docs-lease {
  lease-class = "docs.coordination.SampleLease"
  heartbeat-timeout = 100s
  heartbeat-interval = 1s
  lease-operation-timeout = 1s
  # Any lease specific configuration
}
















 
Split Brain Resolver






Choosing Akka Cluster 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/reactive-streams-interop.html
Reactive Streams Interop • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop




Dependency


Overview


Other implementations




Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop




Dependency


Overview


Other implementations




Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Reactive Streams Interop


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}




Overview


Akka Streams implements the 
Reactive Streams
 standard for asynchronous stream processing with non-blocking back pressure. 


Since Java 9 the APIs of Reactive Streams has been included in the Java Standard library, under the 
java.util.concurrent.Flow
 namespace. For Java 8 there is instead a separate Reactive Streams artifact with the same APIs in the package 
org.reactivestreams
.


Akka streams provides interoperability for both these two API versions, the Reactive Streams interfaces directly through factories on the regular 
Source
 and 
Sink
 APIs. For the Java 9 and later built in interfaces there is a separate set of factories in 
akka.stream.scaladsl.JavaFlowSupport
akka.stream.javadsl.JavaFlowSupport
.


In the following samples the standalone Reactive Stream API factories has been used but each such call can be replaced with the corresponding method from 
JavaFlowSupport
 and the JDK 
java.util.concurrent.Flow._
java.util.concurrent.Flow.*
 interfaces.


Note that it is not possible to use 
JavaFlowSupport
 on Java 8 since the needed interfaces simply is not available in the Java standard library.


The two most important interfaces in Reactive Streams are the 
Publisher
 and 
Subscriber
.




Scala




copy
source
import org.reactivestreams.Publisher
import org.reactivestreams.Subscriber
import org.reactivestreams.Processor


Java




copy
source
import org.reactivestreams.Publisher;
import org.reactivestreams.Subscriber;
import org.reactivestreams.Processor;




Let us assume that a library provides a publisher of tweets:




Scala




copy
source
def tweets: Publisher[Tweet]


Java




copy
source
Publisher<Tweet> tweets();




and another library knows how to store author handles in a database:




Scala




copy
source
def storage: Subscriber[Author]


Java




copy
source
Subscriber<Author> storage();




Using an Akka Streams 
Flow
 we can transform the stream and connect those:




Scala




copy
source
val authors = Flow[Tweet].filter(_.hashtags.contains(akkaTag)).map(_.author)

Source.fromPublisher(tweets).via(authors).to(Sink.fromSubscriber(storage)).run()


Java




copy
source
final Flow<Tweet, Author, NotUsed> authors =
    Flow.of(Tweet.class).filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);

Source.fromPublisher(rs.tweets()).via(authors).to(Sink.fromSubscriber(rs.storage()));




The 
Publisher
 is used as an input 
Source
 to the flow and the 
Subscriber
 is used as an output 
Sink
.


A 
Flow
 can also be converted to a 
RunnableGraph[Processor[In, Out]]
 which materializes to a 
Processor
 when 
run()
 is called. 
run()
 itself can be called multiple times, resulting in a new 
Processor
 instance each time.




Scala




copy
source
val processor: Processor[Tweet, Author] = authors.toProcessor.run()

tweets.subscribe(processor)
processor.subscribe(storage)


Java




copy
source
final Processor<Tweet, Author> processor = authors.toProcessor().run(system);

rs.tweets().subscribe(processor);
processor.subscribe(rs.storage());




A publisher can be connected to a subscriber with the 
subscribe
 method.


It is also possible to expose a 
Source
 as a 
Publisher
 by using the Publisher-
Sink
:




Scala




copy
source
val authorPublisher: Publisher[Author] =
  Source.fromPublisher(tweets).via(authors).runWith(Sink.asPublisher(fanout = false))

authorPublisher.subscribe(storage)


Java




copy
source
final Publisher<Author> authorPublisher =
    Source.fromPublisher(rs.tweets())
        .via(authors)
        .runWith(Sink.asPublisher(AsPublisher.WITHOUT_FANOUT), system);

authorPublisher.subscribe(rs.storage());




A publisher that is created with 
Sink.asPublisher(fanout = false)
Sink.asPublisher(AsPublisher.WITHOUT_FANOUT)
 supports only a single subscription. Additional subscription attempts will be rejected with an 
IllegalStateException
.


A publisher that supports multiple subscribers using fan-out/broadcasting is created as follows:




Scala




copy
source
def alert: Subscriber[Author]
def storage: Subscriber[Author]


Java




copy
source
Subscriber<Author> alert();
Subscriber<Author> storage();






Scala




copy
source
val authorPublisher: Publisher[Author] =
  Source.fromPublisher(tweets).via(authors).runWith(Sink.asPublisher(fanout = true))

authorPublisher.subscribe(storage)
authorPublisher.subscribe(alert)


Java




copy
source
final Publisher<Author> authorPublisher =
    Source.fromPublisher(rs.tweets())
        .via(authors)
        .runWith(Sink.asPublisher(AsPublisher.WITH_FANOUT), system);

authorPublisher.subscribe(rs.storage());
authorPublisher.subscribe(rs.alert());




The input buffer size of the operator controls how far apart the slowest subscriber can be from the fastest subscriber before slowing down the stream.


To make the picture complete, it is also possible to expose a 
Sink
 as a 
Subscriber
 by using the Subscriber-
Source
:




Scala




copy
source
val tweetSubscriber: Subscriber[Tweet] =
  authors.to(Sink.fromSubscriber(storage)).runWith(Source.asSubscriber[Tweet])

tweets.subscribe(tweetSubscriber)


Java




copy
source
final Subscriber<Author> storage = rs.storage();

final Subscriber<Tweet> tweetSubscriber =
    authors.to(Sink.fromSubscriber(storage)).runWith(Source.asSubscriber(), system);

rs.tweets().subscribe(tweetSubscriber);




It is also possible to use re-wrap 
Processor
 instances as a 
Flow
 by passing a factory function that will create the 
Processor
 instances:




Scala




copy
source
// An example Processor factory
def createProcessor: Processor[Int, Int] = Flow[Int].toProcessor.run()

val flow: Flow[Int, Int, NotUsed] = Flow.fromProcessor(() => createProcessor)


Java




copy
source
// An example Processor factory
final Creator<Processor<Integer, Integer>> factory =
    new Creator<Processor<Integer, Integer>>() {
      public Processor<Integer, Integer> create() {
        return Flow.of(Integer.class).toProcessor().run(system);
      }
    };

final Flow<Integer, Integer, NotUsed> flow = Flow.fromProcessor(factory);





Please note that a factory is necessary to achieve reusability of the resulting 
Flow
.


Other implementations


Implementing Reactive Streams makes it possible to plug Akka Streams together with other stream libraries that adhere to the standard. An incomplete list of other implementations:




Reactor (1.1+)


RxJava


Ratpack


Slick
















 
Actors interop






Error Handling in Streams 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/native-image.html
Building Native Images • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Unsupported features


Features requiring additional metadata






Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Unsupported features


Features requiring additional metadata






Project Information


Akka Classic




















Building Native Images


Building native images with Akka is supported, both for local and cluster actor system applications. 


Most built-in features in Akka can be used as is, however some functionality is not available and some extension points require additional metadata for reflective access to work. Akka and the Akka umbrella tries to provide metadata for third party dependencies where needed and not provided by the dependency to allow applications with zero or little extra native-image metadata.


A full sample including many of the Akka projects and build tool set up can be found in the 
Akka Edge Documentation
. 


Unsupported features


The following features and aspects are not currently supported out of the box:




Lightbend Telemetry


Aeron UDP remoting


Testkits


LevelDB and InMem Akka Persistence plugins


Durable storage
 for Akka Distributed Data


Scala 3




Features requiring additional metadata


In general, any feature that allows a custom implementation provided by a third party library or user code that is then plugged into Akka through an entry in the application config file needs to have reflection metadata explicitly added as described in the 
GraalVM docs here
.


Examples of such classes are:




Custom Serializers


Custom Mailbox Types


Akka Persistence plugins


Akka Discovery implementations


Akka Lease implementations




For plugins provided in libraries, metadata is preferably provided with the library so that end user applications do not need to provide metadata for them.


Serialization with Jackson


When using the built-in 
JsonSerializable
JsonSerializable
 and 
CborSerializable
CborSerializable
 marker traits, message types are automatically added for reflective access by Akka. But there are a few important caveats:


Messages with only primitives, standard library, your own or Akka provided types are automatically registered. More complex message structures and special Jackson annotations must be carefully tested as it is hard to predict if and how Jackson will reflectively interact with them. 


Types only referenced in fields as type parameters of other generic types, like for example in a collection 
List[MyClass]
List
, will require explicit marker traits to be picked up 
class MyClass() extends JsonSerializable
 
class MyClass implements JsonSerializable
 or explicit entries in the 
reflect-config.json
 of your application.


Scala standard library enumerations are not supported for serialization out of the box.


If self-defined marker traits are being used, then the marker trait defined in 
serialization-bindings
, as well as each concrete message type (lookup, constructor fields as needed by Jackson) and the field types, need to be added to the reflection metadata.


Applications that define and configure 
JacksonMigration
JacksonMigration
 to evolve data formats need to list each concrete migration implementation (lookup and zero parameter constructor) in reflection metadata.


Additional object mappers added through config 
akka.serialization.jackson.jackson-modules
 need entries in the reflective metadata (lookup and constructor).


Third party serializers


Third party serializers will need reflection metadata for the serializer implementation, the types used in 
serialization-bindings
 and possibly further per-message metadata depending on the specific serializer logic.


Extensions


Classic and typed 
Extension
s loaded via configuration (
akka.extensions
, 
akka.actor.typed.extensions
, 
akka.actor.library-extensions
 or 
akka.actor.typed.library-extensions
) need an entry in the reflection metadata (class and constructor).


Akka Persistence Event Adapters


Event adapters defined in an application need to be listed in reflection metadata (class and constructor).


Reflective classic actor construction


Classic actors that have a 
Props
Props
 defined using the 
type 
Props[T]()
 or class 
Props(classOf[T], ...)
 
class 
Props.create(T.getClass, ...)
 need reflection entries for lookup and the constructor matching the set of passed parameters. 


An easier path is to instead use lambda factories 
type 
Props(new T)
 
class 
Props.create(T.getClass, () -> new T())
 to define props. The only reason to use the reflection based approach is the classic remote deploy feature.


Logging


When using 
akka-slf4j
 for logging, automatically used for 
akka-actor-typed
, it is very likely that the concrete logger chosen needs extra configuration.


While Akka does not mandate a logger implementation, 
logback-classic
 is used in many Akka samples throughout the Akka projects. Because of this Akka provides reflection metadata for logback out of the box, however projects using it will need an extra native image flag 
--initialize-at-build-time=ch.qos.logback
.


Special care needs to be taken if using the async logback appender. Either avoid using 
ch.qos.logback.classic.AsyncAppender
 or declare your own lazy version of the appender (not starting any threads at native image build time). You can see an example of such a lazy appender in the 
Akka Projections edge replication sample
Akka Projections edge replication sample














 
Rolling Updates






Project Information 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/actors-motivation.html
Why modern systems need a new programming model • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model




The challenge of encapsulation


The illusion of shared memory on modern computer architectures


The illusion of a call stack




How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model




The challenge of encapsulation


The illusion of shared memory on modern computer architectures


The illusion of a call stack




How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Why modern systems need a new programming model


The actor model was proposed decades ago by 
Carl Hewitt
 as a way to handle parallel processing in a high performance network — an environment that was not available at the time. Today, hardware and infrastructure capabilities have caught up with and exceeded Hewitt’s vision. Consequently, organizations building distributed systems with demanding requirements encounter challenges that cannot fully be solved with a traditional object-oriented programming (OOP) model, but that can benefit from the actor model.


Today, the actor model is not only recognized as a highly effective solution — it has been proven in production for some of the world’s most demanding applications. To highlight issues that the actor model addresses, this topic discusses the following mismatches between traditional programming assumptions and the reality of modern multi-threaded, multi-CPU architectures:




The challenge of encapsulation


The illusion of shared memory on modern computer architectures


The illusion of a call stack




The challenge of encapsulation


A core pillar of OOP is 
encapsulation
. Encapsulation dictates that the internal data of an object is not accessible directly from the outside; it can only be modified by invoking a set of curated methods. The object is responsible for exposing safe operations that protect the invariant nature of its encapsulated data.


For example, operations on an ordered binary tree implementation must not allow violation of the tree ordering invariant. Callers expect the ordering to be intact and when querying the tree for a certain piece of data, they need to be able to rely on this constraint.


When we analyze OOP runtime behavior, we sometimes draw a message sequence chart showing the interactions of method calls. For example:




Unfortunately, the above diagram does not accurately represent the 
lifelines
 of the instances during execution. In reality, a 
thread
 executes all these calls, and the enforcement of invariants occurs on the same thread from which the method was called. Updating the diagram with the thread of execution, it looks like this:




The significance of this clarification becomes clear when you try to model what happens with 
multiple threads
. Suddenly, our neatly drawn diagram becomes inadequate. We can try to illustrate multiple threads accessing the same instance:




There is a section of execution where two threads enter the same method. Unfortunately, the encapsulation model of objects does not guarantee anything about what happens in that section. Instructions of the two invocations can be interleaved in arbitrary ways which eliminate any hope for keeping the invariants intact without some type of coordination between two threads. Now, imagine this issue compounded by the existence of many threads.


The common approach to solving this problem is to add a lock around these methods. While this ensures that at most one thread will enter the method at any given time, this is a very costly strategy:




Locks 
seriously limit
 concurrency, they are very costly on modern CPU architectures,  requiring heavy-lifting from the operating system to suspend the thread and restore it later.


The caller thread is now blocked, so it cannot do any other meaningful work. Even in desktop applications this is  unacceptable, we want to keep user-facing parts of applications (its UI) to be responsive even when a  long background job is running. In the backend, blocking is outright wasteful.  One might think that this can be compensated by launching new threads, but threads are also a costly abstraction.


Locks introduce a new menace: deadlocks.




These realities result in a no-win situation:




Without sufficient locks, the state gets corrupted.


With many locks in place, performance suffers and very easily leads to deadlocks.




Additionally, locks only really work well locally. When it comes to coordinating across multiple machines, the only alternative is distributed locks. Unfortunately, distributed locks are several magnitudes less efficient than local locks and usually impose a hard limit on scaling out. Distributed lock protocols require several communication round-trips over the network across multiple machines, so latency goes through the roof.


In Object Oriented languages we rarely think about threads or linear execution paths in general. We often envision a system as a network of object instances that react to method calls, modify their internal state, then communicate with each other via method calls driving the whole application state forward:




However, in a multi-threaded distributed environment, what actually happens is that threads “traverse” this network of object instances by following method calls. As a result, threads are what really drive execution:




In summary
:




Objects can only guarantee encapsulation (protection of invariants) in the face of single-threaded access,  multi-thread execution almost always leads to corrupted internal state. Every invariant can be violated by  having two contending threads in the same code segment.


While locks seem to be the natural remedy to uphold encapsulation with multiple threads, in practice they  are inefficient and easily lead to deadlocks in any application of real-world scale.


Locks work locally, attempts to make them distributed exist, but offer limited potential for scaling out.




The illusion of shared memory on modern computer architectures


Programming models of the 80’-90’s conceptualize that writing to a variable means writing to a memory location directly (which somewhat muddies the water that local variables might exist only in registers). On modern architectures - if we simplify things a bit - CPUs are writing to 
cache lines
 instead of writing to memory directly. Most of these caches are local to the CPU core, that is, writes by one core are not visible by another. In order to make local changes visible to another core, and hence to another thread, the cache line needs to be shipped to the other core’s cache.


On the JVM, we have to explicitly denote memory locations to be shared across threads by using 
volatile
 markers or 
Atomic
 wrappers. Otherwise, we can access them only in a locked section. Why don’t we just mark all variables as volatile? Because shipping cache lines across cores is a very costly operation! Doing so would implicitly stall the cores involved from doing additional work, and result in bottlenecks on the cache coherence protocol (the protocol CPUs use to transfer cache lines between main memory and other CPUs). The result is magnitudes of slowdown.


Even for developers aware of this situation, figuring out which memory locations should be marked as volatile, or which atomic structures to use is a dark art.


In summary
:




There is no real shared memory anymore, CPU cores pass chunks of data (cache lines) explicitly to each other  just as computers on a network do. Inter-CPU communication and network communication have more in common than many realize. Passing messages is the norm now be it across CPUs or networked computers.


Instead of hiding the message passing aspect through variables marked as shared or using atomic data structures,  a more disciplined and principled approach is to keep state local to a concurrent entity and propagate data or events  between concurrent entities explicitly via messages.




The illusion of a call stack


Today, we often take call stacks for granted. But, they were invented in an era where concurrent programming was not as important because multi-CPU systems were not common. Call stacks do not cross threads and hence, do not model asynchronous call chains.


The problem arises when a thread intends to delegate a task to the “background”. In practice, this really means delegating to another thread. This cannot be a simple method/function call because calls are strictly local to the thread. What usually happens, is that the “caller” puts an object into a memory location shared by a worker thread (“callee”), which in turn, picks it up in some event loop. This allows the “caller” thread to move on and do other tasks.


The first issue is, how can the “caller” be notified of the completion of the task? But a more serious issue arises when a task fails with an exception. Where does the exception propagate to? It will propagate to the exception handler of the worker thread completely ignoring who the actual “caller” was:




This is a serious problem. How does the worker thread deal with the situation? It likely cannot fix the issue as it is usually oblivious of the purpose of the failed task. The “caller” thread needs to be notified somehow, but there is no call stack to unwind with an exception. Failure notification can only be done via a side-channel, for example putting an error code where the “caller” thread otherwise expects the result once ready. If this notification is not in place, the “caller” never gets notified of a failure and the task is lost! 
This is surprisingly similar to how networked systems work where messages/requests can get lost/fail without any notification.


This bad situation gets worse when things go really wrong and a worker backed by a thread encounters a bug and ends up in an unrecoverable situation. For example, an internal exception caused by a bug bubbles up to the root of the thread and makes the thread shut down. This immediately raises the question, who should restart the normal operation of the service hosted by the thread, and how should it be restored to a known-good state? At first glance, this might seem manageable, but we are suddenly faced by a new, unexpected phenomena: the actual task, that the thread was currently working on, is no longer in the shared memory location where tasks are taken from (usually a queue). In fact, due to the exception reaching to the top, unwinding all of the call stack, the task state is fully lost! 
We have lost a message even though this is local communication with no networking involved (where message losses are to be expected).
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.


In summary:




To achieve any meaningful concurrency and performance on current systems, threads must delegate tasks among each  other in an efficient way without blocking. With this style of task-delegating concurrency  (and even more so with networked/distributed computing) call stack-based error handling breaks down and new,  explicit error signaling mechanisms need to be introduced. Failures become part of the domain model.


Concurrent systems with work delegation need to handle service faults and have principled means to recover from them.  Clients of such services need to be aware that tasks/messages might get lost during restarts.  Even if loss does not happen, a response might be delayed arbitrarily due to previously enqueued tasks  (a long queue), delays caused by garbage collection, etc. In face of these, concurrent systems should handle response  deadlines in the form of timeouts, just like networked/distributed systems.




Next, let’s see how use of the actor model can overcome these challenges.














 
Introduction to Akka






How the Actor Model Meets the Needs of Modern, Distributed Systems 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/actors.html
What is an Actor? • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?




Actor Reference


State


Behavior


Mailbox


Child Actors


Supervisor Strategy


When an Actor Terminates




Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?




Actor Reference


State


Behavior


Mailbox


Child Actors


Supervisor Strategy


When an Actor Terminates




Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















What is an Actor?


The previous section about 
Actor Systems
 explained how actors form hierarchies and are the smallest unit when building an application. This section looks at one such actor in isolation, explaining the concepts you encounter while implementing it. For a more in depth reference with all the details please refer to 
Introduction to Actors
.


The 
Actor Model
 as defined by Hewitt, Bishop and Steiger in 1973 is a computational model that expresses exactly what it means for computation to be distributed. The processing unitsâActorsâcan only communicate by exchanging messages and upon reception of a message an Actor can do the following three fundamental actions:




send a finite number of messages to Actors it knows


create a finite number of new Actors


designate the behavior to be applied to the next message




An actor is a container for 
State
, 
Behavior
, a 
Mailbox
, 
Child Actors
 and a 
Supervisor Strategy
. All of this is encapsulated behind an 
Actor Reference
. One noteworthy aspect is that actors have an explicit lifecycle, they are not automatically destroyed when no longer referenced; after having created one, it is your responsibility to make sure that it will eventually be terminated as wellâwhich also gives you control over how resources are released 
When an Actor Terminates
.


Actor Reference


As detailed below, an actor object needs to be shielded from the outside in order to benefit from the actor model. Therefore, actors are represented to the outside using actor references, which are objects that can be passed around freely and without restriction. This split into inner and outer object enables transparency for all the desired operations: restarting an actor without needing to update references elsewhere, placing the actual actor object on remote hosts, sending messages to actors independent of where they are running. But the most important aspect is that it is not possible to look inside an actor and get hold of its state from the outside, unless the actor unwisely publishes this information itself.


Actor references are parameterized and only messages that are of the specified type can be sent to them.


State


Actor objects will typically contain some variables which reflect possible states the actor may be in. This can be an explicit state machine, or it could be a counter, set of listeners, pending requests, etc. These data are what make an actor valuable, and they must be protected from corruption by other actors. The good news is that Akka actors conceptually each have their own light-weight thread, which is completely shielded from the rest of the system. This means that instead of having to synchronize access using locks you can write your actor code without worrying about concurrency at all.


Behind the scenes Akka will run sets of actors on sets of real threads, where typically many actors share one thread, and subsequent invocations of one actor may end up being processed on different threads. Akka ensures that this implementation detail does not affect the single-threadedness of handling the actorâs state.


Because the internal state is vital to an actorâs operations, having inconsistent state is fatal. Thus, when the actor fails and is restarted by its supervisor, the state will be created from scratch, like upon first creating the actor. This is to enable the ability of self-healing of the system.


Optionally, an actor’s state can be automatically recovered to the state before a restart by persisting received messages and replaying them after restart (see 
Event Sourcing
).


Behavior


Every time a message is processed, it is matched against the current behavior of the actor. Behavior means a function which defines the actions to be taken in reaction to the message at that point in time, say forward a request if the client is authorized, deny it otherwise. This behavior may change over time, e.g. because different clients obtain authorization over time, or because the actor may go into an âout-of-serviceâ mode and later come back. These changes are achieved by either encoding them in state variables which are read from the behavior logic, or the function itself may be swapped out at runtime, by returning a different behavior to be used for next message. However, the initial behavior defined during construction of the actor object is special in the sense that a restart of the actor will reset its behavior to this initial one.


Messages can be sent to an 
actor Reference
 and behind this faÃ§ade there is a behavior that receives the message and acts upon it. The binding between Actor reference and behavior can change over time, but that is not visible on the outside.


Actor references are parameterized and only messages that are of the specified type can be sent to them. The association between an actor reference and its type parameter must be made when the actor reference (and its Actor) is created. For this purpose each behavior is also parameterized with the type of messages it is able to process. Since the behavior can change behind the actor reference faÃ§ade, designating the next behavior is a constrained operation: the successor must handle the same type of messages as its predecessor. This is necessary in order to not invalidate the actor references that refer to this Actor.


What this enables is that whenever a message is sent to an Actor we can statically ensure that the type of the message is one that the Actor declares to handleâwe can avoid the mistake of sending completely pointless messages. What we cannot statically ensure, though, is that the behavior behind the actor reference will be in a given state when our message is received. The fundamental reason is that the association between actor reference and behavior is a dynamic runtime property, the compiler cannot know it while it translates the source code.


This is the same as for normal Java objects with internal variables: when compiling the program we cannot know what their value will be, and if the result of a method call depends on those variables then the outcome is uncertain to a degreeâwe can only be certain that the returned value is of a given type.


The reply message type of an Actor command is described by the type of the actor reference for the reply-to that is contained within the message. This allows a conversation to be described in terms of its types: the reply will be of type A, but it might also contain an address of type B, which then allows the other Actor to continue the conversation by sending a message of type B to this new actor reference. While we cannot statically express the âcurrentâ state of an Actor, we can express the current state of a protocol between two Actors, since that is just given by the last message type that was received or sent.


Mailbox


An actorâs purpose is the processing of messages, and these messages were sent to the actor from other actors (or from outside the actor system). The piece which connects sender and receiver is the actorâs mailbox: each actor has exactly one mailbox to which all senders enqueue their messages. Enqueuing happens in the time-order of send operations, which means that messages sent from different actors may not have a defined order at runtime due to the apparent randomness of distributing actors across threads. Sending multiple messages to the same target from the same actor, on the other hand, will enqueue them in the same order.


There are different mailbox implementations to choose from, the default being a FIFO: the order of the messages processed by the actor matches the order in which they were enqueued. This is usually a good default, but applications may need to prioritize some messages over others. In this case, a priority mailbox will enqueue not always at the end but at a position as given by the message priority, which might even be at the front. While using such a queue, the order of messages processed will naturally be defined by the queueâs algorithm and in general not be FIFO.


An important feature in which Akka differs from some other actor model implementations is that the current behavior must always handle the next dequeued message, there is no scanning the mailbox for the next matching one. Failure to handle a message will typically be treated as a failure, unless this behavior is overridden.


Child Actors


Each actor is potentially a parent: if it creates children for delegating sub-tasks, it will automatically supervise them. The list of children is maintained within the actorâs context and the actor has access to it. Modifications to the list are done by spawning or stopping children and these actions are reflected immediately. The actual creation and termination actions happen behind the scenes in an asynchronous way, so they do not âblockâ their parent.


Supervisor Strategy


The final piece of an actor is its strategy for handling unexpected exceptions - failures. Fault handling is then done transparently by Akka, applying one of the strategies described in 
Fault Tolerance
 for each failure.


When an Actor Terminates


Once an actor terminates, i.e. fails in a way which is not handled by a restart, stops itself or is stopped by its supervisor, it will free up its resources, draining all remaining messages from its mailbox into the systemâs âdead letter mailboxâ which will forward them to the EventStream as DeadLetters. The mailbox is then replaced within the actor reference with a system mailbox, redirecting all new messages to the EventStream as DeadLetters. This is done on a best effort basis, though, so do not rely on it in order to construct âguaranteed deliveryâ.
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.














 
Actor Systems






Supervision and Monitoring 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/testing.html
Testing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing




Module info


Introduction


Asynchronous testing


Synchronous behavior testing




Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing




Module info


Introduction


Asynchronous testing


Synchronous behavior testing




Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Testing


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Testing
.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Actor TestKit add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-testkit-typed" % AkkaVersion % Test
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-testkit-typed_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  testImplementation "com.typesafe.akka:akka-actor-testkit-typed_${versions.ScalaBinary}"
}


We recommend using Akka TestKit with ScalaTest:
sbt
libraryDependencies += "org.scalatest" %% "scalatest" % "3.2.17" % Test
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencies>
  <dependency>
    <groupId>org.scalatest</groupId>
    <artifactId>scalatest_${scala.binary.version}</artifactId>
    <version>3.2.17</version>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  testImplementation "org.scalatest:scalatest_${versions.ScalaBinary}:3.2.17"
}




Project Info: Akka Actor Testkit (typed)


Artifact
com.typesafe.akka


akka-actor-testkit-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.actor.testkit.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


Testing can either be done asynchronously using a real 
ActorSystem
ActorSystem
 or synchronously on the testing thread using the 
BehaviorTestKit
BehaviorTestKit
.


For testing logic in a 
Behavior
Behavior
 in isolation synchronous testing is preferred, but the features that can be tested are limited. For testing interactions between multiple actors a more realistic asynchronous test is preferred.


Those two testing approaches are described in:






Asynchronous testing




Basic example


Observing mocked behavior


Test framework integration


Configuration


Controlling the scheduler


Test of logging


Silence logging output from tests




Synchronous behavior testing




Spawning children


Sending messages


Testing other effects


Checking for Log Messages




















 
Mailboxes






Asynchronous testing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/java/index.html
Developing :: Akka Documentation




























 














 


























Developers








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












SDK 








SDK


Key Value Entities


Event Sourced Entities


Views






 


Timers


Workflows










 


Contact Us


Sign In




Get Started











                    Try Akka for free
                    


Deploy and scale a multi-region app
No credit card required







                    Develop your own Akka app
                    


Akka account not required
Free and open SDK for offline dev







                    Request demo
                    


Personalized demo
Akka app design consultation












 


































Akka














Understanding






Architecture model






Deployment model






Development process






Declarative effects






Entity state models






Multi-region operations






Saga patterns






















Developing






Author your first service










Components






Event Sourced Entities






Key Value Entities






HTTP Endpoints






Views






Workflows






Timers






Consumers














Integrations






Component and service calls






Configure message brokers






Streaming














Setup and configuration






Setup and dependency injection






Serialization






Errors and failures






Access Control Lists (ACLs)






JSON Web Tokens (JWT)






Run a service locally










Developer best practices










Samples






Shopping cart quickstart


























Operating










Organizations






Manage users






Regions






Billing














Projects






Create






Manage users










Configure a container registry






Configure an external container registry














Configure message brokers






Aiven for Kafka






AWS MSK Kafka






Confluent Cloud






Google Pub/Sub










Manage secrets














Services






Deploy and manage services






Invoking Akka services






Viewing data






Data migration










Regions










Observability and monitoring






View logs






View metrics






View traces






Exporting metrics, logs, and traces














Integrating with CI/CD tools






CI/CD with GitHub Actions










Operator best practices






















Securing






Access Control Lists (ACLs)






TLS certificates






JSON Web Tokens (JWTs)






















Support






Community Forum






Email






Frequently Asked Questions






Paid Plans






Status






Request a Demo






Troubleshooting






















Reference






Service descriptor






Route descriptor






Observability descriptor






Glossary of terms






Security announcements






Release notes






Migration guide






API documentation






View query language










CLI






Install the Akka CLI






Using the Akka CLI






Enable CLI command completion










CLI command reference






akka






akka auth






akka auth container-registry






akka auth container-registry clear-cached-token






akka auth container-registry configure






akka auth container-registry credentials






akka auth container-registry install-helper






akka auth current-login






akka auth login






akka auth logout






akka auth signup






akka auth tokens






akka auth tokens create






akka auth tokens list






akka auth tokens revoke






akka auth use-token






akka completion






akka config






akka config clear-cache






akka config clear






akka config current-context






akka config delete-context






akka config get-organization






akka config get-project






akka config get






akka config list-contexts






akka config list






akka config rename-context






akka config set






akka config use-context






akka container-registry






akka container-registry delete-image






akka container-registry list-images






akka container-registry list-tags






akka container-registry list






akka container-registry print






akka container-registry push






akka docker






akka docker add-credentials






akka docker list-credentials






akka docker remove-credentials






akka docs






akka local






akka local console






akka local services






akka local services components






akka local services components get-state






akka local services components get-workflow






akka local services components list-events






akka local services components list-ids






akka local services components list-timers






akka local services components list






akka local services connectivity






akka local services list






akka local services views






akka local services views describe






akka local services views drop






akka local services views list






akka logs






akka organizations






akka organizations auth






akka organizations auth add






akka organizations auth add openid






akka organizations auth list






akka organizations auth remove






akka organizations auth show






akka organizations auth update






akka organizations auth update openid






akka organizations get






akka organizations invitations






akka organizations invitations cancel






akka organizations invitations create






akka organizations invitations list






akka organizations list






akka organizations users






akka organizations users add-binding






akka organizations users delete-binding






akka organizations users list-bindings






akka projects






akka projects config






akka projects config get






akka projects config get broker






akka projects config set






akka projects config set broker






akka projects config unset






akka projects config unset broker






akka projects delete






akka projects get






akka projects hostnames






akka projects hostnames add






akka projects hostnames list






akka projects hostnames remove






akka projects list






akka projects new






akka projects observability






akka projects observability apply






akka projects observability config






akka projects observability config traces






akka projects observability edit






akka projects observability export






akka projects observability get






akka projects observability set






akka projects observability set default






akka projects observability set default akka-console






akka projects observability set default google-cloud






akka projects observability set default otlp






akka projects observability set default splunk-hec






akka projects observability set logs






akka projects observability set logs google-cloud






akka projects observability set logs otlp






akka projects observability set logs splunk-hec






akka projects observability set metrics






akka projects observability set metrics google-cloud






akka projects observability set metrics otlp






akka projects observability set metrics prometheus






akka projects observability set metrics splunk-hec






akka projects observability set traces






akka projects observability set traces google-cloud






akka projects observability set traces otlp






akka projects observability unset






akka projects observability unset default






akka projects observability unset logs






akka projects observability unset metrics






akka projects observability unset traces






akka projects open






akka projects regions






akka projects regions add






akka projects regions list






akka projects regions set-primary






akka projects tokens






akka projects tokens create






akka projects tokens list






akka projects tokens revoke






akka projects update






akka quickstart






akka quickstart download






akka quickstart list






akka regions






akka regions list






akka roles






akka roles add-binding






akka roles delete-binding






akka roles invitations






akka roles invitations delete






akka roles invitations invite-user






akka roles invitations list






akka roles list-bindings






akka roles list






akka routes






akka routes create






akka routes delete






akka routes edit






akka routes export






akka routes get






akka routes list






akka routes update






akka secrets






akka secrets create






akka secrets create asymmetric






akka secrets create generic






akka secrets create symmetric






akka secrets create tls-ca






akka secrets create tls






akka secrets delete






akka secrets get






akka secrets list






akka services






akka services apply






akka services components






akka services components get-state






akka services components get-workflow






akka services components list-events






akka services components list-ids






akka services components list-timers






akka services components list






akka services connectivity






akka services data






akka services data cancel-task






akka services data export






akka services data get-task






akka services data import






akka services data list-tasks






akka services data watch-task






akka services delete






akka services deploy






akka services edit






akka services export






akka services expose






akka services get






akka services jwts






akka services jwts add






akka services jwts generate






akka services jwts list-algorithms






akka services jwts list






akka services jwts remove






akka services jwts update






akka services list






akka services logging






akka services logging list






akka services logging set-level






akka services logging unset-level






akka services pause






akka services proxy






akka services restart






akka services restore






akka services resume






akka services unexpose






akka services views






akka services views describe






akka services views drop






akka services views list






akka version


























Akka Libraries


















Akka


default








Akka
























Akka


Developing












Developing












The Akka SDK provides you proven design patterns that enable your apps to remain responsive to change. It frees you from infrastructure concerns and lets you focus on the application logic.






With its few, concise components, the Akka SDK is easy to learn, and you can develop services in quick, iterative steps by running your code locally with full insight through Akka’s console.






Akka services let you build REST endpoints with flexible access control and multiple ways to expose these endpoints to their consuming systems or applications. Akka is secure by default, and you explicitly express the desired access through code and configuration.






Akka encapsulates data together with the logic to access and modify it. The data itself is expressed in regular Java records (plain old Java objects). The same goes for the events that change the data, these are expressed in pure Java to reflect business events that lead to data updates. Akka enables you to build fully event-driven services by combining logic and data into one thing: entities.






Data and changes to it are managed by Akka’s runtime without the need to manage database storage. Changes to your data can be automatically replicated to multiple places, not only within a single service, but also across applications and even cloud providers. An SQL-like language lets you design read access that ensures the data is properly indexed for your application needs.






Integrations with message systems like Kafka are already built-in and the Akka SDK enables message consumers to listen to topics and queues.










Prerequisites






The following are required to develop services with the Akka SDK:










Java 21, we recommend 
Eclipse Adoptium






Apache Maven
 version 3.9 or later






curl
 command-line tool






Docker Engine
 27 or later














Getting Started






Follow 
Author your first Akka service
 to implement your first service. If you prefer to first explore working example code, you can check out the 
Shopping cart quickstart
 or our other 
samples
.






On the other hand, if you would rather spend some time exploring our documentation, here are some main features you will find in this section:










Event Sourced Entities






Key Value Entities






HTTP Endpoints






Views






Workflows






Timed Actions






Consuming and Producing
















Saga patterns


Author your first service












































© 2011 - 
, Lightbend, Inc. All rights reserved. | 
Licenses
 | 
Terms
 | 
Privacy Policy
 | 
Cookie Listing
 | 
Cookie Settings
 | 
RSS

URL: https://doc.akka.io/libraries/akka/snapshot/additional/books.html
Books and Videos • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos




Books


Videos




Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos




Books


Videos




Example projects


Project




Akka Classic




















Books and Videos


Books


Recommended reads




Akka in Action, second edition
, by Francisco Lopez-Sancho Abraham. Manning Publications Co., ISBN: 9781617299216, Jun 2023


Reactive Design Patterns
, by Roland Kuhn with Jamie Allen and Brian Hanafee, Manning Publications Co., ISBN 9781617291807, Feb 2017


Akka in Action
, by Raymond Roestenburg and Rob Bakker, Manning Publications Co., ISBN: 9781617291012, September 2016




Other reads about Akka and the Actor model




Akka Cookbook
, by HÃ©ctor Veiga Ortiz & Piyush Mishra, PACKT Publishing, ISBN: 9781785288180, May 2017


Mastering Akka
, by Christian Baxter, PACKT Publishing, ISBN: 9781786465023, October 2016


Reactive Messaging Patterns with the Actor Model
, by Vaughn Vernon, Addison-Wesley Professional, ISBN: 0133846830, August 2015


Developing an Akka Edge
, by Thomas Lockney and Raymond Tay, Bleeding Edge Press, ISBN: 9781939902054, April 2014


Effective Akka
, by Jamie Allen, O’Reilly Media, ISBN: 1449360076, August 2013


Akka Concurrency
, by Derek Wyatt, artima developer, ISBN: 0981531660, May 2013


Akka Essentials
, by Munish K. Gupta, PACKT Publishing, ISBN: 1849518289, October 2012


Start Building RESTful Microservices using Akka HTTP with Scala
, by Ayush Kumar Mishra, Knoldus Software LLP, ISBN: 9781976762543, December 2017




Videos




Effective Akka HTTP
, by Johannes Rudolph, Reactive Systems Meetup Hamburg, November 2016


Zen of Akka
 - an overview of good and bad practices in Akka, by Konrad Malawski, ScalaDays New York, June 2016
















 
Frequently Asked Questions






Example projects 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://console.akka.io/


URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial_5.html
Part 5: Querying Device Groups • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




Introduction


Dealing with possible scenarios


Implementing the query


Adding query capability to the group


Summary


What’s Next?






General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




Introduction


Dealing with possible scenarios


Implementing the query


Adding query capability to the group


Summary


What’s Next?






General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Part 5: Querying Device Groups


Introduction


The conversational patterns that we have seen so far are simple in the sense that they require the actor to keep little or no state. Specifically:




Device actors return a reading, which requires no state change


Record a temperature, which updates a single field


Device Group actors maintain group membership by adding or removing entries from a map




In this part, we will use a more complex example. Since homeowners will be interested in the temperatures throughout their home, our goal is to be able to query all of the device actors in a group. Let us start by investigating how such a query API should behave.


Dealing with possible scenarios


The very first issue we face is that the membership of a group is dynamic. Each sensor device is represented by an actor that can stop at any time. At the beginning of the query, we can ask all of the existing device actors for the current temperature. However, during the lifecycle of the query:




A device actor might stop and not be able to respond back with a temperature reading.


A new device actor might start up and not be included in the query because we weren’t aware of it.




These issues can be addressed in many different ways, but the important point is to settle on the desired behavior. The following works well for our use case:




When a query arrives, the group actor takes a 
snapshot
 of the existing device actors and will only ask those actors for the temperature.


Actors that start up 
after
 the query arrives are ignored.


If an actor in the snapshot stops during the query without answering, we will report the fact that it stopped to the sender of the query message.




Apart from device actors coming and going dynamically, some actors might take a long time to answer. For example, they could be stuck in an accidental infinite loop, or fail due to a bug and drop our request. We don’t want the query to continue indefinitely, so we will consider it complete in either of the following cases:




All actors in the snapshot have either responded or have confirmed being stopped.


We reach a pre-defined deadline.




Given these decisions, along with the fact that a device in the snapshot might have just started and not yet received a temperature to record, we can define four states for each device actor, with respect to a temperature query:




It has a temperature available: 
Temperature
.


It has responded, but has no temperature available yet: 
TemperatureNotAvailable
.


It has stopped before answering: 
DeviceNotAvailable
.


It did not respond before the deadline: 
DeviceTimedOut
.




Summarizing these in message types we can add the following to the message protocol:




Scala




copy
source
final case class RequestAllTemperatures(requestId: Long, groupId: String, replyTo: ActorRef[RespondAllTemperatures])
    extends DeviceGroupQuery.Command
    with DeviceGroup.Command
    with DeviceManager.Command

final case class RespondAllTemperatures(requestId: Long, temperatures: Map[String, TemperatureReading])

sealed trait TemperatureReading
final case class Temperature(value: Double) extends TemperatureReading
case object TemperatureNotAvailable extends TemperatureReading
case object DeviceNotAvailable extends TemperatureReading
case object DeviceTimedOut extends TemperatureReading


Java




copy
source
public static final class RequestAllTemperatures
    implements DeviceGroupQuery.Command, DeviceGroup.Command, Command {

  final long requestId;
  final String groupId;
  final ActorRef<RespondAllTemperatures> replyTo;

  public RequestAllTemperatures(
      long requestId, String groupId, ActorRef<RespondAllTemperatures> replyTo) {
    this.requestId = requestId;
    this.groupId = groupId;
    this.replyTo = replyTo;
  }
}

public static final class RespondAllTemperatures {
  final long requestId;
  final Map<String, TemperatureReading> temperatures;

  public RespondAllTemperatures(long requestId, Map<String, TemperatureReading> temperatures) {
    this.requestId = requestId;
    this.temperatures = temperatures;
  }
}

public interface TemperatureReading {}

public static final class Temperature implements TemperatureReading {
  public final double value;

  public Temperature(double value) {
    this.value = value;
  }

  @Override
  public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;

    Temperature that = (Temperature) o;

    return Double.compare(that.value, value) == 0;
  }

  @Override
  public int hashCode() {
    long temp = Double.doubleToLongBits(value);
    return (int) (temp ^ (temp >>> 32));
  }

  @Override
  public String toString() {
    return "Temperature{" + "value=" + value + '}';
  }
}

public enum TemperatureNotAvailable implements TemperatureReading {
  INSTANCE
}

public enum DeviceNotAvailable implements TemperatureReading {
  INSTANCE
}

public enum DeviceTimedOut implements TemperatureReading {
  INSTANCE
}




Implementing the query


One approach for implementing the query involves adding code to the device group actor. However, in practice this can be very cumbersome and error-prone. Remember that when we start a query, we need to take a snapshot of the devices present and start a timer so that we can enforce the deadline. In the meantime, 
another query
 can arrive. For the second query we need to keep track of the exact same information but in isolation from the previous query. This would require us to maintain separate mappings between queries and device actors.


Instead, we will implement a simpler, and superior approach. We will create an actor that represents a 
single query
 and that performs the tasks needed to complete the query on behalf of the group actor. So far we have created actors that belonged to classical domain objects, but now, we will create an actor that represents a process or a task rather than an entity. We benefit by keeping our group device actor simple and being able to better test query capability in isolation.


Defining the query actor


First, we need to design the lifecycle of our query actor. This consists of identifying its initial state, the first action it will take, and the cleanup — if necessary. The query actor will need the following information:




The snapshot and IDs of active device actors to query.


The ID of the request that started the query (so that we can include it in the reply).


The reference of the actor who sent the query. We will send the reply to this actor directly.


A deadline that indicates how long the query should wait for replies. Making this a parameter will simplify testing.




Scheduling the query timeout


Since we need a way to indicate how long we are willing to wait for responses, it is time to introduce a new Akka feature that we have not used yet, the built-in scheduler facility. Using 
Behaviors.withTimers
Behaviors.withTimers
 and 
startSingleTimer
startSingleTimer
 to schedule a message that will be sent after a given delay.


We need to create a message that represents the query timeout. We create a simple message 
CollectionTimeout
 without any parameters for this purpose.


At the start of the query, we need to ask each of the device actors for the current temperature. To be able to quickly detect devices that stopped before they got the 
ReadTemperature
 message we will also watch each of the actors. This way, we get 
DeviceTerminated
 messages for those that stop during the lifetime of the query, so we don’t need to wait until the timeout to mark these as not available.


Putting this together, the outline of our 
DeviceGroupQuery
 actor looks like this:




Scala




copy
source
object DeviceGroupQuery {

  def apply(
      deviceIdToActor: Map[String, ActorRef[Device.Command]],
      requestId: Long,
      requester: ActorRef[DeviceManager.RespondAllTemperatures],
      timeout: FiniteDuration): Behavior[Command] = {
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        new DeviceGroupQuery(deviceIdToActor, requestId, requester, timeout, context, timers)
      }
    }
  }

  trait Command

  private case object CollectionTimeout extends Command

  final case class WrappedRespondTemperature(response: Device.RespondTemperature) extends Command

  private final case class DeviceTerminated(deviceId: String) extends Command
}

class DeviceGroupQuery(
    deviceIdToActor: Map[String, ActorRef[Device.Command]],
    requestId: Long,
    requester: ActorRef[DeviceManager.RespondAllTemperatures],
    timeout: FiniteDuration,
    context: ActorContext[DeviceGroupQuery.Command],
    timers: TimerScheduler[DeviceGroupQuery.Command])
    extends AbstractBehavior[DeviceGroupQuery.Command](context) {

  import DeviceGroupQuery._
  import DeviceManager.DeviceNotAvailable
  import DeviceManager.DeviceTimedOut
  import DeviceManager.RespondAllTemperatures
  import DeviceManager.Temperature
  import DeviceManager.TemperatureNotAvailable
  import DeviceManager.TemperatureReading

  timers.startSingleTimer(CollectionTimeout, CollectionTimeout, timeout)

  private val respondTemperatureAdapter = context.messageAdapter(WrappedRespondTemperature.apply)


  deviceIdToActor.foreach {
    case (deviceId, device) =>
      context.watchWith(device, DeviceTerminated(deviceId))
      device ! Device.ReadTemperature(0, respondTemperatureAdapter)
  }

}


Java




copy
source
public class DeviceGroupQuery extends AbstractBehavior<DeviceGroupQuery.Command> {

  public interface Command {}

  private static enum CollectionTimeout implements Command {
    INSTANCE
  }

  static class WrappedRespondTemperature implements Command {
    final Device.RespondTemperature response;

    WrappedRespondTemperature(Device.RespondTemperature response) {
      this.response = response;
    }
  }

  private static class DeviceTerminated implements Command {
    final String deviceId;

    private DeviceTerminated(String deviceId) {
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(
      Map<String, ActorRef<Device.Command>> deviceIdToActor,
      long requestId,
      ActorRef<DeviceManager.RespondAllTemperatures> requester,
      Duration timeout) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(
                timers ->
                    new DeviceGroupQuery(
                        deviceIdToActor, requestId, requester, timeout, context, timers)));
  }

  private final long requestId;
  private final ActorRef<DeviceManager.RespondAllTemperatures> requester;

  private DeviceGroupQuery(
      Map<String, ActorRef<Device.Command>> deviceIdToActor,
      long requestId,
      ActorRef<DeviceManager.RespondAllTemperatures> requester,
      Duration timeout,
      ActorContext<Command> context,
      TimerScheduler<Command> timers) {
    super(context);
    this.requestId = requestId;
    this.requester = requester;

    timers.startSingleTimer(CollectionTimeout.INSTANCE, timeout);

    ActorRef<Device.RespondTemperature> respondTemperatureAdapter =
        context.messageAdapter(Device.RespondTemperature.class, WrappedRespondTemperature::new);

    for (Map.Entry<String, ActorRef<Device.Command>> entry : deviceIdToActor.entrySet()) {
      context.watchWith(entry.getValue(), new DeviceTerminated(entry.getKey()));
      entry.getValue().tell(new Device.ReadTemperature(0L, respondTemperatureAdapter));
    }
    stillWaiting = new HashSet<>(deviceIdToActor.keySet());
  }


}




Note that we have to convert the 
RespondTemperature
 replies from the device actor to the message protocol that the 
DeviceGroupQuery
 actor understands, i.e. 
DeviceGroupQuery.Command
. For this we use a 
messageAdapter
 that wraps the 
RespondTemperature
 in a 
WrappedRespondTemperature
, which 
extends
implements
 
DeviceGroupQuery.Command
.


Tracking actor state


The query actor, apart from the pending timer, has one stateful aspect, tracking the set of actors that: have replied, have stopped, or have not replied. We track this state 
in a 
var
 field of an immutable 
Map
in a mutable 
HashMap
 in the actor.


For our use case:




We keep track of the state with:
    


a 
Map
 of already received replies


a 
Set
 of actors that we still wait on






We have three events to act on:
    


We can receive a 
RespondTemperature
 message from one of the devices.


We can receive a 
DeviceTerminated
 message for a device actor that has been stopped in the meantime.


We can reach the deadline and receive a 
CollectionTimeout
.








To accomplish this, add the following to your 
DeviceGroupQuery
 source file:




Scala




copy
source
private var repliesSoFar = Map.empty[String, TemperatureReading]
private var stillWaiting = deviceIdToActor.keySet

override def onMessage(msg: Command): Behavior[Command] =
  msg match {
    case WrappedRespondTemperature(response) => onRespondTemperature(response)
    case DeviceTerminated(deviceId)          => onDeviceTerminated(deviceId)
    case CollectionTimeout                   => onCollectionTimout()
  }

private def onRespondTemperature(response: Device.RespondTemperature): Behavior[Command] = {
  val reading = response.value match {
    case Some(value) => Temperature(value)
    case None        => TemperatureNotAvailable
  }

  val deviceId = response.deviceId
  repliesSoFar += (deviceId -> reading)
  stillWaiting -= deviceId

  respondWhenAllCollected()
}

private def onDeviceTerminated(deviceId: String): Behavior[Command] = {
  if (stillWaiting(deviceId)) {
    repliesSoFar += (deviceId -> DeviceNotAvailable)
    stillWaiting -= deviceId
  }
  respondWhenAllCollected()
}

private def onCollectionTimout(): Behavior[Command] = {
  repliesSoFar ++= stillWaiting.map(deviceId => deviceId -> DeviceTimedOut)
  stillWaiting = Set.empty
  respondWhenAllCollected()
}


Java




copy
source
private Map<String, DeviceManager.TemperatureReading> repliesSoFar = new HashMap<>();
private final Set<String> stillWaiting;

@Override
public Receive<Command> createReceive() {
  return newReceiveBuilder()
      .onMessage(WrappedRespondTemperature.class, this::onRespondTemperature)
      .onMessage(DeviceTerminated.class, this::onDeviceTerminated)
      .onMessage(CollectionTimeout.class, this::onCollectionTimeout)
      .build();
}

private Behavior<Command> onRespondTemperature(WrappedRespondTemperature r) {
  DeviceManager.TemperatureReading reading =
      r.response
          .value
          .map(v -> (DeviceManager.TemperatureReading) new DeviceManager.Temperature(v))
          .orElse(DeviceManager.TemperatureNotAvailable.INSTANCE);

  String deviceId = r.response.deviceId;
  repliesSoFar.put(deviceId, reading);
  stillWaiting.remove(deviceId);

  return respondWhenAllCollected();
}

private Behavior<Command> onDeviceTerminated(DeviceTerminated terminated) {
  if (stillWaiting.contains(terminated.deviceId)) {
    repliesSoFar.put(terminated.deviceId, DeviceManager.DeviceNotAvailable.INSTANCE);
    stillWaiting.remove(terminated.deviceId);
  }
  return respondWhenAllCollected();
}

private Behavior<Command> onCollectionTimeout(CollectionTimeout timeout) {
  for (String deviceId : stillWaiting) {
    repliesSoFar.put(deviceId, DeviceManager.DeviceTimedOut.INSTANCE);
  }
  stillWaiting.clear();
  return respondWhenAllCollected();
}




For 
RespondTemperature
 and 
DeviceTerminated
 we keep track of the replies by updating 
repliesSoFar
 and remove the actor from 
stillWaiting
. For this, we can use the actor’s identifier already present in the 
DeviceTerminated
 message. For our 
RespondTemperature
 message we will need to add this information as follows:




Scala




copy
source
final case class RespondTemperature(requestId: Long, deviceId: String, value: Option[Double])


Java




copy
source
public static final class RespondTemperature {
  final long requestId;
  final String deviceId;
  final Optional<Double> value;

  public RespondTemperature(long requestId, String deviceId, Optional<Double> value) {
    this.requestId = requestId;
    this.deviceId = deviceId;
    this.value = value;
  }
}




And:




Scala




copy
source
case ReadTemperature(id, replyTo) =>
  replyTo ! RespondTemperature(id, deviceId, lastTemperatureReading)
  this


Java




copy
source
private Behavior<Command> onReadTemperature(ReadTemperature r) {
  r.replyTo.tell(new RespondTemperature(r.requestId, deviceId, lastTemperatureReading));
  return this;
}




After processing each message we delegate to a method 
respondWhenAllCollected
, which we will discuss soon.


In the case of timeout, we need to take all the actors that have not yet replied (the members of the set 
stillWaiting
) and put a 
DeviceTimedOut
 as the status in the final reply.


We now have to figure out what to do in 
respondWhenAllCollected
. First, we need to record the new result in the map 
repliesSoFar
 and remove the actor from 
stillWaiting
. The next step is to check if there are any remaining actors we are waiting for. If there is none, we send the result of the query to the original requester and stop the query actor. Otherwise, we need to update the 
repliesSoFar
 and 
stillWaiting
 structures and wait for more messages.


With all this knowledge, we can create the 
respondWhenAllCollected
 method:




Scala




copy
source
private def respondWhenAllCollected(): Behavior[Command] = {
  if (stillWaiting.isEmpty) {
    requester ! RespondAllTemperatures(requestId, repliesSoFar)
    Behaviors.stopped
  } else {
    this
  }
}


Java




copy
source
private Behavior<Command> respondWhenAllCollected() {
  if (stillWaiting.isEmpty()) {
    requester.tell(new DeviceManager.RespondAllTemperatures(requestId, repliesSoFar));
    return Behaviors.stopped();
  } else {
    return this;
  }
}




Our query actor is now done:




Scala




copy
source
object DeviceGroupQuery {

  def apply(
      deviceIdToActor: Map[String, ActorRef[Device.Command]],
      requestId: Long,
      requester: ActorRef[DeviceManager.RespondAllTemperatures],
      timeout: FiniteDuration): Behavior[Command] = {
    Behaviors.setup { context =>
      Behaviors.withTimers { timers =>
        new DeviceGroupQuery(deviceIdToActor, requestId, requester, timeout, context, timers)
      }
    }
  }

  trait Command

  private case object CollectionTimeout extends Command

  final case class WrappedRespondTemperature(response: Device.RespondTemperature) extends Command

  private final case class DeviceTerminated(deviceId: String) extends Command
}

class DeviceGroupQuery(
    deviceIdToActor: Map[String, ActorRef[Device.Command]],
    requestId: Long,
    requester: ActorRef[DeviceManager.RespondAllTemperatures],
    timeout: FiniteDuration,
    context: ActorContext[DeviceGroupQuery.Command],
    timers: TimerScheduler[DeviceGroupQuery.Command])
    extends AbstractBehavior[DeviceGroupQuery.Command](context) {

  import DeviceGroupQuery._
  import DeviceManager.DeviceNotAvailable
  import DeviceManager.DeviceTimedOut
  import DeviceManager.RespondAllTemperatures
  import DeviceManager.Temperature
  import DeviceManager.TemperatureNotAvailable
  import DeviceManager.TemperatureReading

  timers.startSingleTimer(CollectionTimeout, CollectionTimeout, timeout)

  private val respondTemperatureAdapter = context.messageAdapter(WrappedRespondTemperature.apply)

  private var repliesSoFar = Map.empty[String, TemperatureReading]
  private var stillWaiting = deviceIdToActor.keySet


  deviceIdToActor.foreach {
    case (deviceId, device) =>
      context.watchWith(device, DeviceTerminated(deviceId))
      device ! Device.ReadTemperature(0, respondTemperatureAdapter)
  }

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      case WrappedRespondTemperature(response) => onRespondTemperature(response)
      case DeviceTerminated(deviceId)          => onDeviceTerminated(deviceId)
      case CollectionTimeout                   => onCollectionTimout()
    }

  private def onRespondTemperature(response: Device.RespondTemperature): Behavior[Command] = {
    val reading = response.value match {
      case Some(value) => Temperature(value)
      case None        => TemperatureNotAvailable
    }

    val deviceId = response.deviceId
    repliesSoFar += (deviceId -> reading)
    stillWaiting -= deviceId

    respondWhenAllCollected()
  }

  private def onDeviceTerminated(deviceId: String): Behavior[Command] = {
    if (stillWaiting(deviceId)) {
      repliesSoFar += (deviceId -> DeviceNotAvailable)
      stillWaiting -= deviceId
    }
    respondWhenAllCollected()
  }

  private def onCollectionTimout(): Behavior[Command] = {
    repliesSoFar ++= stillWaiting.map(deviceId => deviceId -> DeviceTimedOut)
    stillWaiting = Set.empty
    respondWhenAllCollected()
  }

  private def respondWhenAllCollected(): Behavior[Command] = {
    if (stillWaiting.isEmpty) {
      requester ! RespondAllTemperatures(requestId, repliesSoFar)
      Behaviors.stopped
    } else {
      this
    }
  }
}


Java




copy
source
public class DeviceGroupQuery extends AbstractBehavior<DeviceGroupQuery.Command> {

  public interface Command {}

  private static enum CollectionTimeout implements Command {
    INSTANCE
  }

  static class WrappedRespondTemperature implements Command {
    final Device.RespondTemperature response;

    WrappedRespondTemperature(Device.RespondTemperature response) {
      this.response = response;
    }
  }

  private static class DeviceTerminated implements Command {
    final String deviceId;

    private DeviceTerminated(String deviceId) {
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(
      Map<String, ActorRef<Device.Command>> deviceIdToActor,
      long requestId,
      ActorRef<DeviceManager.RespondAllTemperatures> requester,
      Duration timeout) {
    return Behaviors.setup(
        context ->
            Behaviors.withTimers(
                timers ->
                    new DeviceGroupQuery(
                        deviceIdToActor, requestId, requester, timeout, context, timers)));
  }

  private final long requestId;
  private final ActorRef<DeviceManager.RespondAllTemperatures> requester;
  private Map<String, DeviceManager.TemperatureReading> repliesSoFar = new HashMap<>();
  private final Set<String> stillWaiting;


  private DeviceGroupQuery(
      Map<String, ActorRef<Device.Command>> deviceIdToActor,
      long requestId,
      ActorRef<DeviceManager.RespondAllTemperatures> requester,
      Duration timeout,
      ActorContext<Command> context,
      TimerScheduler<Command> timers) {
    super(context);
    this.requestId = requestId;
    this.requester = requester;

    timers.startSingleTimer(CollectionTimeout.INSTANCE, timeout);

    ActorRef<Device.RespondTemperature> respondTemperatureAdapter =
        context.messageAdapter(Device.RespondTemperature.class, WrappedRespondTemperature::new);

    for (Map.Entry<String, ActorRef<Device.Command>> entry : deviceIdToActor.entrySet()) {
      context.watchWith(entry.getValue(), new DeviceTerminated(entry.getKey()));
      entry.getValue().tell(new Device.ReadTemperature(0L, respondTemperatureAdapter));
    }
    stillWaiting = new HashSet<>(deviceIdToActor.keySet());
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(WrappedRespondTemperature.class, this::onRespondTemperature)
        .onMessage(DeviceTerminated.class, this::onDeviceTerminated)
        .onMessage(CollectionTimeout.class, this::onCollectionTimeout)
        .build();
  }

  private Behavior<Command> onRespondTemperature(WrappedRespondTemperature r) {
    DeviceManager.TemperatureReading reading =
        r.response
            .value
            .map(v -> (DeviceManager.TemperatureReading) new DeviceManager.Temperature(v))
            .orElse(DeviceManager.TemperatureNotAvailable.INSTANCE);

    String deviceId = r.response.deviceId;
    repliesSoFar.put(deviceId, reading);
    stillWaiting.remove(deviceId);

    return respondWhenAllCollected();
  }

  private Behavior<Command> onDeviceTerminated(DeviceTerminated terminated) {
    if (stillWaiting.contains(terminated.deviceId)) {
      repliesSoFar.put(terminated.deviceId, DeviceManager.DeviceNotAvailable.INSTANCE);
      stillWaiting.remove(terminated.deviceId);
    }
    return respondWhenAllCollected();
  }

  private Behavior<Command> onCollectionTimeout(CollectionTimeout timeout) {
    for (String deviceId : stillWaiting) {
      repliesSoFar.put(deviceId, DeviceManager.DeviceTimedOut.INSTANCE);
    }
    stillWaiting.clear();
    return respondWhenAllCollected();
  }

  private Behavior<Command> respondWhenAllCollected() {
    if (stillWaiting.isEmpty()) {
      requester.tell(new DeviceManager.RespondAllTemperatures(requestId, repliesSoFar));
      return Behaviors.stopped();
    } else {
      return this;
    }
  }

}




Testing the query actor


Now let’s verify the correctness of the query actor implementation. There are various scenarios we need to test individually to make sure everything works as expected. To be able to do this, we need to simulate the device actors somehow to exercise various normal or failure scenarios. Thankfully we took the list of collaborators (actually a 
Map
) as a parameter to the query actor, so we can pass in 
TestProbe
TestProbe
 references. In our first test, we try out the case when there are two devices and both report a temperature:




Scala




copy
source
"return temperature value for working devices" in {
  val requester = createTestProbe[RespondAllTemperatures]()

  val device1 = createTestProbe[Command]()
  val device2 = createTestProbe[Command]()

  val deviceIdToActor = Map("device1" -> device1.ref, "device2" -> device2.ref)

  val queryActor =
    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))

  device1.expectMessageType[Device.ReadTemperature]
  device2.expectMessageType[Device.ReadTemperature]

  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device1", Some(1.0)))
  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device2", Some(2.0)))

  requester.expectMessage(
    RespondAllTemperatures(
      requestId = 1,
      temperatures = Map("device1" -> Temperature(1.0), "device2" -> Temperature(2.0))))
}


Java




copy
source
@Test
public void testReturnTemperatureValueForWorkingDevices() {
  TestProbe<RespondAllTemperatures> requester =
      testKit.createTestProbe(RespondAllTemperatures.class);
  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);
  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);

  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();
  deviceIdToActor.put("device1", device1.getRef());
  deviceIdToActor.put("device2", device2.getRef());

  ActorRef<DeviceGroupQuery.Command> queryActor =
      testKit.spawn(
          DeviceGroupQuery.create(
              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));

  device1.expectMessageClass(Device.ReadTemperature.class);
  device2.expectMessageClass(Device.ReadTemperature.class);

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device1", Optional.of(1.0))));

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device2", Optional.of(2.0))));

  RespondAllTemperatures response = requester.receiveMessage();
  assertEquals(1L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", new Temperature(1.0));
  expectedTemperatures.put("device2", new Temperature(2.0));

  assertEquals(expectedTemperatures, response.temperatures);
}




That was the happy case, but we know that sometimes devices cannot provide a temperature measurement. This scenario is just slightly different from the previous:




Scala




copy
source
"return TemperatureNotAvailable for devices with no readings" in {
  val requester = createTestProbe[RespondAllTemperatures]()

  val device1 = createTestProbe[Command]()
  val device2 = createTestProbe[Command]()

  val deviceIdToActor = Map("device1" -> device1.ref, "device2" -> device2.ref)

  val queryActor =
    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))

  device1.expectMessageType[Device.ReadTemperature]
  device2.expectMessageType[Device.ReadTemperature]

  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device1", None))
  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device2", Some(2.0)))

  requester.expectMessage(
    RespondAllTemperatures(
      requestId = 1,
      temperatures = Map("device1" -> TemperatureNotAvailable, "device2" -> Temperature(2.0))))
}


Java




copy
source
@Test
public void testReturnTemperatureNotAvailableForDevicesWithNoReadings() {
  TestProbe<RespondAllTemperatures> requester =
      testKit.createTestProbe(RespondAllTemperatures.class);
  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);
  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);

  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();
  deviceIdToActor.put("device1", device1.getRef());
  deviceIdToActor.put("device2", device2.getRef());

  ActorRef<DeviceGroupQuery.Command> queryActor =
      testKit.spawn(
          DeviceGroupQuery.create(
              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));

  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);
  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device1", Optional.empty())));

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device2", Optional.of(2.0))));

  RespondAllTemperatures response = requester.receiveMessage();
  assertEquals(1L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", TemperatureNotAvailable.INSTANCE);
  expectedTemperatures.put("device2", new Temperature(2.0));

  assertEquals(expectedTemperatures, response.temperatures);
}




We also know, that sometimes device actors stop before answering:




Scala




copy
source
"return DeviceNotAvailable if device stops before answering" in {
  val requester = createTestProbe[RespondAllTemperatures]()

  val device1 = createTestProbe[Command]()
  val device2 = createTestProbe[Command]()

  val deviceIdToActor = Map("device1" -> device1.ref, "device2" -> device2.ref)

  val queryActor =
    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))

  device1.expectMessageType[Device.ReadTemperature]
  device2.expectMessageType[Device.ReadTemperature]

  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device1", Some(2.0)))

  device2.stop()

  requester.expectMessage(
    RespondAllTemperatures(
      requestId = 1,
      temperatures = Map("device1" -> Temperature(2.0), "device2" -> DeviceNotAvailable)))
}


Java




copy
source
@Test
public void testReturnDeviceNotAvailableIfDeviceStopsBeforeAnswering() {
  TestProbe<RespondAllTemperatures> requester =
      testKit.createTestProbe(RespondAllTemperatures.class);
  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);
  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);

  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();
  deviceIdToActor.put("device1", device1.getRef());
  deviceIdToActor.put("device2", device2.getRef());

  ActorRef<DeviceGroupQuery.Command> queryActor =
      testKit.spawn(
          DeviceGroupQuery.create(
              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));

  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);
  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device1", Optional.of(1.0))));

  device2.stop();

  RespondAllTemperatures response = requester.receiveMessage();
  assertEquals(1L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", new Temperature(1.0));
  expectedTemperatures.put("device2", DeviceNotAvailable.INSTANCE);

  assertEquals(expectedTemperatures, response.temperatures);
}




If you remember, there is another case related to device actors stopping. It is possible that we get a normal reply from a device actor, but then receive a 
Terminated
Terminated
 for the same actor later. In this case, we would like to keep the first reply and not mark the device as 
DeviceNotAvailable
. We should test this, too:




Scala




copy
source
"return temperature reading even if device stops after answering" in {
  val requester = createTestProbe[RespondAllTemperatures]()

  val device1 = createTestProbe[Command]()
  val device2 = createTestProbe[Command]()

  val deviceIdToActor = Map("device1" -> device1.ref, "device2" -> device2.ref)

  val queryActor =
    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))

  device1.expectMessageType[Device.ReadTemperature]
  device2.expectMessageType[Device.ReadTemperature]

  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device1", Some(1.0)))
  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device2", Some(2.0)))

  device2.stop()

  requester.expectMessage(
    RespondAllTemperatures(
      requestId = 1,
      temperatures = Map("device1" -> Temperature(1.0), "device2" -> Temperature(2.0))))
}


Java




copy
source
@Test
public void testReturnTemperatureReadingEvenIfDeviceStopsAfterAnswering() {
  TestProbe<RespondAllTemperatures> requester =
      testKit.createTestProbe(RespondAllTemperatures.class);
  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);
  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);

  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();
  deviceIdToActor.put("device1", device1.getRef());
  deviceIdToActor.put("device2", device2.getRef());

  ActorRef<DeviceGroupQuery.Command> queryActor =
      testKit.spawn(
          DeviceGroupQuery.create(
              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));

  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);
  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device1", Optional.of(1.0))));

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device2", Optional.of(2.0))));

  device2.stop();

  RespondAllTemperatures response = requester.receiveMessage();
  assertEquals(1L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", new Temperature(1.0));
  expectedTemperatures.put("device2", new Temperature(2.0));

  assertEquals(expectedTemperatures, response.temperatures);
}




The final case is when not all devices respond in time. To keep our test relatively fast, we will construct the 
DeviceGroupQuery
 actor with a smaller timeout:




Scala




copy
source
"return DeviceTimedOut if device does not answer in time" in {
  val requester = createTestProbe[RespondAllTemperatures]()

  val device1 = createTestProbe[Command]()
  val device2 = createTestProbe[Command]()

  val deviceIdToActor = Map("device1" -> device1.ref, "device2" -> device2.ref)

  val queryActor =
    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 200.millis))

  device1.expectMessageType[Device.ReadTemperature]
  device2.expectMessageType[Device.ReadTemperature]

  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, "device1", Some(1.0)))

  // no reply from device2

  requester.expectMessage(
    RespondAllTemperatures(
      requestId = 1,
      temperatures = Map("device1" -> Temperature(1.0), "device2" -> DeviceTimedOut)))
}


Java




copy
source
@Test
public void testReturnDeviceTimedOutIfDeviceDoesNotAnswerInTime() {
  TestProbe<RespondAllTemperatures> requester =
      testKit.createTestProbe(RespondAllTemperatures.class);
  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);
  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);

  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();
  deviceIdToActor.put("device1", device1.getRef());
  deviceIdToActor.put("device2", device2.getRef());

  ActorRef<DeviceGroupQuery.Command> queryActor =
      testKit.spawn(
          DeviceGroupQuery.create(
              deviceIdToActor, 1L, requester.getRef(), Duration.ofMillis(200)));

  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);
  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);

  queryActor.tell(
      new DeviceGroupQuery.WrappedRespondTemperature(
          new Device.RespondTemperature(0L, "device1", Optional.of(1.0))));

  // no reply from device2

  RespondAllTemperatures response = requester.receiveMessage();
  assertEquals(1L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", new Temperature(1.0));
  expectedTemperatures.put("device2", DeviceTimedOut.INSTANCE);

  assertEquals(expectedTemperatures, response.temperatures);
}




Our query works as expected now, it is time to include this new functionality in the 
DeviceGroup
 actor now.


Adding query capability to the group


Including the query feature in the group actor is fairly simple now. We did all the heavy lifting in the query actor itself, the group actor only needs to create it with the right initial parameters and nothing else.




Scala




copy
source
class DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)
    extends AbstractBehavior[DeviceGroup.Command](context) {
  import DeviceGroup._
  import DeviceManager.{
    DeviceRegistered,
    ReplyDeviceList,
    RequestAllTemperatures,
    RequestDeviceList,
    RequestTrackDevice
  }

  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]

  context.log.info("DeviceGroup {} started", groupId)

  override def onMessage(msg: Command): Behavior[Command] =
    msg match {
      // ... other cases omitted

      case RequestAllTemperatures(requestId, gId, replyTo) =>
        if (gId == groupId) {
          context.spawnAnonymous(
            DeviceGroupQuery(deviceIdToActor, requestId = requestId, requester = replyTo, 3.seconds))
          this
        } else
          Behaviors.unhandled
    }

  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {
    case PostStop =>
      context.log.info("DeviceGroup {} stopped", groupId)
      this
  }
}


Java




copy
source
public class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {

  public interface Command {}

  private class DeviceTerminated implements Command {
    public final ActorRef<Device.Command> device;
    public final String groupId;
    public final String deviceId;

    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {
      this.device = device;
      this.groupId = groupId;
      this.deviceId = deviceId;
    }
  }

  public static Behavior<Command> create(String groupId) {
    return Behaviors.setup(context -> new DeviceGroup(context, groupId));
  }

  private final String groupId;
  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();

  private DeviceGroup(ActorContext<Command> context, String groupId) {
    super(context);
    this.groupId = groupId;
    context.getLog().info("DeviceGroup {} started", groupId);
  }


  private DeviceGroup onAllTemperatures(DeviceManager.RequestAllTemperatures r) {
    // since Java collections are mutable, we want to avoid sharing them between actors (since
    // multiple Actors (threads)
    // modifying the same mutable data-structure is not safe), and perform a defensive copy of the
    // mutable map:
    //
    // Feel free to use your favourite immutable data-structures library with Akka in Java
    // applications!
    Map<String, ActorRef<Device.Command>> deviceIdToActorCopy = new HashMap<>(this.deviceIdToActor);

    getContext()
        .spawnAnonymous(
            DeviceGroupQuery.create(
                deviceIdToActorCopy, r.requestId, r.replyTo, Duration.ofSeconds(3)));

    return this;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        // ... other cases omitted
        .onMessage(
            DeviceManager.RequestAllTemperatures.class,
            r -> r.groupId.equals(groupId),
            this::onAllTemperatures)
        .build();
  }
}




It is probably worth restating what we said at the beginning of the chapter. By keeping the temporary state that is only relevant to the query itself in a separate actor we keep the group actor implementation very simple. It delegates everything to child actors and therefore does not have to keep state that is not relevant to its core business. Also, multiple queries can now run parallel to each other, in fact, as many as needed. In our case querying an individual device actor is a fast operation, but if this were not the case, for example, because the remote sensors need to be contacted over the network, this design would significantly improve throughput.


We close this chapter by testing that everything works together. This test is a variant of the previous ones, now exercising the group query feature:




Scala




copy
source
"be able to collect temperatures from all active devices" in {
  val registeredProbe = createTestProbe[DeviceRegistered]()
  val groupActor = spawn(DeviceGroup("group"))

  groupActor ! RequestTrackDevice("group", "device1", registeredProbe.ref)
  val deviceActor1 = registeredProbe.receiveMessage().device

  groupActor ! RequestTrackDevice("group", "device2", registeredProbe.ref)
  val deviceActor2 = registeredProbe.receiveMessage().device

  groupActor ! RequestTrackDevice("group", "device3", registeredProbe.ref)
  registeredProbe.receiveMessage()

  // Check that the device actors are working
  val recordProbe = createTestProbe[TemperatureRecorded]()
  deviceActor1 ! RecordTemperature(requestId = 0, 1.0, recordProbe.ref)
  recordProbe.expectMessage(TemperatureRecorded(requestId = 0))
  deviceActor2 ! RecordTemperature(requestId = 1, 2.0, recordProbe.ref)
  recordProbe.expectMessage(TemperatureRecorded(requestId = 1))
  // No temperature for device3

  val allTempProbe = createTestProbe[RespondAllTemperatures]()
  groupActor ! RequestAllTemperatures(requestId = 0, groupId = "group", allTempProbe.ref)
  allTempProbe.expectMessage(
    RespondAllTemperatures(
      requestId = 0,
      temperatures =
        Map("device1" -> Temperature(1.0), "device2" -> Temperature(2.0), "device3" -> TemperatureNotAvailable)))
}


Java




copy
source
@Test
public void testCollectTemperaturesFromAllActiveDevices() {
  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);
  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create("group"));

  groupActor.tell(new RequestTrackDevice("group", "device1", registeredProbe.getRef()));
  ActorRef<Device.Command> deviceActor1 = registeredProbe.receiveMessage().device;

  groupActor.tell(new RequestTrackDevice("group", "device2", registeredProbe.getRef()));
  ActorRef<Device.Command> deviceActor2 = registeredProbe.receiveMessage().device;

  groupActor.tell(new RequestTrackDevice("group", "device3", registeredProbe.getRef()));
  ActorRef<Device.Command> deviceActor3 = registeredProbe.receiveMessage().device;

  // Check that the device actors are working
  TestProbe<Device.TemperatureRecorded> recordProbe =
      testKit.createTestProbe(Device.TemperatureRecorded.class);
  deviceActor1.tell(new Device.RecordTemperature(0L, 1.0, recordProbe.getRef()));
  assertEquals(0L, recordProbe.receiveMessage().requestId);
  deviceActor2.tell(new Device.RecordTemperature(1L, 2.0, recordProbe.getRef()));
  assertEquals(1L, recordProbe.receiveMessage().requestId);
  // No temperature for device 3

  TestProbe<RespondAllTemperatures> allTempProbe =
      testKit.createTestProbe(RespondAllTemperatures.class);
  groupActor.tell(new RequestAllTemperatures(0L, "group", allTempProbe.getRef()));
  RespondAllTemperatures response = allTempProbe.receiveMessage();
  assertEquals(0L, response.requestId);

  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();
  expectedTemperatures.put("device1", new Temperature(1.0));
  expectedTemperatures.put("device2", new Temperature(2.0));
  expectedTemperatures.put("device3", TemperatureNotAvailable.INSTANCE);

  assertEquals(expectedTemperatures, response.temperatures);
}




Summary


In the context of the IoT system, this guide introduced the following concepts, among others. You can follow the links to review them if necessary:




The hierarchy of actors and their lifecycle


The importance of designing messages for flexibility


How to watch and stop actors, if necessary




What’s Next?


To continue your journey with Akka, we recommend:




Start building your own applications with Akka, make sure you 
get involved in our amazing community
 for help if you get stuck.


If youâd like some additional background, and detail, read the rest of the 
reference documentation
, check out some 
books and videos
, or even explore the free online courses 
Akka Basics for Java
Akka Basics for Scala
.


If you are interested in functional programming, read how actors can be defined in a 
functional style
. In this guide the object-oriented style was used, but you can mix both as you like.




To get from this guide to a complete application you would likely need to provide either an UI or an API. For this we recommend that you look at the following technologies and see what fits you:




Microservices with Akka tutorial
 illustrates how to implement an Event Sourced CQRS application with Akka Persistence and Akka Projections


Akka HTTP
 is a HTTP server and client library, making it possible to publish and consume HTTP endpoints


Akka gRPC
 supports fully typed, streaming gRPC servers and clients.
















 
Part 4: Working with Device Groups






General Concepts 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/index-persistence.html
Persistence (Event Sourcing) • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence (Event Sourcing)






Event Sourcing




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and EventSourcedBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Recovery


Tagging


Event adapters


Wrapping EventSourcedBehavior


Journal failures


Stash


Scaling out


Configuration


Example project




Replicated Event Sourcing




Relaxing the single-writer principle for availability


API


Resolving conflicting updates


Side effects


How it works


Running projections


Examples


Journal Support


Migrating from non-replicated




CQRS


Style Guide




Event handlers in the state


Command handlers in the state


Optional initial state


Mutable state


Leveraging Java 21 features




Snapshotting




Snapshots


Snapshot failures


Snapshot deletion


Event deletion




Testing




Module info


Unit testing with the BehaviorTestKit


Unit testing with the the ActorTestKit and EventSourcedBehaviorTestKit


Persistence TestKit


Integration testing




Schema Evolution for Event Sourced Actors




Dependency


Introduction


Schema evolution in event-sourced systems


Picking the right serialization format


Schema evolution in action




Persistence Query




Dependency


Introduction


Design overview


Read Journals


Performance and denormalization


Query plugins


Scaling out


Example project




Persistence Query for LevelDB




Dependency


Introduction


How to get the ReadJournal


Supported Queries


Configuration




Persistence Plugins




R2DBC plugin


Cassandra plugin


AWS DynamoDB plugin


JDBC plugin


Feature limitations


Enabling a plugin


Eager initialization of persistence plugin


Pre-packaged plugins




Persistence - Building a storage backend




Journal plugin API


Snapshot store plugin API


Plugin TCK


Corrupt event logs




Replicated Event Sourcing replication via direct access to replica databases




Sharded Replicated Event Sourced entities


Direct Replication of Events


Hot Standby


Examples


Journal Support




Replicated Event Sourcing Examples




Additional samples


Auction example


Shopping cart example




















 
Choosing Akka Cluster






Event Sourcing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/index-persistence-durable-state.html
Persistence (Durable State) • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence (Durable State)






Durable State




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and DurableStateBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Tagging


Wrapping DurableStateBehavior




Style Guide




Command handlers in the state


Optional initial state


Leveraging Java 21 features




CQRS


Persistence Query




Dependency


Introduction


Using query with Akka Projections




Building a storage backend for Durable State




State Store plugin API


State Store provider


Configure the State Store




















 
Shopping cart example






Durable State 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/downstream-upgrade-strategy.html
Downstream upgrade strategy • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy




Patch versions


Minor versions




Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy




Patch versions


Minor versions




Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Downstream upgrade strategy


When a new Akka version is released, downstream projects (such as 
Akka Management
, 
Akka HTTP
 and 
Akka gRPC
) do not need to update immediately: because of our 
binary compatibility
 approach, applications can take advantage of the latest version of Akka without having to wait for intermediate libraries to update.


Patch versions


When releasing a new patch version of Akka (e.g. 2.5.22), we typically don’t immediately bump the Akka version in satellite projects.


The reason for this is this will make it more low-friction for users to update those satellite projects: say their project is on Akka 2.5.22 and Akka Management 1.0.0, and we release Akka Management 1.0.1 (still built with Akka 2.5.22) and Akka 2.5.23. They can safely update to Akka Management 1.0.1 without also updating to Akka 2.5.23, or update to Akka 2.5.23 without updating to Akka Management 1.0.1.


When there is reason for a satellite project to upgrade the Akka patch version, they are free to do so at any time.


Minor versions


When releasing a new minor version of Akka (e.g. 2.6.0), satellite projects are also usually not updated immediately, but as needed.


When a satellite project does update to a new minor version of Akka, it will also increase its own minor version. The previous stable branch will enter the usual end-of-support lifecycle for Lightbend customers, and only important bugfixes will be backported to the previous version and released.


For example, when Akka 2.5.0 was released, Akka HTTP 10.0.x continued to depend on Akka 2.4. When it was time to update Akka HTTP to Akka 2.5, 10.1.0 was created, but 10.0.x was maintained for backward compatibility for a period of time according to Lightbend’s support policy.














 
Binary Compatibility Rules






Modules marked May Change 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/tutorial.html
Introduction to the Example • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example




Prerequisites


IoT example use case


What you will learn in this tutorial




Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example




Prerequisites


IoT example use case


What you will learn in this tutorial




Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Introduction to the Example


When writing prose, the hardest part is often composing the first few sentences. There is a similar “blank canvas” feeling when starting to build an Akka system. You might wonder: Which should be the first actor? Where should it live? What should it do? Fortunately — unlike with prose — established best practices can guide us through these initial steps. In the remainder of this guide, we examine the core logic of a simple Akka application to introduce you to actors and show you how to formulate solutions with them. The example demonstrates common patterns that will help you kickstart your Akka projects.


Prerequisites


You should have already followed the instructions in the 
first Hello World example
 to download and run the Hello World example. You will use this as a seed project and add the functionality described in this tutorial.
Note


Both the Java and Scala DSLs of Akka modules bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting 
javadsl
 imports when working in Scala, or viceversa. See 
IDE Tips
. 


IoT example use case


In this tutorial, we’ll use Akka to build out part of an Internet of Things (IoT) system that reports data from sensor devices installed in customers’ homes. The example focuses on temperature readings. The target use case allows customers to log in and view the last reported temperature from different areas of their homes. You can imagine that such sensors could also collect relative humidity or other interesting data and an application would likely support reading and changing device configuration, maybe even alerting home owners when sensor state falls outside of a particular range.


In a real system, the application would be exposed to customers through a mobile app or browser. This guide concentrates only on the core logic for storing temperatures that would be called over a network protocol, such as HTTP. It also includes writing tests to help you get comfortable and proficient with testing actors.


The tutorial application consists of two main components:




Device data collection:
 — maintains a local representation of the remote devices. Multiple sensor devices for a home are organized into one device group.


User dashboard:
 — periodically collects data from the devices for a  logged in user’s home and presents the results as a report.




The following diagram illustrates the example application architecture. Since we are interested in the state of each sensor device, we will model devices as actors. The running application will create as many instances of device actors and device groups as necessary.




What you will learn in this tutorial


This tutorial introduces and illustrates:




The actor hierarchy and how it influences actor behavior


How to choose the right granularity for actors


How to define protocols as messages


Typical conversational styles




Let’s get started by learning more about actors.














 
Overview of Akka libraries and modules






Part 1: Actor Architecture 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-concepts.html
Cluster Specification • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification




Introduction


Terms




Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification




Introduction


Terms




Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Specification


This document describes the design concepts of Akka Cluster. For the guide on using Akka Cluster please see either




Cluster Usage


Cluster Usage with classic Akka APIs


Cluster Membership Service




Introduction


Akka Cluster provides a fault-tolerant decentralized peer-to-peer based 
Cluster Membership Service
 with no single point of failure or single point of bottleneck. It does this using 
gossip
 protocols and an automatic 
failure detector
.


Akka Cluster allows for building distributed applications, where one application or service spans multiple nodes (in practice multiple 
ActorSystem
ActorSystem
s). 


Terms




node


A logical member of a cluster. There could be multiple nodes on a physical machine. Defined by a 
hostname:port:uid
 tuple.


cluster


A set of nodes joined together through the 
Cluster Membership Service
.


leader


A single node in the cluster that acts as the leader. Managing cluster convergence and membership state transitions.




Gossip


The cluster membership used in Akka is based on Amazon’s 
Dynamo
 system and particularly the approach taken in Basho’s’ 
Riak
 distributed database. Cluster membership is communicated using a 
Gossip Protocol
, where the current state of the cluster is gossiped randomly through the cluster, with preference to members that have not seen the latest version.


Vector Clocks


Vector clocks
 are a type of data structure and algorithm for generating a partial ordering of events in a distributed system and detecting causality violations.


We use vector clocks to reconcile and merge differences in cluster state during gossiping. A vector clock is a set of (node, counter) pairs. Each update to the cluster state has an accompanying update to the vector clock.


Gossip Convergence


Information about the cluster converges locally at a node at certain points in time. This is when a node can prove that the cluster state it is observing has been observed by all other nodes in the cluster. Convergence is implemented by passing a set of nodes that have seen current state version during gossip. This information is referred to as the seen set in the gossip overview. When all nodes are included in the seen set there is convergence.


Gossip convergence cannot occur while any nodes are 
unreachable
. The nodes need to become 
reachable
 again, or moved to the 
down
 and 
removed
 states (see the 
Cluster Membership Lifecycle
 section). This only blocks the leader from performing its cluster membership management and does not influence the application running on top of the cluster. For example this means that during a network partition it is not possible to add more nodes to the cluster. The nodes can join, but they will not be moved to the 
up
 state until the partition has healed or the unreachable nodes have been downed.


Failure Detector


The failure detector in Akka Cluster is responsible for trying to detect if a node is 
unreachable
 from the rest of the cluster. For this we are using the 
Phi Accrual Failure Detector
 implementation. To be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is easily 
configurable
 for tuning to your environments and needs.


In a cluster each node is monitored by a few (default maximum 5) other nodes. The nodes to monitor are selected from neighbors in a hashed ordered node ring. This is to increase the likelihood to monitor across racks and data centers, but the order is the same on all nodes, which ensures full coverage.


When any node is detected to be 
unreachable
 this data is spread to the rest of the cluster through the 
gossip
. In other words, only one node needs to mark a node 
unreachable
 to have the rest of the cluster mark that node 
unreachable
.


The failure detector will also detect if the node becomes 
reachable
 again. When all nodes that monitored the 
unreachable
 node detect it as 
reachable
 again the cluster, after gossip dissemination, will consider it as 
reachable
.




If system messages cannot be delivered to a node it will be quarantined and then it cannot come back from 
unreachable
. This can happen if the there are too many unacknowledged system messages (e.g. watch, Terminated, remote actor deployment, failures of actors supervised by remote parent). Then the node needs to be moved to the 
down
 or 
removed
 states (see 
Cluster Membership Lifecycle
) and the actor system of the quarantined node must be restarted before it can join the cluster again.


See the following for more details:




Phi Accrual Failure Detector
 implementation


Using the Failure Detector




Leader


After gossip convergence a 
leader
 for the cluster can be determined. There is no 
leader
 election process, the 
leader
 can always be recognised deterministically by any node whenever there is gossip convergence. The leader is only a role, any node can be the leader and it can change between convergence rounds. The 
leader
 is the first node in sorted order that is able to take the leadership role, where the preferred member states for a 
leader
 are 
up
 and 
leaving
 (see the 
Cluster Membership Lifecycle
 for more information about member states).


The role of the 
leader
 is to shift members in and out of the cluster, changing 
joining
 members to the 
up
 state or 
exiting
 members to the 
removed
 state. Currently 
leader
 actions are only triggered by receiving a new cluster state with gossip convergence.


Seed Nodes


The seed nodes are contact points for new nodes joining the cluster. When a new node is started it sends a message to all seed nodes and then sends a join command to the seed node that answers first.


The seed nodes configuration value does not have any influence on the running cluster itself, it is only relevant for new nodes joining the cluster as it helps them to find contact points to send the join command to; a new member can send this command to any current member of the cluster, not only to the seed nodes.


Gossip Protocol


A variation of 
push-pull gossip
 is used to reduce the amount of gossip information sent around the cluster. In push-pull gossip a digest is sent representing current versions but not actual values; the recipient of the gossip can then send back any values for which it has newer versions and also request values for which it has outdated versions. Akka uses a single shared state with a vector clock for versioning, so the variant of push-pull gossip used in Akka makes use of this version to only push the actual state as needed.


Periodically, the default is every 1 second, each node chooses another random node to initiate a round of gossip with. If less than Â½ of the nodes resides in the seen set (have seen the new state) then the cluster gossips 3 times instead of once every second. This adjusted gossip interval is a way to speed up the convergence process in the early dissemination phase after a state change.


The choice of node to gossip with is random but biased towards nodes that might not have seen the current state version. During each round of gossip exchange, when convergence is not yet reached, a node uses a very high probability (which is configurable) to gossip with another node which is not part of the seen set, i.e. which is likely to have an older version of the state. Otherwise it gossips with any random live node.


This biased selection is a way to speed up the convergence process in the late dissemination phase after a state change.


For clusters larger than 400 nodes (configurable, and suggested by empirical evidence) the 0.8 probability is gradually reduced to avoid overwhelming single stragglers with too many concurrent gossip requests. The gossip receiver also has a mechanism to protect itself from too many simultaneous gossip messages by dropping messages that have been enqueued in the mailbox for too long of a time.


While the cluster is in a converged state the gossiper only sends a small gossip status message containing the gossip version to the chosen node. As soon as there is a change to the cluster (meaning non-convergence) then it goes back to biased gossip again.


The recipient of the gossip state or the gossip status can use the gossip version (vector clock) to determine whether:




it has a newer version of the gossip state, in which case it sends that back to the gossiper


it has an outdated version of the state, in which case the recipient requests the current state from the gossiper by sending back its version of the gossip state


it has conflicting gossip versions, in which case the different versions are merged and sent back




If the recipient and the gossip have the same version then the gossip state is not sent or requested.


The periodic nature of the gossip has a nice batching effect of state changes, e.g. joining several nodes quickly after each other to one node will result in only one state change to be spread to other members in the cluster.


The gossip messages are serialized with 
protobuf
 and also gzipped to reduce payload size.














 
Cluster Usage






Cluster Membership Service 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-sharding.html
Cluster Sharding • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding




Module info


Introduction


Basic example


Persistence example


Shard allocation


How it works


Passivation


Automatic Passivation


Sharding State


Remembering Entities


Startup after minimum number of members


Health check


Inspecting cluster sharding state


Lease


Removal of internal Cluster Sharding data


Configuration


Example project




Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding




Module info


Introduction


Basic example


Persistence example


Shard allocation


How it works


Passivation


Automatic Passivation


Sharding State


Remembering Entities


Startup after minimum number of members


Health check


Inspecting cluster sharding state


Lease


Removal of internal Cluster Sharding data


Configuration


Example project




Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Sharding


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Cluster Sharding


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Cluster Sharding, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-sharding-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-sharding-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-sharding-typed_${versions.ScalaBinary}"
}




Project Info: Akka Cluster Sharding (typed)


Artifact
com.typesafe.akka


akka-cluster-sharding-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster.sharding.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


Cluster sharding is useful when you need to distribute actors across several nodes in the cluster and want to be able to interact with them using their logical identifier, but without having to care about their physical location in the cluster, which might also change over time.


It could for example be actors representing Aggregate Roots in Domain-Driven Design terminology. Here we call these actors “entities”. These actors typically have persistent (durable) state, but this feature is not limited to actors with persistent state.


The 
Introduction to Akka Cluster Sharding video
 is a good starting point for learning Cluster Sharding.


Cluster sharding is typically used when you have many stateful actors that together consume more resources (e.g. memory) than fit on one machine. If you only have a few stateful actors it might be easier to run them on a 
Cluster Singleton
 node. 


In this context sharding means that actors with an identifier, so called entities, can be automatically distributed across multiple nodes in the cluster. Each entity actor runs only at one place, and messages can be sent to the entity without requiring the sender to know the location of the destination actor. This is achieved by sending the messages via a 
ShardRegion
 actor provided by this extension, which knows how to route the message with the entity id to the final destination.


Cluster sharding will not be active on members with status 
WeaklyUp
 if that feature is enabled.


Warning


Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in 
multiple shards and entities
 being started, one in each separate cluster! See 
Downing
.


Basic example


Sharding is accessed via the 
ClusterSharding
ClusterSharding
 extension




Scala




copy
source
import akka.cluster.sharding.typed.ShardingEnvelope
import akka.cluster.sharding.typed.scaladsl.ClusterSharding
import akka.cluster.sharding.typed.scaladsl.EntityTypeKey
import akka.cluster.sharding.typed.scaladsl.EntityRef

val sharding = ClusterSharding(system)


Java




copy
source
import akka.cluster.sharding.typed.ShardingEnvelope;
import akka.cluster.sharding.typed.javadsl.ClusterSharding;
import akka.cluster.sharding.typed.javadsl.EntityTypeKey;
import akka.cluster.sharding.typed.javadsl.EntityRef;
import akka.cluster.sharding.typed.javadsl.Entity;
import akka.persistence.typed.PersistenceId;

ClusterSharding sharding = ClusterSharding.get(system);




It is common for sharding to be used with persistence however any 
Behavior
Behavior
 can be used with sharding e.g. a basic counter:




Scala




copy
source
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Int]) extends Command

  def apply(entityId: String): Behavior[Command] = {
    def updated(value: Int): Behavior[Command] = {
      Behaviors.receiveMessage[Command] {
        case Increment =>
          updated(value + 1)
        case GetValue(replyTo) =>
          replyTo ! value
          Behaviors.same
      }
    }

    updated(0)

  }
}


Java




copy
source
public class Counter extends AbstractBehavior<Counter.Command> {

  public interface Command {}

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    private final ActorRef<Integer> replyTo;

    public GetValue(ActorRef<Integer> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static Behavior<Command> create(String entityId) {
    return Behaviors.setup(context -> new Counter(context, entityId));
  }

  private final String entityId;
  private int value = 0;

  private Counter(ActorContext<Command> context, String entityId) {
    super(context);
    this.entityId = entityId;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, msg -> onIncrement())
        .onMessage(GetValue.class, this::onGetValue)
        .build();
  }

  private Behavior<Command> onIncrement() {
    value++;
    return this;
  }

  private Behavior<Command> onGetValue(GetValue msg) {
    msg.replyTo.tell(value);
    return this;
  }
}




Each Entity type has a key that is then used to retrieve an EntityRef for a given entity identifier. Note in the sample’s 
Counter.apply
Counter.create
 function that the 
entityId
 parameter is not called, it is included to demonstrate how one can pass it to an entity. Another way to do this is by sending the 
entityId
 as part of the message if needed.




Scala




copy
source
val TypeKey = EntityTypeKey[Counter.Command]("Counter")

val shardRegion: ActorRef[ShardingEnvelope[Counter.Command]] =
  sharding.init(Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId)))


Java




copy
source
EntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, "Counter");

ActorRef<ShardingEnvelope<Counter.Command>> shardRegion =
    sharding.init(Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())));




Messages to a specific entity are then sent via an 
EntityRef
EntityRef
. The 
entityId
 and the name of the Entity’s key can be retrieved from the 
EntityRef
. It is also possible to wrap methods in a 
ShardingEnvelope
ShardingEnvelope
 or define extractor functions and send messages directly to the shard region.




Scala




copy
source
// With an EntityRef
val counterOne: EntityRef[Counter.Command] = sharding.entityRefFor(TypeKey, "counter-1")
counterOne ! Counter.Increment

// Entity id is specified via an `ShardingEnvelope`
shardRegion ! ShardingEnvelope("counter-1", Counter.Increment)


Java




copy
source
EntityRef<Counter.Command> counterOne = sharding.entityRefFor(typeKey, "counter-1");
counterOne.tell(Counter.Increment.INSTANCE);

shardRegion.tell(new ShardingEnvelope<>("counter-1", Counter.Increment.INSTANCE));




Cluster sharding 
init
init
 should be called on every node for each entity type. Which nodes entity actors are created on can be controlled with 
roles
. 
init
 will create a 
ShardRegion
 or a proxy depending on whether the node’s role matches the entity’s role. 


The behavior factory lambda passed to the init method is defined on each node and only used locally, this means it is safe to use it for injecting for example a node local 
ActorRef
ActorRef
 that each sharded actor should have access to or some object that is not possible to serialize.


Specifying the role:




Scala




copy
source
sharding.init(
  Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId)).withRole("backend"))


Java




copy
source
EntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, "Counter");

ActorRef<ShardingEnvelope<Counter.Command>> shardRegionOrProxy =
    sharding.init(
        Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())).withRole("backend"));




A note about EntityRef and serialization


If including 
EntityRef
EntityRef
’s in messages or the 
State
/
Event
s of an 
EventSourcedBehavior
EventSourcedBehavior
, those 
EntityRef
s will need to be serialized. The 
entityId
 and 
typeKey
 of an 
EntityRef
getEntityId
 and 
getTypeKey
 methods of an 
EntityRef
 provide exactly the information needed upon deserialization to regenerate an 
EntityRef
 equivalent to the one serialized, given an expected type of messages to send to the entity.


At this time, serialization of 
EntityRef
s requires a 
custom serializer
, as the specific 
EntityTypeKey
EntityTypeKey
 (including the type of message which the desired entity type accepts) should not simply be encoded in the serialized representation but looked up on the deserializing side.


Persistence example


When using sharding, entities can be moved to different nodes in the cluster. Persistence can be used to recover the state of an actor after it has moved.


Akka Persistence is based on the single-writer principle, for a particular 
PersistenceId
PersistenceId
 only one persistent actor instance should be active. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding is typically used together with persistence to ensure that there is only one active entity for each 
PersistenceId
 (
entityId
).


Here is an example of a persistent actor that is used as a sharded entity:




Scala




copy
source
import akka.actor.typed.Behavior
import akka.cluster.sharding.typed.scaladsl.EntityTypeKey
import akka.persistence.typed.scaladsl.Effect

object HelloWorld {

  // Command
  sealed trait Command extends CborSerializable
  final case class Greet(whom: String)(val replyTo: ActorRef[Greeting]) extends Command
  // Response
  final case class Greeting(whom: String, numberOfPeople: Int) extends CborSerializable

  // Event
  final case class Greeted(whom: String) extends CborSerializable

  // State
  final case class KnownPeople(names: Set[String]) extends CborSerializable {
    def add(name: String): KnownPeople = copy(names = names + name)

    def numberOfPeople: Int = names.size
  }

  private val commandHandler: (KnownPeople, Command) => Effect[Greeted, KnownPeople] = { (_, cmd) =>
    cmd match {
      case cmd: Greet => greet(cmd)
    }
  }

  private def greet(cmd: Greet): Effect[Greeted, KnownPeople] =
    Effect.persist(Greeted(cmd.whom)).thenRun(state => cmd.replyTo ! Greeting(cmd.whom, state.numberOfPeople))

  private val eventHandler: (KnownPeople, Greeted) => KnownPeople = { (state, evt) =>
    state.add(evt.whom)
  }

  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("HelloWorld")

  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {
    Behaviors.setup { context =>
      context.log.info("Starting HelloWorld {}", entityId)
      EventSourcedBehavior(persistenceId, emptyState = KnownPeople(Set.empty), commandHandler, eventHandler)
    }
  }

}


Java




copy
source
import akka.actor.typed.javadsl.Behaviors;
import akka.cluster.sharding.typed.javadsl.EntityTypeKey;
import akka.persistence.typed.PersistenceId;
import akka.persistence.typed.javadsl.CommandHandler;
import akka.persistence.typed.javadsl.Effect;
import akka.persistence.typed.javadsl.EventHandler;

public static class HelloWorld
    extends EventSourcedBehavior<HelloWorld.Command, HelloWorld.Greeted, HelloWorld.KnownPeople> {

  // Command
  public interface Command extends CborSerializable {}

  public static final class Greet implements Command {
    public final String whom;
    public final ActorRef<Greeting> replyTo;

    public Greet(String whom, ActorRef<Greeting> replyTo) {
      this.whom = whom;
      this.replyTo = replyTo;
    }
  }

  // Response
  public static final class Greeting implements CborSerializable {
    public final String whom;
    public final int numberOfPeople;

    public Greeting(String whom, int numberOfPeople) {
      this.whom = whom;
      this.numberOfPeople = numberOfPeople;
    }
  }

  // Event
  public static final class Greeted implements CborSerializable {
    public final String whom;

    @JsonCreator
    public Greeted(String whom) {
      this.whom = whom;
    }
  }

  // State
  static final class KnownPeople implements CborSerializable {
    private Set<String> names = Collections.emptySet();

    KnownPeople() {}

    private KnownPeople(Set<String> names) {
      this.names = names;
    }

    KnownPeople add(String name) {
      Set<String> newNames = new HashSet<>(names);
      newNames.add(name);
      return new KnownPeople(newNames);
    }

    int numberOfPeople() {
      return names.size();
    }
  }

  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =
      EntityTypeKey.create(Command.class, "HelloWorld");

  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {
    return Behaviors.setup(context -> new HelloWorld(context, entityId, persistenceId));
  }

  private HelloWorld(
      ActorContext<Command> context, String entityId, PersistenceId persistenceId) {
    super(persistenceId);
    context.getLog().info("Starting HelloWorld {}", entityId);
  }

  @Override
  public KnownPeople emptyState() {
    return new KnownPeople();
  }

  @Override
  public CommandHandler<Command, Greeted, KnownPeople> commandHandler() {
    return newCommandHandlerBuilder().forAnyState().onCommand(Greet.class, this::greet).build();
  }

  private Effect<Greeted, KnownPeople> greet(KnownPeople state, Greet cmd) {
    return Effect()
        .persist(new Greeted(cmd.whom))
        .thenRun(newState -> cmd.replyTo.tell(new Greeting(cmd.whom, newState.numberOfPeople())));
  }

  @Override
  public EventHandler<KnownPeople, Greeted> eventHandler() {
    return (state, evt) -> state.add(evt.whom);
  }
}




To initialize and use the entity:




Scala




copy
source
import akka.cluster.sharding.typed.scaladsl.ClusterSharding
import akka.cluster.sharding.typed.scaladsl.Entity
import akka.util.Timeout

class HelloWorldService(system: ActorSystem[_]) {
  import system.executionContext

  private val sharding = ClusterSharding(system)

  // registration at startup
  sharding.init(Entity(typeKey = HelloWorld.TypeKey) { entityContext =>
    HelloWorld(entityContext.entityId, PersistenceId(entityContext.entityTypeKey.name, entityContext.entityId))
  })

  private implicit val askTimeout: Timeout = Timeout(5.seconds)

  def greet(worldId: String, whom: String): Future[Int] = {
    val entityRef = sharding.entityRefFor(HelloWorld.TypeKey, worldId)
    val greeting = entityRef ? HelloWorld.Greet(whom)
    greeting.map(_.numberOfPeople)
  }

}


Java




copy
source
import akka.cluster.sharding.typed.javadsl.ClusterSharding;
import akka.cluster.sharding.typed.javadsl.EntityRef;
import akka.cluster.sharding.typed.javadsl.Entity;
import akka.persistence.typed.javadsl.EventSourcedBehavior;
import akka.serialization.jackson.CborSerializable;
import akka.util.Timeout;
import com.fasterxml.jackson.annotation.JsonCreator;

public static class HelloWorldService {
  private final ActorSystem<?> system;
  private final ClusterSharding sharding;
  private final Duration askTimeout = Duration.ofSeconds(5);

  // registration at startup
  public HelloWorldService(ActorSystem<?> system) {
    this.system = system;
    sharding = ClusterSharding.get(system);

    // registration at startup
    sharding.init(
        Entity.of(
            HelloWorld.ENTITY_TYPE_KEY,
            entityContext ->
                HelloWorld.create(
                    entityContext.getEntityId(),
                    PersistenceId.of(
                        entityContext.getEntityTypeKey().name(), entityContext.getEntityId()))));
  }

  // usage example
  public CompletionStage<Integer> sayHello(String worldId, String whom) {
    EntityRef<HelloWorld.Command> entityRef =
        sharding.entityRefFor(HelloWorld.ENTITY_TYPE_KEY, worldId);
    CompletionStage<HelloWorld.Greeting> result =
        entityRef.ask(replyTo -> new HelloWorld.Greet(whom, replyTo), askTimeout);
    return result.thenApply(greeting -> greeting.numberOfPeople);
  }
}




Note how an unique 
PersistenceId
PersistenceId
 can be constructed from the 
EntityTypeKey
EntityTypeKey
 and the 
entityId
 provided by the 
EntityContext
EntityContext
 in the factory function for the 
Behavior
Behavior
. This is a typical way of defining the 
PersistenceId
 but other formats are possible, as described in the 
PersistenceId section
.


Sending messages to persistent entities is the same as if the entity wasn’t persistent. The only difference is when an entity is moved the state will be restored. In the above example 
ask
 is used but 
tell
 or any of the other 
Interaction Patterns
 can be used.


See 
persistence
 for more details.


Shard allocation


A shard is a group of entities that will be managed together. The grouping is typically defined by a hashing function of the 
entityId
. For a specific entity identifier the shard identifier must always be the same. Otherwise the entity actor might accidentally be started in several places at the same time.


By default the shard identifier is the absolute value of the 
hashCode
 of the entity identifier modulo the total number of shards. The number of shards is configured by:


copy
source
akka.cluster.sharding {
  # Number of shards used by the default HashCodeMessageExtractor
  # when no other message extractor is defined. This value must be
  # the same for all nodes in the cluster and that is verified by
  # configuration check when joining. Changing the value requires
  # stopping all nodes in the cluster.
  number-of-shards = 1000
}


As a rule of thumb, the number of shards should be a factor ten greater than the planned maximum number of cluster nodes. It doesn’t have to be exact. Fewer shards than number of nodes will result in that some nodes will not host any shards. Too many shards will result in less efficient management of the shards, e.g. rebalancing overhead, and increased latency because the coordinator is involved in the routing of the first message for each shard.


The 
number-of-shards
 configuration value must be the same for all nodes in the cluster and that is verified by configuration check when joining. Changing the value requires stopping all nodes in the cluster.


The shards are allocated to the nodes in the cluster. The decision of where to allocate a shard is done by a shard allocation strategy. 


The default implementation 
LeastShardAllocationStrategy
 allocates new shards to the 
ShardRegion
 (node) with least number of previously allocated shards. This strategy can be replaced by an application specific implementation.


When a node is added to the cluster the shards on the existing nodes will be rebalanced to the new node. The 
LeastShardAllocationStrategy
 picks shards for rebalancing from the 
ShardRegion
s with most number of previously allocated shards. They will then be allocated to the 
ShardRegion
 with least number of previously allocated shards, i.e. new members in the cluster. The amount of shards to rebalance in each round can be limited to make it progress slower since rebalancing too many shards at the same time could result in additional load on the system. For example, causing many Event Sourced entites to be started at the same time.


A new rebalance algorithm was included in Akka 2.6.10. It can reach optimal balance in a few rebalance rounds (typically 1 or 2 rounds). For backwards compatibility the new algorithm is not enabled by default. The new algorithm is recommended and will become the default in future versions of Akka. You enable the new algorithm by setting 
rebalance-absolute-limit
 > 0, for example:


akka.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit = 20



The 
rebalance-absolute-limit
 is the maximum number of shards that will be rebalanced in one rebalance round.


You may also want to tune the 
akka.cluster.sharding.least-shard-allocation-strategy.rebalance-relative-limit
. The 
rebalance-relative-limit
 is a fraction (< 1.0) of total number of (known) shards that will be rebalanced in one rebalance round. The lower result of 
rebalance-relative-limit
 and 
rebalance-absolute-limit
 will be used.


External shard allocation


An alternative allocation strategy is the 
ExternalShardAllocationStrategy
ExternalShardAllocationStrategy
 which allows explicit control over where shards are allocated via the 
ExternalShardAllocation
ExternalShardAllocation
 extension.


This can be used, for example, to match up Kafka Partition consumption with shard locations. The video 
How to co-locate Kafka Partitions with Akka Cluster Shards
 explains a setup for it. Alpakka Kafka provides 
an extension for Akka Cluster Sharding
.


To use it set it as the allocation strategy on your 
Entity
Entity
:




Scala




copy
source
val TypeKey = EntityTypeKey[Counter.Command]("Counter")

val entity = Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId))
  .withAllocationStrategy(ExternalShardAllocationStrategy(system, TypeKey.name))


Java




copy
source
EntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, "Counter");

ActorRef<ShardingEnvelope<Counter.Command>> shardRegion =
    sharding.init(
        Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId()))
            .withAllocationStrategy(
                ExternalShardAllocationStrategy.create(system, typeKey.name())));




For any shardId that has not been allocated it will be allocated to the requesting node. To make explicit allocations:




Scala




copy
source
val client: ExternalShardAllocationClient = ExternalShardAllocation(system).clientFor(TypeKey.name)
val done: Future[Done] = client.updateShardLocation("shard-id-1", Address("akka", "system", "127.0.0.1", 2552))


Java




copy
source
ExternalShardAllocationClient client =
    ExternalShardAllocation.get(system).getClient(typeKey.name());
CompletionStage<Done> done =
    client.setShardLocation("shard-id-1", new Address("akka", "system", "127.0.0.1", 2552));




Any new or moved shard allocations will be moved on the next rebalance.


The communication from the client to the shard allocation strategy is via 
Distributed Data
. It uses a single 
LWWMap
LWWMap
 that can support 10s of thousands of shards. Later versions could use multiple keys to support a greater number of shards.


Example project for external allocation strategy


akka-sample-kafka-to-sharding-scala.zip
 is an example project that can be downloaded, and with instructions of how to run, that demonstrates how to use external sharding to co-locate Kafka partition consumption with shards.


Colocate Shards


When using the default shard allocation strategy the shards for different entity types are allocated independent of each other, i.e. the same shard identifier for the different entity types may be allocated to different nodes. Colocating shards can be useful if it’s known that certain entities interact or share resources with some other entities and that can be defined by using the same shard identifier.


To colocate such shards you can use the 
ConsistentHashingShardAllocationStrategy
ConsistentHashingShardAllocationStrategy
.


Let’s look at an example where the purpose is to colocate 
Device
 entities with the 
Building
 entity they belong to. To use the same shard identifier we need to use a custom 
ShardingMessageExtractor
ShardingMessageExtractor
 for the 
Device
 and 
Building
 entities:




Scala




copy
source
object Building {
  val TypeKey = EntityTypeKey[Command]("Building")

  val NumberOfShards = 100

  final class MessageExtractor extends ShardingMessageExtractor[ShardingEnvelope[Command], Command] {

    override def entityId(envelope: ShardingEnvelope[Command]): String =
      envelope.entityId

    override def shardId(entityId: String): String =
      math.abs(entityId.hashCode % NumberOfShards).toString

    override def unwrapMessage(envelope: ShardingEnvelope[Command]): Command =
      envelope.message
  }

  sealed trait Command

  def apply(entityId: String): Behavior[Command] = ???
}

object Device {
  val TypeKey = EntityTypeKey[Command]("Device")

  final class MessageExtractor extends ShardingMessageExtractor[ShardingEnvelope[Command], Command] {

    override def entityId(envelope: ShardingEnvelope[Command]): String =
      envelope.entityId

    override def shardId(entityId: String): String = {
      // Use same shardId as the Building to colocate Building and Device
      // we have the buildingId as prefix in the entityId
      val buildingId = entityId.split(':').head
      math.abs(buildingId.hashCode % Building.NumberOfShards).toString
    }

    override def unwrapMessage(envelope: ShardingEnvelope[Command]): Command =
      envelope.message
  }

  sealed trait Command

  def apply(entityId: String): Behavior[Command] = ???
}



Java




copy
source
public static class Building extends AbstractBehavior<Building.Command> {

  static int NUMBER_OF_SHARDS = 100;

  static final class MessageExtractor
      extends ShardingMessageExtractor<ShardingEnvelope<Building.Command>, Building.Command> {
    @Override
    public String entityId(ShardingEnvelope<Command> envelope) {
      return envelope.entityId();
    }

    @Override
    public String shardId(String entityId) {
      return String.valueOf(Math.abs(entityId.hashCode() % NUMBER_OF_SHARDS));
    }

    @Override
    public Command unwrapMessage(ShardingEnvelope<Command> envelope) {
      return envelope.message();
    }
  }

  static EntityTypeKey<Building.Command> typeKey =
      EntityTypeKey.create(Building.Command.class, "Building");

  public interface Command {}

  public static Behavior<Building.Command> create(String entityId) {
    return Behaviors.setup(context -> new Building(context, entityId));
  }

  private Building(ActorContext<Building.Command> context, String entityId) {
    super(context);
  }

  @Override
  public Receive<Building.Command> createReceive() {
    return newReceiveBuilder().build();
  }
}

public static class Device extends AbstractBehavior<Device.Command> {

  static final class MessageExtractor
      extends ShardingMessageExtractor<ShardingEnvelope<Device.Command>, Device.Command> {
    @Override
    public String entityId(ShardingEnvelope<Command> envelope) {
      return envelope.entityId();
    }

    @Override
    public String shardId(String entityId) {
      // Use same shardId as the Building to colocate Building and Device
      // we have the buildingId as prefix in the entityId
      String buildingId = entityId.split(":")[0];
      return String.valueOf(Math.abs(buildingId.hashCode() % Building.NUMBER_OF_SHARDS));
    }

    @Override
    public Command unwrapMessage(ShardingEnvelope<Command> envelope) {
      return envelope.message();
    }
  }

  static EntityTypeKey<Device.Command> typeKey =
      EntityTypeKey.create(Device.Command.class, "Device");

  public interface Command {}

  public static Behavior<Device.Command> create(String entityId) {
    return Behaviors.setup(context -> new Device(context, entityId));
  }

  private Device(ActorContext<Device.Command> context, String entityId) {
    super(context);
  }

  @Override
  public Receive<Device.Command> createReceive() {
    return newReceiveBuilder().build();
  }
}





Set the allocation strategy and message extractor on your 
Entity
Entity
:




Scala




copy
source
ClusterSharding(system).init(
  Entity(Building.TypeKey)(createBehavior = entityContext => Building(entityContext.entityId))
    .withMessageExtractor(new Building.MessageExtractor)
    .withAllocationStrategy(new ConsistentHashingShardAllocationStrategy(rebalanceLimit = 10)))

ClusterSharding(system).init(
  Entity(Device.TypeKey)(createBehavior = entityContext => Device(entityContext.entityId))
    .withMessageExtractor(new Device.MessageExtractor)
    .withAllocationStrategy(new ConsistentHashingShardAllocationStrategy(rebalanceLimit = 10)))


Java




copy
source
int rebalanceLimit = 10;

ClusterSharding.get(system)
    .init(
        Entity.of(Building.typeKey, ctx -> Building.create(ctx.getEntityId()))
            .withMessageExtractor(new Building.MessageExtractor())
            .withAllocationStrategy(
                new ConsistentHashingShardAllocationStrategy(rebalanceLimit)));

ClusterSharding.get(system)
    .init(
        Entity.of(Device.typeKey, ctx -> Device.create(ctx.getEntityId()))
            .withMessageExtractor(new Device.MessageExtractor())
            .withAllocationStrategy(
                new ConsistentHashingShardAllocationStrategy(rebalanceLimit)));


Note


Create a new instance of the 
ConsistentHashingShardAllocationStrategy
 for each entity type, i.e. a 
ConsistentHashingShardAllocationStrategy
 instance must not be shared between different entity types.


The allocation strategy is using 
Consistent Hashing
 of the Cluster membership ring to assign a shard to a node. When adding or removing nodes it will rebalance according to the new consistent hashing, but that means that only a few shards will be rebalanced and others remain on the same location. When there are changes to the Cluster membership the shards may be on different nodes for a while, but eventually, when the membership is stable, the shards with the same identifier will end up on the same node.


Custom shard allocation


An optional custom shard allocation strategy can be passed into the optional parameter when initializing an entity type or explicitly using the 
withAllocationStrategy
withAllocationStrategy
 function. See the API documentation of 
ShardAllocationStrategy
AbstractShardAllocationStrategy
 for details of how to implement a custom 
ShardAllocationStrategy
.


How it works


See 
Cluster Sharding concepts
.


Passivation


If the state of the entities are persistent you may stop entities that are not used to reduce memory consumption. This is done by the application specific implementation of the entity actors for example by defining receive timeout (
context.setReceiveTimeout
context.setReceiveTimeout
). If a message is already enqueued to the entity when it stops itself the enqueued message in the mailbox will be dropped. To support graceful passivation without losing such messages the entity actor can send 
ClusterSharding.Passivate
ClusterSharding.Passivate
 to the 
ActorRef
ActorRef
[
ShardCommand
]
<
ShardCommand
>
 that was passed in to the factory method when creating the entity. The optional 
stopMessage
 message will be sent back to the entity, which is then supposed to stop itself, otherwise it will be stopped automatically. Incoming messages will be buffered by the 
Shard
 between reception of 
Passivate
 and termination of the entity. Such buffered messages are thereafter delivered to a new incarnation of the entity.




Scala




copy
source
object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Int]) extends Command
  private case object Idle extends Command
  case object GoodByeCounter extends Command

  def apply(shard: ActorRef[ClusterSharding.ShardCommand], entityId: String): Behavior[Command] = {
    Behaviors.setup { ctx =>
      def updated(value: Int): Behavior[Command] =
        Behaviors.receiveMessage[Command] {
          case Increment =>
            updated(value + 1)
          case GetValue(replyTo) =>
            replyTo ! value
            Behaviors.same
          case Idle =>
            // after receive timeout
            shard ! ClusterSharding.Passivate(ctx.self)
            Behaviors.same
          case GoodByeCounter =>
            // the stopMessage, used for rebalance and passivate
            Behaviors.stopped
        }

      ctx.setReceiveTimeout(30.seconds, Idle)
      updated(0)
    }
  }
}


Java




copy
source
public class Counter2 extends AbstractBehavior<Counter2.Command> {

  public interface Command {}

  private enum Idle implements Command {
    INSTANCE
  }

  public enum GoodByeCounter implements Command {
    INSTANCE
  }

  public enum Increment implements Command {
    INSTANCE
  }

  public static class GetValue implements Command {
    private final ActorRef<Integer> replyTo;

    public GetValue(ActorRef<Integer> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static Behavior<Command> create(
      ActorRef<ClusterSharding.ShardCommand> shard, String entityId) {
    return Behaviors.setup(
        ctx -> {
          ctx.setReceiveTimeout(Duration.ofSeconds(30), Idle.INSTANCE);
          return new Counter2(ctx, shard, entityId);
        });
  }

  private final ActorRef<ClusterSharding.ShardCommand> shard;
  private final String entityId;
  private int value = 0;

  private Counter2(
      ActorContext<Command> context,
      ActorRef<ClusterSharding.ShardCommand> shard,
      String entityId) {
    super(context);
    this.shard = shard;
    this.entityId = entityId;
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Increment.class, msg -> onIncrement())
        .onMessage(GetValue.class, this::onGetValue)
        .onMessage(Idle.class, msg -> onIdle())
        .onMessage(GoodByeCounter.class, msg -> onGoodByeCounter())
        .build();
  }

  private Behavior<Command> onIncrement() {
    value++;
    return this;
  }

  private Behavior<Command> onGetValue(GetValue msg) {
    msg.replyTo.tell(value);
    return this;
  }

  private Behavior<Command> onIdle() {
    // after receive timeout
    shard.tell(new ClusterSharding.Passivate<>(getContext().getSelf()));
    return this;
  }

  private Behavior<Command> onGoodByeCounter() {
    // the stopMessage, used for rebalance and passivate
    return Behaviors.stopped();
  }
}




and then initialized with:




Scala




copy
source
val TypeKey = EntityTypeKey[Counter.Command]("Counter")

ClusterSharding(system).init(Entity(TypeKey)(createBehavior = entityContext =>
  Counter(entityContext.shard, entityContext.entityId)).withStopMessage(Counter.GoodByeCounter))


Java




copy
source
EntityTypeKey<Counter2.Command> typeKey =
    EntityTypeKey.create(Counter2.Command.class, "Counter");

sharding.init(
    Entity.of(typeKey, ctx -> Counter2.create(ctx.getShard(), ctx.getEntityId()))
        .withStopMessage(Counter2.GoodByeCounter.INSTANCE));




Note that in the above example the 
stopMessage
 is specified as 
GoodByeCounter
. That message will be sent to the entity when it’s supposed to stop itself due to rebalance or passivation. If the 
stopMessage
 is not defined it will be stopped automatically without receiving a specific message. It can be useful to define a custom stop message if the entity needs to perform some asynchronous cleanup or interactions before stopping.


The stop message is only sent locally, from the shard to the entity so does not require an entity id to end up in the right actor. When using a custom 
ShardingMessageExtractor
ShardingMessageExtractor
 without envelopes, the extractor will still have to handle the stop message type to please the compiler, even though it will never actually be passed to the extractor.


Automatic Passivation


Entities are automatically passivated based on a passivation strategy. The default passivation strategy is to 
passivate idle entities
 when they haven’t received a message within a specified interval, and this is the current default strategy to maintain compatibility with earlier versions. It’s recommended to switch to a 
passivation strategy with an active entity limit
 and a pre-configured default strategy is provided. Active entity limits and idle entity timeouts can also be used together.
Note


The automatic passivation strategies, except 
passivate idle entities
 are marked as 
may change
 in the sense of being the subject of final development. This means that the configuration or semantics can change without warning or deprecation period. The passivation strategies can be used in production, but we reserve the right to adjust the configuration after additional testing and feedback.


Automatic passivation can be disabled by setting 
akka.cluster.sharding.passivation.strategy = none
. It is disabled automatically if 
Remembering Entities
 is enabled.
Note


Only messages sent through Cluster Sharding are counted as entity activity for automatic passivation. Messages sent directly to the 
ActorRef
ActorRef
, including messages that the actor sends to itself, are not counted as entity activity.


Idle entity passivation


Idle entities can be automatically passivated when they have not received a message for a specified length of time. This is currently the default strategy, for compatibility, and is enabled automatically with a timeout of 2 minutes. Specify a different idle timeout with configuration:


copy
source
akka.cluster.sharding.passivation {
  default-idle-strategy.idle-entity.timeout = 3 minutes
}


Or specify the idle timeout as a duration using the 
withPassivationStrategy
withPassivationStrategy
 method on 
ClusterShardingSettings
.


Idle entity timeouts can be enabled and configured for any passivation strategy.


Active entity limits


Automatic passivation strategies can limit the number of active entities. Limit-based passivation strategies use a replacement policy to determine which active entities should be passivated when the active entity limit is exceeded. The configurable limit is for a whole shard region and is divided evenly among the active shards in each region.


A recommended passivation strategy, which will become the new default passivation strategy in future versions of Akka Cluster Sharding, can be enabled with configuration:


copy
source
akka.cluster.sharding.passivation {
  strategy = default-strategy
}


This default strategy uses a 
composite passivation strategy
 which combines recency-based and frequency-based tracking: the main area is configured with a 
segmented least recently used policy
 with a frequency-biased 
admission filter
, fronted by a recency-biased 
admission window
 with 
adaptive sizing
 enabled.


The active entity limit for the default strategy can be configured:


copy
source
akka.cluster.sharding.passivation {
  strategy = default-strategy
  default-strategy {
    active-entity-limit = 1000000
  }
}


Or using the 
withActiveEntityLimit
withActiveEntityLimit
 method on 
ClusterShardingSettings.PassivationStrategySettings
.


An 
idle entity timeout
 can also be enabled and configured for this strategy:


copy
source
akka.cluster.sharding.passivation {
  strategy = default-strategy
  default-strategy {
    idle-entity.timeout = 30.minutes
  }
}


Or using the 
withIdleEntityPassivation
withIdleEntityPassivation
 method on 
ClusterShardingSettings.PassivationStrategySettings
.


If the default strategy is not appropriate for particular workloads and access patterns, a 
custom passivation strategy
 can be created with configurable replacement policies, active entity limits, and idle entity timeouts.


Custom passivation strategies


To configure a custom passivation strategy, create a configuration section for the strategy under 
akka.cluster.sharding.passivation
 and select this strategy using the 
strategy
 setting. The strategy needs a 
replacement policy
 to be chosen, an 
active entity limit
 to be set, and can optionally 
passivate idle entities
. For example, a custom strategy can be configured to use the 
least recently used policy
:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-lru-strategy
  custom-lru-strategy {
    active-entity-limit = 1000000
    replacement.policy = least-recently-used
  }
}


The active entity limit and replacement policy can also be configured using the 
withPassivationStrategy
 method on 
ClusterShardingSettings
, passing custom 
ClusterShardingSettings.PassivationStrategySettings
.


Least recently used policy


The 
least recently used
 policy passivates those entities that have the least recent activity when the number of active entities passes the specified limit.


When to use
: the least recently used policy should be used when access patterns are recency biased, where entities that were recently accessed are likely to be accessed again. See the 
segmented least recently used policy
 for a variation that also distinguishes frequency of access.


Configure a passivation strategy to use the least recently used policy:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-lru-strategy
  custom-lru-strategy {
    active-entity-limit = 1000000
    replacement.policy = least-recently-used
  }
}


Or using the 
withLeastRecentlyUsedReplacement
withLeastRecentlyUsedReplacement
 method on 
ClusterShardingSettings.PassivationStrategySettings
.


Segmented least recently used policy


A variation of the least recently used policy can be enabled that divides the active entity space into multiple segments to introduce frequency information into the passivation strategy. Higher-level segments contain entities that have been accessed more often. The first segment is for entities that have only been accessed once, the second segment for entities that have been accessed at least twice, and so on. When an entity is accessed again, it will be promoted to the most recent position of the next-level or highest-level segment. The higher-level segments are limited, where the total limit is either evenly divided among segments, or proportions of the segments can be configured. When a higher-level segment exceeds its limit, the least recently used active entity tracked in that segment will be demoted to the level below. Only the least recently used entities in the lowest level will be candidates for passivation. The higher levels are considered “protected”, where entities will have additional opportunities to be accessed before being considered for passivation.


When to use
: the segmented least recently used policy can be used for workloads where some entities are more popular than others, to prioritize those entities that are accessed more frequently.


To configure a segmented least recently used (SLRU) policy, with two levels and a protected segment limited to 80% of the total limit:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-slru-strategy
  custom-slru-strategy {
    active-entity-limit = 1000000
    replacement {
      policy = least-recently-used
      least-recently-used {
        segmented {
          levels = 2
          proportions = [0.2, 0.8]
        }
      }
    }
  }
}


Or to configure a 4-level segmented least recently used (S4LRU) policy, with 4 evenly divided levels:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-s4lru-strategy
  custom-s4lru-strategy {
    active-entity-limit = 1000000
    replacement {
      policy = least-recently-used
      least-recently-used {
        segmented.levels = 4
      }
    }
  }
}


Or using custom 
ClusterShardingSettings.PassivationStrategySettings.LeastRecentlyUsedSettings
.


Most recently used policy


The 
most recently used
 policy passivates those entities that have the most recent activity when the number of active entities passes the specified limit.


When to use
: the most recently used policy is most useful when the older an entity is, the more likely that entity will be accessed again; as seen in cyclic access patterns.


Configure a passivation strategy to use the most recently used policy:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-mru-strategy
  custom-mru-strategy {
    active-entity-limit = 1000000
    replacement.policy = most-recently-used
  }
}


Or using the 
withMostRecentlyUsedReplacement
withMostRecentlyUsedReplacement
 method on 
ClusterShardingSettings.PassivationStrategySettings
.


Least frequently used policy


The 
least frequently used
 policy passivates those entities that have the least frequent activity when the number of active entities passes the specified limit.


When to use
: the least frequently used policy should be used when access patterns are frequency biased, where some entities are much more popular than others and should be prioritized. See the 
least frequently used with dynamic aging policy
 for a variation that also handles shifts in popularity.


Configure automatic passivation to use the least frequently used policy:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-lfu-strategy
  custom-lfu-strategy {
    active-entity-limit = 1000000
    replacement.policy = least-frequently-used
  }
}


Or using the 
withLeastFrequentlyUsedReplacement
withLeastFrequentlyUsedReplacement
 method on 
ClusterShardingSettings.PassivationStrategySettings
.


Least frequently used with dynamic aging policy


A variation of the least frequently used policy can be enabled that uses “dynamic aging” to adapt to shifts in the set of popular entities, which is useful for smaller active entity limits and when shifts in popularity are common. If entities were frequently accessed in the past but then become unpopular, they can still remain active for a long time given their high frequency counts. Dynamic aging effectively increases the frequencies for recently accessed entities so they can more easily become higher priority over entities that are no longer accessed.


When to use
: the least frequently used with dynamic aging policy can be used when workloads are frequency biased (there are some entities that are much more popular), but which entities are most popular changes over time. Shifts in popularity can have more impact on a least frequently used policy if the active entity limit is small.


Configure dynamic aging with the least frequently used policy:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-lfu-with-dynamic-aging
  custom-lfu-with-dynamic-aging {
    active-entity-limit = 1000
    replacement {
      policy = least-frequently-used
      least-frequently-used {
        dynamic-aging = on
      }
    }
  }
}


Or using custom 
ClusterShardingSettings.PassivationStrategySettings.LeastFrequentlyUsedSettings
.


Composite passivation strategies


Passivation strategies can be combined using an admission window and admission filter. The admission window tracks newly activated entities. Entities are replaced in the admission window using one of the replacement policies, such as the least recently used replacement policy. When an entity is replaced in the window area it has an opportunity to enter the main entity tracking area, based on the admission filter. The admission filter determines whether an entity that has left the window area should be admitted into the main area, or otherwise be passivated. A frequency sketch is the default admission filter and estimates the access frequency of entities over the lifespan of the cluster sharding node, selecting the entity that is estimated to be accessed more frequently. Composite passivation strategies with an admission window and admission filter are implementing the 
Window-TinyLFU
 caching algorithm.


Admission window policy


The admission window tracks newly activated entities. When an entity is replaced in the window area, it has an opportunity to enter the main entity tracking area, based on the 
admission filter
. The admission window can be enabled by selecting a policy (while the regular replacement policy is for the main area):


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-strategy-with-admission-window
  custom-strategy-with-admission-window {
    active-entity-limit = 1000000
    admission.window.policy = least-recently-used
    replacement.policy = least-frequently-used
  }
}


The proportion of the active entity limit used for the admission window can be configured (the default is 1%):


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-strategy-with-admission-window
  custom-strategy-with-admission-window {
    active-entity-limit = 1000000
    admission.window {
      policy = least-recently-used
      proportion = 0.1 # 10%
    }
    replacement.policy = least-frequently-used
  }
}


The proportion for the admission window can also be adapted and optimized dynamically, by enabling an 
admission window optimizer
.


Admission window optimizer


The proportion of the active entity limit used for the admission window can be adapted dynamically using an optimizer. The window area will usually retain entities that are accessed again in a short time (recency-biased), while the main area can track entities that are accessed more frequently over longer times (frequency-biased). If access patterns for entities are changeable, then the adaptive sizing of the window allows the passivation strategy to adapt between recency-biased and frequency-biased workloads.


The optimizer currently available uses a simple hill-climbing algorithm, which searches for a window proportion that provides an optimal active rate (where entities are already active when accessed, the 
cache hit rate
). Enable adaptive window sizing by configuring the 
hill-climbing
 window optimizer:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-strategy-with-admission-window
  custom-strategy-with-admission-window {
    active-entity-limit = 1000000
    admission.window {
      policy = least-recently-used
      optimizer = hill-climbing
    }
    replacement.policy = least-frequently-used
  }
}


See the 
reference.conf
 for parameters that can be tuned for the hill climbing admission window optimizer.


Admission filter


An admission filter can be enabled, which determines whether an entity that has left the window area (or a newly activated entity if there is no admission window) should be admitted into the main entity tracking area, or otherwise be passivated. If no admission filter is configured, then entities will always be admitted into the main area.


A frequency sketch is the default admission filter and estimates the access frequency of entities over the lifespan of the cluster sharding node, selecting the entity that is estimated to be accessed more frequently. The frequency sketch automatically ages entries, using the approach from the 
TinyLFU
 cache admission algorithm. Enable an admission filter by configuring the 
frequency-sketch
 admission filter:


copy
source
akka.cluster.sharding.passivation {
  strategy = custom-strategy-with-admission
  custom-strategy-with-admission {
    active-entity-limit = 1000000
    admission {
      window {
        policy = least-recently-used
        optimizer = hill-climbing
      }
      filter = frequency-sketch
    }
    replacement {
      policy = least-recently-used
      least-recently-used {
        segmented {
          levels = 2
          proportions = [0.2, 0.8]
        }
      }
    }
  }
}


See the 
reference.conf
 for parameters that can be tuned for the frequency sketch admission filter.


Sharding State


There are two types of state managed:




ShardCoordinator State
 - the 
Shard
 locations. This is stored in the 
State Store
.


Remembering Entities
 - the active shards and the entities in each 
Shard
, which is optional, and disabled by default. This is stored in the 
Remember Entities Store
.




State Store


A state store is mandatory for sharding, it contains the location of shards. The 
ShardCoordinator
 needs to load this state after it moves between nodes.


There are two options for the state store:




Distributed Data Mode
 - uses Akka 
Distributed Data
 (CRDTs) (the default)


Persistence Mode
 - (deprecated) uses Akka 
Persistence
 (Event Sourcing)


Warning


Persistence for state store mode is deprecated. It is recommended to migrate to 
ddata
 for the coordinator state and if using replicated entities migrate to 
eventsourced
 for the replicated entities state.


The data written by the deprecated 
persistence
 state store mode for remembered entities can be read by the new remember entities 
eventsourced
 mode.


Once you’ve migrated you can not go back to 
persistence
 mode.


Distributed Data Mode


To enable distributed data store mode (the default):


akka.cluster.sharding.state-store-mode = ddata



The state of the 
ShardCoordinator
 is replicated across the cluster but is not stored to disk. 
Distributed Data
 handles the 
ShardCoordinator
’s state with 
WriteMajorityPlus
WriteMajorityPlus
 / 
ReadMajorityPlus
ReadMajorityPlus
 consistency. When all nodes in the cluster have been stopped, the state is no longer needed and dropped.


Cluster Sharding uses its own Distributed Data 
Replicator
Replicator
 per node. If using roles with sharding there is one 
Replicator
 per role, which enables a subset of all nodes for some entity types and another subset for other entity types. Each replicator has a name that contains the node role and therefore the role configuration must be the same on all nodes in the cluster, for example you can’t change the roles when performing a rolling update. Changing roles requires 
a full cluster restart
.


The 
akka.cluster.sharding.distributed-data
 config section configures the settings for Distributed Data. It’s not possible to have different 
distributed-data
 settings for different sharding entity types.


Persistence mode


To enable persistence store mode:


akka.cluster.sharding.state-store-mode = persistence



Since it is running in a cluster 
Persistence
 must be configured with a distributed journal.
Warning


Persistence mode for 
Remembering Entities
 has been replaced by a remember entities state mode. It should not be used for new projects and existing projects should migrate as soon as possible.


Remembering Entities


Remembering entities automatically restarts entities after a rebalance or entity crash. Without remembered entities restarts happen on the arrival of a message.


Enabling remembered entities disables 
Automatic Passivation
.


The state of the entities themselves is not restored unless they have been made persistent, for example with 
Event Sourcing
.


To enable remember entities set 
rememberEntities
 flag to true in 
ClusterShardingSettings
ClusterShardingSettings
 when starting a shard region (or its proxy) for a given 
entity
 type or configure 
akka.cluster.sharding.remember-entities = on
.


Starting and stopping entities has an overhead but this is limited by batching operations to the underlying remember entities store.


Behavior When Enabled


When 
rememberEntities
 is enabled, whenever a 
Shard
 is rebalanced onto another node or recovers after a crash, it will recreate all the entities which were previously running in that 
Shard
. 


To permanently stop entities send a 
ClusterSharding.Passivate
ClusterSharding.Passivate
 to the 
ActorRef
ActorRef
[
ShardCommand
]
<
ShardCommand
>
 that was passed in to the factory method when creating the entity. Otherwise, the entity will be automatically restarted after the entity restart backoff specified in the configuration.


Remember entities store


There are two options for the remember entities store:




ddata


eventsourced




Remember entities distributed data mode


Enable ddata mode with (enabled by default):


akka.cluster.sharding.remember-entities-store = ddata



To support restarting entities after a full cluster restart (non-rolling) the remember entities store is persisted to disk by distributed data. This can be disabled if not needed:


akka.cluster.sharding.distributed-data.durable.keys = []



Reasons for disabling:




No requirement for remembering entities after a full cluster shutdown


Running in an environment without access to disk between restarts e.g. Kubernetes without persistent volumes




For supporting remembered entities in an environment without disk storage use 
eventsourced
 mode instead.
Java 17


When using 
remember-entities-store=ddata
 the remember entities store is persisted to disk by LMDB. When running with Java 17 you have to add JVM flags 
--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED
.


Event sourced mode


Enable 
eventsourced
 mode with:


akka.cluster.sharding.remember-entities-store = eventsourced



This mode uses 
Event Sourcing
 to store the active shards and active entities for each shard so a persistence and snapshot plugin must be configured.


akka.cluster.sharding.journal-plugin-id = <plugin>
akka.cluster.sharding.snapshot-plugin-id = <plugin>



Migrating from deprecated persistence mode


If not using remembered entities you can migrate to ddata with a full cluster restart.


If using remembered entities there are two migration options: 




ddata
 for the state store and 
ddata
 for remembering entities. All remembered entities will be lost after a full cluster restart.


ddata
 for the state store and 
eventsourced
 for remembering entities. The new 
eventsourced
 remembering entities store  reads the data written by the old 
persistence
 mode. Your remembered entities will be remembered after a full cluster restart.




For migrating existing remembered entities an event adapter needs to be configured in the config for the journal you use in your 
application.conf
. In this example 
cassandra
 is the used journal:


akka.persistence.cassandra.journal {
  event-adapters {
    coordinator-migration = "akka.cluster.sharding.OldCoordinatorStateMigrationEventAdapter"
  }

  event-adapter-bindings {
    "akka.cluster.sharding.ShardCoordinator$Internal$DomainEvent" = coordinator-migration
  }
}



Once you have migrated you cannot go back to the old persistence store, a rolling update is therefore not possible.


When 
Distributed Data mode
 is used the identifiers of the entities are stored in 
Durable Storage
 of Distributed Data. You may want to change the configuration of the 
akka.cluster.sharding.distributed-data.durable.lmdb.dir
, since the default directory contains the remote port of the actor system. If using a dynamically assigned port (0) it will be different each time and the previously stored data will not be loaded.


The reason for storing the identifiers of the active entities in durable storage, i.e. stored to disk, is that the same entities should be started also after a complete cluster restart. If this is not needed you can disable durable storage and benefit from better performance by using the following configuration:


akka.cluster.sharding.distributed-data.durable.keys = []



Startup after minimum number of members


It’s recommended to use Cluster Sharding with the Cluster setting 
akka.cluster.min-nr-of-members
 or 
akka.cluster.role.<role-name>.min-nr-of-members
. 
min-nr-of-members
 will defer the allocation of the shards until at least that number of regions have been started and registered to the coordinator. This avoids that many shards are allocated to the first region that registers and only later are rebalanced to other nodes.


See 
How To Startup when Cluster Size Reached
 for more information about 
min-nr-of-members
.


Health check


An 
Akka Management compatible health check
 is included that returns healthy once the local shard region has registered with the coordinator. This health check should be used in cases where you don’t want to receive production traffic until the local shard region is ready to retrieve locations for shards. For shard regions that aren’t critical and therefore should not block this node becoming ready do not include them.


The health check does not fail after an initial successful check. Once a shard region is registered and is operational it stays available for incoming message. 


Cluster sharding enables the health check automatically. To disable:


akka.management.health-checks.readiness-checks {
  sharding = ""
}



Monitoring of each shard region is off by default. Add them by defining the entity type names (
EntityTypeKey.name
):


akka.cluster.sharding.healthcheck.names = ["counter-1", "HelloWorld"]



The health check is disabled (always returns success true) after a duration of failing checks after the Cluster member is up. Otherwise, it would stall a Kubernetes rolling update when adding a new entity type in the new version.


See also additional information about how to make 
smooth rolling updates
.


Inspecting cluster sharding state


Two requests to inspect the cluster state are available:


GetShardRegionState
GetShardRegionState
 which will reply with a 
ShardRegion.CurrentShardRegionState
ShardRegion.CurrentShardRegionState
 that contains the identifiers of the shards running in a Region and what entities are alive for each of them.




Scala




copy
source
import akka.cluster.sharding.typed.GetShardRegionState
import akka.cluster.sharding.ShardRegion.CurrentShardRegionState

val replyTo: ActorRef[CurrentShardRegionState] = replyMessageAdapter

ClusterSharding(system).shardState ! GetShardRegionState(Counter.TypeKey, replyTo)


Java




copy
source
import akka.cluster.sharding.typed.GetShardRegionState;
import akka.cluster.sharding.ShardRegion.CurrentShardRegionState;

ActorRef<CurrentShardRegionState> replyTo = replyMessageAdapter;

ClusterSharding.get(system).shardState().tell(new GetShardRegionState(typeKey, replyTo));




GetClusterShardingStats
GetClusterShardingStats
 which will query all the regions in the cluster and reply with a 
ShardRegion.ClusterShardingStats
ShardRegion.ClusterShardingStats
 containing the identifiers of the shards running in each region and a count of entities that are alive in each shard.




Scala




copy
source
import akka.cluster.sharding.typed.GetClusterShardingStats
import akka.cluster.sharding.ShardRegion.ClusterShardingStats
import scala.concurrent.duration._

val replyTo: ActorRef[ClusterShardingStats] = replyMessageAdapter
val timeout: FiniteDuration = 5.seconds

ClusterSharding(system).shardState ! GetClusterShardingStats(Counter.TypeKey, timeout, replyTo)


Java




copy
source
import akka.cluster.sharding.typed.GetClusterShardingStats;
import akka.cluster.sharding.ShardRegion.ClusterShardingStats;

ActorRef<ClusterShardingStats> replyTo = replyMessageAdapter;
Duration timeout = Duration.ofSeconds(5);

ClusterSharding.get(system)
    .shardState()
    .tell(new GetClusterShardingStats(typeKey, timeout, replyTo));




If any shard queries failed, for example due to timeout if a shard was too busy to reply within the configured 
akka.cluster.sharding.shard-region-query-timeout
, 
ShardRegion.CurrentShardRegionState
 and 
ShardRegion.ClusterShardingStats
 will also include the set of shard identifiers by region that failed.


The purpose of these messages is testing and monitoring, they are not provided to give access to directly sending messages to the individual entities.


Lease


A 
lease
 can be used as an additional safety measure to ensure a shard does not run on two nodes.


Reasons for how this can happen:




Network partitions without an appropriate downing provider


Mistakes in the deployment process leading to two separate Akka Clusters


Timing issues between removing members from the Cluster on one side of a network partition and shutting them down on the other side




A lease can be a final backup that means that each shard won’t create child entity actors unless it has the lease. 


To use a lease for sharding set 
akka.cluster.sharding.use-lease
 to the configuration location of the lease to use. Each shard will try and acquire a lease with with the name 
<actor system name>-shard-<type name>-<shard id>
 and the owner is set to the 
Cluster(system).selfAddress.hostPort
.


If a shard can’t acquire a lease it will remain uninitialized so messages for entities it owns will be buffered in the 
ShardRegion
. If the lease is lost after initialization the Shard will be terminated.


Removal of internal Cluster Sharding data


Removal of internal Cluster Sharding data is only relevant for “Persistent Mode”. The Cluster Sharding 
ShardCoordinator
 stores locations of the shards. This data is safely be removed when restarting the whole Akka Cluster. Note that this does not include application data.


There is a utility program 
RemoveInternalClusterShardingData
RemoveInternalClusterShardingData
 that removes this data.
Warning


Never use this program while there are running Akka Cluster nodes that are using Cluster Sharding. Stop all Cluster nodes before using this program.


It can be needed to remove the data if the Cluster Sharding coordinator cannot startup because of corrupt data, which may happen if accidentally two clusters were running at the same time, e.g. caused by an invalid downing provider when there was a network partition.


Use this program as a standalone Java main program:


java -classpath <jar files, including akka-cluster-sharding>
  akka.cluster.sharding.RemoveInternalClusterShardingData
    -2.3 entityType1 entityType2 entityType3



The program is included in the 
akka-cluster-sharding
 jar file. It is easiest to run it with same classpath and configuration as your ordinary application. It can be run from sbt or Maven in similar way.


Specify the entity type names (same as you use in the 
init
 method of 
ClusterSharding
) as program arguments.


If you specify 
-2.3
 as the first program argument it will also try to remove data that was stored by Cluster Sharding in Akka 2.3.x using different persistenceId.


Configuration


The 
ClusterSharding
ClusterSharding
 extension can be configured with the following properties. These configuration properties are read by the 
ClusterShardingSettings
ClusterShardingSettings
 when created with an ActorSystem parameter. It is also possible to amend the 
ClusterShardingSettings
 or create it from another config section with the same layout as below. 


One important configuration property is 
number-of-shards
 as described in 
Shard allocation
.


You may also need to tune the configuration properties is 
rebalance-absolute-limit
 and 
rebalance-relative-limit
 as described in 
Shard allocation
.


copy
source
# Settings for the ClusterShardingExtension
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = off

  # When 'remember-entities' is enabled and the state store mode is ddata this controls
  # how the remembered entities and shards are stored. Possible values are "eventsourced" and "ddata"
  # Default is ddata for backwards compatibility.
  remember-entities-store = "ddata"

  # Deprecated: use the `passivation.default-idle-strategy.idle-entity.timeout` setting instead.
  # Set this to a time duration to have sharding passivate entities when they have not
  # received any message in this length of time. Set to 'off' to disable.
  # It is always disabled if `remember-entities` is enabled.
  passivate-idle-entity-after = null

  # Automatic entity passivation settings.
  passivation {
    # If an entity doesn't stop itself from a passivation request it will be forcefully stopped
    # after this timeout.
    stop-timeout = 10s

    # Automatic passivation strategy to use.
    # Set to "none" or "off" to disable automatic passivation.
    # Set to "default-strategy" to switch to the recommended default strategy with an active entity limit.
    # See the strategy-defaults section for possible passivation strategy settings and default values.
    # Passivation strategies are always disabled if `remember-entities` is enabled.
    #
    # API MAY CHANGE: Configuration for passivation strategies, except default-idle-strategy,
    # may change after additional testing and feedback.
    strategy = "default-idle-strategy"

    # Default passivation strategy without active entity limit; time out idle entities after 2 minutes.
    default-idle-strategy {
      idle-entity.timeout = 120s
    }

    # Recommended default strategy for automatic passivation with an active entity limit.
    # Configured with an adaptive recency-based admission window, a frequency-based admission filter, and
    # a segmented least recently used (SLRU) replacement policy for the main active entity tracking.
    default-strategy {
      # Default limit of 100k active entities in a shard region (in a cluster node).
      active-entity-limit = 100000

      # Admission window with LRU policy and adaptive sizing, and a frequency sketch admission filter to the main area.
      admission {
        window {
          policy = least-recently-used
          optimizer = hill-climbing
        }
        filter = frequency-sketch
      }

      # Main area with segmented LRU replacement policy with an 80% "protected" level by default.
      replacement {
        policy = least-recently-used
        least-recently-used {
          segmented {
            levels = 2
            proportions = [0.2, 0.8]
          }
        }
      }
    }

    strategy-defaults {
      # Passivate entities when they have not received a message for a specified length of time.
      idle-entity {
        # Passivate idle entities after the timeout. Set to "none" or "off" to disable.
        timeout = none

        # Check idle entities every interval. Set to "default" to use half the timeout by default.
        interval = default
      }

      # Limit of active entities in a shard region.
      # Passivate entities when the number of active entities in a shard region reaches this limit.
      # The per-region limit is divided evenly among the active shards in a region.
      # Set to "none" or "off" to disable limit-based automatic passivation, to only use idle entity timeouts.
      active-entity-limit = none

      # Entity replacement settings, for when the active entity limit is reached.
      replacement {
        # Entity replacement policy to use when the active entity limit is reached. Possible values are:
        #   - "least-recently-used"
        #   - "most-recently-used"
        #   - "least-frequently-used"
        # Set to "none" or "off" to disable the replacement policy and ignore the active entity limit.
        policy = none

        # Least recently used entity replacement policy.
        least-recently-used {
          # Optionally use a "segmented" least recently used strategy.
          # Disabled when segmented.levels are set to "none" or "off".
          segmented {
            # Number of segmented levels.
            levels = none

            # Fractional proportions for the segmented levels.
            # If empty then segments are divided evenly by the number of levels.
            proportions = []
          }
        }

        # Most recently used entity replacement policy.
        most-recently-used {}

        # Least frequently used entity replacement policy.
        least-frequently-used {
          # New frequency counts will be "dynamically aged" when enabled.
          dynamic-aging = off
        }
      }

      # An optional admission area, with a window for newly and recently activated entities, and an admission filter
      # to determine whether a candidate should be admitted to the main area of the passivation strategy.
      admission {
        # An optional window area, where newly created entities will be admitted initially, and when evicted
        # from the window area have an opportunity to move to the main area based on the admission filter.
        window {
          # The initial sizing for the window area (if enabled), as a fraction of the total active entity limit.
          proportion = 0.01

          # The minimum adaptive sizing for the window area, as a fraction of the total active entity limit.
          # Only applies when an adaptive window optimizer is enabled.
          minimum-proportion = 0.01

          # The maximum adaptive sizing for the window area, as a fraction of the total active entity limit.
          # Only applies when an adaptive window optimizer is enabled.
          maximum-proportion = 1.0

          # Adaptive optimizer to use for dynamically resizing the window area. Possible values are:
          #   - "hill-climbing"
          # Set to "none" or "off" to disable adaptive sizing of the window area.
          optimizer = off

          # A window proportion optimizer using a simple hill-climbing algorithm.
          hill-climbing {
            # Multiplier of the active entity limit for how often (in accesses) to adjust the window proportion.
            adjust-multiplier = 10.0

            # The size of the initial step to take (also used when the climbing restarts).
            initial-step = 0.0625

            # A threshold for the change in active rate (hit rate) to restart climbing.
            restart-threshold = 0.05

            # The decay ratio applied on each climbing step.
            step-decay = 0.98
          }

          # Replacement policy to use for the window area.
          # Entities that are evicted from the window area may move to the main area, based on the admission filter.
          # Possible values are the same as for the main replacement policy.
          # Set to "none" or "off" to disable the window area.
          policy = none

          least-recently-used {
            segmented {
              levels = none
              proportions = []
            }
          }

          most-recently-used {}

          least-frequently-used {
            dynamic-aging = off
          }
        }

        # The admission filter for the main area of the passivation strategy. Possible values are:
        #   - "frequency-sketch"
        # Set to "none" or "off" to disable the admission filter and always admit to the main area.
        filter = none

        # An admission filter based on a frequency sketch (a variation of a count-min sketch).
        frequency-sketch {
          # The depth of the frequency sketch (the number of hash functions).
          depth = 4

          # The size of the frequency counters in bits: 2, 4, 8, 16, 32, or 64 bits.
          counter-bits = 4

          # Multiplier of the active entity limit for the width of the frequency sketch.
          width-multiplier = 4

          # Multiplier of the active entity limit for how often the reset operation of the frequency sketch is applied.
          reset-multiplier = 10.0
        }
      }
    }
  }

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  # Additionally, if an entity doesn't handle the stopMessage
  # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully
  #
  # Note that, by default, if running in Kubernetes, the default time from SIGTERM
  # (which triggers a rebalance) and SIGKILL (forcible stop of the process) is 30 seconds.
  # This delay is set by `terminationGracePeriodSeconds` in Kubernetes.
  #
  # In the shutdown case, coordinated shutdown (see `akka.coordinated-shutdown`) may
  # also stop the JVM before this timeout is hit.
  handoff-timeout = 60 s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10 s

  # If the shard is remembering entities and can't store state changes, it
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10 s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate, the entity will be restarted after this duration or when
  # the next message for it is received, whichever occurs first.
  entity-restart-backoff = 10 s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined,
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  journal-plugin-id = ""

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined,
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  snapshot-plugin-id = ""

  # Defines how the coordinator stores its state. Same is also used by the
  # shards for rememberEntities.
  # Valid values are "ddata" or "persistence".
  # "persistence" mode is deprecated
  state-store-mode = "ddata"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times. A snapshot trigger might be delayed if a batch of updates is processed.
  # Only used when state-store-mode=persistence
  snapshot-after = 1000

  # The shard deletes persistent events (messages and snapshots) after doing snapshot
  # keeping this number of old persistent batches.
  # Batch is of size `snapshot-after`.
  # When set to 0, after snapshot is successfully done, all events with equal or lower sequence number will be deleted.
  # Default value of 2 leaves last maximum 2*`snapshot-after` events and 3 snapshots (2 old ones + latest snapshot).
  # If larger than 0, one additional batch of journal messages is kept when state-store-mode=persistence to include messages from delayed snapshots.
  keep-nr-of-batches = 2

  # Settings for LeastShardAllocationStrategy.
  #
  # A new rebalance algorithm was included in Akka 2.6.10. It can reach optimal balance in
  # less rebalance rounds (typically 1 or 2 rounds). The amount of shards to rebalance in each
  # round can still be limited to make it progress slower. For backwards compatibility,
  # the new algorithm is not enabled by default. Enable the new algorithm by setting
  # `rebalance-absolute-limit` > 0, for example:
  # akka.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit=20
  # The new algorithm is recommended and will become the default in future versions of Akka.
  least-shard-allocation-strategy {
    # Maximum number of shards that will be rebalanced in one rebalance round.
    # The lower of this and `rebalance-relative-limit` will be used.
    rebalance-absolute-limit = 0

    # Maximum number of shards that will be rebalanced in one rebalance round.
    # Fraction of total number of (known) shards.
    # The lower of this and `rebalance-absolute-limit` will be used.
    rebalance-relative-limit = 0.1

    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
    # when rebalance-absolute-limit > 0.
    #
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    # The difference between number of shards in the region with most shards and
    # the region with least shards must be greater than (>) the `rebalanceThreshold`
    # for the rebalance to occur.
    # It is also the maximum number of shards that will start rebalancing per rebalance-interval
    # 1 gives the best distribution and therefore typically the best choice.
    # Increasing the threshold can result in quicker rebalance but has the
    # drawback of increased difference between number of shards (and therefore load)
    # on different nodes before rebalance will occur.
    rebalance-threshold = 1

    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used
    # when rebalance-absolute-limit > 0.
    #
    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  external-shard-allocation-strategy {
    # How long to wait for the client to persist an allocation to ddata or get all shard locations
    client-timeout = 5s
  }

  # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)
  # and for a shard to get its state when remembered entities is enabled
  # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond
  # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening
  waiting-for-state-timeout = 2 s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # Also used as timeout for writes of remember entities when that is enabled
  updating-state-timeout = 5 s

  # Timeout to wait for querying all shards for a given `ShardRegion`.
  shard-region-query-timeout = 3 s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting and is applied per remembered shard starting up (not for
  # entire shard region). The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy.
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    # The frequency is per sharding region (entity type).
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  event-sourced-remember-entities-store {
    # When using remember entities and the event sourced remember entities store the batches
    # written to the store are limited by this number to avoid getting a too large event for
    # the journal to handle. If using long persistence ids you may have to increase this.
    max-updates-per-write = 100
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role" if
  # "akka.cluster.sharding.coordinator-singleton-role-override" is enabled. Disabling it will allow to
  # use separate nodes for the shard coordinator and the shards themselves.
  # A lease can be configured in these settings for the coordinator singleton
  coordinator-singleton = ${akka.cluster.singleton}


  # By default, the role for the coordinator singleton is the same as the role for the shards
  # "akka.cluster.sharding.role". Set this to off to use the role from
  # "akka.cluster.sharding.coordinator-singleton.role" for the coordinator singleton.
  coordinator-singleton-role-override = on

  coordinator-state {
    # State updates are required to be written to a majority of nodes plus this
    # number of additional nodes. Can also be set to "all" to require
    # writes to all nodes. The reason for write/read to more than majority
    # is to have more tolerance for membership changes between write and read.
    # The tradeoff of increasing this is that updates will be slower.
    # It is more important to increase the `read-majority-plus`.
    write-majority-plus = 3
    # State retrieval when ShardCoordinator is started is required to be read
    # from a majority of nodes plus this number of additional nodes. Can also
    # be set to "all" to require reads from all nodes. The reason for write/read
    # to more than majority is to have more tolerance for membership changes between
    # write and read.
    # The tradeoff of increasing this is that coordinator startup will be slower.
    read-majority-plus = 5
  }
  
  # Settings for the Distributed Data replicator. 
  # Same layout as akka.cluster.distributed-data.
  # The "role" of the distributed-data configuration is not used. The distributed-data
  # role will be the same as "akka.cluster.sharding.role".
  # Note that there is one Replicator per role and it's not possible
  # to have different distributed-data settings for different sharding entity types.
  # Only used when state-store-mode=ddata
  distributed-data = ${akka.cluster.distributed-data}
  distributed-data {
    # minCap parameter to MajorityWrite and MajorityRead consistency level.
    majority-min-cap = 5
    durable.keys = ["shard-*"]
    
    # When using many entities with "remember entities" the Gossip message
    # can become too large if including too many in same message. Limit to
    # the same number as the number of ORSet per shard.
    max-delta-elements = 5

    # ShardCoordinator is singleton running on oldest
    prefer-oldest = on
  }

  # The id of the dispatcher to use for ClusterSharding actors.
  # If specified, you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = "akka.actor.internal-dispatcher"

  # Config path of the lease that each shard must acquire before starting entity actors
  # default is no lease
  # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
  use-lease = ""

  # The interval between retries for acquiring the lease
  lease-retry-interval = 5s

  # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling
  # in production systems.
  verbose-debug-logging = off

  # Throw an exception if the internal state machine in the Shard actor does an invalid state transition.
  # Mostly for the Akka test suite. If off, the invalid transition is logged as a warning instead of throwing and
  # crashing the shard.
  fail-on-invalid-entity-state-transition = off

  # Healthcheck that can be used with Akka management health checks: https://doc.akka.io/libraries/akka-management/current/healthchecks.html
  healthcheck {
    # sharding names to check have registered with the coordinator for the health check to pass
    # once initial registration has taken place the health check always returns true to prevent the coordinator
    # moving making the health check of all nodes fail
    # by default no sharding instances are monitored
    names = []

    # Timeout for the local shard region to respond. This should be lower than your monitoring system's
    # timeout for health checks
    timeout = 5s

    # The health check is only performed during this duration after
    # the member is up. After that the sharding check will not be performed (always returns success).
    # The purpose is to wait for Cluster Sharding registration to complete on initial startup.
    # After that, in case of Sharding Coordinator movement or reachability we still want to be ready
    # because requests can typically be served without involving the coordinator.
    # Another reason is that when a new entity type is added in a rolling update we don't want to fail
    # the ready check forever, which would stall the rolling update. Sharding Coordinator is expected
    # run on the oldest member, but in this scenario that is in the old deployment hasn't started the
    # coordinator for that entity type.
    disabled-after = 10s
  }
}


copy
source
akka.cluster.sharding {
  # Number of shards used by the default HashCodeMessageExtractor
  # when no other message extractor is defined. This value must be
  # the same for all nodes in the cluster and that is verified by
  # configuration check when joining. Changing the value requires
  # stopping all nodes in the cluster.
  number-of-shards = 1000
}


Example project


Sharding example project
 
Sharding example project
 is an example project that can be downloaded, and with instructions of how to run.


This project contains a KillrWeather sample illustrating how to use Cluster Sharding.














 
Cluster Singleton






Cluster Sharding concepts 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/2017-02-10-java-serialization.html
Java Serialization, Fixed in Akka 2.4.17 • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17




Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements




Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17




Date


Description of Vulnerability


Severity


Affected Versions


Fixed Versions


Acknowledgements




Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Java Serialization, Fixed in Akka 2.4.17


Date


10 February 2017


Description of Vulnerability


An attacker that can connect to an 
ActorSystem
 exposed via Akka Remote over TCP can gain remote code execution capabilities in the context of the JVM process that runs the ActorSystem if:




JavaSerializer
 is enabled (default in Akka 2.4.x)


and TLS is disabled 
or
 TLS is enabled with 
akka.remote.netty.ssl.security.require-mutual-authentication = false
 (which is still the default in Akka 2.4.x)


or if TLS is enabled with mutual authentication and the authentication keys of a host that is allowed to connect have been compromised, an attacker gained access to a valid certificate (e.g. by compromising a node with certificates issued by the same internal PKI tree to get access of the certificate)


regardless of whether 
untrusted
 mode is enabled or not




Java deserialization is 
known to be vulnerable
 to attacks when attacker can provide arbitrary types.


Akka Remoting uses Java serializer as default configuration which makes it vulnerable in its default form. The documentation of how to disable Java serializer was not complete. The documentation of how to enable mutual authentication was missing (only described in reference.conf).


To protect against such attacks the system should be updated to Akka 
2.4.17
 or later and be configured with 
disabled Java serializer
. Additional protection can be achieved when running in an untrusted network by enabling 
TLS with mutual authentication
.


Please subscribe to the 
akka-security
 mailing list to be notified promptly about future security issues.


Severity


The 
CVSS
 score of this vulnerability is 6.8 (Medium), based on vector [AV:A/AC:M/Au:N/C:C/I:C/A:C/E:F/RL:TF/RC:C](
https://nvd.nist.gov/vuln-metrics/cvss/v2-calculator?calculator&version=2&vector=%5C(AV:A/AC:M/Au:N/C:C/I:C/A:C/E:F/RL:TF/RC:C%5C))
.


Rationale for the score:




AV:A - Best practice is that Akka remoting nodes should only be accessible from the adjacent network, so in good setups, this will be adjacent.


AC:M - Any one in the adjacent network can launch the attack with non-special access privileges.


C:C, I:C, A:C - Remote Code Execution vulnerabilities are by definition CIA:C.




Affected Versions




Akka 
2.4.16
 and prior


Akka 
2.5-M1
 (milestone not intended for production)




Fixed Versions


We have prepared patches for the affected versions, and have released the following versions which resolve the issue: 




Akka 
2.4.17
 (Scala 2.11, 2.12)




Binary and source compatibility has been maintained for the patched releases so the upgrade procedure is as simple as changing the library dependency.


It will also be fixed in 2.5-M2 or 2.5.0-RC1.


Acknowledgements


We would like to thank Alvaro Munoz at Hewlett Packard Enterprise Security & Adrian Bravo at Workday for their thorough investigation and bringing this issue to our attention.














 
Security Announcements






Camel Dependency, Fixed in Akka 2.5.4 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/index.html
Project Information • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Project Information






Binary Compatibility Rules




Binary compatibility rules explained


Change in versioning scheme, stronger compatibility since 2.4


Mixed versioning is not allowed


The meaning of “may change”


API stability annotations and comments


Binary Compatibility Checking Toolchain


Serialization compatibility across Scala versions




Downstream upgrade strategy




Patch versions


Minor versions




Modules marked “May Change”


IDE Tips




Configure the auto-importer in IntelliJ / Eclipse




Immutability using Lombok




Adding Lombok to your project


Using lombok




Migration Guides




Migration Guide 2.9.x to 2.10.x


Migration Guide 2.8.x to 2.9.x


Migration Guide 2.7.x to 2.8.x


Migration Guide 2.6.x to 2.7.x


Migration Guide 2.5.x to 2.6.x


Older Migration Guides




Rolling Updates and Versions




Akka upgrades


Change log




Issue Tracking




Browsing


Creating tickets


Submitting Pull Requests




Licenses




Akka License


Documentation and test sources license


Akka Committer License Agreement


Licenses for Dependency Libraries




Frequently Asked Questions




Akka Project


Resources with Explicit Lifecycle


Actors


Cluster


Debugging


Other questions?




Books and Videos




Books


Videos




Example projects




Quickstart


FSM


Cluster


Distributed Data


Cluster Sharding


Persistence and CQRS


Replicated Event Sourcing


Kafka to Cluster Sharding




Project




Commercial Support


Sponsors


Akka Discuss Forums


Source Code


Releases Repository


Snapshots Repository




















 
Building Native Images






Binary Compatibility Rules 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/alpakka/current/
Alpakka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Alpakka Documentation





Version 9.0.1





Java
Scala










Overview


Data Transformations


AMQP


Apache Camel


Apache Cassandra


Apache Geode


Apache Kafka


Apache Kudu


Apache Solr


Avro Parquet


AWS EventBridge


AWS DynamoDB


AWS Kinesis and Firehose


AWS Lambda


AWS S3


AWS SNS


AWS SQS


Azure Storage


Azure Storage Queue


Couchbase


Elasticsearch


Eventuate


File


FS2


FTP


Google Common


Google Cloud BigQuery


Google Cloud BigQuery Storage


Google Cloud Pub/Sub


Google Cloud Pub/Sub gRPC


Google Cloud Storage


Google FCM


gRPC


Hadoop Distributed File System - HDFS


HBase


Huawei Push Kit


HTTP


IBM Bluemix Cloud Object Storage


InfluxDB


IronMQ


Jakarta Messaging (JMS)


JMS


MongoDB


MQTT


MQTT Streaming


Opensearch


Pulsar


Pravega


Server-sent Events (SSE)


Slick (JDBC)


Spring Web


TCP


UDP


Unix Domain Socket


















Alpakka Documentation





Version 9.0.1





Java
Scala












Overview


Data Transformations


AMQP


Apache Camel


Apache Cassandra


Apache Geode


Apache Kafka


Apache Kudu


Apache Solr


Avro Parquet


AWS EventBridge


AWS DynamoDB


AWS Kinesis and Firehose


AWS Lambda


AWS S3


AWS SNS


AWS SQS


Azure Storage


Azure Storage Queue


Couchbase


Elasticsearch


Eventuate


File


FS2


FTP


Google Common


Google Cloud BigQuery


Google Cloud BigQuery Storage


Google Cloud Pub/Sub


Google Cloud Pub/Sub gRPC


Google Cloud Storage


Google FCM


gRPC


Hadoop Distributed File System - HDFS


HBase


Huawei Push Kit


HTTP


IBM Bluemix Cloud Object Storage


InfluxDB


IronMQ


Jakarta Messaging (JMS)


JMS


MongoDB


MQTT


MQTT Streaming


Opensearch


Pulsar


Pravega


Server-sent Events (SSE)


Slick (JDBC)


Spring Web


TCP


UDP


Unix Domain Socket




















Alpakka Documentation


The 
Alpakka project
 is an initiative to implement stream-aware and reactive integration pipelines for Java and Scala. It is built on top of 
Akka Streams
, and has been designed from the ground up to understand streaming natively and provide a DSL for reactive and stream-oriented programming, with built-in support for backpressure. Akka Streams is a 
Reactive Streams
 and JDK 9+ 
java.util.concurrent.Flow
-compliant implementation and therefore 
fully interoperable
 with other implementations.






Overview




Versions


Contributing


External Components


Self-contained examples


Other documentation


Integration Patterns


Release Notes




Data Transformations




Parsing Lines


JSON


Compressing/decompressing


Comma-Separated Values - CSV


RecordIO Framing


Text and charsets


Extensible Markup Language - XML




AMQP




Artifacts


Connecting to server


Sending messages


Receiving messages


Using Pub/Sub


Using rabbitmq as an RPC mechanism


Acknowledging messages downstream




Apache Camel




External library




Apache Cassandra




Artifacts


Sessions


Reading from Cassandra


Writing to Cassandra


Custom Session creation




Apache Geode




Artifacts


Setup


Writing to Geode


Reading from Geode


Geode basic commands




Apache Kafka




Separate repository




Apache Kudu




Artifacts


Configuration


Writing to Kudu in a Flow


Writing to Kudu with a Sink




Apache Solr




Artifacts


Set up a Solr client


Reading from Solr


Writing to Solr


Update documents


Delete documents by ids


Delete documents by query




Avro Parquet




Artifacts


Source Initiation


Sink Initiation


Flow Initiation


Running the example code




AWS EventBridge




Artifacts


Setup


Publish messages to AWS EventBridge Event Bus


Integration testing


Shared AWS client configuration




AWS DynamoDB




Artifacts


Setup


Sending requests and receiving responses


Error Retries and Exponential Backoff




AWS Kinesis and Firehose




Artifacts


Kinesis Data Streams


AWS KCL Scheduler Source & checkpointer




AWS Lambda




Artifacts


Setup


Sending messages


AwsLambdaFlow configuration




AWS S3




Artifacts


Configuration


Store a file in S3


Download a file from S3


Access object metadata without downloading object from S3


List bucket contents


List bucket contents and common prefixes


Copy upload (multi part)


Apply S3 settings to a part of the stream


Bucket management


Running the example code




AWS SNS




Artifacts


Setup


Publish messages to an SNS topic




AWS SQS




Artifacts


Setup


Read from an SQS queue


Publish messages to an SQS queue


Updating message statuses


Integration testing




Azure Storage




Artifacts


Configuration


Building request


Supported operations on Blob service


Supported operations on File service




Azure Storage Queue




Artifacts


Init Azure Storage API


Queuing a message


Processing and deleting messages




Couchbase




Artifacts


Overview


Reading from Couchbase in Akka Streams


Writing to Couchbase in Akka Streams


Using 
CouchbaseSession
 directly




Elasticsearch




Artifacts


Elasticsearch parameters


Elasticsearch as Source and Sink


Elasticsearch as Flow




Eventuate




External library




File




Artifacts


Writing to and reading from files


Tailing a file into a stream


Creating directories


Listing directory contents


Listening to changes in a directory


Rotating the file to stream into


ZIP Archive


TAR Archive




FS2




External library




FTP




Artifacts


Configuring the connection settings


Traversing a remote FTP folder recursively


Retrieving files


Writing files


Removing files


Moving files


Creating directory




Google Common




Artifacts


Configuration


Credentials


Accessing settings


Apply custom settings to a part of the stream


Interop with Google Java client libraries


Accessing other Google APIs




Google Cloud BigQuery




Artifacts


Configuration


Imports


Setup data classes


Run a query


Load data into BigQuery


Managing datasets and tables


Apply custom settings to a part of the stream


Make raw API requests




Google Cloud BigQuery Storage




Artifacts


Build setup


Configuration


Reading


Running the test code




Google Cloud Pub/Sub




Artifacts


Usage


Running the examples




Google Cloud Pub/Sub gRPC




Artifacts


Binary compatibility


Build setup


Configuration


Publishing


Subscribing


Running the test code




Google Cloud Storage




Artifacts


Configuration


Store a file in Google Cloud Storage


Download a file from Google Cloud Storage


Access object metadata without downloading object from Google Cloud Storage


List bucket contents


Rewrite (multi part)


Apply Google Cloud Storage settings to a part of the stream


Bucket management


Running the example code




Google FCM




Artifacts


Settings


Sending notifications


Scala only




gRPC




Akka gRPC




Hadoop Distributed File System - HDFS




Artifacts


Specifying a Hadoop Version


Set up client


Writing


Configuration


Reading


Running the example code




HBase




Artifacts


Converters


Settings


Source


Flow


Sink


HBase administration commands




Huawei Push Kit




Artifacts


Settings


Sending notifications


Scala only




HTTP




Akka HTTP




IBM Bluemix Cloud Object Storage




Connection limitations




InfluxDB




Influxdata, the makers of InfluxDB now offer an Akka Streams-aware client library in 
https://github.com/influxdata/influxdb-client-java/tree/master/client-scala


Artifacts


Set up InfluxDB client


InfluxDB as Source and Sink




IronMQ




Artifacts


Consumer


Producer




Jakarta Messaging (JMS)




Artifacts


Producer


Consumer


Browse




JMS




Artifacts


Producer


Consumer


Browse


Using IBM MQ




MongoDB




Artifacts


Initialization


Source


Flow and Sink




MQTT




Artifacts


Settings


Reading from MQTT


Publishing to MQTT


Publish and subscribe in a single flow


Using flow with Acknowledge on message sent


Capturing MQTT client logging


Running the example code




MQTT Streaming




Artifacts


Flow through a client session


Flow through a server session




Opensearch




Artifacts


Opensearch parameters


Opensearch as Source and Sink


Opensearch as Flow




Pulsar




External library




Pravega




Artifacts


Concepts


Configuration




Server-sent Events (SSE)




Artifacts


Usage




Slick (JDBC)




Artifacts


Initialization


Starting a Database Session


Closing a Database Session


Using a Slick Source


Using a Slick Flow or Sink


Flow


Flow with pass-through




Spring Web




Artifacts


Usage


Shameless plug: Akka HTTP




TCP




Akka TCP




UDP




Artifacts


Sending


Receiving


Running the example code




Unix Domain Socket




Artifacts


Binding to a file


Connecting to a file
























Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Alpakka is available under the 
Business Source License 1.1
.



© 2011-2024 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/fault-tolerance.html
Fault Tolerance • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance




Supervision


Child actors are stopped when parent is restarting


The PreRestart signal


Bubble failures up through the hierarchy




Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance




Supervision


Child actors are stopped when parent is restarting


The PreRestart signal


Bubble failures up through the hierarchy




Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Fault Tolerance


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Fault Tolerance
.


When an actor throws an unexpected exception, a failure, while processing a message or during initialization, the actor will by default be stopped.
Note


An important difference between 
Typed actors
 and 
Classic actors
 is that by default: the former are stopped if an exception is thrown and no supervision strategy is defined while in Classic they are restarted.


Note that there is an important distinction between failures and validation errors:


A 
validation error
 means that the data of a command sent to an actor is not valid, this should rather be modelled as a part of the actor protocol than make the actor throw exceptions.


A 
failure
 is instead something unexpected or outside the control of the actor itself, for example a database connection that broke. Opposite to validation errors, it is seldom useful to model failures as part of the protocol as a sending actor can very seldomly do anything useful about it.


For failures it is useful to apply the “let it crash” philosophy: instead of mixing fine grained recovery and correction of internal state that may have become partially invalid because of the failure with the business logic we move that responsibility somewhere else. For many cases the resolution can then be to “crash” the actor, and start a new one, with a fresh state that we know is valid.


Supervision


In Akka this “somewhere else” is called supervision. Supervision allows you to declaratively describe what should happen when certain types of exceptions are thrown inside an actor. 


The default 
supervision
 strategy is to stop the actor if an exception is thrown. In many cases you will want to further customize this behavior. To use supervision the actual Actor behavior is wrapped using 
Behaviors.supervise
Behaviors.supervise
. Typically you would wrap the actor with supervision in the parent when spawning it as a child.


This example restarts the actor when it fails with an 
IllegalStateException
: 




Scala




copy
source
Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart)


Java




copy
source
Behaviors.supervise(behavior)
    .onFailure(IllegalStateException.class, SupervisorStrategy.restart());




Or to resume, ignore the failure and process the next message, instead:




Scala




copy
source
Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.resume)


Java




copy
source
Behaviors.supervise(behavior)
    .onFailure(IllegalStateException.class, SupervisorStrategy.resume());




More complicated restart strategies can be used e.g. to restart no more than 10 times in a 10 second period:




Scala




copy
source
Behaviors
  .supervise(behavior)
  .onFailure[IllegalStateException](
    SupervisorStrategy.restart.withLimit(maxNrOfRetries = 10, withinTimeRange = 10.seconds))


Java




copy
source
Behaviors.supervise(behavior)
    .onFailure(
        IllegalStateException.class,
        SupervisorStrategy.restart().withLimit(10, Duration.ofSeconds(10)));




To handle different exceptions with different strategies calls to 
supervise
supervise
 can be nested:




Scala




copy
source
Behaviors
  .supervise(Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart))
  .onFailure[IllegalArgumentException](SupervisorStrategy.stop)


Java




copy
source
Behaviors.supervise(
        Behaviors.supervise(behavior)
            .onFailure(IllegalStateException.class, SupervisorStrategy.restart()))
    .onFailure(IllegalArgumentException.class, SupervisorStrategy.stop());




For a full list of strategies see the public methods on 
SupervisorStrategy
SupervisorStrategy
.
Note


When the behavior is restarted the original 
Behavior
Behavior
 that was given to 
Behaviors.supervise
Behaviors.supervise
 is re-installed, which means that if it contains mutable state it must be a factory via 
Behaviors.setup
Behaviors.setup
. When using the object-oriented style with a class extending 
AbstractBehavior
AbstractBehavior
 it’s always recommended to create it via 
Behaviors.setup
Behaviors.setup
 as described in 
Behavior factory method
. For the function style there is typically no need for the factory if the state is captured in immutable parameters.


Wrapping behaviors


With the 
functional style
 it is very common to store state by changing behavior e.g.




Scala




copy
source
object Counter {
  sealed trait Command
  case class Increment(nr: Int) extends Command
  case class GetCount(replyTo: ActorRef[Int]) extends Command

  def apply(): Behavior[Command] =
    Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart)

  private def counter(count: Int): Behavior[Command] =
    Behaviors.receiveMessage[Command] {
      case Increment(nr: Int) =>
        counter(count + nr)
      case GetCount(replyTo) =>
        replyTo ! count
        Behaviors.same
    }
}


Java




copy
source
public static class Counter {
  public interface Command {}

  public static final class Increase implements Command {}

  public static final class Get implements Command {
    public final ActorRef<Got> replyTo;

    public Get(ActorRef<Got> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static final class Got {
    public final int n;

    public Got(int n) {
      this.n = n;
    }
  }

  public static Behavior<Command> create() {
    return Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart());
  }

  private static Behavior<Command> counter(int currentValue) {
    return Behaviors.receive(Command.class)
        .onMessage(Increase.class, o -> onIncrease(currentValue))
        .onMessage(Get.class, command -> onGet(currentValue, command))
        .build();
  }

  private static Behavior<Command> onIncrease(int currentValue) {
    return counter(currentValue + 1);
  }

  private static Behavior<Command> onGet(int currentValue, Get command) {
    command.replyTo.tell(new Got(currentValue));
    return Behaviors.same();
  }
}




When doing this supervision only needs to be added to the top level:




Scala




copy
source
def apply(): Behavior[Command] =
  Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart)


Java




copy
source
public static Behavior<Command> create() {
  return Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart());
}




Each returned behavior will be re-wrapped automatically with the supervisor.


Child actors are stopped when parent is restarting


Child actors are often started in a 
setup
setup
 block that is run again when the parent actor is restarted. The child actors are stopped to avoid resource leaks of creating new child actors each time the parent is restarted.




Scala




copy
source
def child(size: Long): Behavior[String] =
  Behaviors.receiveMessage(msg => child(size + msg.length))

def parent: Behavior[String] = {
  Behaviors
    .supervise[String] {
      Behaviors.setup { ctx =>
        val child1 = ctx.spawn(child(0), "child1")
        val child2 = ctx.spawn(child(0), "child2")

        Behaviors.receiveMessage[String] { msg =>
          // message handling that might throw an exception
          val parts = msg.split(" ")
          child1 ! parts(0)
          child2 ! parts(1)
          Behaviors.same
        }
      }
    }
    .onFailure(SupervisorStrategy.restart)
}


Java




copy
source
static Behavior<String> child(long size) {
  return Behaviors.receiveMessage(msg -> child(size + msg.length()));
}

static Behavior<String> parent() {
  return Behaviors.<String>supervise(
          Behaviors.setup(
              ctx -> {
                final ActorRef<String> child1 = ctx.spawn(child(0), "child1");
                final ActorRef<String> child2 = ctx.spawn(child(0), "child2");

                return Behaviors.receiveMessage(
                    msg -> {
                      // message handling that might throw an exception
                      String[] parts = msg.split(" ");
                      child1.tell(parts[0]);
                      child2.tell(parts[1]);
                      return Behaviors.same();
                    });
              }))
      .onFailure(SupervisorStrategy.restart());
}




It is possible to override this so that child actors are not influenced when the parent actor is restarted. The restarted parent instance will then have the same children as before the failure.


If child actors are created from 
setup
setup
 like in the previous example and they should remain intact (not stopped) when parent is restarted the 
supervise
supervise
 should be placed inside the 
setup
setup
 and using 
SupervisorStrategy.restart.withStopChildren(false)
SupervisorStrategy.restart().withStopChildren(false)
 like this:




Scala




copy
source
def parent2: Behavior[String] = {
  Behaviors.setup { ctx =>
    val child1 = ctx.spawn(child(0), "child1")
    val child2 = ctx.spawn(child(0), "child2")

    // supervision strategy inside the setup to not recreate children on restart
    Behaviors
      .supervise {
        Behaviors.receiveMessage[String] { msg =>
          // message handling that might throw an exception
          val parts = msg.split(" ")
          child1 ! parts(0)
          child2 ! parts(1)
          Behaviors.same
        }
      }
      .onFailure(SupervisorStrategy.restart.withStopChildren(false))
  }
}


Java




copy
source
static Behavior<String> parent2() {
  return Behaviors.setup(
      ctx -> {
        final ActorRef<String> child1 = ctx.spawn(child(0), "child1");
        final ActorRef<String> child2 = ctx.spawn(child(0), "child2");

        // supervision strategy inside the setup to not recreate children on restart
        return Behaviors.<String>supervise(
                Behaviors.receiveMessage(
                    msg -> {
                      // message handling that might throw an exception
                      String[] parts = msg.split(" ");
                      child1.tell(parts[0]);
                      child2.tell(parts[1]);
                      return Behaviors.same();
                    }))
            .onFailure(SupervisorStrategy.restart().withStopChildren(false));
      });
}




That means that the 
setup
setup
 block will only be run when the parent actor is first started, and not when it is restarted.


The PreRestart signal


Before a supervised actor is restarted it is sent the 
PreRestart
PreRestart
 signal giving it a chance to clean up resources it has created, much like the 
PostStop
PostStop
 signal when the 
actor stops
. The returned behavior from the 
PreRestart
PreRestart
 signal is ignored.




Scala




copy
source
def withPreRestart: Behavior[String] = {
  Behaviors
    .supervise[String] {
      Behaviors.setup { ctx =>
        val resource = claimResource()

        Behaviors
          .receiveMessage[String] { msg =>
            // message handling that might throw an exception

            val parts = msg.split(" ")
            resource.process(parts)
            Behaviors.same
          }
          .receiveSignal {
            case (_, signal) if signal == PreRestart || signal == PostStop =>
              resource.close()
              Behaviors.same
          }
      }
    }
    .onFailure[Exception](SupervisorStrategy.restart)
}



Java




copy
source
Behaviors.supervise(
        Behaviors.<String>setup(
            ctx -> {
              final Resource resource = claimResource();

              return Behaviors.receive(String.class)
                  .onMessage(
                      String.class,
                      msg -> {
                        // message handling that might throw an exception
                        String[] parts = msg.split(" ");
                        resource.process(parts);
                        return Behaviors.same();
                      })
                  .onSignal(
                      PreRestart.class,
                      signal -> {
                        resource.close();
                        return Behaviors.same();
                      })
                  .onSignal(
                      PostStop.class,
                      signal -> {
                        resource.close();
                        return Behaviors.same();
                      })
                  .build();
            }))
    .onFailure(Exception.class, SupervisorStrategy.restart());




Note that 
PostStop
PostStop
 is not emitted for a restart, so typically you need to handle both 
PreRestart
PreRestart
 and 
PostStop
PostStop
 to cleanup resources.




Bubble failures up through the hierarchy


In some scenarios it may be useful to push the decision about what to do on a failure upwards in the Actor hierarchy  and let the parent actor handle what should happen on failures (in classic Akka Actors this is how it works by default).


For a parent to be notified when a child is terminated it has to 
watch
 the child. If the child was stopped because of a failure the 
ChildFailed
ChildFailed
 signal will be received which will contain the cause. 
ChildFailed
ChildFailed
 extends 
Terminated
Terminated
 so if your use case does not need to distinguish between stopping and failing you can handle both cases with the 
Terminated
Terminated
 signal.


If the parent in turn does not handle the 
Terminated
Terminated
 message it will itself fail with an 
DeathPactException
DeathPactException
.


This means that a hierarchy of actors can have a child failure bubble up making each actor on the way stop but informing the top-most parent that there was a failure and how to deal with it, however, the original exception that caused the failure will only be available to the immediate parent out of the box (this is most often a good thing, not leaking implementation details). 


There might be cases when you want the original exception to bubble up the hierarchy, this can be done by handling the 
Terminated
Terminated
 signal, and rethrowing the exception in each actor.




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.DeathPactException
import akka.actor.typed.SupervisorStrategy
import akka.actor.typed.scaladsl.Behaviors

object Protocol {
  sealed trait Command
  case class Fail(text: String) extends Command
  case class Hello(text: String, replyTo: ActorRef[String]) extends Command
}
import Protocol._

object Worker {
  def apply(): Behavior[Command] =
    Behaviors.receiveMessage {
      case Fail(text) =>
        throw new RuntimeException(text)
      case Hello(text, replyTo) =>
        replyTo ! text
        Behaviors.same
    }
}

object MiddleManagement {
  def apply(): Behavior[Command] =
    Behaviors.setup[Command] { context =>
      context.log.info("Middle management starting up")
      // default supervision of child, meaning that it will stop on failure
      val child = context.spawn(Worker(), "child")
      // we want to know when the child terminates, but since we do not handle
      // the Terminated signal, we will in turn fail on child termination
      context.watch(child)

      // here we don't handle Terminated at all which means that
      // when the child fails or stops gracefully this actor will
      // fail with a DeathPactException
      Behaviors.receiveMessage { message =>
        child ! message
        Behaviors.same
      }
    }
}

object Boss {
  def apply(): Behavior[Command] =
    Behaviors
      .supervise(Behaviors.setup[Command] { context =>
        context.log.info("Boss starting up")
        // default supervision of child, meaning that it will stop on failure
        val middleManagement = context.spawn(MiddleManagement(), "middle-management")
        context.watch(middleManagement)

        // here we don't handle Terminated at all which means that
        // when middle management fails with a DeathPactException
        // this actor will also fail
        Behaviors.receiveMessage[Command] { message =>
          middleManagement ! message
          Behaviors.same
        }
      })
      .onFailure[DeathPactException](SupervisorStrategy.restart)
}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.DeathPactException;
import akka.actor.typed.SupervisorStrategy;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;

public interface Protocol {
  public interface Command {}

  public static class Fail implements Command {
    public final String text;

    public Fail(String text) {
      this.text = text;
    }
  }

  public static class Hello implements Command {
    public final String text;
    public final ActorRef<String> replyTo;

    public Hello(String text, ActorRef<String> replyTo) {
      this.text = text;
      this.replyTo = replyTo;
    }
  }
}

public static class Worker extends AbstractBehavior<Protocol.Command> {

  public static Behavior<Protocol.Command> create() {
    return Behaviors.setup(Worker::new);
  }

  private Worker(ActorContext<Protocol.Command> context) {
    super(context);
  }

  @Override
  public Receive<Protocol.Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Protocol.Fail.class, this::onFail)
        .onMessage(Protocol.Hello.class, this::onHello)
        .build();
  }

  private Behavior<Protocol.Command> onFail(Protocol.Fail message) {
    throw new RuntimeException(message.text);
  }

  private Behavior<Protocol.Command> onHello(Protocol.Hello message) {
    message.replyTo.tell(message.text);
    return this;
  }
}

public static class MiddleManagement extends AbstractBehavior<Protocol.Command> {

  public static Behavior<Protocol.Command> create() {
    return Behaviors.setup(MiddleManagement::new);
  }

  private final ActorRef<Protocol.Command> child;

  private MiddleManagement(ActorContext<Protocol.Command> context) {
    super(context);

    context.getLog().info("Middle management starting up");
    // default supervision of child, meaning that it will stop on failure
    child = context.spawn(Worker.create(), "child");

    // we want to know when the child terminates, but since we do not handle
    // the Terminated signal, we will in turn fail on child termination
    context.watch(child);
  }

  @Override
  public Receive<Protocol.Command> createReceive() {
    // here we don't handle Terminated at all which means that
    // when the child fails or stops gracefully this actor will
    // fail with a DeathPactException
    return newReceiveBuilder().onMessage(Protocol.Command.class, this::onCommand).build();
  }

  private Behavior<Protocol.Command> onCommand(Protocol.Command message) {
    // just pass messages on to the child
    child.tell(message);
    return this;
  }
}

public static class Boss extends AbstractBehavior<Protocol.Command> {

  public static Behavior<Protocol.Command> create() {
    return Behaviors.supervise(Behaviors.setup(Boss::new))
        .onFailure(DeathPactException.class, SupervisorStrategy.restart());
  }

  private final ActorRef<Protocol.Command> middleManagement;

  private Boss(ActorContext<Protocol.Command> context) {
    super(context);
    context.getLog().info("Boss starting up");
    // default supervision of child, meaning that it will stop on failure
    middleManagement = context.spawn(MiddleManagement.create(), "middle-management");
    context.watch(middleManagement);
  }

  @Override
  public Receive<Protocol.Command> createReceive() {
    // here we don't handle Terminated at all which means that
    // when middle management fails with a DeathPactException
    // this actor will also fail
    return newReceiveBuilder().onMessage(Protocol.Command.class, this::onCommand).build();
  }

  private Behavior<Protocol.Command> onCommand(Protocol.Command message) {
    // just pass messages on to the child
    middleManagement.tell(message);
    return this;
  }
}

















 
Interaction Patterns






Actor discovery 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/cluster-sharding-concepts.html
Cluster Sharding concepts • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts




Scenarios


Shard location


Shard rebalancing


ShardCoordinator state


Message ordering


Reliable delivery


Overhead




Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts




Scenarios


Shard location


Shard rebalancing


ShardCoordinator state


Message ordering


Reliable delivery


Overhead




Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster Sharding concepts


The 
ShardRegion
 actor is started on each node in the cluster, or group of nodes tagged with a specific role. The 
ShardRegion
 is created with two application specific functions to extract the entity identifier and the shard identifier from incoming messages. A 
Shard
 is a group of entities that will be managed together. For the first message in a specific shard the 
ShardRegion
 requests the location of the shard from a central coordinator, the 
ShardCoordinator
.


The 
ShardCoordinator
 decides which 
ShardRegion
 shall own the 
Shard
 and informs that 
ShardRegion
. The region will confirm this request and create the 
Shard
 supervisor as a child actor. The individual 
Entities
 will then be created when needed by the 
Shard
 actor. Incoming messages thus travel via the 
ShardRegion
 and the 
Shard
 to the target 
Entity
.


If the shard home is another 
ShardRegion
 instance messages will be forwarded to that 
ShardRegion
 instance instead. While resolving the location of a shard incoming messages for that shard are buffered and later delivered when the shard home is known. Subsequent messages to the resolved shard can be delivered to the target destination immediately without involving the 
ShardCoordinator
.


Scenarios


Once a 
Shard
 location is known 
ShardRegion
s send messages directly. Here are the scenarios for getting to this state. In the scenarios the following notation is used:




SC
 - ShardCoordinator


M#
 - Message 1, 2, 3, etc


SR#
 - ShardRegion 1, 2 3, etc


S#
 - Shard 1 2 3, etc


E#
 - Entity 1 2 3, etc. An entity refers to an Actor managed by Cluster Sharding.




Where 
#
 is a number to distinguish between instances as there are multiple in the Cluster.


Scenario 1: Message to an unknown shard that belongs to the local ShardRegion




Incoming message 
M1
 to 
ShardRegion
 instance 
SR1
.


M1
 is mapped to shard 
S1
. 
SR1
 doesn’t know about 
S1
, so it asks the 
SC
 for the location of 
S1
.


SC
 answers that the home of 
S1
 is 
SR1
.


SR1
 creates child actor shard 
S1
 and forwards the message to it.


S1
 creates child actor for 
E1
 and forwards the message to it.


All incoming messages for 
S1
 which arrive at 
SR1
 can be handled by 
SR1
 without 
SC
.




Scenario 2: Message to an unknown shard that belongs to a remote ShardRegion




Incoming message 
M2
 to 
ShardRegion
 instance 
SR1
.


M2
 is mapped to 
S2
. 
SR1
 doesn’t know about 
S2
, so it asks 
SC
 for the location of 
S2
.


SC
 answers that the home of 
S2
 is 
SR2
.


SR1
 sends buffered messages for 
S2
 to 
SR2
.


All incoming messages for 
S2
 which arrive at 
SR1
 can be handled by 
SR1
 without 
SC
. It forwards messages to 
SR2
.


SR2
 receives message for 
S2
, ask 
SC
, which answers that the home of 
S2
 is 
SR2
, and we are in Scenario 1 (but for 
SR2
).




Shard location


To make sure that at most one instance of a specific entity actor is running somewhere in the cluster it is important that all nodes have the same view of where the shards are located. Therefore the shard allocation decisions are taken by the central 
ShardCoordinator
, which is running as a cluster singleton, i.e. one instance on the oldest member among all cluster nodes or a group of nodes tagged with a specific role.


The logic that decides where a shard is to be located is defined in a pluggable 
shard allocation strategy
.


Shard rebalancing


To be able to use newly added members in the cluster the coordinator facilitates rebalancing of shards, i.e. migrate entities from one node to another. In the rebalance process the coordinator first notifies all 
ShardRegion
 actors that a handoff for a shard has started. That means they will start buffering incoming messages for that shard, in the same way as if the shard location is unknown. During the rebalance process the coordinator will not answer any requests for the location of shards that are being rebalanced, i.e. local buffering will continue until the handoff is completed. The 
ShardRegion
 responsible for the rebalanced shard will stop all entities in that shard by sending the specified 
stopMessage
 (default 
PoisonPill
) to them. When all entities have been terminated the 
ShardRegion
 owning the entities will acknowledge the handoff as completed to the coordinator. Thereafter the coordinator will reply to requests for the location of the shard, thereby allocating a new home for the shard, and then buffered messages in the 
ShardRegion
 actors are delivered to the new location. This means that the state of the entities are not transferred or migrated. If the state of the entities are of importance it should be persistent (durable), e.g. with 
Persistence
 (or see 
Classic Persistence
), so that it can be recovered at the new location.


The logic that decides which shards to rebalance is defined in a pluggable shard allocation strategy. The default implementation 
LeastShardAllocationStrategy
 allocates new shards to the 
ShardRegion
 (node) with least number of previously allocated shards. 


See also 
Shard allocation
.


ShardCoordinator state


The state of shard locations in the 
ShardCoordinator
 is persistent (durable) with 
Distributed Data
 (or see 
Classic Distributed Data
) to survive failures. 


When a crashed or unreachable coordinator node has been removed (via down) from the cluster a new 
ShardCoordinator
 singleton actor will take over and the state is recovered. During such a failure period shards with a known location are still available, while messages for new (unknown) shards are buffered until the new 
ShardCoordinator
 becomes available.


Message ordering


As long as a sender uses the same 
ShardRegion
 actor to deliver messages to an entity actor the order of the messages is preserved. As long as the buffer limit is not reached messages are delivered on a best effort basis, with at-most once delivery semantics, in the same way as ordinary message sending.


Reliable delivery


Reliable end-to-end messaging, with at-least-once semantics can be added by using the 
Reliable Delivery
 feature.


Overhead


Some additional latency is introduced for messages targeted to new or previously unused shards due to the round-trip to the coordinator. Rebalancing of shards may also add latency. This should be considered when designing the application specific shard resolution, e.g. to avoid too fine grained shards. Once a shard’s location is known the only overhead is sending a message via the 
ShardRegion
 rather than directly.














 
Cluster Sharding






Sharded Daemon Process 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-edge/current/index.html
Akka Edge







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Edge





Version 1.6.8





Java
Scala










Architectural Overview


Example Use Cases


Feature Summary


Guide - Java/Scala


Guide - Rust


Lightweight deployments


















Akka Edge





Version 1.6.8





Java
Scala












Architectural Overview


Example Use Cases


Feature Summary


Guide - Java/Scala


Guide - Rust


Lightweight deployments




















Akka Edge


Akka Edge is a set of features in Akka that will help you with using Akka at the edge of the cloud for higher availability and lower latency.




Akka Edge is similar to 
Akka Distributed Cluster
 and they share features and implementation, but Akka Edge is intended for use cases with even more geographically distributed services and possibly in more resource constrained environments. In contrast, Akka Distributed Cluster is intended for connecting rather few services in different cloud regions. Akka Edge and Akka Distributed Cluster are designed to be used together.


.






Architectural Overview




Topology


Communication transport


Event Replication


Dynamic filters




Example Use Cases




Energy management


Vehicle communications


High availability retail


Sporting events


Agriculture


Factories




Feature Summary




Projections over gRPC


Replicated Event Sourcing over gRPC




Guide - Java/Scala




Local Drone Control Service


Coarse Grained Location Replication


Restaurant deliveries


Local Drone Delivery Selection


Drone Charging Station


Deploying the Restaurant Delivery Service




Guide - Rust




Getting started


Running the sample


The temperature entity


Consuming registration events


Producing temperature events


HTTP temperature events


UDP observations


The main function




Lightweight deployments




Lightweight Kubernetes


Cloud-optimized JVMs


GraalVM Native Image


Multidimensional autoscaling
























Architectural Overview 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka Edge is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#how-it-works
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/persistence.html
Event Sourcing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and EventSourcedBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Recovery


Tagging


Event adapters


Wrapping EventSourcedBehavior


Journal failures


Stash


Scaling out


Configuration


Example project




Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and EventSourcedBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Recovery


Tagging


Event adapters


Wrapping EventSourcedBehavior


Journal failures


Stash


Scaling out


Configuration


Example project




Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Event Sourcing


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Akka Persistence
.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Persistence, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-persistence-typed" % AkkaVersion,
  "com.typesafe.akka" %% "akka-persistence-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-typed_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-persistence-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-persistence-typed_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-persistence-testkit_${versions.ScalaBinary}"
}


You also have to select journal plugin and optionally snapshot store plugin, see 
Persistence Plugins
.




Project Info: Akka Event Sourcing (typed)


Artifact
com.typesafe.akka


akka-persistence-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.persistence.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


Akka Persistence enables stateful actors to persist their state so that it can be recovered when an actor is either restarted, such as after a JVM crash, by a supervisor or a manual stop-start, or migrated within a cluster. The key concept behind Akka Persistence is that only the 
events
 that are persisted by the actor are stored, not the actual state of the actor (although actor state snapshot support is available). The events are persisted by appending to storage (nothing is ever mutated) which allows for very high transaction rates and efficient replication. A stateful actor is recovered by replaying the stored events to the actor, allowing it to rebuild its state. This can be either the full history of changes or starting from a checkpoint in a snapshot, which can dramatically reduce recovery times. 


Akka Persistence also supports 
Durable State Behaviors
, which is based on persistence of the latest state of the actor. In this implementation, the 
latest
 state is persisted, instead of events. Hence this is more similar to CRUD based applications.


The 
Event Sourcing with Akka 2.6 video
 is a good starting point for learning Event Sourcing, together with the 
Microservices with Akka tutorial
 that illustrates how to implement an Event Sourced CQRS application with Akka Persistence and Akka Projections.


Event Sourcing concepts


See an 
introduction to Event Sourcing
 at MSDN.


Another excellent article about “thinking in Events” is 
Events As First-Class Citizens
 by Randy Shoup. It is a short and recommended read if you’re starting developing Events based applications.


What follows is Akka’s implementation via event sourced actors. 


An event sourced actor (also known as a persistent actor) receives a (non-persistent) command which is first validated if it can be applied to the current state. Here validation can mean anything, from simple inspection of a command message’s fields up to a conversation with several external services, for example. If validation succeeds, events are generated from the command, representing the effect of the command. These events are then persisted and, after successful persistence, used to change the actor’s state. When the event sourced actor needs to be recovered, only the persisted events are replayed of which we know that they can be successfully applied. In other words, events cannot fail when being replayed to a persistent actor, in contrast to commands. Event sourced actors may also process commands that do not change application state such as query commands for example.


Example and core API


Let’s start with a simple example. The minimum required for a 
EventSourcedBehavior
EventSourcedBehavior
 is:




Scala




copy
source
import akka.persistence.typed.scaladsl.EventSourcedBehavior
import akka.persistence.typed.PersistenceId

object MyPersistentBehavior {
  sealed trait Command
  sealed trait Event
  final case class State()

  def apply(): Behavior[Command] =
    EventSourcedBehavior[Command, Event, State](
      persistenceId = PersistenceId.ofUniqueId("abc"),
      emptyState = State(),
      commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
      eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
}


Java




copy
source
public class MyPersistentBehavior
    extends EventSourcedBehavior<
        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {

  interface Command {}

  interface Event {}

  public static class State {}

  public static Behavior<Command> create(PersistenceId persistenceId) {
    return new MyPersistentBehavior(persistenceId);
  }

  private MyPersistentBehavior(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return new State();
  }

  @Override
  public CommandHandler<Command, Event, State> commandHandler() {
    return (state, command) -> {
      throw new RuntimeException("TODO: process the command & return an Effect");
    };
  }

  @Override
  public EventHandler<State, Event> eventHandler() {
    return (state, event) -> {
      throw new RuntimeException("TODO: process the event return the next state");
    };
  }
}




The first important thing to notice is the 
Behavior
Behavior
 of a persistent actor is typed to the type of the 
Command
 because this is the type of message a persistent actor should receive. In Akka this is now enforced by the type system.


The components that make up an 
EventSourcedBehavior
EventSourcedBehavior
 are:




persistenceId
 is the stable unique identifier for the persistent actor.


emptyState
 defines the 
State
 when the entity is first created e.g. a Counter would start with 0 as state.


commandHandler
 defines how to handle command by producing Effects e.g. persisting events, stopping the persistent actor.


eventHandler
 returns the new state given the current state when an event has been persisted.




Note that the concrete class does not contain any fields with state like a regular POJO. All state of the 
EventSourcedBehavior
 must be represented in the 
State
 or else they will not be persisted and therefore be lost when the actor is stopped or restarted. Updates to the State are always performed in the eventHandler based on the events.


Next we’ll discuss each of these in detail.


PersistenceId


The 
PersistenceId
PersistenceId
 is the stable unique identifier for the persistent actor in the backend event journal and snapshot store.


Cluster Sharding
 is typically used together with 
EventSourcedBehavior
 to ensure that there is only one active entity for each 
PersistenceId
 (
entityId
). There are techniques to ensure this uniqueness, an example of which can be found in the 
Persistence example in the Cluster Sharding documentation
. This illustrates how to construct the 
PersistenceId
 from the 
entityTypeKey
 and 
entityId
 provided by the 
EntityContext
EntityContext
.


The 
entityId
 in Cluster Sharding is the business domain identifier of the entity. The 
entityId
 might not be unique enough to be used as the 
PersistenceId
 by itself. For example two different types of entities may have the same 
entityId
. To create a unique 
PersistenceId
 the 
entityId
 should be prefixed with a stable name of the entity type, which typically is the same as the 
EntityTypeKey.name
 that is used in Cluster Sharding. There are 
PersistenceId.apply
PersistenceId.of
 factory methods to help with constructing such 
PersistenceId
 from an 
entityTypeHint
 and 
entityId
.


The default separator when concatenating the 
entityTypeHint
 and 
entityId
 is 
|
, but a custom separator is supported.
Note


The 
|
 separator is also used in Lagom’s 
scaladsl.PersistentEntity
 but no separator is used in Lagom’s 
javadsl.PersistentEntity
. For compatibility with Lagom’s 
javadsl.PersistentEntity
 you should use 
""
 as the separator.


A custom identifier can be created with 
PersistenceId.ofUniqueId
PersistenceId.ofUniqueId
. 


Command handler


The command handler is a function with 2 parameters, the current 
State
 and the incoming 
Command
.


A command handler returns an 
Effect
Effect
 directive that defines what event or events, if any, to persist. Effects are created using 
a factory that is returned via the 
Effect()
 method
 
the 
Effect
 factory
.


The two most commonly used effects are: 




persist
 will persist one single event or several events atomically, i.e. all events  are stored or none of them are stored if there is an error


none
 no events are to be persisted, for example a read-only command




More effects are explained in 
Effects and Side Effects
.


In addition to returning the primary 
Effect
 for the command 
EventSourcedBehavior
s can also chain side effects that are to be performed after successful persist which is achieved with the 
thenRun
 function e.g. 
Effect.persist(..).thenRun
Effect().persist(..).thenRun
.


Event handler


When an event has been persisted successfully the new state is created by applying the event to the current state with the 
eventHandler
. In the case of multiple persisted events, the 
eventHandler
 is called with each event in the same order as they were passed to 
Effect.persist(..)
Effect().persist(..)
.


The state is typically defined as an immutable class and then the event handler returns a new instance of the state. You may choose to use a mutable class for the state, and then the event handler may update the state instance and return the same instance. Both immutable and mutable state is supported, but it must only be modified in the event handler.


The same event handler is also used when the entity is started up to recover its state from the stored events.


The event handler must only update the state and never perform side effects, as those would also be executed during recovery of the persistent actor. Side effects should be performed in 
thenRun
 from the 
command handler
 after persisting the event or from the 
RecoveryCompleted
RecoveryCompleted
 after 
Recovery
.


Completing the example


Let’s fill in the details of the example.


Command and event:




Scala




copy
source
sealed trait Command
final case class Add(data: String) extends Command
case object Clear extends Command

sealed trait Event
final case class Added(data: String) extends Event
case object Cleared extends Event


Java




copy
source
interface Command {}

public static class Add implements Command {
  public final String data;

  public Add(String data) {
    this.data = data;
  }
}

public enum Clear implements Command {
  INSTANCE
}

interface Event {}

public static class Added implements Event {
  public final String data;

  public Added(String data) {
    this.data = data;
  }
}

public enum Cleared implements Event {
  INSTANCE
}




State is a List containing the 5 latest items:




Scala




copy
source
final case class State(history: List[String] = Nil)


Java




copy
source
public static class State {
  private final List<String> items;

  private State(List<String> items) {
    this.items = items;
  }

  public State() {
    this.items = new ArrayList<>();
  }

  public State addItem(String data) {
    List<String> newItems = new ArrayList<>(items);
    newItems.add(0, data);
    // keep 5 items
    List<String> latest = newItems.subList(0, Math.min(5, newItems.size()));
    return new State(latest);
  }
}




The command handler persists the 
Add
 payload in an 
Added
 event:




Scala




copy
source
import akka.persistence.typed.scaladsl.Effect

val commandHandler: (State, Command) => Effect[Event, State] = { (state, command) =>
  command match {
    case Add(data) => Effect.persist(Added(data))
    case Clear     => Effect.persist(Cleared)
  }
}


Java




copy
source
@Override
public CommandHandler<Command, Event, State> commandHandler() {
  return newCommandHandlerBuilder()
      .forAnyState()
      .onCommand(Add.class, command -> Effect().persist(new Added(command.data)))
      .onCommand(Clear.class, command -> Effect().persist(Cleared.INSTANCE))
      .build();
}




The event handler appends the item to the state and keeps 5 items. This is called after successfully persisting the event in the database:




Scala




copy
source
val eventHandler: (State, Event) => State = { (state, event) =>
  event match {
    case Added(data) => state.copy((data :: state.history).take(5))
    case Cleared     => State(Nil)
  }
}


Java




copy
source
@Override
public EventHandler<State, Event> eventHandler() {
  return newEventHandlerBuilder()
      .forAnyState()
      .onEvent(Added.class, (state, event) -> state.addItem(event.data))
      .onEvent(Cleared.class, () -> new State())
      .build();
}




These are used to create an 
EventSourcedBehavior
:
 
These are defined in an 
EventSourcedBehavior
:




Scala




copy
source
import akka.persistence.typed.scaladsl.EventSourcedBehavior
import akka.persistence.typed.PersistenceId

def apply(id: String): Behavior[Command] =
  EventSourcedBehavior[Command, Event, State](
    persistenceId = PersistenceId.ofUniqueId(id),
    emptyState = State(Nil),
    commandHandler = commandHandler,
    eventHandler = eventHandler)


Java




copy
source
import akka.persistence.typed.javadsl.EventSourcedBehavior;
import akka.persistence.typed.PersistenceId;

public class MyPersistentBehavior
    extends EventSourcedBehavior<
        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {

  // commands, events and state defined here

  public static Behavior<Command> create(PersistenceId persistenceId) {
    return new MyPersistentBehavior(persistenceId);
  }

  private MyPersistentBehavior(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return new State();
  }

  @Override
  public CommandHandler<Command, Event, State> commandHandler() {
    return newCommandHandlerBuilder()
        .forAnyState()
        .onCommand(Add.class, command -> Effect().persist(new Added(command.data)))
        .onCommand(Clear.class, command -> Effect().persist(Cleared.INSTANCE))
        .build();
  }

  @Override
  public EventHandler<State, Event> eventHandler() {
    return newEventHandlerBuilder()
        .forAnyState()
        .onEvent(Added.class, (state, event) -> state.addItem(event.data))
        .onEvent(Cleared.class, () -> new State())
        .build();
  }
}




Effects and Side Effects


A command handler returns an 
Effect
Effect
 directive that defines what event or events, if any, to persist. Effects are created using 
a factory that is returned via the 
Effect()
 method
 
the 
Effect
 factory
 and can be one of: 




persist
persist
 will persist one single event or several events atomically, i.e. all events  are stored or none of them are stored if there is an error


none
none
 no events are to be persisted, for example a read-only command


unhandled
unhandled
 the command is unhandled (not supported) in current state


stop
stop
 stop this actor


stash
stash
 the current command is stashed


unstashAll
unstashAll
 process the commands that were stashed with 
Effect.stash
Effect().stash


reply
reply
 send a reply message to the given 
ActorRef
ActorRef


async
reply
 Asynchronous command handling


asyncReply
reply
 Asynchronous command handling and then reply




Note that only one of those can be chosen per incoming command. It is not possible to both persist and say none/unhandled.


In addition to returning the primary 
Effect
 for the command 
EventSourcedBehavior
EventSourcedBehavior
s can also chain side effects that are to be performed after successful persist which is achieved with the 
thenRun
thenRun
 function e.g. 
Effect.persist(..).thenRun
Effect().persist(..).thenRun
.


In the example below the state is sent to the 
subscriber
 ActorRef. Note that the new state after applying the event is passed as parameter of the 
thenRun
 function. In the case where multiple events have been persisted, the state passed to 
thenRun
 is the updated state after all events have been handled.


All 
thenRun
 registered callbacks are executed sequentially after successful execution of the persist statement (or immediately, in case of 
none
 and 
unhandled
).


In addition to 
thenRun
 the following actions can also be performed after successful persist:




thenStop
thenStop
 the actor will be stopped


thenUnstashAll
thenUnstashAll
 process the commands that were stashed with 
Effect.stash
Effect().stash


thenReply
thenReply
 send a reply message to the given 
ActorRef
ActorRef




Example of effects:




Scala




copy
source
def onCommand(subscriber: ActorRef[State], state: State, command: Command): Effect[Event, State] = {
  command match {
    case Add(data) =>
      Effect.persist(Added(data)).thenRun(newState => subscriber ! newState)
    case Clear =>
      Effect.persist(Cleared).thenRun((newState: State) => subscriber ! newState).thenStop()
  }
}


Java




copy
source
private final ActorRef<State> subscriber;

@Override
public CommandHandler<Command, Event, State> commandHandler() {
  return newCommandHandlerBuilder()
      .forAnyState()
      .onCommand(Add.class, this::onAdd)
      .onCommand(Clear.class, this::onClear)
      .build();
}

private Effect<Event, State> onAdd(Add command) {
  return Effect()
      .persist(new Added(command.data))
      .thenRun(newState -> subscriber.tell(newState));
}

private Effect<Event, State> onClear(Clear command) {
  return Effect()
      .persist(Cleared.INSTANCE)
      .thenRun(newState -> subscriber.tell(newState))
      .thenStop();
}





Most of the time this will be done with the 
thenRun
thenRun
 method on the 
Effect
 above. You can factor out common side effects into functions and reuse for several commands. For example:




Scala




copy
source
// Example factoring out a chained effect to use in several places with `thenRun`
val commonChainedEffects: Mood => Unit = _ => println("Command processed")
// Then in a command handler:
Effect
  .persist(Remembered("Yep")) // persist event
  .thenRun(commonChainedEffects) // add on common chained effect


Java




copy
source
// Example factoring out a chained effect to use in several places with `thenRun`
static final Procedure<ExampleState> commonChainedEffect =
    state -> System.out.println("Command handled!");

      @Override
      public CommandHandler<MyCommand, MyEvent, ExampleState> commandHandler() {
        return newCommandHandlerBuilder()
            .forStateType(ExampleState.class)
            .onCommand(
                Cmd.class,
                (state, cmd) ->
                    Effect()
                        .persist(new Evt(cmd.data))
                        .thenRun(() -> cmd.replyTo.tell(new Ack()))
                        .thenRun(commonChainedEffect))
            .build();
      }




Side effects ordering and guarantees


Any side effects are executed on an at-most-once basis and will not be executed if the persist fails.


Side effects are not run when the actor is restarted or started again after being stopped. You may inspect the state when receiving the 
RecoveryCompleted
RecoveryCompleted
 signal and execute side effects that have not been acknowledged at that point. That may possibly result in executing side effects more than once.


The side effects are executed sequentially, it is not possible to execute side effects in parallel, unless they call out to something that is running concurrently (for example sending a message to another actor).


It’s possible to execute a side effects before persisting the event, but that can result in that the side effect is performed but the event is not stored if the persist fails.


Atomic writes


It is possible to store several events atomically by using the 
persist
persist
 effect with a list of events. That means that all events passed to that method are stored or none of them are stored if there is an error.


The recovery of a persistent actor will therefore never be done partially with only a subset of events persisted by a single 
persist
persist
 effect.


Some journals may not support atomic writes of several events and they will then reject the 
persist
 with multiple events. This is signalled to an 
EventSourcedBehavior
EventSourcedBehavior
 via an 
PersistRejected
PersistRejected
 signal. An 
EventRejectedException
EventRejectedException
 is also thrown (typically with a 
UnsupportedOperationException
) and can be handled with a 
supervisor
.


Cluster Sharding and EventSourcedBehavior


Cluster Sharding
 is an excellent fit to spread persistent actors over a cluster, addressing them by id. It makes it possible to have more persistent actors exist in the cluster than what would fit in the memory of one node. Cluster sharding improves the resilience of the cluster. If a node crashes, the persistent actors are quickly started on a new node and can resume operations.


The 
EventSourcedBehavior
EventSourcedBehavior
 can then be run as with any plain actor as described in 
actors documentation
, but since Akka Persistence is based on the single-writer principle the persistent actors are typically used together with Cluster Sharding. For a particular 
persistenceId
 only one persistent actor instance should be active at one time. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding ensures that there is only one active entity for each id. The 
Cluster Sharding example
 illustrates this common combination.


Accessing the ActorContext


If the 
EventSourcedBehavior
EventSourcedBehavior
 needs to use the 
ActorContext
ActorContext
, for example to spawn child actors, it can be obtained by wrapping construction with 
Behaviors.setup
Behaviors.setup
:




Scala




copy
source
import akka.persistence.typed.scaladsl.Effect
import akka.persistence.typed.scaladsl.EventSourcedBehavior.CommandHandler

def apply(): Behavior[String] =
  Behaviors.setup { context =>
    EventSourcedBehavior[String, String, State](
      persistenceId = PersistenceId.ofUniqueId("myPersistenceId"),
      emptyState = State(),
      commandHandler = CommandHandler.command { cmd =>
        context.log.info("Got command {}", cmd)
        Effect.none
      },
      eventHandler = {
        case (state, _) => state
      })
  }


Java




copy
source
public class MyPersistentBehavior
    extends EventSourcedBehavior<
        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {

  public static Behavior<Command> create(PersistenceId persistenceId) {
    return Behaviors.setup(ctx -> new MyPersistentBehavior(persistenceId, ctx));
  }

  // this makes the context available to the command handler etc.
  private final ActorContext<Command> context;

  // optionally if you only need `ActorContext.getSelf()`
  private final ActorRef<Command> self;

  public MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> ctx) {
    super(persistenceId);
    this.context = ctx;
    this.self = ctx.getSelf();
  }

}




Changing Behavior


After processing a message, actors are able to return the 
Behavior
Behavior
 that is used for the next message.


As you can see in the above examples this is not supported by persistent actors. Instead, the state is returned by 
eventHandler
. The reason a new behavior can’t be returned is that behavior is part of the actor’s state and must also carefully be reconstructed during recovery. If it would have been supported it would mean that the behavior must be restored when replaying events and also encoded in the state anyway when snapshots are used. That would be very prone to mistakes and thus not allowed in Akka Persistence.


For basic actors you can use the same set of command handlers independent of what state the entity is in, as shown in above example. For more complex actors it’s useful to be able to change the behavior in the sense that different functions for processing commands may be defined depending on what state the actor is in. This is useful when implementing finite state machine (FSM) like entities.


The next example demonstrates how to define different behavior based on the current 
State
. It shows an actor that represents the state of a blog post. Before a post is started the only command it can process is to 
AddPost
. Once it is started then one can look it up with 
GetPost
, modify it with 
ChangeBody
 or publish it with 
Publish
.


The state is captured by:




Scala




copy
source
sealed trait State

case object BlankState extends State

final case class DraftState(content: PostContent) extends State {
  def withBody(newBody: String): DraftState =
    copy(content = content.copy(body = newBody))

  def postId: String = content.postId
}

final case class PublishedState(content: PostContent) extends State {
  def postId: String = content.postId
}


Java




copy
source
interface State {}

enum BlankState implements State {
  INSTANCE
}

static class DraftState implements State {
  final PostContent content;

  DraftState(PostContent content) {
    this.content = content;
  }

  DraftState withContent(PostContent newContent) {
    return new DraftState(newContent);
  }

  DraftState withBody(String newBody) {
    return withContent(new PostContent(postId(), content.title, newBody));
  }

  String postId() {
    return content.postId;
  }
}

static class PublishedState implements State {
  final PostContent content;

  PublishedState(PostContent content) {
    this.content = content;
  }

  PublishedState withContent(PostContent newContent) {
    return new PublishedState(newContent);
  }

  PublishedState withBody(String newBody) {
    return withContent(new PostContent(postId(), content.title, newBody));
  }

  String postId() {
    return content.postId;
  }
}




The commands, of which only a subset are valid depending on the state:




Scala




copy
source
sealed trait Command
final case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command
final case class AddPostDone(postId: String)
final case class GetPost(replyTo: ActorRef[PostContent]) extends Command
final case class ChangeBody(newBody: String, replyTo: ActorRef[Done]) extends Command
final case class Publish(replyTo: ActorRef[Done]) extends Command
final case class PostContent(postId: String, title: String, body: String)


Java




copy
source
public interface Command {}
public static class AddPost implements Command {
  final PostContent content;
  final ActorRef<AddPostDone> replyTo;

  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {
    this.content = content;
    this.replyTo = replyTo;
  }
}

public static class AddPostDone implements Command {
  final String postId;

  public AddPostDone(String postId) {
    this.postId = postId;
  }
}
public static class GetPost implements Command {
  final ActorRef<PostContent> replyTo;

  public GetPost(ActorRef<PostContent> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class ChangeBody implements Command {
  final String newBody;
  final ActorRef<Done> replyTo;

  public ChangeBody(String newBody, ActorRef<Done> replyTo) {
    this.newBody = newBody;
    this.replyTo = replyTo;
  }
}

public static class Publish implements Command {
  final ActorRef<Done> replyTo;

  public Publish(ActorRef<Done> replyTo) {
    this.replyTo = replyTo;
  }
}

public static class PostContent implements Command {
  final String postId;
  final String title;
  final String body;

  public PostContent(String postId, String title, String body) {
    this.postId = postId;
    this.title = title;
    this.body = body;
  }
}




The command handler to process each command is decided by the state class (or state predicate) that is given to the 
forStateType
 of the 
CommandHandlerBuilder
 and the match cases in the builders.
 
The command handler to process each command is decided by first looking at the state and then the command. It typically becomes two levels of pattern matching, first on the state and then on the command.
 Delegating to methods is a good practice because the one-line cases give a nice overview of the message dispatch.




Scala




copy
source
private val commandHandler: (State, Command) => Effect[Event, State] = { (state, command) =>
  state match {

    case BlankState =>
      command match {
        case cmd: AddPost => addPost(cmd)
        case _            => Effect.unhandled
      }

    case draftState: DraftState =>
      command match {
        case cmd: ChangeBody  => changeBody(draftState, cmd)
        case Publish(replyTo) => publish(draftState, replyTo)
        case GetPost(replyTo) => getPost(draftState, replyTo)
        case AddPost(_, replyTo) =>
          Effect.unhandled.thenRun(_ => replyTo ! StatusReply.Error("Cannot add post while in draft state"))
      }

    case publishedState: PublishedState =>
      command match {
        case GetPost(replyTo) => getPost(publishedState, replyTo)
        case AddPost(_, replyTo) =>
          Effect.unhandled.thenRun(_ => replyTo ! StatusReply.Error("Cannot add post, already published"))
        case _ => Effect.unhandled
      }
  }
}

private def addPost(cmd: AddPost): Effect[Event, State] = {
  val evt = PostAdded(cmd.content.postId, cmd.content)
  Effect.persist(evt).thenRun { _ =>
    // After persist is done additional side effects can be performed
    cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))
  }
}

private def changeBody(state: DraftState, cmd: ChangeBody): Effect[Event, State] = {
  val evt = BodyChanged(state.postId, cmd.newBody)
  Effect.persist(evt).thenRun { _ =>
    cmd.replyTo ! Done
  }
}

private def publish(state: DraftState, replyTo: ActorRef[Done]): Effect[Event, State] = {
  Effect.persist(Published(state.postId)).thenRun { _ =>
    println(s"Blog post ${state.postId} was published")
    replyTo ! Done
  }
}

private def getPost(state: DraftState, replyTo: ActorRef[PostContent]): Effect[Event, State] = {
  replyTo ! state.content
  Effect.none
}

private def getPost(state: PublishedState, replyTo: ActorRef[PostContent]): Effect[Event, State] = {
  replyTo ! state.content
  Effect.none
}


Java




copy
source
@Override
public CommandHandler<Command, Event, State> commandHandler() {
  CommandHandlerBuilder<Command, Event, State> builder = newCommandHandlerBuilder();

  builder.forStateType(BlankState.class).onCommand(AddPost.class, this::onAddPost);

  builder
      .forStateType(DraftState.class)
      .onCommand(ChangeBody.class, this::onChangeBody)
      .onCommand(Publish.class, this::onPublish)
      .onCommand(GetPost.class, this::onGetPost);

  builder
      .forStateType(PublishedState.class)
      .onCommand(ChangeBody.class, this::onChangeBody)
      .onCommand(GetPost.class, this::onGetPost);

  builder.forAnyState().onCommand(AddPost.class, (state, cmd) -> Effect().unhandled());

  return builder.build();
}

private Effect<Event, State> onAddPost(AddPost cmd) {
  PostAdded event = new PostAdded(cmd.content.postId, cmd.content);
  return Effect()
      .persist(event)
      .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));
}

private Effect<Event, State> onChangeBody(DraftState state, ChangeBody cmd) {
  BodyChanged event = new BodyChanged(state.postId(), cmd.newBody);
  return Effect().persist(event).thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
}

private Effect<Event, State> onChangeBody(PublishedState state, ChangeBody cmd) {
  BodyChanged event = new BodyChanged(state.postId(), cmd.newBody);
  return Effect().persist(event).thenRun(() -> cmd.replyTo.tell(Done.getInstance()));
}

private Effect<Event, State> onPublish(DraftState state, Publish cmd) {
  return Effect()
      .persist(new Published(state.postId()))
      .thenRun(
          () -> {
            System.out.println("Blog post published: " + state.postId());
            cmd.replyTo.tell(Done.getInstance());
          });
}

private Effect<Event, State> onGetPost(DraftState state, GetPost cmd) {
  cmd.replyTo.tell(state.content);
  return Effect().none();
}

private Effect<Event, State> onGetPost(PublishedState state, GetPost cmd) {
  cmd.replyTo.tell(state.content);
  return Effect().none();
}




The event handler:




Scala




copy
source
private val eventHandler: (State, Event) => State = { (state, event) =>
  state match {

    case BlankState =>
      event match {
        case PostAdded(_, content) =>
          DraftState(content)
        case _ => throw new IllegalStateException(s"unexpected event [$event] in state [$state]")
      }

    case draftState: DraftState =>
      event match {

        case BodyChanged(_, newBody) =>
          draftState.withBody(newBody)

        case Published(_) =>
          PublishedState(draftState.content)

        case _ => throw new IllegalStateException(s"unexpected event [$event] in state [$state]")
      }

    case _: PublishedState =>
      // no more changes after published
      throw new IllegalStateException(s"unexpected event [$event] in state [$state]")
  }
}


Java




copy
source
@Override
public EventHandler<State, Event> eventHandler() {

  EventHandlerBuilder<State, Event> builder = newEventHandlerBuilder();

  builder
      .forStateType(BlankState.class)
      .onEvent(PostAdded.class, event -> new DraftState(event.content));

  builder
      .forStateType(DraftState.class)
      .onEvent(BodyChanged.class, (state, chg) -> state.withBody(chg.newBody))
      .onEvent(Published.class, (state, event) -> new PublishedState(state.content));

  builder
      .forStateType(PublishedState.class)
      .onEvent(BodyChanged.class, (state, chg) -> state.withBody(chg.newBody));

  return builder.build();
}




And finally the behavior is created 
from the 
EventSourcedBehavior.apply
:




Scala




copy
source
object BlogPostEntity {
  // commands, events, state defined here

  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {
    Behaviors.setup { context =>
      context.log.info("Starting BlogPostEntity {}", entityId)
      EventSourcedBehavior[Command, Event, State](persistenceId, emptyState = BlankState, commandHandler, eventHandler)
    }
  }

  // commandHandler and eventHandler defined here
}


Java




copy
source
public class BlogPostEntity
    extends EventSourcedBehavior<
        BlogPostEntity.Command, BlogPostEntity.Event, BlogPostEntity.State> {
  // commands, events and state as in above snippets

  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {
    return Behaviors.setup(
        context -> {
          context.getLog().info("Starting BlogPostEntity {}", entityId);
          return new BlogPostEntity(persistenceId);
        });
  }

  private BlogPostEntity(PersistenceId persistenceId) {
    super(persistenceId);
  }

  @Override
  public State emptyState() {
    return BlankState.INSTANCE;
  }

  // commandHandler, eventHandler as in above snippets
}




This can be taken one or two steps further by defining the event and command handlers in the state class as illustrated in 
event handlers in the state
 and 
command handlers in the state
.


There is also an example illustrating an 
optional initial state
.


Replies


The 
Request-Response interaction pattern
 is very common for persistent actors, because you typically want to know if the command was rejected due to validation errors and when accepted you want a confirmation when the events have been successfully stored.


Therefore you typically include a 
ActorRef
ActorRef
[ReplyMessageType]
<ReplyMessageType>
. If the command can either have a successful response or a validation error returned, the generic response type 
StatusReply
StatusReply
[ReplyType]
 
<ReplyType>
 can be used. If the successful reply does not contain a value but is more of an acknowledgement a pre defined 
StatusReply.Ack
StatusReply.ack()
 of type 
StatusReply[Done]
StatusReply<Done>
 can be used.


After validation errors or after persisting events, using a 
thenRun
thenRun
 side effect, the reply message can be sent to the 
ActorRef
ActorRef
.




Scala




copy
source
final case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command
final case class AddPostDone(postId: String)


Java




copy
source
public static class AddPost implements Command {
  final PostContent content;
  final ActorRef<AddPostDone> replyTo;

  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {
    this.content = content;
    this.replyTo = replyTo;
  }
}

public static class AddPostDone implements Command {
  final String postId;

  public AddPostDone(String postId) {
    this.postId = postId;
  }
}






Scala




copy
source
val evt = PostAdded(cmd.content.postId, cmd.content)
Effect.persist(evt).thenRun { _ =>
  // After persist is done additional side effects can be performed
  cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))
}


Java




copy
source
PostAdded event = new PostAdded(cmd.content.postId, cmd.content);
return Effect()
    .persist(event)
    .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));




Since this is such a common pattern there is a reply effect for this purpose. It has the nice property that it can be used to enforce that replies are not forgotten when implementing the 
EventSourcedBehavior
EventSourcedBehavior
. If it’s defined with 
EventSourcedBehavior.withEnforcedReplies
EventSourcedBehaviorWithEnforcedReplies
 there will be compilation errors if the returned effect isn’t a 
ReplyEffect
ReplyEffect
, which can be created with 
Effect.reply
Effect().reply
, 
Effect.noReply
Effect().noReply
, 
Effect.thenReply
Effect().thenReply
, or 
Effect.thenNoReply
Effect().thenNoReply
.




Scala




copy
source
def apply(accountNumber: String, persistenceId: PersistenceId): Behavior[Command] = {
  EventSourcedBehavior.withEnforcedReplies(persistenceId, EmptyAccount, commandHandler(accountNumber), eventHandler)
}


Java




copy
source
public class AccountEntity
    extends EventSourcedBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {




The commands must have a field of 
ActorRef
ActorRef
[ReplyMessageType]
<ReplyMessageType>
 that can then be used to send a reply.




Scala




copy
source
sealed trait Command extends CborSerializable
final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command


Java




copy
source
interface Command extends CborSerializable {}




The 
ReplyEffect
ReplyEffect
 is created with 
Effect.reply
Effect().reply
, 
Effect.noReply
Effect().noReply
, 
Effect.thenReply
Effect().thenReply
, or 
Effect.thenNoReply
Effect().thenNoReply
.


Note that command handlers are defined with 
newCommandHandlerWithReplyBuilder
 when using 
EventSourcedBehaviorWithEnforcedReplies
, as opposed to 
newCommandHandlerBuilder
 when using 
EventSourcedBehavior
.




Scala




copy
source
private def withdraw(acc: OpenedAccount, cmd: Withdraw): ReplyEffect[Event, Account] = {
  if (acc.canWithdraw(cmd.amount))
    Effect.persist(Withdrawn(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
  else
    Effect.reply(cmd.replyTo)(
      StatusReply.Error(s"Insufficient balance ${acc.balance} to be able to withdraw ${cmd.amount}"))
}


Java




copy
source
private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {
  if (!account.canWithdraw(command.amount)) {
    return Effect()
        .reply(
            command.replyTo,
            StatusReply.error("not enough funds to withdraw " + command.amount));
  } else {
    return Effect()
        .persist(new Withdrawn(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }
}




These effects will send the reply message even when 
EventSourcedBehavior.withEnforcedReplies
EventSourcedBehaviorWithEnforcedReplies
 is not used, but then there will be no compilation errors if the reply decision is left out.


Note that the 
noReply
 is a way of making conscious decision that a reply shouldn’t be sent for a specific command or the reply will be sent later, perhaps after some asynchronous interaction with other actors or services.


Serialization


The same 
serialization
 mechanism as for actor messages is also used for persistent actors. When picking a serialization solution for the events you should also consider that it must be possible to read old events when the application has evolved. Strategies for that can be found in the 
schema evolution
.


You need to enable 
serialization
 for your commands (messages), events, and state (snapshot). 
Serialization with Jackson
 is a good choice in many cases and our recommendation if you don’t have other preference.


Recovery


An event sourced actor is automatically recovered on start and on restart by replaying journaled events. New messages sent to the actor during recovery do not interfere with replayed events. They are stashed and received by the 
EventSourcedBehavior
EventSourcedBehavior
 after the recovery phase completes.


The number of concurrent recoveries that can be in progress at the same time is limited to not overload the system and the backend data store. When exceeding the limit the actors will wait until other recoveries have been completed. This is configured by:


akka.persistence.max-concurrent-recoveries = 50



The 
event handler
 is used for updating the state when replaying the journaled events.


It is strongly discouraged to perform side effects in the event handler, so side effects should be performed once recovery has completed as a reaction to the 
RecoveryCompleted
RecoveryCompleted
 signal 
in the 
receiveSignal
 handler
 
by overriding 
receiveSignal




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = persistenceId,
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .receiveSignal {
    case (state, RecoveryCompleted) =>
      throw new NotImplementedError("TODO: add some end-of-recovery side-effect here")
  }


Java




copy
source
@Override
public SignalHandler<State> signalHandler() {
  return newSignalHandlerBuilder()
      .onSignal(
          RecoveryCompleted.instance(),
          state -> {
            throw new RuntimeException("TODO: add some end-of-recovery side-effect here");
          })
      .build();
}




The 
RecoveryCompleted
 contains the current 
State
.


The actor will always receive a 
RecoveryCompleted
 signal, even if there are no events in the journal and the snapshot store is empty, or if it’s a new persistent actor with a previously unused 
PersistenceId
.


Snapshots
 can be used for optimizing recovery times.


Replay filter


There could be cases where event streams are corrupted and multiple writers (i.e. multiple persistent actor instances) journaled different messages with the same sequence number. In such a case, you can configure how you filter replayed messages from multiple writers, upon recovery.


In your configuration, under the 
akka.persistence.journal.xxx.replay-filter
 section (where 
xxx
 is your journal plugin id), you can select the replay filter 
mode
 from one of the following values:




repair-by-discard-old


fail


warn


off




For example, if you configure the replay filter for leveldb plugin, it looks like this:


# The replay filter can detect a corrupt event stream by inspecting
# sequence numbers and writerUuid when replaying events.
akka.persistence.journal.leveldb.replay-filter {
  # What the filter should do when detecting invalid events.
  # Supported values:
  # `repair-by-discard-old` : discard events from old writers,
  #                           warning is logged
  # `fail` : fail the replay, error is logged
  # `warn` : log warning but emit events untouched
  # `off` : disable this feature completely
  mode = repair-by-discard-old
}



Disable recovery


You can also completely disable the recovery of events and snapshots:




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withRecovery(Recovery.disabled)


Java




copy
source
@Override
public Recovery recovery() {
  return Recovery.disabled();
}




Please refer to 
snapshots
 if you need to disable only the snapshot recovery, or you need to select specific snapshots.


In any case, the highest sequence number will always be recovered so you can keep persisting new events without corrupting your event log.
Warning


Disable of recovery is not normal behavior of an event sourced actor, since events and snapshots are not used for the recovery of the actor.


Recovery from only last event


For some use cases it is enough to recover the actor from the last event, as an optimization to not replay all events. You can enable this recovery mode with:




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withRecovery(Recovery.replayOnlyLast)


Java




copy
source
@Override
public Recovery recovery() {
  return Recovery.replayOnlyLast();
}




Snapshots are not loaded when recovery from last event is selected.
Warning


Recovery from only last event is not normal behavior of an event sourced actor, since it typically would need all events, or a snapshot and events after the snapshot, to recover its state.


This feature is currently only supported by the R2DBC plugin.


Tagging


Persistence allows you to use event tags without using an 
EventAdapter
:




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .withTagger(_ => Set("tag1", "tag2"))


Java




copy
source
@Override
public Set<String> tagsFor(Event event) {
  Set<String> tags = new HashSet<>();
  tags.add("tag1");
  tags.add("tag2");
  return tags;
}




Event adapters


Event adapters can be programmatically added to your 
EventSourcedBehavior
EventSourcedBehavior
s that can convert from your 
Event
 type to another type that is then passed to the journal.


Defining an event adapter is done by extending an EventAdapter:




Scala




copy
source
case class Wrapper[T](event: T)
class WrapperEventAdapter[T] extends EventAdapter[T, Wrapper[T]] {
  override def toJournal(e: T): Wrapper[T] = Wrapper(e)
  override def fromJournal(p: Wrapper[T], manifest: String): EventSeq[T] = EventSeq.single(p.event)
  override def manifest(event: T): String = ""
}


Java




copy
source
public static class Wrapper<T> {
  private final T event;

  public Wrapper(T event) {
    this.event = event;
  }

  public T getEvent() {
    return event;
  }
}

public static class EventAdapterExample
    extends EventAdapter<SimpleEvent, Wrapper<SimpleEvent>> {
  @Override
  public Wrapper<SimpleEvent> toJournal(SimpleEvent simpleEvent) {
    return new Wrapper<>(simpleEvent);
  }

  @Override
  public String manifest(SimpleEvent event) {
    return "";
  }

  @Override
  public EventSeq<SimpleEvent> fromJournal(
      Wrapper<SimpleEvent> simpleEventWrapper, String manifest) {
    return EventSeq.single(simpleEventWrapper.getEvent());
  }
}




Then install it on an 
EventSourcedBehavior
:




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .eventAdapter(new WrapperEventAdapter[Event])


Java




copy
source
@Override
public EventAdapter<SimpleEvent, Wrapper<SimpleEvent>> eventAdapter() {
  return new EventAdapterExample();
}




Wrapping EventSourcedBehavior


When creating an 
EventSourcedBehavior
EventSourcedBehavior
, it is possible to wrap 
EventSourcedBehavior
 in other behaviors such as 
Behaviors.setup
Behaviors.setup
 in order to access the 
ActorContext
ActorContext
 object. For instance to access the actor logging upon taking snapshots for debug purpose.




Scala




copy
source
Behaviors.setup[Command] { context =>
  EventSourcedBehavior[Command, Event, State](
    persistenceId = PersistenceId.ofUniqueId("abc"),
    emptyState = State(),
    commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
    eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
    .snapshotWhen((state, _, _) => {
      context.log.info("Snapshot actor {} => state: {}", context.self.path.name, state)
      true
    })
}


Java




copy
source
public class MyPersistentBehavior
    extends EventSourcedBehavior<
        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {


  public static Behavior<Command> create(PersistenceId persistenceId) {
    return Behaviors.setup(context -> new MyPersistentBehavior(persistenceId, context));
  }

  private final ActorContext<Command> context;

  private MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> context) {
    super(
        persistenceId,
        SupervisorStrategy.restartWithBackoff(
            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));
    this.context = context;
  }

  @Override
  public boolean shouldSnapshot(State state, Event event, long sequenceNr) {
    context
        .getLog()
        .info("Snapshot actor {} => state: {}", context.getSelf().path().name(), state);
    return true;
  }
}




Journal failures


By default an 
EventSourcedBehavior
EventSourcedBehavior
 will stop if an exception is thrown from the journal. It is possible to override this with any 
BackoffSupervisorStrategy
BackoffSupervisorStrategy
. It is not possible to use the normal supervision wrapping for this as it isn’t valid to 
resume
 a behavior on a journal failure as it is not known if the event was persisted.




Scala




copy
source
EventSourcedBehavior[Command, Event, State](
  persistenceId = PersistenceId.ofUniqueId("abc"),
  emptyState = State(),
  commandHandler = (state, cmd) => throw new NotImplementedError("TODO: process the command & return an Effect"),
  eventHandler = (state, evt) => throw new NotImplementedError("TODO: process the event return the next state"))
  .onPersistFailure(
    SupervisorStrategy.restartWithBackoff(minBackoff = 10.seconds, maxBackoff = 60.seconds, randomFactor = 0.1))


Java




copy
source
public class MyPersistentBehavior
    extends EventSourcedBehavior<
        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {


  public static Behavior<Command> create(PersistenceId persistenceId) {
    return new MyPersistentBehavior(persistenceId);
  }

  private MyPersistentBehavior(PersistenceId persistenceId) {
    super(
        persistenceId,
        SupervisorStrategy.restartWithBackoff(
            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));
  }

}




If there is a problem with recovering the state of the actor from the journal, a 
RecoveryFailed
RecoveryFailed
 signal is emitted to the 
receiveSignal
 handler
 
receiveSignal
 method
 and the actor will be stopped (or restarted with backoff).


If there is a problem with persisting an event to the journal, a 
PersistFailed
PersistFailed
 signal is emitted to the 
receiveSignal
 handler
 
receiveSignal
 method
 and the actor will be stopped (or restarted with backoff).


Journal rejections


Journals can reject events. The difference from a failure is that the journal must decide to reject an event before trying to persist it e.g. because of a serialization exception. If an event is rejected it definitely won’t be in the journal. This is signalled to an 
EventSourcedBehavior
EventSourcedBehavior
 via an 
PersistRejected
PersistRejected
 signal. An 
EventRejectedException
EventRejectedException
 is also thrown and can be handled with a 
supervisor
. Not all journal implementations use rejections and treat these kind of problems also as journal failures. 


Stash


When persisting events with 
persist
persist
 it is guaranteed that the 
EventSourcedBehavior
EventSourcedBehavior
 will not receive further commands until after the events have been confirmed to be persisted and additional side effects have been run. Incoming messages are stashed automatically until the 
persist
 is completed.


Commands are also stashed during recovery and will not interfere with replayed events. Commands will be received when recovery has been completed.


The stashing described above is handled automatically, but there is also a possibility to stash commands when they are received to defer processing of them until later. One example could be waiting for some external condition or interaction to complete before processing additional commands. That is accomplished by returning a 
stash
stash
 effect and later use 
thenUnstashAll
thenUnstashAll
.


Let’s use an example of a task manager to illustrate how the stashing effects can be used. It handles three commands; 
StartTask
, 
NextStep
 and 
EndTask
. Those commands are associated with a given 
taskId
 and the manager processes one 
taskId
 at a time. A task is started when receiving 
StartTask
, and continues when receiving 
NextStep
 commands until the final 
EndTask
 is received. Commands with another 
taskId
 than the one in progress are deferred by stashing them. When 
EndTask
 is processed a new task can start and the stashed commands are processed.




Scala




copy
source
object TaskManager {

  sealed trait Command
  final case class StartTask(taskId: String) extends Command
  final case class NextStep(taskId: String, instruction: String) extends Command
  final case class EndTask(taskId: String) extends Command

  sealed trait Event
  final case class TaskStarted(taskId: String) extends Event
  final case class TaskStep(taskId: String, instruction: String) extends Event
  final case class TaskCompleted(taskId: String) extends Event

  final case class State(taskIdInProgress: Option[String])

  def apply(persistenceId: PersistenceId): Behavior[Command] =
    EventSourcedBehavior[Command, Event, State](
      persistenceId = persistenceId,
      emptyState = State(None),
      commandHandler = (state, command) => onCommand(state, command),
      eventHandler = (state, event) => applyEvent(state, event))
      .onPersistFailure(SupervisorStrategy.restartWithBackoff(1.second, 30.seconds, 0.2))

  private def onCommand(state: State, command: Command): Effect[Event, State] = {
    state.taskIdInProgress match {
      case None =>
        command match {
          case StartTask(taskId) =>
            Effect.persist(TaskStarted(taskId))
          case _ =>
            Effect.unhandled
        }

      case Some(inProgress) =>
        command match {
          case StartTask(taskId) =>
            if (inProgress == taskId)
              Effect.none // duplicate, already in progress
            else
              // other task in progress, wait with new task until later
              Effect.stash()

          case NextStep(taskId, instruction) =>
            if (inProgress == taskId)
              Effect.persist(TaskStep(taskId, instruction))
            else
              // other task in progress, wait with new task until later
              Effect.stash()

          case EndTask(taskId) =>
            if (inProgress == taskId)
              Effect.persist(TaskCompleted(taskId)).thenUnstashAll() // continue with next task
            else
              // other task in progress, wait with new task until later
              Effect.stash()
        }
    }
  }

  private def applyEvent(state: State, event: Event): State = {
    event match {
      case TaskStarted(taskId) => State(Option(taskId))
      case TaskStep(_, _)      => state
      case TaskCompleted(_)    => State(None)
    }
  }
}


Java




copy
source
public class TaskManager
    extends EventSourcedBehavior<TaskManager.Command, TaskManager.Event, TaskManager.State> {

  public interface Command {}

  public static final class StartTask implements Command {
    public final String taskId;

    public StartTask(String taskId) {
      this.taskId = taskId;
    }
  }

  public static final class NextStep implements Command {
    public final String taskId;
    public final String instruction;

    public NextStep(String taskId, String instruction) {
      this.taskId = taskId;
      this.instruction = instruction;
    }
  }

  public static final class EndTask implements Command {
    public final String taskId;

    public EndTask(String taskId) {
      this.taskId = taskId;
    }
  }

  public interface Event {}

  public static final class TaskStarted implements Event {
    public final String taskId;

    public TaskStarted(String taskId) {
      this.taskId = taskId;
    }
  }

  public static final class TaskStep implements Event {
    public final String taskId;
    public final String instruction;

    public TaskStep(String taskId, String instruction) {
      this.taskId = taskId;
      this.instruction = instruction;
    }
  }

  public static final class TaskCompleted implements Event {
    public final String taskId;

    public TaskCompleted(String taskId) {
      this.taskId = taskId;
    }
  }

  public static class State {
    public final Optional<String> taskIdInProgress;

    public State(Optional<String> taskIdInProgress) {
      this.taskIdInProgress = taskIdInProgress;
    }
  }

  public static Behavior<Command> create(PersistenceId persistenceId) {
    return new TaskManager(persistenceId);
  }

  public TaskManager(PersistenceId persistenceId) {
    super(
        persistenceId,
        SupervisorStrategy.restartWithBackoff(
            Duration.ofSeconds(1), Duration.ofSeconds(30), 0.2));
  }

  @Override
  public State emptyState() {
    return new State(Optional.empty());
  }

  @Override
  public CommandHandler<Command, Event, State> commandHandler() {
    return newCommandHandlerBuilder()
        .forAnyState()
        .onCommand(StartTask.class, this::onStartTask)
        .onCommand(NextStep.class, this::onNextStep)
        .onCommand(EndTask.class, this::onEndTask)
        .build();
  }

  private Effect<Event, State> onStartTask(State state, StartTask command) {
    if (state.taskIdInProgress.isPresent()) {
      if (state.taskIdInProgress.get().equals(command.taskId))
        return Effect().none(); // duplicate, already in progress
      else return Effect().stash(); // other task in progress, wait with new task until later
    } else {
      return Effect().persist(new TaskStarted(command.taskId));
    }
  }

  private Effect<Event, State> onNextStep(State state, NextStep command) {
    if (state.taskIdInProgress.isPresent()) {
      if (state.taskIdInProgress.get().equals(command.taskId))
        return Effect().persist(new TaskStep(command.taskId, command.instruction));
      else return Effect().stash(); // other task in progress, wait with new task until later
    } else {
      return Effect().unhandled();
    }
  }

  private Effect<Event, State> onEndTask(State state, EndTask command) {
    if (state.taskIdInProgress.isPresent()) {
      if (state.taskIdInProgress.get().equals(command.taskId))
        return Effect()
            .persist(new TaskCompleted(command.taskId))
            .thenUnstashAll(); // continue with next task
      else return Effect().stash(); // other task in progress, wait with new task until later
    } else {
      return Effect().unhandled();
    }
  }

  @Override
  public EventHandler<State, Event> eventHandler() {
    return newEventHandlerBuilder()
        .forAnyState()
        .onEvent(TaskStarted.class, (state, event) -> new State(Optional.of(event.taskId)))
        .onEvent(TaskStep.class, (state, event) -> state)
        .onEvent(TaskCompleted.class, (state, event) -> new State(Optional.empty()))
        .build();
  }
}




You should be careful to not send more messages to a persistent actor than it can keep up with, otherwise the stash buffer will fill up and when reaching its maximum capacity the commands will be dropped. The capacity can be configured with:


akka.persistence.typed.stash-capacity = 10000



To override the global config from above, use the following api to define a custom stash buffer capacity per entity:




Scala




copy
source
.withStashCapacity(100)


Java




copy
source
@Override
public Optional<Integer> stashCapacity() {
  return Optional.of(100);
}




Note that the stashed commands are kept in an in-memory buffer, so in case of a crash they will not be processed.




Stashed commands are discarded in case the actor (entity) is passivated or rebalanced by Cluster Sharding.


Stashed commands are discarded in case the actor is restarted (or stopped) due to a thrown exception while processing a command or side effect after persisting.


Stashed commands are preserved and processed later in case of a failure while storing events but only if an 
onPersistFailure
 backoff supervisor strategy is defined.




It’s allowed to stash messages while unstashing. Those newly added commands will not be processed by the 
unstashAll
unstashAll
 effect that was in progress and have to be unstashed by another 
unstashAll
.


Scaling out


In a use case where the number of persistent actors needed is higher than what would fit in the memory of one node or where resilience is important so that if a node crashes the persistent actors are quickly started on a new node and can resume operations 
Cluster Sharding
 is an excellent fit to spread persistent actors over a cluster and address them by id.


Akka Persistence is based on the single-writer principle. For a particular 
PersistenceId
PersistenceId
 only one 
EventSourcedBehavior
EventSourcedBehavior
 instance should be active at one time. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding ensures that there is only one active entity (
EventSourcedBehavior
) for each id within the cluster. 
Replicated Event Sourcing
 supports active-active persistent entities.


Configuration


There are several configuration properties for the persistence module, please refer to the 
reference configuration
.


The 
journal and snapshot store plugins
 have specific configuration, see reference documentation of the chosen plugin.


Example project


The 
Microservices with Akka tutorial
 contains a Shopping Cart sample illustrating how to use Event Sourcing and Projections together. The events are consumed by even processors to build other representations from the events, or publish the events to other services.


The 
Akka Distributed Cluster Guide
 illustrates how to use 
Replicated Event Sourcing
 that supports active-active persistent entities.














 
Persistence (Event Sourcing)






Replicated Event Sourcing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/coexisting.html
Coexistence • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence




Dependency


Introduction


Classic to typed


Typed to classic


Supervision




Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence




Dependency


Introduction


Classic to typed


Typed to classic


Supervision




Style guide


Learning Akka Typed from Classic




Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Coexistence


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Actor Typed, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


We believe Akka Typed will be adopted in existing systems gradually and therefore it’s important to be able to use typed and classic actors together, within the same 
ActorSystem
. Also, we will not be able to integrate with all existing modules in one big bang release and that is another reason for why these two ways of writing actors must be able to coexist.


There are two different 
ActorSystem
s: 
akka.actor.ActorSystem
akka.actor.ActorSystem
 and 
akka.actor.typed.ActorSystem
akka.actor.typed.ActorSystem
. 


Currently the typed actor system is implemented using the classic actor system under the hood. This may change in the future.


Typed and classic can interact the following ways:




classic actor systems can create typed actors


typed actors can send messages to classic actors, and opposite


spawn and supervise typed child from classic parent, and opposite


watch typed from classic, and opposite


classic actor system can be converted to a typed actor system




In the examples the 
akka.actor
 package is aliased to 
classic
.




Scala




copy
source
import akka.{ actor => classic }




The examples use fully qualified class names for the classic classes to distinguish between typed and classic classes with the same name.


Classic to typed


While coexisting your application will likely still have a classic ActorSystem. This can be converted to a typed ActorSystem so that new code and migrated parts don’t rely on the classic system:




Scala




copy
source
// adds support for actors to a classic actor system and context
import akka.actor.typed.scaladsl.adapter._

val system = akka.actor.ActorSystem("ClassicToTypedSystem")
val typedSystem: ActorSystem[Nothing] = system.toTyped


Java




copy
source
// In java use the static methods on Adapter to convert from typed to classic
import akka.actor.typed.javadsl.Adapter;
akka.actor.ActorSystem classicActorSystem = akka.actor.ActorSystem.create();
ActorSystem<Void> typedActorSystem = Adapter.toTyped(classicActorSystem);




Then for new typed actors here’s how you create, watch and send messages to it from a classic actor.




Scala




copy
source
object Typed {
  sealed trait Command
  final case class Ping(replyTo: ActorRef[Pong.type]) extends Command
  case object Pong

  def apply(): Behavior[Command] =
    Behaviors.receive { (context, message) =>
      message match {
        case Ping(replyTo) =>
          context.log.info(s"${context.self} got Ping from $replyTo")
          // replyTo is a classic actor that has been converted for coexistence
          replyTo ! Pong
          Behaviors.same
      }
    }
}


Java




copy
source
public abstract static class Typed {
  interface Command {}

  public static class Ping implements Command {
    public final akka.actor.typed.ActorRef<Pong> replyTo;

    public Ping(ActorRef<Pong> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Pong {}

  public static Behavior<Command> behavior() {
    return Behaviors.receive(Typed.Command.class)
        .onMessage(
            Typed.Ping.class,
            message -> {
              message.replyTo.tell(new Pong());
              return same();
            })
        .build();
  }
}




The top level classic actor is created in the usual way:




Scala




copy
source
val classicActor = system.actorOf(Classic.props())


Java




copy
source
akka.actor.ActorSystem as = akka.actor.ActorSystem.create();
akka.actor.ActorRef classic = as.actorOf(Classic.props());




Then it can create a typed actor, watch it, and send a message to it:




Scala




copy
source
class Classic extends classic.Actor with ActorLogging {
  // context.spawn is an implicit extension method
  val second: ActorRef[Typed.Command] =
    context.spawn(Typed(), "second")

  // context.watch is an implicit extension method
  context.watch(second)

  // self can be used as the `replyTo` parameter here because
  // there is an implicit conversion from akka.actor.ActorRef to
  // akka.actor.typed.ActorRef
  // An equal alternative would be `self.toTyped`
  second ! Typed.Ping(self)

  override def receive = {
    case Typed.Pong =>
      log.info(s"$self got Pong from ${sender()}")
      // context.stop is an implicit extension method
      context.stop(second)
    case classic.Terminated(ref) =>
      log.info(s"$self observed termination of $ref")
      context.stop(self)
  }
}


Java




copy
source
public static class Classic extends AbstractActor {
  public static akka.actor.Props props() {
    return akka.actor.Props.create(Classic.class);
  }

  private final akka.actor.typed.ActorRef<Typed.Command> second =
      Adapter.spawn(getContext(), Typed.behavior(), "second");

  @Override
  public void preStart() {
    Adapter.watch(getContext(), second);
    second.tell(new Typed.Ping(Adapter.toTyped(getSelf())));
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            Typed.Pong.class,
            message -> {
              Adapter.stop(getContext(), second);
            })
        .match(
            akka.actor.Terminated.class,
            t -> {
              getContext().stop(getSelf());
            })
        .build();
  }
}




There is one 
import
 that is needed to make that work.
 
We import the Adapter class and call static methods for conversion.




Scala




copy
source
// adds support for actors to a classic actor system and context
import akka.actor.typed.scaladsl.adapter._


Java




copy
source
// In java use the static methods on Adapter to convert from typed to classic
import akka.actor.typed.javadsl.Adapter;




That adds some implicit extension methods that are added to classic and typed 
ActorSystem
, 
ActorContext
 and 
ActorRef
 in both directions.
 
To convert between typed and classic 
ActorSystem
, 
ActorContext
 and 
ActorRef
 in both directions there are adapter methods in 
akka.actor.typed.javadsl.Adapter
.
 Note the inline comments in the example above. 


This method of using a top level classic actor is the suggested path for this type of co-existence. However, if you prefer to start with a typed top level actor then you can use the 
implicit 
spawn
 -method
Adapter.spawn
 directly from the typed system:




Scala




copy
source
val system = classic.ActorSystem("TypedWatchingClassic")
val typed = system.spawn(Typed.behavior, "Typed")


Java




copy
source
ActorSystem as = ActorSystem.create();
ActorRef<Typed.Command> typed = Adapter.spawn(as, Typed.create(), "Typed");




The above classic-typed difference is further elaborated in 
the 
ActorSystem
 section
 of “Learning Akka Typed from Classic”. 


Typed to classic


Let’s turn the example upside down and first start the typed actor and then the classic as a child.


The following will show how to create, watch and send messages back and forth from a typed actor to this classic actor:




Scala




copy
source
object Classic {
  def props(): classic.Props = classic.Props(new Classic)
}
class Classic extends classic.Actor {
  override def receive = {
    case Typed.Ping(replyTo) =>
      replyTo ! Typed.Pong
  }
}


Java




copy
source
public static class Classic extends AbstractActor {
  public static akka.actor.Props props() {
    return akka.actor.Props.create(Classic.class);
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder().match(Typed.Ping.class, this::onPing).build();
  }

  private void onPing(Typed.Ping message) {
    message.replyTo.tell(Typed.Pong.INSTANCE);
  }
}






Creating the actor system and the typed actor:




Scala




copy
source
val system = classic.ActorSystem("TypedWatchingClassic")
val typed = system.spawn(Typed.behavior, "Typed")


Java




copy
source
ActorSystem as = ActorSystem.create();
ActorRef<Typed.Command> typed = Adapter.spawn(as, Typed.create(), "Typed");




Then the typed actor creates the classic actor, watches it and sends and receives a response:




Scala




copy
source
object Typed {
  final case class Ping(replyTo: akka.actor.typed.ActorRef[Pong.type])
  sealed trait Command
  case object Pong extends Command

  val behavior: Behavior[Command] =
    Behaviors.setup { context =>
      // context.actorOf is an implicit extension method
      val classic = context.actorOf(Classic.props(), "second")

      // context.watch is an implicit extension method
      context.watch(classic)

      // illustrating how to pass sender, toClassic is an implicit extension method
      classic.tell(Typed.Ping(context.self), context.self.toClassic)

      Behaviors
        .receiveMessagePartial[Command] {
          case Pong =>
            // it's not possible to get the sender, that must be sent in message
            // context.stop is an implicit extension method
            context.stop(classic)
            Behaviors.same
        }
        .receiveSignal {
          case (_, akka.actor.typed.Terminated(_)) =>
            Behaviors.stopped
        }
    }
}


Java




copy
source
public static class Typed extends AbstractBehavior<Typed.Command> {

  public static class Ping {
    public final akka.actor.typed.ActorRef<Pong> replyTo;

    public Ping(ActorRef<Pong> replyTo) {
      this.replyTo = replyTo;
    }
  }

  interface Command {}

  public enum Pong implements Command {
    INSTANCE
  }

  private final akka.actor.ActorRef second;

  private Typed(ActorContext<Command> context, akka.actor.ActorRef second) {
    super(context);
    this.second = second;
  }

  public static Behavior<Command> create() {
    return akka.actor.typed.javadsl.Behaviors.setup(
        context -> {
          akka.actor.ActorRef second = Adapter.actorOf(context, Classic.props(), "second");

          Adapter.watch(context, second);

          second.tell(
              new Typed.Ping(context.getSelf().narrow()), Adapter.toClassic(context.getSelf()));

          return new Typed(context, second);
        });
  }

  @Override
  public Receive<Command> createReceive() {
    return newReceiveBuilder()
        .onMessage(Typed.Pong.class, message -> onPong())
        .onSignal(akka.actor.typed.Terminated.class, sig -> Behaviors.stopped())
        .build();
  }

  private Behavior<Command> onPong() {
    Adapter.stop(getContext(), second);
    return this;
  }
}




Note that when sending from a typed actor to a classic 
ActorRef
ActorRef
 there is no sender in scope as in classic. The typed sender should use its own 
ActorContext[T].self
 explicitly, as shown in the snippet.


Supervision


The default supervision for classic actors is to restart whereas for typed it is to stop. When combining classic and typed actors the default supervision is based on the default behavior of the child, for example if a classic actor creates a typed child, its default supervision will be to stop. If a typed actor creates a classic child, its default supervision will be to restart.














 
Synchronous behavior testing






Style guide 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/index.html#reporting-vulnerabilities
Security Announcements • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Security Announcements
Note


Security announcements has moved to a shared page for all Akka projects and can now be found at 
akka.io/security


Receiving Security Advisories


The best way to receive any and all security announcements is to subscribe to the 
Akka security list
.


The mailing list is very low traffic, and receives notifications only after security reports have been managed by the core team and fixes are publicly available.


Reporting Vulnerabilities


We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.


Following best practice, we strongly encourage anyone to report potential security vulnerabilities to 
[email protected]
 before disclosing them in a public forum like the mailing list or as a GitHub issue.


Reports to this email address will be handled by our security team, who will work together with you to ensure that a fix can be provided without delay.


Security Related Documentation




Java Serialization


Remote deployment allow list


Remote Security




Fixed Security Vulnerabilities






Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


















 
Akka Documentation






Java Serialization, Fixed in Akka 2.4.17 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-dynamic.html
Dynamic stream handling • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling




Dependency


Introduction


Controlling stream completion with KillSwitch


Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub




Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling




Dependency


Introduction


Controlling stream completion with KillSwitch


Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub




Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Dynamic stream handling


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction




Controlling stream completion with KillSwitch


A 
KillSwitch
KillSwitch
 allows the completion of operators of 
FlowShape
FlowShape
 from the outside. It consists of a flow element that can be linked to an operator of 
FlowShape
 needing completion control. The 
KillSwitch
 
trait
 
interface
 allows to:




complete the stream(s) via 
shutdown()
shutdown()


fail the stream(s) via 
abort(Throwable error)
abort(Throwable error)






Scala




copy
source
trait KillSwitch {

  /**
   * After calling [[KillSwitch#shutdown]] the linked [[Graph]]s of [[FlowShape]] are completed normally.
   */
  def shutdown(): Unit

  /**
   * After calling [[KillSwitch#abort]] the linked [[Graph]]s of [[FlowShape]] are failed.
   */
  def abort(ex: Throwable): Unit
}




After the first call to either 
shutdown
 or 
abort
, all subsequent calls to any of these methods will be ignored. Stream completion is performed by both




cancelling its upstream.


completing (in case of 
shutdown
) or failing (in case of 
abort
) its downstream




A 
KillSwitch
 can control the completion of one or multiple streams, and therefore comes in two different flavours.




UniqueKillSwitch


UniqueKillSwitch
UniqueKillSwitch
 allows to control the completion of 
one
 materialized 
Graph
Graph
 of 
FlowShape
FlowShape
. Refer to the below for usage examples.




Shutdown






Scala




copy
source
val countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)
val lastSnk = Sink.last[Int]

val (killSwitch, last) = countingSrc
  .viaMat(KillSwitches.single)(Keep.right)
  .toMat(lastSnk)(Keep.both)
  .run()

doSomethingElse()

killSwitch.shutdown()

Await.result(last, 1.second) shouldBe 2


Java




copy
source
final Source<Integer, NotUsed> countingSrc =
    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))
        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());
final Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();

final Pair<UniqueKillSwitch, CompletionStage<Integer>> stream =
    countingSrc
        .viaMat(KillSwitches.single(), Keep.right())
        .toMat(lastSnk, Keep.both())
        .run(system);

final UniqueKillSwitch killSwitch = stream.first();
final CompletionStage<Integer> completionStage = stream.second();

doSomethingElse();
killSwitch.shutdown();

final int finalCount = completionStage.toCompletableFuture().get(1, TimeUnit.SECONDS);
assertEquals(2, finalCount);






Abort






Scala




copy
source
val countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)
val lastSnk = Sink.last[Int]

val (killSwitch, last) = countingSrc
  .viaMat(KillSwitches.single)(Keep.right)
  .toMat(lastSnk)(Keep.both).run()

val error = new RuntimeException("boom!")
killSwitch.abort(error)

Await.result(last.failed, 1.second) shouldBe error


Java




copy
source
final Source<Integer, NotUsed> countingSrc =
    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))
        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());
final Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();

final Pair<UniqueKillSwitch, CompletionStage<Integer>> stream =
    countingSrc
        .viaMat(KillSwitches.single(), Keep.right())
        .toMat(lastSnk, Keep.both())
        .run(system);

final UniqueKillSwitch killSwitch = stream.first();
final CompletionStage<Integer> completionStage = stream.second();

final Exception error = new Exception("boom!");
killSwitch.abort(error);

final int result =
    completionStage.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);
assertEquals(-1, result);






SharedKillSwitch


A 
SharedKillSwitch
SharedKillSwitch
 allows to control the completion of an arbitrary number operators of 
FlowShape
FlowShape
. It can be materialized multiple times via its 
flow
flow
 method, and all materialized operators linked to it are controlled by the switch. Refer to the below for usage examples.




Shutdown






Scala




copy
source
val countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)
val lastSnk = Sink.last[Int]
val sharedKillSwitch = KillSwitches.shared("my-kill-switch")

val last = countingSrc
  .via(sharedKillSwitch.flow)
  .runWith(lastSnk)

val delayedLast = countingSrc
  .delay(1.second, DelayOverflowStrategy.backpressure)
  .via(sharedKillSwitch.flow)
  .runWith(lastSnk)

doSomethingElse()

sharedKillSwitch.shutdown()

Await.result(last, 1.second) shouldBe 2
Await.result(delayedLast, 1.second) shouldBe 1


Java




copy
source
final Source<Integer, NotUsed> countingSrc =
    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))
        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());
final Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();
final SharedKillSwitch killSwitch = KillSwitches.shared("my-kill-switch");

final CompletionStage<Integer> completionStage =
    countingSrc
        .viaMat(killSwitch.flow(), Keep.right())
        .toMat(lastSnk, Keep.right())
        .run(system);
final CompletionStage<Integer> completionStageDelayed =
    countingSrc
        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure())
        .viaMat(killSwitch.flow(), Keep.right())
        .toMat(lastSnk, Keep.right())
        .run(system);

doSomethingElse();
killSwitch.shutdown();

final int finalCount = completionStage.toCompletableFuture().get(1, TimeUnit.SECONDS);
final int finalCountDelayed =
    completionStageDelayed.toCompletableFuture().get(1, TimeUnit.SECONDS);

assertEquals(2, finalCount);
assertEquals(1, finalCountDelayed);






Abort






Scala




copy
source
val countingSrc = Source(Stream.from(1)).delay(1.second)
val lastSnk = Sink.last[Int]
val sharedKillSwitch = KillSwitches.shared("my-kill-switch")

val last1 = countingSrc.via(sharedKillSwitch.flow).runWith(lastSnk)
val last2 = countingSrc.via(sharedKillSwitch.flow).runWith(lastSnk)

val error = new RuntimeException("boom!")
sharedKillSwitch.abort(error)

Await.result(last1.failed, 1.second) shouldBe error
Await.result(last2.failed, 1.second) shouldBe error


Java




copy
source
final Source<Integer, NotUsed> countingSrc =
    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))
        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());
final Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();
final SharedKillSwitch killSwitch = KillSwitches.shared("my-kill-switch");

final CompletionStage<Integer> completionStage1 =
    countingSrc
        .viaMat(killSwitch.flow(), Keep.right())
        .toMat(lastSnk, Keep.right())
        .run(system);
final CompletionStage<Integer> completionStage2 =
    countingSrc
        .viaMat(killSwitch.flow(), Keep.right())
        .toMat(lastSnk, Keep.right())
        .run(system);

final Exception error = new Exception("boom!");
killSwitch.abort(error);

final int result1 =
    completionStage1.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);
final int result2 =
    completionStage2.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);

assertEquals(-1, result1);
assertEquals(-1, result2);


Note


A 
UniqueKillSwitch
UniqueKillSwitch
 is always a result of a materialization, whilst 
SharedKillSwitch
SharedKillSwitch
 needs to be constructed before any materialization takes place.


Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub


There are many cases when consumers or producers of a certain service (represented as a Sink, Source, or possibly Flow) are dynamic and not known in advance. The Graph DSL does not allow to represent this, all connections of the graph must be known in advance and must be connected upfront. To allow dynamic fan-in and fan-out streaming, the Hubs should be used. They provide means to construct 
Sink
Sink
 and 
Source
Source
 pairs that are “attached” to each other, but one of them can be materialized multiple times to implement dynamic fan-in or fan-out.


Using the MergeHub


A 
MergeHub
MergeHub
 allows to implement a dynamic fan-in junction point in a graph where elements coming from different producers are emitted in a First-Comes-First-Served fashion. If the consumer cannot keep up then 
all
 of the producers are backpressured. The hub itself comes as a 
Source
Source
 to which the single consumer can be attached. It is not possible to attach any producers until this 
Source
 has been materialized (started). This is ensured by the fact that we only get the corresponding 
Sink
Sink
 as a materialized value. Usage might look like this:




Scala




copy
source
// A simple consumer that will print to the console for now
val consumer = Sink.foreach(println)

// Attach a MergeHub Source to the consumer. This will materialize to a
// corresponding Sink.
val runnableGraph: RunnableGraph[Sink[String, NotUsed]] =
  MergeHub.source[String](perProducerBufferSize = 16).to(consumer)

// By running/materializing the consumer we get back a Sink, and hence
// now have access to feed elements into it. This Sink can be materialized
// any number of times, and every element that enters the Sink will
// be consumed by our consumer.
val toConsumer: Sink[String, NotUsed] = runnableGraph.run()

// Feeding two independent sources into the hub.
Source.single("Hello!").runWith(toConsumer)
Source.single("Hub!").runWith(toConsumer)


Java




copy
source
// A simple consumer that will print to the console for now
Sink<String, CompletionStage<Done>> consumer = Sink.foreach(System.out::println);

// Attach a MergeHub Source to the consumer. This will materialize to a
// corresponding Sink.
RunnableGraph<Sink<String, NotUsed>> runnableGraph = MergeHub.of(String.class, 16).to(consumer);

// By running/materializing the consumer we get back a Sink, and hence
// now have access to feed elements into it. This Sink can be materialized
// any number of times, and every element that enters the Sink will
// be consumed by our consumer.
Sink<String, NotUsed> toConsumer = runnableGraph.run(system);

Source.single("Hello!").runWith(toConsumer, system);
Source.single("Hub!").runWith(toConsumer, system);




This sequence, while might look odd at first, ensures proper startup order. Once we get the 
Sink
, we can use it as many times as wanted. Everything that is fed to it will be delivered to the consumer we attached previously until it cancels.


Using the BroadcastHub


A 
BroadcastHub
BroadcastHub
 can be used to consume elements from a common producer by a dynamic set of consumers. The rate of the producer will be automatically adapted to the slowest consumer. In this case, the hub is a 
Sink
Sink
 to which the single producer must be attached first. Consumers can only be attached once the 
Sink
 has been materialized (i.e. the producer has been started). One example of using the 
BroadcastHub
:




Scala




copy
source
// A simple producer that publishes a new "message" every second
val producer = Source.tick(1.second, 1.second, "New message")

// Attach a BroadcastHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
val runnableGraph: RunnableGraph[Source[String, NotUsed]] =
  producer.toMat(BroadcastHub.sink(bufferSize = 256))(Keep.right)

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
val fromProducer: Source[String, NotUsed] = runnableGraph.run()

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg => println("consumer1: " + msg))
fromProducer.runForeach(msg => println("consumer2: " + msg))


Java




copy
source
// A simple producer that publishes a new "message" every second
Source<String, Cancellable> producer =
    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), "New message");

// Attach a BroadcastHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
RunnableGraph<Source<String, NotUsed>> runnableGraph =
    producer.toMat(BroadcastHub.of(String.class, 256), Keep.right());

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
Source<String, NotUsed> fromProducer = runnableGraph.run(materializer);

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg -> System.out.println("consumer1: " + msg), materializer);
fromProducer.runForeach(msg -> System.out.println("consumer2: " + msg), materializer);




The resulting 
Source
Source
 can be materialized any number of times, each materialization effectively attaching a new subscriber. If there are no subscribers attached to this hub then it will not drop any elements but instead backpressure the upstream producer until subscribers arrive. This behavior can be tweaked by using the operators 
.buffer
.buffer
 for example with a drop strategy, or attaching a subscriber that drops all messages. If there are no other subscribers, this will ensure that the producer is kept drained (dropping all elements) and once a new subscriber arrives it will adaptively slow down, ensuring no more messages are dropped.


Combining dynamic operators to build a simple Publish-Subscribe service


The features provided by the Hub implementations are limited by default. This is by design, as various combinations can be used to express additional features like unsubscribing producers or consumers externally. We show here an example that builds a 
Flow
Flow
 representing a publish-subscribe channel. The input of the 
Flow
 is published to all subscribers while the output streams all the elements published.


First, we connect a 
MergeHub
MergeHub
 and a 
BroadcastHub
BroadcastHub
 together to form a publish-subscribe channel. Once we materialize this small stream, we get back a pair of 
Source
Source
 and 
Sink
Sink
 that together define the publish and subscribe sides of our channel.




Scala




copy
source
// Obtain a Sink and Source which will publish and receive from the "bus" respectively.
val (sink, source) =
  MergeHub.source[String](perProducerBufferSize = 16).toMat(BroadcastHub.sink(bufferSize = 256))(Keep.both).run()


Java




copy
source
// Obtain a Sink and Source which will publish and receive from the "bus" respectively.
Pair<Sink<String, NotUsed>, Source<String, NotUsed>> sinkAndSource =
    MergeHub.of(String.class, 16)
        .toMat(BroadcastHub.of(String.class, 256), Keep.both())
        .run(system);

Sink<String, NotUsed> sink = sinkAndSource.first();
Source<String, NotUsed> source = sinkAndSource.second();




We now use a few tricks to add more features. First of all, we attach a 
Sink.ignore
Sink.ignore
 at the broadcast side of the channel to keep it drained when there are no subscribers. If this behavior is not the desired one this line can be dropped.




Scala




copy
source
// Ensure that the Broadcast output is dropped if there are no listening parties.
// If this dropping Sink is not attached, then the broadcast hub will not drop any
// elements itself when there are no subscribers, backpressuring the producer instead.
source.runWith(Sink.ignore)


Java




copy
source
// Ensure that the Broadcast output is dropped if there are no listening parties.
// If this dropping Sink is not attached, then the broadcast hub will not drop any
// elements itself when there are no subscribers, backpressuring the producer instead.
source.runWith(Sink.ignore(), system);




We now wrap the 
Sink
Sink
 and 
Source
Source
 in a 
Flow
Flow
 using 
Flow.fromSinkAndSource
Flow.fromSinkAndSource
. This bundles up the two sides of the channel into one and forces users of it to always define a publisher and subscriber side (even if the subscriber side is dropping). It also allows us to attach a 
KillSwitch
KillSwitch
 as a 
BidiStage
 which in turn makes it possible to close both the original 
Sink
 and 
Source
 at the same time. Finally, we add 
backpressureTimeout
 on the consumer side to ensure that subscribers that block the channel for more than 3 seconds are forcefully removed (and their stream failed).




Scala




copy
source
// We create now a Flow that represents a publish-subscribe channel using the above
// started stream as its "topic". We add two more features, external cancellation of
// the registration and automatic cleanup for very slow subscribers.
val busFlow: Flow[String, String, UniqueKillSwitch] =
  Flow
    .fromSinkAndSource(sink, source)
    .joinMat(KillSwitches.singleBidi[String, String])(Keep.right)
    .backpressureTimeout(3.seconds)


Java




copy
source
// We create now a Flow that represents a publish-subscribe channel using the above
// started stream as its "topic". We add two more features, external cancellation of
// the registration and automatic cleanup for very slow subscribers.
Flow<String, String, UniqueKillSwitch> busFlow =
    Flow.fromSinkAndSource(sink, source)
        .joinMat(KillSwitches.singleBidi(), Keep.right())
        .backpressureTimeout(Duration.ofSeconds(1));




The resulting Flow now has a type of 
Flow[String, String, UniqueKillSwitch]
 representing a publish-subscribe channel which can be used any number of times to attach new producers or consumers. In addition, it materializes to a 
UniqueKillSwitch
 (see 
UniqueKillSwitch
) that can be used to deregister a single user externally:




Scala




copy
source
val switch: UniqueKillSwitch =
  Source.repeat("Hello world!").viaMat(busFlow)(Keep.right).to(Sink.foreach(println)).run()

// Shut down externally
switch.shutdown()


Java




copy
source
UniqueKillSwitch killSwitch =
    Source.repeat("Hello World!")
        .viaMat(busFlow, Keep.right())
        .to(Sink.foreach(System.out::println))
        .run(system);

// Shut down externally
killSwitch.shutdown();




Using the PartitionHub


This is a 
may change
 feature
*


A 
PartitionHub
PartitionHub
 can be used to route elements from a common producer to a dynamic set of consumers. The selection of consumer is done with a function. Each element can be routed to only one consumer. 


The rate of the producer will be automatically adapted to the slowest consumer. In this case, the hub is a 
Sink
Sink
 to which the single producer must be attached first. Consumers can only be attached once the 
Sink
 has been materialized (i.e. the producer has been started). One example of using the 
PartitionHub
:




Scala




copy
source
// A simple producer that publishes a new "message-" every second
val producer = Source.tick(1.second, 1.second, "message").zipWith(Source(1 to 100))((a, b) => s"$a-$b")

// Attach a PartitionHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
val runnableGraph: RunnableGraph[Source[String, NotUsed]] =
  producer.toMat(
    PartitionHub.sink(
      (size, elem) => math.abs(elem.hashCode % size),
      startAfterNrOfConsumers = 2,
      bufferSize = 256))(Keep.right)

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
val fromProducer: Source[String, NotUsed] = runnableGraph.run()

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg => println("consumer1: " + msg))
fromProducer.runForeach(msg => println("consumer2: " + msg))


Java




copy
source
// A simple producer that publishes a new "message-n" every second
Source<String, Cancellable> producer =
    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), "message")
        .zipWith(Source.range(0, 100), (a, b) -> a + "-" + b);

// Attach a PartitionHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
RunnableGraph<Source<String, NotUsed>> runnableGraph =
    producer.toMat(
        PartitionHub.of(String.class, (size, elem) -> Math.abs(elem.hashCode() % size), 2, 256),
        Keep.right());

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
Source<String, NotUsed> fromProducer = runnableGraph.run(materializer);

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg -> System.out.println("consumer1: " + msg), materializer);
fromProducer.runForeach(msg -> System.out.println("consumer2: " + msg), materializer);




The 
partitioner
 function takes two parameters; the first is the number of active consumers and the second is the stream element. The function should return the index of the selected consumer for the given element, i.e. 
int
 greater than or equal to 0 and less than number of consumers.


The resulting 
Source
Source
 can be materialized any number of times, each materialization effectively attaching a new consumer. If there are no consumers attached to this hub then it will not drop any elements but instead backpressure the upstream producer until consumers arrive. This behavior can be tweaked by using an operator, for example 
.buffer
.buffer
 with a drop strategy, or attaching a consumer that drops all messages. If there are no other consumers, this will ensure that the producer is kept drained (dropping all elements) and once a new consumer arrives and messages are routed to the new consumer it will adaptively slow down, ensuring no more messages are dropped.


It is possible to define how many initial consumers that are required before it starts emitting any messages to the attached consumers. While not enough consumers have been attached messages are buffered and when the buffer is full the upstream producer is backpressured. No messages are dropped.


The above example illustrate a stateless partition function. For more advanced stateful routing the 
ofStateful
 
statefulSink
 can be used. Here is an example of a stateful round-robin function:




Scala




copy
source
// A simple producer that publishes a new "message-" every second
val producer = Source.tick(1.second, 1.second, "message").zipWith(Source(1 to 100))((a, b) => s"$a-$b")

// New instance of the partitioner function and its state is created
// for each materialization of the PartitionHub.
def roundRobin(): (PartitionHub.ConsumerInfo, String) => Long = {
  var i = -1L

  (info, elem) => {
    i += 1
    info.consumerIdByIdx((i % info.size).toInt)
  }
}

// Attach a PartitionHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
val runnableGraph: RunnableGraph[Source[String, NotUsed]] =
  producer.toMat(PartitionHub.statefulSink(() => roundRobin(), startAfterNrOfConsumers = 2, bufferSize = 256))(
    Keep.right)

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
val fromProducer: Source[String, NotUsed] = runnableGraph.run()

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg => println("consumer1: " + msg))
fromProducer.runForeach(msg => println("consumer2: " + msg))


Java




copy
source
// A simple producer that publishes a new "message-n" every second
Source<String, Cancellable> producer =
    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), "message")
        .zipWith(Source.range(0, 100), (a, b) -> a + "-" + b);

// Attach a PartitionHub Sink to the producer. This will materialize to a
// corresponding Source.
// (We need to use toMat and Keep.right since by default the materialized
// value to the left is used)
RunnableGraph<Source<String, NotUsed>> runnableGraph =
    producer.toMat(
        PartitionHub.ofStateful(String.class, () -> new RoundRobin<String>(), 2, 256),
        Keep.right());

// By running/materializing the producer, we get back a Source, which
// gives us access to the elements published by the producer.
Source<String, NotUsed> fromProducer = runnableGraph.run(materializer);

// Print out messages from the producer in two independent consumers
fromProducer.runForeach(msg -> System.out.println("consumer1: " + msg), materializer);
fromProducer.runForeach(msg -> System.out.println("consumer2: " + msg), materializer);




Note that it is a factory of a function to be able to hold stateful variables that are unique for each materialization. 
In this example the 
partitioner
 function is implemented as a class to be able to hold the mutable variable. A new instance of 
RoundRobin
 is created for each materialization of the hub.


copy
source
// Using a class since variable must otherwise be final.
// New instance is created for each materialization of the PartitionHub.
static class RoundRobin<T> implements ToLongBiFunction<ConsumerInfo, T> {

  private long i = -1;

  @Override
  public long applyAsLong(ConsumerInfo info, T elem) {
    i++;
    return info.consumerIdByIdx((int) (i % info.size()));
  }
}


The function takes two parameters; the first is information about active consumers, including an array of consumer identifiers and the second is the stream element. The function should return the selected consumer identifier for the given element. The function will never be called when there are no active consumers, i.e. there is always at least one element in the array of identifiers.


Another interesting type of routing is to prefer routing to the fastest consumers. The 
ConsumerInfo
ConsumerInfo
 has an accessor 
queueSize
 that is approximate number of buffered elements for a consumer. Larger value than other consumers could be an indication of that the consumer is slow. Note that this is a moving target since the elements are consumed concurrently. Here is an example of a hub that routes to the consumer with least buffered elements:




Scala




copy
source
val producer = Source(0 until 100)

// ConsumerInfo.queueSize is the approximate number of buffered elements for a consumer.
// Note that this is a moving target since the elements are consumed concurrently.
val runnableGraph: RunnableGraph[Source[Int, NotUsed]] =
  producer.toMat(
    PartitionHub.statefulSink(
      () => (info, elem) => info.consumerIds.minBy(id => info.queueSize(id)),
      startAfterNrOfConsumers = 2,
      bufferSize = 16))(Keep.right)

val fromProducer: Source[Int, NotUsed] = runnableGraph.run()

fromProducer.runForeach(msg => println("consumer1: " + msg))
fromProducer.throttle(10, 100.millis).runForeach(msg => println("consumer2: " + msg))


Java




copy
source
Source<Integer, NotUsed> producer = Source.range(0, 100);

// ConsumerInfo.queueSize is the approximate number of buffered elements for a consumer.
// Note that this is a moving target since the elements are consumed concurrently.
RunnableGraph<Source<Integer, NotUsed>> runnableGraph =
    producer.toMat(
        PartitionHub.ofStateful(
            Integer.class,
            () ->
                (info, elem) -> {
                  final List<Object> ids = info.getConsumerIds();
                  int minValue = info.queueSize(0);
                  long fastest = info.consumerIdByIdx(0);
                  for (int i = 1; i < ids.size(); i++) {
                    int value = info.queueSize(i);
                    if (value < minValue) {
                      minValue = value;
                      fastest = info.consumerIdByIdx(i);
                    }
                  }
                  return fastest;
                },
            2,
            8),
        Keep.right());

Source<Integer, NotUsed> fromProducer = runnableGraph.run(materializer);

fromProducer.runForeach(msg -> System.out.println("consumer1: " + msg), materializer);
fromProducer
    .throttle(10, Duration.ofMillis(100))
    .runForeach(msg -> System.out.println("consumer2: " + msg), materializer);
















 
Context Propagation






Custom stream processing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/legal/privacy
Privacy Policy


























































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



































































          Akka Privacy Policy
        






Lightbend Inc, d.b.a Akka
October 22, 2024






































Introduction


In this policy, we describe in summary the personal information we collect and how we use it, share it, and protect it.


The details on how we handle your personal information are enumerated in the related policies.


Scope


This policy applies to all personally identifiable information (PII) collected by Akka, except for that information related to Employees, Contractors and Applicants, which are covered under their own policies.


Definitions


Please see the 
Policy Definitions
 document distributed with this policy for the definition of terms and abbreviations


Referenced
 
Policies




Terms of Use


Personal Information Handling Policy




1. Policy


1.1 Summary




Akka respects privacy on all its websites and services.


We collect information from you including identifiers, customer records, usage data, professional affiliations, and build profiles.


No spam promise; we only send you operational, security, and transaction-related emails (unless you opt-in to our newsletter).


We share data with affiliated service providers and resellers and also if we have your consent or during corporate transactions.


We use cookies for session info and visitor identification.


We take strong measures to protect your data from unauthorized access.


Data is retained as necessary for its collected purpose.


We comply with international standards and privacy regulations.




Akka, Inc. is committed to respecting your privacy on all of our websites and the products and services offered on these websites.


In this policy, we describe the personal information we collect and how we use it, share it, and protect it.


1.2 Categories Of Personal Information Collected


We collect the following types of personal information:




Identifiers:
 Includes direct identifiers, such as name, alias user ID, username, account number or unique personal identifier; email address, phone number, address and other contact information; IP address and other online identifiers.


Customer Records:
 Includes personal information, such as name, account name, user ID, contact information, education and employment information, payment information that individuals provide to us in order to purchase or obtain our products and services.


Commercial Information:
 Includes records of products or services purchased, obtained, or considered, or other purchasing or use histories or tendencies.


Audio, Video And Electronic Data:
 Includes audio, electronic, visual, thermal, olfactory, or similar information such as photographs and images (e.g., that you provide us) and call recordings (e.g., of customer support calls).


Usage Data:
 Includes browsing history, clickstream data, search history, access logs and other usage data and information regarding an individual’s interaction with our websites, mobile apps and other services, and our marketing messages and online ads.


Professional:
 Includes professional and employment-related information (such as current and former employer(s) and position(s), business contact information and professional memberships).


Education:
 Information about an individual’s educational history (such as the schools attended, degrees you were awarded, and associated dates).


Inferences:
 Includes inferences drawn from other personal information that we collect to create a profile reflecting an individual’s preferences, characteristics, predispositions, behavior, attitudes, abilities or aptitudes.


Information You Provide To Akka:
 We collect personal data that you choose to provide to us when you are using our sites and services. Personal data may include your name, email address, state and country of residence, job title and company name. As part of the services, we may collect information about the courses you complete and any accreditations you receive. We may also collect personal data from your communications with us, including through email or forms on our sites, such as your phone number, and address. When engaging with our sales teams, we may collect your contact information, as well as information about your previous purchases and sales activity.


Information Automatically Collected:
 Whenever you visit or interact with the sites, we automatically collect data about your visit. Such information includes your computer's Internet Protocol ("IP") address, device ID, device location information based on IP address, browser type, browser version, the pages you visit, the time and date of your visit, the time spent on those pages and other statistics.




1.3 No Spam Promise


We hate spam. You hate spam.


We don’t send promotional messages or bulk email messages to you unless you have explicitly opted-in to our newsletter.


We crafted this policy to quell the frustration  with  spam and to better comply with US Federal Law, the CAN-SPAM Act of 2003, California State Law SB186, and Directives 2000/31/EC and 2002/58/EC of the European Parliament and of the Council, among others.


You will only receive email by an  email address provided during a sign-up registration procedure or an email address provided voluntarily when filling out a newsletter or contact form.


We will contact you via email for business correspondence intended to inform customers of service, billing or business related issues. Also, to comply with several international information security standards including NIST CSF and the EU CRA, we are required to provide security and operational notifications to you.


Receipt emails or messages referring to transactions executed on our sites are NOT considered spam, since they are meant to confirm the status of a transaction for customers.


1.4 Who We Share Your Personal Data With


We do not share your personal data other than:




Affiliates, Service Providers And Processors:
  We engage third party companies, individuals and affiliated entities to facilitate our sites and services and to provide the sites and services on our behalf, to perform related services, to assist us in analyzing how our sites and services are used, and to provide data storage and other services. These third parties have access to your personal data only to perform specific tasks on our behalf and are obligated to 
not
 retain, disclose, sell or use your personal data for any other purpose.


Resellers
: We engage third party value added service providers and their affiliates to facilitate execution of business transactions. These third parties have access to data collected about you and your entity only to perform specific tasks on our behalf are obligated to not retain, disclose, sell or use your personal data for any other purpose.


Third Parties In Case Of Legal Requirement:
 We may share your personal data with third parties if we are legally required to do so. This includes situations where we need to comply with legal processes, assist law enforcement in investigating fraud or legal violations, respond to claims against us, or protect the rights, property, or safety of Akka, our customers, or the public.


Third Parties With Consent:
 We will also disclose information about you, including personal data, to any other third parties, where you have expressly consented or requested that we do so.


Third Parties In Case Of A Corporate Transaction:
  In addition, your personal data may be disclosed as part of any merger, sale, reorganization, transfer of Akka's assets or businesses, acquisition, bankruptcy, or similar event.




1.5 Cookie Policy


We use so-called “Cookies” and other similar technical means to track repeat visits to our sites and services in order to maintain session information, identify repeat visitors, and for other purposes relating to the delivery of our services.


See our Terms of Use for details.


1.6 Security


In order to protect your personal data we have implemented reasonable, commercially acceptable security procedures and practices appropriate to the nature of the personal data we store, in order to protect it from unauthorized access, destruction, use, modification or disclosure. Your personal data is contained behind secured networks and is only accessible by a limited number of people, who have special access rights and are required to keep the personal data confidential. Please see our 
Trust Center
 for information about our Information Security standards and policies. We require all third-parties with whom we share your data for processing to have equivalent security measures to our own.


1.7 Purpose and Legal Basis for Processing


We use personal information for various business purposes, including providing and improving our services, marketing, to provide customer support, and compliance with legal obligations.


1.8 Retention


We will retain your personal data for as long as necessary to fulfill the purposes we collected it for, including for the purposes of satisfying any legal, accounting, or reporting requirements.


1.9 Effective Date and Updates


This policy is affective as of August 1st, 2024. We may update this policy from time-to-time, and you are welcome to subscribe on our 
Trust Center
 to be informed of such updates.


1.10 Contact and Request


We are Akka (Lightbend, Inc. d.b.a Akka), located at 580 California, #1231, San Francisco, CA 94104.


You can contact us via email at 
security@akka.io
 or by phone at +1 877 989-7372, or our website at 
https://www.akka.io
 to make requests about your personal information or any other privacy matters.


2. Supported Frameworks


Below is the required language for the privacy regulations and standards we comply with


2.1 EU And UK General Data Protection Regulation


Identity And Contact Details Of The Data Controller


Our Data Protection Officer for the EU can be reached at 
privacy_eu@akka.io
.


Our Data Protection Officer for the UK can be reached at 
privacy_uk@akka.io
.


For the UK,  any complaints or for further information, you can contact the Information Commissioner's Office (ICO) in the UK at the 
ICO Official Website
, the Helpline 0303 123 1113 or the Information Commissioner's Office, Wycliffe House, Water Lane, Wilmslow, Cheshire, SK9 5AF, United Kingdom.


You may find corresponding contact information for each of the EU member states at the 
European Data Protection Board Members Website


Description Of Data Subjects' Rights


You have the following rights regarding your personal data:




The right to determine if we are holding any personal data relating to you (right to be informed).


The right to access your data and request a copy (right of access).


The right to rectify any inaccurate or incomplete data (right to rectification).


The right to request the erasure of your data (right to erasure).


The right to restrict the processing of your data.


The right to data portability.


The right to object to the processing of your data.


The right to object to automated decision-making relating to your data, including profiling.


The right to withdraw consent at any time.




Information On Data Transfers


Your data may be transferred to third countries outside the EU or UK. We ensure that appropriate safeguards are in place, such as standard contractual clauses, to protect your data.


Right To Lodge A Complaint


If you believe that your data protection rights have been violated, you have the right to lodge a complaint with the supervisory authority in your country (as listed above).


Automated Decision-Making And Profiling


We do not use automated decision-making or profiling that produces legal effects or that significantly affects you.


Source Of Data


If we have not collected data directly from you, we obtained it from publicly available sources. The categories of personal data we collect include those enumerated above in 1.2. You must provide your content by explicit actions, either by registering on our website, or accepting a question asking for your consent to cookies and other such tracking, before we will collect data. You may withdraw such consent at any time by contacting us at 
privacy@akka.io
, or using unsubscribe links in communications or on our website, which we will honor immediately.


2.2 California Consumer Privacy Act


Rights Under CCPA


You have the right to know what personal information we collect, use, disclose, and sell. You also have the right to request the deletion of your personal information, opt-out of the sale of your personal information, and not be discriminated against for exercising these rights.


Methods For Submitting Requests


You can submit requests to know or delete your personal information by contacting us as listed above in 1.10.


Do Not Sell My Personal Information Link


If you wish to opt-out of the sale of your personal information, please email us at 
privacy@akka.io
 or use the “Do Not Sell My Personal Information” link on our website.


2.3 Canada's Anti-Spam Legislation


Consent For Communications


We obtain explicit consent before sending any commercial electronic messages. By subscribing to our newsletter or other services, you agree to receive promotional content from Akka. You can withdraw your consent at any time by using the unsubscribe link provided in our communications or by contacting us as detailed in 1.10.


Unsubscribe Mechanism


To unsubscribe from our newsletter, click the unsubscribe link at the bottom of any of our messages. Your request will be processed immediately, and you will no longer receive newsletter emails from us.


2.4 EU-US Data Privacy Framework


We comply with the EU-US Data Privacy Framework as set forth by the U.S. Department of Commerce regarding the collection, use, and retention of personal information transferred from the European Union to the United States. We have certified our adherence to the Privacy Framework Principles of Notice, Choice, Accountability for Onward Transfer, Security, Data Integrity and Purpose Limitation, Access, and Recourse, Enforcement, and Liability.


In compliance with the EU-US Data Privacy Framework, we are committed to resolving complaints about your privacy and our collection or use of your personal data. EU individuals with inquiries or complaints regarding our Privacy Policy should first contact us as described in 1.10.


Our compliance with the EU-US Data Privacy Framework is subject to the investigatory and enforcement powers of the U.S. Federal Trade Commission (FTC).


We may be required to disclose personal data in response to lawful requests by public authorities, including to meet national security or law enforcement requirements.


We remain liable under the EU-US Data Privacy Framework Principles if our third-party service providers process your personal information in a manner inconsistent with the Principles, unless we can prove that we are not responsible for the event giving rise to the damage.


If you have any questions or concerns about this Privacy Policy or our data handling practices, you can contact us as listed in 1.10.














 Legal










Privacy Policy 


Terms of Use 


 Business Source License 1.1 










































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/distributed-data.html
Distributed Data • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data




Module info


Introduction


Using the Replicator


Replicated data types


Durable Storage


Limitations


Learn More about CRDTs


Configuration


Example project




Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data




Module info


Introduction


Using the Replicator


Replicated data types


Durable Storage


Limitations


Learn More about CRDTs


Configuration


Example project




Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Distributed Data


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Distributed Data
.


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Cluster Distributed Data, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-cluster-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-cluster-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-cluster-typed_${versions.ScalaBinary}"
}




Project Info: Akka Cluster (typed)


Artifact
com.typesafe.akka


akka-cluster-typed


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.cluster.typed


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.6.0, 2019-11-06




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




Introduction


Akka Distributed Data
 is useful when you need to share data between nodes in an Akka Cluster. The data is accessed with an actor providing a key-value store like API. The keys are unique identifiers with type information of the data values. The values are 
Conflict Free Replicated Data Types
 (CRDTs).


All data entries are spread to all nodes, or nodes with a certain role, in the cluster via direct replication and gossip based dissemination. You have fine grained control of the consistency level for reads and writes.


The nature of CRDTs makes it possible to perform updates from any node without coordination. Concurrent updates from different nodes will automatically be resolved by the monotonic merge function, which all data types must provide. The state changes always converge. Several useful data types for counters, sets, maps and registers are provided and you can also implement your own custom data types.


It is eventually consistent and geared toward providing high read and write availability (partition tolerance), with low latency. Note that in an eventually consistent system a read may return an out-of-date value.


Using the Replicator


You can interact with the data through the replicator actor which can be accessed through the 
DistributedData
DistributedData
 extension.


The messages for the replicator, such as 
Replicator.Update
Replicator.Update
 are defined as subclasses of 
Replicator.Command
Replicator.Command
 and the actual CRDTs are defined in the 
akka.cluster.ddata
 package, for example 
GCounter
GCounter
. It requires a 
implicit
 
akka.cluster.ddata.SelfUniqueAddress
, available from:




Scala




copy
source
implicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(context.getSystem()).selfUniqueAddress();




The replicator can contain multiple entries each containing a replicated data type, we therefore need to create a key identifying the entry and helping us know what type it has, and then use that key for every interaction with the replicator. Each replicated data type contains a factory for defining such a key.


Cluster members with status 
WeaklyUp
, will participate in Distributed Data. This means that the data will be replicated to the 
WeaklyUp
 nodes with the background gossip protocol. Note that it will not participate in any actions where the consistency mode is to read/write from all nodes or the majority of nodes. The 
WeaklyUp
 node is not counted as part of the cluster. So 3 nodes + 5 
WeaklyUp
 is essentially a 3 node cluster as far as consistent actions are concerned.


This sample uses the replicated data type 
GCounter
 to implement a counter that can be written to on any node of the cluster: 




Scala




copy
source
import akka.actor.typed.ActorRef
import akka.actor.typed.Behavior
import akka.actor.typed.scaladsl.Behaviors
import akka.cluster.ddata.GCounter
import akka.cluster.ddata.GCounterKey
import akka.cluster.ddata.typed.scaladsl.Replicator._

object Counter {
  sealed trait Command
  case object Increment extends Command
  final case class GetValue(replyTo: ActorRef[Int]) extends Command
  final case class GetCachedValue(replyTo: ActorRef[Int]) extends Command
  case object Unsubscribe extends Command
  private sealed trait InternalCommand extends Command
  private case class InternalUpdateResponse(rsp: Replicator.UpdateResponse[GCounter]) extends InternalCommand
  private case class InternalGetResponse(rsp: Replicator.GetResponse[GCounter], replyTo: ActorRef[Int])
      extends InternalCommand
  private case class InternalSubscribeResponse(chg: Replicator.SubscribeResponse[GCounter]) extends InternalCommand

  def apply(key: GCounterKey): Behavior[Command] =
    Behaviors.setup[Command] { context =>
      implicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress

      // adapter that turns the response messages from the replicator into our own protocol
      DistributedData.withReplicatorMessageAdapter[Command, GCounter] { replicatorAdapter =>
        // Subscribe to changes of the given `key`.
        replicatorAdapter.subscribe(key, InternalSubscribeResponse.apply)

        def updated(cachedValue: Int): Behavior[Command] = {
          Behaviors.receiveMessage[Command] {
            case Increment =>
              replicatorAdapter.askUpdate(
                askReplyTo => Replicator.Update(key, GCounter.empty, Replicator.WriteLocal, askReplyTo)(_ :+ 1),
                InternalUpdateResponse.apply)

              Behaviors.same

            case GetValue(replyTo) =>
              replicatorAdapter.askGet(
                askReplyTo => Replicator.Get(key, Replicator.ReadLocal, askReplyTo),
                value => InternalGetResponse(value, replyTo))

              Behaviors.same

            case GetCachedValue(replyTo) =>
              replyTo ! cachedValue
              Behaviors.same

            case Unsubscribe =>
              replicatorAdapter.unsubscribe(key)
              Behaviors.same

            case internal: InternalCommand =>
              internal match {
                case InternalUpdateResponse(_) => Behaviors.same // ok

                case InternalGetResponse(rsp @ Replicator.GetSuccess(`key`), replyTo) =>
                  val value = rsp.get(key).value.toInt
                  replyTo ! value
                  Behaviors.same

                case InternalGetResponse(_, _) =>
                  Behaviors.unhandled // not dealing with failures
                case InternalSubscribeResponse(chg @ Replicator.Changed(`key`)) =>
                  val value = chg.get(key).value.intValue
                  updated(value)

                case InternalSubscribeResponse(Replicator.Deleted(_)) =>
                  Behaviors.unhandled // no deletes

                case InternalSubscribeResponse(_) => // changed but wrong key
                  Behaviors.unhandled

              }
          }
        }

        updated(cachedValue = 0)
      }
    }
}


Java




copy
source
import akka.actor.typed.ActorRef;
import akka.actor.typed.Behavior;
import akka.actor.typed.javadsl.AbstractBehavior;
import akka.actor.typed.javadsl.ActorContext;
import akka.actor.typed.javadsl.Behaviors;
import akka.actor.typed.javadsl.Receive;
import akka.cluster.ddata.GCounter;
import akka.cluster.ddata.Key;
import akka.cluster.ddata.SelfUniqueAddress;
import akka.cluster.ddata.typed.javadsl.DistributedData;
import akka.cluster.ddata.typed.javadsl.Replicator;
import akka.cluster.ddata.typed.javadsl.ReplicatorMessageAdapter;

  public class Counter extends AbstractBehavior<Counter.Command> {
    interface Command {}

    enum Increment implements Command {
      INSTANCE
    }

    public static class GetValue implements Command {
      public final ActorRef<Integer> replyTo;

      public GetValue(ActorRef<Integer> replyTo) {
        this.replyTo = replyTo;
      }
    }

    public static class GetCachedValue implements Command {
      public final ActorRef<Integer> replyTo;

      public GetCachedValue(ActorRef<Integer> replyTo) {
        this.replyTo = replyTo;
      }
    }

    enum Unsubscribe implements Command {
      INSTANCE
    }

    private interface InternalCommand extends Command {}

    private static class InternalUpdateResponse implements InternalCommand {
      final Replicator.UpdateResponse<GCounter> rsp;

      InternalUpdateResponse(Replicator.UpdateResponse<GCounter> rsp) {
        this.rsp = rsp;
      }
    }

    private static class InternalGetResponse implements InternalCommand {
      final Replicator.GetResponse<GCounter> rsp;
      final ActorRef<Integer> replyTo;

      InternalGetResponse(Replicator.GetResponse<GCounter> rsp, ActorRef<Integer> replyTo) {
        this.rsp = rsp;
        this.replyTo = replyTo;
      }
    }

    private static final class InternalSubscribeResponse implements InternalCommand {
      final Replicator.SubscribeResponse<GCounter> rsp;

      InternalSubscribeResponse(Replicator.SubscribeResponse<GCounter> rsp) {
        this.rsp = rsp;
      }
    }

    public static Behavior<Command> create(Key<GCounter> key) {
      return Behaviors.setup(
          ctx ->
              DistributedData.withReplicatorMessageAdapter(
                  (ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter) ->
                      new Counter(ctx, replicatorAdapter, key)));
    }

    // adapter that turns the response messages from the replicator into our own protocol
    private final ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter;
    private final SelfUniqueAddress node;
    private final Key<GCounter> key;

    private int cachedValue = 0;

    private Counter(
        ActorContext<Command> context,
        ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter,
        Key<GCounter> key) {
      super(context);

      this.replicatorAdapter = replicatorAdapter;
      this.key = key;

      final SelfUniqueAddress node = DistributedData.get(context.getSystem()).selfUniqueAddress();

      this.node = DistributedData.get(context.getSystem()).selfUniqueAddress();

      this.replicatorAdapter.subscribe(this.key, InternalSubscribeResponse::new);
    }

    @Override
    public Receive<Command> createReceive() {
      return newReceiveBuilder()
          .onMessage(Increment.class, this::onIncrement)
          .onMessage(InternalUpdateResponse.class, msg -> Behaviors.same())
          .onMessage(GetValue.class, this::onGetValue)
          .onMessage(GetCachedValue.class, this::onGetCachedValue)
          .onMessage(Unsubscribe.class, this::onUnsubscribe)
          .onMessage(InternalGetResponse.class, this::onInternalGetResponse)
          .onMessage(InternalSubscribeResponse.class, this::onInternalSubscribeResponse)
          .build();
    }

    private Behavior<Command> onIncrement(Increment cmd) {
      replicatorAdapter.askUpdate(
          askReplyTo ->
              new Replicator.Update<>(
                  key,
                  GCounter.empty(),
                  Replicator.writeLocal(),
                  askReplyTo,
                  curr -> curr.increment(node, 1)),
          InternalUpdateResponse::new);

      return this;
    }

    private Behavior<Command> onGetValue(GetValue cmd) {
      replicatorAdapter.askGet(
          askReplyTo -> new Replicator.Get<>(key, Replicator.readLocal(), askReplyTo),
          rsp -> new InternalGetResponse(rsp, cmd.replyTo));

      return this;
    }

    private Behavior<Command> onGetCachedValue(GetCachedValue cmd) {
      cmd.replyTo.tell(cachedValue);
      return this;
    }

    private Behavior<Command> onUnsubscribe(Unsubscribe cmd) {
      replicatorAdapter.unsubscribe(key);
      return this;
    }

    private Behavior<Command> onInternalGetResponse(InternalGetResponse msg) {
      if (msg.rsp instanceof Replicator.GetSuccess) {
        int value = ((Replicator.GetSuccess<?>) msg.rsp).get(key).getValue().intValue();
        msg.replyTo.tell(value);
        return this;
      } else {
        // not dealing with failures
        return Behaviors.unhandled();
      }
    }

    private Behavior<Command> onInternalSubscribeResponse(InternalSubscribeResponse msg) {
      if (msg.rsp instanceof Replicator.Changed) {
        GCounter counter = ((Replicator.Changed<?>) msg.rsp).get(key);
        cachedValue = counter.getValue().intValue();
        return this;
      } else {
        // no deletes
        return Behaviors.unhandled();
      }
    }
  }
}




Although you can interact with the 
Replicator
 using the 
ActorRef[Replicator.Command]
ActorRef<Replicator.Command>
 from 
DistributedData(ctx.system).replicator
DistributedData(ctx.getSystem()).replicator()
 it’s often more convenient to use the 
ReplicatorMessageAdapter
 as in the above example.




Update


To modify and replicate a data value you send a 
Replicator.Update
 message to the local 
Replicator
.


In the above example, for an incoming 
Increment
 command, we send the 
replicator
 a 
Replicator.Update
 request, it contains five values:




the 
Key
KEY
 we want to update


the data to use as the empty state if the replicator has not seen the key before


the 
write consistency level
 we want for the update


an 
ActorRef[Replicator.UpdateResponse[GCounter]]
ActorRef<Replicator.UpdateResponse<GCounter>>
 to respond to when the update is completed


a 
modify
 function that takes a previous state and updates it, in our case by incrementing it with 1




There is alternative way of constructing the function for the 
Update
 message:




Scala




copy
source
// alternative way to define the `createRequest` function
// Replicator.Update instance has a curried `apply` method
replicatorAdapter.askUpdate(
  Replicator.Update(key, GCounter.empty, Replicator.WriteLocal)(_ :+ 1),
  InternalUpdateResponse.apply)

// that is the same as
replicatorAdapter.askUpdate(
  askReplyTo => Replicator.Update(key, GCounter.empty, Replicator.WriteLocal, askReplyTo)(_ :+ 1),
  InternalUpdateResponse.apply)




The current data value for the 
key
 of the 
Update
 is passed as parameter to the 
modify
 function of the 
Update
. The function is supposed to return the new value of the data, which will then be replicated according to the given 
write consistency level
.


The 
modify
 function is called by the 
Replicator
 actor and must therefore be a pure function that only uses the data parameter and stable fields from enclosing scope. It must for example not access the 
ActorContext
 or mutable state of an enclosing actor. 
Update
 is intended to only be sent from an actor running in same local 
ActorSystem
  as the 
Replicator
, because the 
modify
 function is typically not serializable.


You will always see your own writes. For example if you send two 
Update
 messages changing the value of the same 
key
, the 
modify
 function of the second message will see the change that was performed by the first 
Update
 message. 


As reply of the 
Update
 a 
Replicator.UpdateSuccess
 is sent to the 
replyTo
 of the 
Update
 if the value was successfully replicated according to the supplied consistency level within the supplied timeout. Otherwise a 
Replicator.UpdateFailure
 subclass is sent back. Note that a 
Replicator.UpdateTimeout
 reply does not mean that the update completely failed or was rolled back. It may still have been replicated to some nodes, and will eventually be replicated to all nodes with the gossip protocol.


It is possible to abort the 
Update
 when inspecting the state parameter that is passed in to the 
modify
 function by throwing an exception. That happens before the update is performed and a 
Replicator.ModifyFailure
 is sent back as reply. 


Get


To retrieve the current value of a data you send 
Replicator.Get
 message to the 
Replicator
. 


The example has the 
GetValue
 command, which is asking the replicator for current value. Note how the 
replyTo
 from the incoming message can be used when the 
GetSuccess
 response from the replicator is received.


Alternative way of constructing the function for the 
Get
 and 
Delete
:




Scala




copy
source
// alternative way to define the `createRequest` function
// Replicator.Get instance has a curried `apply` method
replicatorAdapter.askGet(Replicator.Get(key, Replicator.ReadLocal), value => InternalGetResponse(value, replyTo))

// that is the same as
replicatorAdapter.askGet(
  askReplyTo => Replicator.Get(key, Replicator.ReadLocal, askReplyTo),
  value => InternalGetResponse(value, replyTo))




For a 
Get
 you supply a 
read consistency level
.


You will always read your own writes. For example if you send a 
Update
 message followed by a 
Get
 of the same 
key
 the 
Get
 will retrieve the change that was performed by the preceding 
Update
 message. However, the order of the reply messages are not defined, i.e. in the previous example you may receive the 
GetSuccess
 before the 
UpdateSuccess
.


As reply of the 
Get
 a 
Replicator.GetSuccess
 is sent to the 
replyTo
 of the 
Get
 if the value was successfully retrieved according to the supplied consistency level within the supplied timeout. Otherwise a 
Replicator.GetFailure
 is sent. If the key does not exist the reply will be 
Replicator.NotFound
.


Subscribe


Whenever the distributed counter in the example is updated, we cache the value so that we can answer requests about the value without the extra interaction with the replicator using the 
GetCachedValue
 command.


When we start up the actor we subscribe it to changes for our key, meaning whenever the replicator observes a change for the counter our actor will receive a 
Replicator.Changed[GCounter]
Replicator.Changed<GCounter>
. Since this is not a message in our protocol, we use a message transformation function to wrap it in the internal 
InternalSubscribeResponse
 message, which is then handled in the regular message handling of the behavior, as shown in the above example. Subscribers will be notified of changes, if there are any, based on the configurable 
akka.cluster.distributed-data.notify-subscribers-interval
.


The subscriber is automatically unsubscribed if the subscriber is terminated. A subscriber can also be de-registered with the 
replicatorAdapter.unsubscribe(key)
 function.


In addition to subscribing to individual keys it is possible to subscribe to all keys with a given prefix by using a 
*
 at the end of the key 
id
. For example 
GCounterKey("counter-*")
. Notifications will be sent for all matching keys, also new keys added later.


Delete


A data entry can be deleted by sending a 
Replicator.Delete
 message to the local 
Replicator
. As reply of the 
Delete
 a 
Replicator.DeleteSuccess
 is sent to the 
replyTo
 of the 
Delete
 if the value was successfully deleted according to the supplied consistency level within the supplied timeout. Otherwise a 
Replicator.ReplicationDeleteFailure
 is sent. Note that 
ReplicationDeleteFailure
 does not mean that the delete completely failed or was rolled back. It may still have been replicated to some nodes, and may eventually be replicated to all nodes.


A deleted key cannot be reused again, but it is still recommended to delete unused data entries because that reduces the replication overhead when new nodes join the cluster. Subsequent 
Delete
, 
Update
 and 
Get
 requests will be replied with 
Replicator.DataDeleted
. Subscribers will receive 
Replicator.Deleted
.


The 
automatic expiry
 is an alternative for removing unused data entries.
Warning


As deleted keys continue to be included in the stored data on each node as well as in gossip messages, a continuous series of updates and deletes of top-level entities will result in growing memory usage until an ActorSystem runs out of memory. To use Akka Distributed Data where frequent adds and removes are required, you should use 
automatic expiry
 or a fixed number of top-level data types that support both updates and removals, for example 
ORMap
 or 
ORSet
.


Expire


A data entry can automatically be removed after a period of inactivity, i.e. when there has been no access of the entry with 
Get
, 
Update
 or 
Delete
.


Expiry is enabled for configured keys:


akka.cluster.distributed-data.expire-keys-after-inactivity {
 "key-1" = 10 minutes
 "cache-*" = 2 minutes
}



Prefix matching is supported by using 
*
 at the end of a key.


Expiry can be enabled for all entries by specifying:


akka.cluster.distributed-data.expire-keys-after-inactivity {
  "*" = 10 minutes
}



Subscribers will receive 
Replicator.Expired
 when an entry has expired.


Expired entries are completely removed and does not leave any tombstones as is the case for 
Delete
. Expired keys can be reused again. Also 
deleted
 entries can be expired and then completely removed.


Consistency


The consistency level that is supplied in the 
Update
 and 
Get
 specifies per request how many replicas that must respond successfully to a write and read request.


WriteAll
 and 
ReadAll
 is the strongest consistency level, but also the slowest and with lowest availability. For example, it is enough that one node is unavailable for a 
Get
 request and you will not receive the value.


For low latency reads you use 
ReadLocal
readLocal
 with the risk of retrieving stale data, i.e. updates from other nodes might not be visible yet.


Write consistency


When using 
WriteLocal
writeLocal
 the 
Update
 is only written to the local replica and then disseminated in the background with the gossip protocol, which can take few seconds to spread to all nodes.


For an update you supply a write consistency level which has the following meaning:




WriteLocal
writeLocal
 the value will immediately only be written to the local replica, and later disseminated with gossip


WriteTo(n)
 the value will immediately be written to at least 
n
 replicas, including the local replica


WriteMajority
 the value will immediately be written to a majority of replicas, i.e. at least 
N/2 + 1
 replicas, where N is the number of nodes in the cluster (or cluster role group)


WriteMajorityPlus
 is like 
WriteMajority
 but with the given number of 
additional
 nodes added  to the majority count. At most all nodes. This gives better tolerance for membership changes between  writes and reads. Exiting nodes are excluded using 
WriteMajorityPlus
 because those are typically about to be removed  and will not be able to respond.


WriteAll
 the value will immediately be written to all nodes in the cluster (or all nodes in the cluster role group).  Exiting nodes are excluded using 
WriteAll
 because those are typically about to be removed and will not be able to respond.




When you specify to write to 
n
 out of 
x
 nodes, the update will first replicate to 
n
 nodes. If there are not enough Acks after a 1/5th of the timeout, the update will be replicated to 
n
 other nodes. If there are less than n nodes left all of the remaining nodes are used. Reachable nodes are preferred over unreachable nodes.


Note that 
WriteMajority
 and 
WriteMajorityPlus
 have a 
minCap
 parameter that is useful to specify to achieve better safety for small clusters.


Read consistency


If consistency is a priority, you can ensure that a read always reflects the most recent write by using the following formula:


(nodes_written + nodes_read) > N



where N is the total number of nodes in the cluster, or the number of nodes with the role that is used for the 
Replicator
.


You supply a consistency level which has the following meaning:




ReadLocal
readLocal
 the value will only be read from the local replica


ReadFrom(n)
 the value will be read and merged from 
n
 replicas, including the local replica


ReadMajority
 the value will be read and merged from a majority of replicas, i.e. at least 
N/2 + 1
 replicas, where N is the number of nodes in the cluster (or cluster role group)


ReadMajorityPlus
 is like 
ReadMajority
 but with the given number of 
additional
 nodes added  to the majority count. At most all nodes. This gives better tolerance for membership changes between  writes and reads. Exiting nodes are excluded using 
ReadMajorityPlus
 because those are typically about to be  removed and will not be able to respond.


ReadAll
 the value will be read and merged from all nodes in the cluster (or all nodes in the cluster role group).  Exiting nodes are excluded using 
ReadAll
 because those are typically about to be removed and will not be able to respond.




Note that 
ReadMajority
 and 
ReadMajorityPlus
 have a 
minCap
 parameter that is useful to specify to achieve better safety for small clusters.


Consistency and response types


When using 
ReadLocal
, you will never receive a 
GetFailure
 response, since the local replica is always available to local readers. 
WriteLocal
 however may still reply with 
UpdateFailure
 messages if the 
modify
 function throws an exception, or if it fails to persist to 
durable storage
.


Examples


In a 7 node cluster these consistency properties are achieved by writing to 4 nodes and reading from 4 nodes, or writing to 5 nodes and reading from 3 nodes.


By combining 
WriteMajority
 and 
ReadMajority
 levels a read always reflects the most recent write. The 
Replicator
 writes and reads to a majority of replicas, i.e. 
N / 2 + 1
. For example, in a 5 node cluster it writes to 3 nodes and reads from 3 nodes. In a 6 node cluster it writes to 4 nodes and reads from 4 nodes.


You can define a minimum number of nodes for 
WriteMajority
 and 
ReadMajority
, this will minimize the risk of reading stale data. Minimum cap is provided by minCap property of 
WriteMajority
 and 
ReadMajority
 and defines the required majority. If the minCap is higher then 
N / 2 + 1
 the minCap will be used.


For example if the minCap is 5 the 
WriteMajority
 and 
ReadMajority
 for cluster of 3 nodes will be 3, for cluster of 6 nodes will be 5 and for cluster of 12 nodes will be 7 ( 
N / 2 + 1
 ).


For small clusters (<7) the risk of membership changes between a WriteMajority and ReadMajority is rather high and then the nice properties of combining majority write and reads are not guaranteed. Therefore the 
ReadMajority
 and 
WriteMajority
 have a 
minCap
 parameter that is useful to specify to achieve better safety for small clusters. It means that if the cluster size is smaller than the majority size it will use the 
minCap
 number of nodes but at most the total size of the cluster.


In some rare cases, when performing an 
Update
 it is needed to first try to fetch latest data from other nodes. That can be done by first sending a 
Get
 with 
ReadMajority
 and then continue with the 
Update
 when the 
GetSuccess
, 
GetFailure
 or 
NotFound
 reply is received. This might be needed when you need to base a decision on latest information or when removing entries from an 
ORSet
 or 
ORMap
. If an entry is added to an 
ORSet
 or 
ORMap
 from one node and removed from another node the entry will only be removed if the added entry is visible on the node where the removal is performed (hence the name observed-removed set).
Warning


Caveat:
 Even if you use 
WriteMajority
 and 
ReadMajority
 there is small risk that you may read stale data if the cluster membership has changed between the 
Update
 and the 
Get
. For example, in cluster of 5 nodes when you 
Update
 and that change is written to 3 nodes: n1, n2, n3. Then 2 more nodes are added and a 
Get
 request is reading from 4 nodes, which happens to be n4, n5, n6, n7, i.e. the value on n1, n2, n3 is not seen in the response of the 
Get
 request. For additional tolerance of membership changes between writes and reads you can use 
WriteMajorityPlus
 and 
ReadMajorityPlus
.


Running separate instances of the replicator


For some use cases, for example when limiting the replicator to certain roles, or using different subsets on different roles, it makes sense to start separate replicators, this needs to be done on all nodes, or the group of nodes tagged with a specific role. To do this with Distributed Data you will first have to start a classic 
Replicator
 and pass it to the 
Replicator.behavior
 method that takes a classic actor ref. All such 
Replicator
s must run on the same path in the classic actor hierarchy.


A standalone 
ReplicatorMessageAdapter
 can also be created for a given 
Replicator
 instead of creating one via the 
DistributedData
 extension.


Replicated data types


Akka contains a set of useful replicated data types and it is fully possible to implement custom replicated data types. 


The data types must be convergent (stateful) CRDTs and implement the 
ReplicatedData
 trait
AbstractReplicatedData
 interface
, i.e. they provide a monotonic merge function and the state changes always converge.


You can use your own custom 
ReplicatedData
 or 
DeltaReplicatedData
AbstractReplicatedData
 or 
AbstractDeltaReplicatedData
 types, and several types are provided by this package, such as:




Counters: 
GCounter
, 
PNCounter


Sets: 
GSet
, 
ORSet


Maps: 
ORMap
, 
ORMultiMap
, 
LWWMap
, 
PNCounterMap


Registers: 
LWWRegister
, 
Flag




Counters


GCounter
 is a “grow only counter”. It only supports increments, no decrements.


It works in a similar way as a vector clock. It keeps track of one counter per node and the total value is the sum of these counters. The 
merge
 is implemented by taking the maximum count for each node.


If you need both increments and decrements you can use the 
PNCounter
 (positive/negative counter).


It is tracking the increments (P) separate from the decrements (N). Both P and N are represented as two internal 
GCounter
s. Merge is handled by merging the internal P and N counters. The value of the counter is the value of the P counter minus the value of the N counter.




Scala




copy
source
implicit val node = DistributedData(system).selfUniqueAddress

val c0 = PNCounter.empty
val c1 = c0 :+ 1
val c2 = c1 :+ 7
val c3: PNCounter = c2.decrement(2)
println(c3.value) // 6


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
final PNCounter c0 = PNCounter.create();
final PNCounter c1 = c0.increment(node, 1);
final PNCounter c2 = c1.increment(node, 7);
final PNCounter c3 = c2.decrement(node, 2);
System.out.println(c3.value()); // 6




GCounter
 and 
PNCounter
 have support for 
delta-CRDT
 and don’t need causal delivery of deltas.


Several related counters can be managed in a map with the 
PNCounterMap
 data type. When the counters are placed in a 
PNCounterMap
 as opposed to placing them as separate top level values they are guaranteed to be replicated together as one unit, which is sometimes necessary for related data.




Scala




copy
source
implicit val node = DistributedData(system).selfUniqueAddress
val m0 = PNCounterMap.empty[String]
val m1 = m0.increment(node, "a", 7)
val m2 = m1.decrement(node, "a", 2)
val m3 = m2.increment(node, "b", 1)
println(m3.get("a")) // 5
m3.entries.foreach { case (key, value) => println(s"$key -> $value") }


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
final PNCounterMap<String> m0 = PNCounterMap.create();
final PNCounterMap<String> m1 = m0.increment(node, "a", 7);
final PNCounterMap<String> m2 = m1.decrement(node, "a", 2);
final PNCounterMap<String> m3 = m2.increment(node, "b", 1);
System.out.println(m3.get("a")); // 5
System.out.println(m3.getEntries());




Sets


If you only need to add elements to a set and not remove elements the 
GSet
 (grow-only set) is the data type to use. The elements can be any type of values that can be serialized. Merge is the union of the two sets.




Scala




copy
source
val s0 = GSet.empty[String]
val s1 = s0 + "a"
val s2 = s1 + "b" + "c"
if (s2.contains("a"))
  println(s2.elements) // a, b, c


Java




copy
source
final GSet<String> s0 = GSet.create();
final GSet<String> s1 = s0.add("a");
final GSet<String> s2 = s1.add("b").add("c");
if (s2.contains("a")) System.out.println(s2.getElements()); // a, b, c




GSet
 has support for 
delta-CRDT
 and it doesn’t require causal delivery of deltas.


If you need add and remove operations you should use the 
ORSet
 (observed-remove set). Elements can be added and removed any number of times. If an element is concurrently added and removed, the add will win. You cannot remove an element that you have not seen.


The 
ORSet
 has a version vector that is incremented when an element is added to the set. The version for the node that added the element is also tracked for each element in a so called “birth dot”. The version vector and the dots are used by the 
merge
 function to track causality of the operations and resolve concurrent updates.




Scala




copy
source
implicit val node = DistributedData(system).selfUniqueAddress
val s0 = ORSet.empty[String]
val s1 = s0 :+ "a"
val s2 = s1 :+ "b"
val s3 = s2.remove("a")
println(s3.elements) // b


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
final ORSet<String> s0 = ORSet.create();
final ORSet<String> s1 = s0.add(node, "a");
final ORSet<String> s2 = s1.add(node, "b");
final ORSet<String> s3 = s2.remove(node, "a");
System.out.println(s3.getElements()); // b




ORSet
 has support for 
delta-CRDT
 and it requires causal delivery of deltas.


Maps


ORMap
 (observed-remove map) is a map with keys of 
Any
 type and the values are 
ReplicatedData
 types themselves. It supports add, update and remove any number of times for a map entry.


If an entry is concurrently added and removed, the add will win. You cannot remove an entry that you have not seen. This is the same semantics as for the 
ORSet
.


If an entry is concurrently updated to different values the values will be merged, hence the requirement that the values must be 
ReplicatedData
 types.


While the 
ORMap
 supports removing and re-adding keys any number of times, the impact that this has on the values can be non-deterministic. A merge will always attempt to merge two values for the same key, regardless of whether that key has been removed and re-added in the meantime, an attempt to replace a value with a new one may not have the intended effect. This means that old values can effectively be resurrected if a node, that has seen both the remove and the update,gossips with a node that has seen neither. One consequence of this is that changing the value type of the CRDT, for example, from a 
GCounter
 to a 
GSet
, could result in the merge function for the CRDT always failing. This could be an unrecoverable state for the node, hence, the types of 
ORMap
 values must never change for a given key.


It is rather inconvenient to use the 
ORMap
 directly since it does not expose specific types of the values. The 
ORMap
 is intended as a low level tool for building more specific maps, such as the following specialized maps.


ORMultiMap
 (observed-remove multi-map) is a multi-map implementation that wraps an 
ORMap
 with an 
ORSet
 for the map’s value.


PNCounterMap
 (positive negative counter map) is a map of named counters (where the name can be of any type). It is a specialized 
ORMap
 with 
PNCounter
 values.


LWWMap
 (last writer wins map) is a specialized 
ORMap
 with 
LWWRegister
 (last writer wins register) values.


ORMap
, 
ORMultiMap
, 
PNCounterMap
 and 
LWWMap
 have support for 
delta-CRDT
 and they require causal delivery of deltas. Support for deltas here means that the 
ORSet
 being underlying key type for all those maps uses delta propagation to deliver updates. Effectively, the update for map is then a pair, consisting of delta for the 
ORSet
 being the key and full update for the respective value (
ORSet
, 
PNCounter
 or 
LWWRegister
) kept in the map.




Scala




copy
source
implicit val node = DistributedData(system).selfUniqueAddress
val m0 = ORMultiMap.empty[String, Int]
val m1 = m0 :+ ("a" -> Set(1, 2, 3))
val m2 = m1.addBinding(node, "a", 4)
val m3 = m2.removeBinding(node, "a", 2)
val m4 = m3.addBinding(node, "b", 1)
println(m4.entries)


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
final ORMultiMap<String, Integer> m0 = ORMultiMap.create();
final ORMultiMap<String, Integer> m1 = m0.put(node, "a", new HashSet<>(Arrays.asList(1, 2, 3)));
final ORMultiMap<String, Integer> m2 = m1.addBinding(node, "a", 4);
final ORMultiMap<String, Integer> m3 = m2.removeBinding(node, "a", 2);
final ORMultiMap<String, Integer> m4 = m3.addBinding(node, "b", 1);
System.out.println(m4.getEntries());




When a data entry is changed the full state of that entry is replicated to other nodes, i.e. when you update a map, the whole map is replicated. Therefore, instead of using one 
ORMap
 with 1000 elements it is more efficient to split that up in 10 top level 
ORMap
 entries with 100 elements each. Top level entries are replicated individually, which has the trade-off that different entries may not be replicated at the same time and you may see inconsistencies between related entries. Separate top level entries cannot be updated atomically together.


There is a special version of 
ORMultiMap
, created by using separate constructor 
ORMultiMap.emptyWithValueDeltas[A, B]
, that also propagates the updates to its values (of 
ORSet
 type) as deltas. This means that the 
ORMultiMap
 initiated with 
ORMultiMap.emptyWithValueDeltas
 propagates its updates as pairs consisting of delta of the key and delta of the value. It is much more efficient in terms of network bandwidth consumed.


However, this behavior has not been made default for 
ORMultiMap
 and if you wish to use it in your code, you need to replace invocations of 
ORMultiMap.empty[A, B]
 (or 
ORMultiMap()
) with 
ORMultiMap.emptyWithValueDeltas[A, B]
 where 
A
 and 
B
 are types respectively of keys and values in the map.


Please also note, that despite having the same Scala type, 
ORMultiMap.emptyWithValueDeltas
 is not compatible with ‘vanilla’ 
ORMultiMap
, because of different replication mechanism. One needs to be extra careful not to mix the two, as they have the same type, so compiler will not hint the error. Nonetheless 
ORMultiMap.emptyWithValueDeltas
 uses the same 
ORMultiMapKey
 type as the ‘vanilla’ 
ORMultiMap
 for referencing.


Note that 
LWWRegister
 and therefore 
LWWMap
 relies on synchronized clocks and should only be used when the choice of value is not important for concurrent updates occurring within the clock skew. Read more in the below section about 
LWWRegister
.


Flags and Registers


Flag
 is a data type for a boolean value that is initialized to 
false
 and can be switched to 
true
. Thereafter it cannot be changed. 
true
 wins over 
false
 in merge.




Scala




copy
source
val f0 = Flag.Disabled
val f1 = f0.switchOn
println(f1.enabled)


Java




copy
source
final Flag f0 = Flag.create();
final Flag f1 = f0.switchOn();
System.out.println(f1.enabled());




LWWRegister
 (last writer wins register) can hold any (serializable) value.


Merge of a 
LWWRegister
 takes the register with highest timestamp. Note that this relies on synchronized clocks. 
LWWRegister
 should only be used when the choice of value is not important for concurrent updates occurring within the clock skew.


Merge takes the register updated by the node with lowest address (
UniqueAddress
 is ordered) if the timestamps are exactly the same.




Scala




copy
source
implicit val node = DistributedData(system).selfUniqueAddress
val r1 = LWWRegister.create("Hello")
val r2 = r1.withValueOf("Hi")
println(s"${r1.value} by ${r1.updatedBy} at ${r1.timestamp}")


Java




copy
source
final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
final LWWRegister<String> r1 = LWWRegister.create(node, "Hello");
final LWWRegister<String> r2 = r1.withValue(node, "Hi");
System.out.println(r1.value() + " by " + r1.updatedBy() + " at " + r1.timestamp());




Instead of using timestamps based on 
System.currentTimeMillis()
 time it is possible to use a timestamp value based on something else, for example an increasing version number from a database record that is used for optimistic concurrency control.




Scala




copy
source
case class Record(version: Int, name: String, address: String)

implicit val node = DistributedData(system).selfUniqueAddress
implicit val recordClock: LWWRegister.Clock[Record] = new LWWRegister.Clock[Record] {
  override def apply(currentTimestamp: Long, value: Record): Long =
    value.version
}

val record1 = Record(version = 1, "Alice", "Union Square")
val r1 = LWWRegister(node, record1, recordClock)

val record2 = Record(version = 2, "Alice", "Madison Square")
val r2 = LWWRegister(node, record2, recordClock)

val r3 = r1.merge(r2)
println(r3.value)


Java




copy
source
class Record {
  public final int version;
  public final String name;
  public final String address;

  public Record(int version, String name, String address) {
    this.version = version;
    this.name = name;
    this.address = address;
  }
}


  final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();
  final LWWRegister.Clock<Record> recordClock =
      new LWWRegister.Clock<Record>() {
        @Override
        public long apply(long currentTimestamp, Record value) {
          return value.version;
        }
      };

  final Record record1 = new Record(1, "Alice", "Union Square");
  final LWWRegister<Record> r1 = LWWRegister.create(node, record1);

  final Record record2 = new Record(2, "Alice", "Madison Square");
  final LWWRegister<Record> r2 = LWWRegister.create(node, record2);

  final LWWRegister<Record> r3 = r1.merge(r2);
  System.out.println(r3.value());




For first-write-wins semantics you can use the 
LWWRegister#reverseClock
 instead of the 
LWWRegister#defaultClock
.


The 
defaultClock
 is using max value of 
System.currentTimeMillis()
 and 
currentTimestamp + 1
. This means that the timestamp is increased for changes on the same node that occurs within the same millisecond. It also means that it is safe to use the 
LWWRegister
 without synchronized clocks when there is only one active writer, e.g. a Cluster Singleton. Such a single writer should then first read current value with 
ReadMajority
 (or more) before changing and writing the value with 
WriteMajority
 (or more). When using 
LWWRegister
 with Cluster Singleton it’s also recommended to enable:


# Update and Get operations are sent to oldest nodes first.
akka.cluster.distributed-data.prefer-oldest = on



Delta-CRDT


Delta State Replicated Data Types
 are supported. A delta-CRDT is a way to reduce the need for sending the full state for updates. For example adding element 
'c'
 and 
'd'
 to set 
{'a', 'b'}
 would result in sending the delta 
{'c', 'd'}
 and merge that with the state on the receiving side, resulting in set 
{'a', 'b', 'c', 'd'}
.


The protocol for replicating the deltas supports causal consistency if the data type is marked with 
RequiresCausalDeliveryOfDeltas
. Otherwise it is only eventually consistent. Without causal consistency it means that if elements 
'c'
 and 
'd'
 are added in two separate 
Update
 operations these deltas may occasionally be propagated to nodes in a different order to the causal order of the updates. For this example it can result in that set 
{'a', 'b', 'd'}
 can be seen before element ‘c’ is seen. Eventually it will be 
{'a', 'b', 'c', 'd'}
.


Note that the full state is occasionally also replicated for delta-CRDTs, for example when new nodes are added to the cluster or when deltas could not be propagated because of network partitions or similar problems.


The the delta propagation can be disabled with configuration property:


akka.cluster.distributed-data.delta-crdt.enabled=off



Custom Data Type


You can implement your own data types. The only requirement is that it implements the 
merge
mergeData
 function of the 
ReplicatedData
AbstractReplicatedData
 trait.


A nice property of stateful CRDTs is that they typically compose nicely, i.e. you can combine several smaller data types to build richer data structures. For example, the 
PNCounter
 is composed of two internal 
GCounter
 instances to keep track of increments and decrements separately.


Here is s simple implementation of a custom 
TwoPhaseSet
 that is using two internal 
GSet
 types to keep track of addition and removals. A 
TwoPhaseSet
 is a set where an element may be added and removed, but never added again thereafter.




Scala




copy
source
case class TwoPhaseSet(adds: GSet[String] = GSet.empty, removals: GSet[String] = GSet.empty) extends ReplicatedData {
  type T = TwoPhaseSet

  def add(element: String): TwoPhaseSet =
    copy(adds = adds.add(element))

  def remove(element: String): TwoPhaseSet =
    copy(removals = removals.add(element))

  def elements: Set[String] = adds.elements.diff(removals.elements)

  override def merge(that: TwoPhaseSet): TwoPhaseSet =
    copy(adds = this.adds.merge(that.adds), removals = this.removals.merge(that.removals))
}


Java




copy
source
public class TwoPhaseSet extends AbstractReplicatedData<TwoPhaseSet> {

  public final GSet<String> adds;
  public final GSet<String> removals;

  public TwoPhaseSet(GSet<String> adds, GSet<String> removals) {
    this.adds = adds;
    this.removals = removals;
  }

  public static TwoPhaseSet create() {
    return new TwoPhaseSet(GSet.create(), GSet.create());
  }

  public TwoPhaseSet add(String element) {
    return new TwoPhaseSet(adds.add(element), removals);
  }

  public TwoPhaseSet remove(String element) {
    return new TwoPhaseSet(adds, removals.add(element));
  }

  public Set<String> getElements() {
    Set<String> result = new HashSet<>(adds.getElements());
    result.removeAll(removals.getElements());
    return result;
  }

  @Override
  public TwoPhaseSet mergeData(TwoPhaseSet that) {
    return new TwoPhaseSet(this.adds.merge(that.adds), this.removals.merge(that.removals));
  }
}




Data types should be immutable, i.e. “modifying” methods should return a new instance.


Implement the additional methods of 
DeltaReplicatedData
AbstractDeltaReplicatedData
 if it has support for delta-CRDT replication.


Serialization


The data types must be serializable with an 
Akka Serializer
. It is highly recommended that you implement efficient serialization with Protobuf or similar for your custom data types. The built in data types are marked with 
ReplicatedDataSerialization
 and serialized with 
akka.cluster.ddata.protobuf.ReplicatedDataSerializer
.


Serialization of the data types are used in remote messages and also for creating message digests (SHA-1) to detect changes. Therefore it is important that the serialization is efficient and produce the same bytes for the same content. For example sets and maps should be sorted deterministically in the serialization.


This is a protobuf representation of the above 
TwoPhaseSet
:


copy
source
option java_package = "docs.ddata.protobuf.msg";
option optimize_for = SPEED;

message TwoPhaseSet {
  repeated string adds = 1;
  repeated string removals = 2;
}


The serializer for the 
TwoPhaseSet
:




Scala




copy
source
import java.util.ArrayList
import java.util.Collections
import scala.jdk.CollectionConverters._
import akka.actor.ExtendedActorSystem
import akka.cluster.ddata.GSet
import akka.cluster.ddata.protobuf.SerializationSupport
import akka.serialization.Serializer
import docs.ddata.TwoPhaseSet
import docs.ddata.protobuf.msg.TwoPhaseSetMessages

class TwoPhaseSetSerializer(val system: ExtendedActorSystem) extends Serializer with SerializationSupport {

  override def includeManifest: Boolean = false

  override def identifier = 99999

  override def toBinary(obj: AnyRef): Array[Byte] = obj match {
    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray
    case _              => throw new IllegalArgumentException(s"Can't serialize object of type ${obj.getClass}")
  }

  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
    twoPhaseSetFromBinary(bytes)
  }

  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet = {
    val b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder()
    // using java collections and sorting for performance (avoid conversions)
    val adds = new ArrayList[String]
    twoPhaseSet.adds.elements.foreach(adds.add)
    if (!adds.isEmpty) {
      Collections.sort(adds)
      b.addAllAdds(adds)
    }
    val removals = new ArrayList[String]
    twoPhaseSet.removals.elements.foreach(removals.add)
    if (!removals.isEmpty) {
      Collections.sort(removals)
      b.addAllRemovals(removals)
    }
    b.build()
  }

  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {
    val msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes)
    val addsSet = msg.getAddsList.iterator.asScala.toSet
    val removalsSet = msg.getRemovalsList.iterator.asScala.toSet
    val adds = addsSet.foldLeft(GSet.empty[String])((acc, el) => acc.add(el))
    val removals = removalsSet.foldLeft(GSet.empty[String])((acc, el) => acc.add(el))
    // GSet will accumulate deltas when adding elements,
    // but those are not of interest in the result of the deserialization
    TwoPhaseSet(adds.resetDelta, removals.resetDelta)
  }
}


Java




copy
source
import akka.actor.ExtendedActorSystem;
import akka.cluster.ddata.GSet;
import akka.cluster.ddata.protobuf.AbstractSerializationSupport;
import docs.ddata.protobuf.msg.TwoPhaseSetMessages;
import docs.ddata.protobuf.msg.TwoPhaseSetMessages.TwoPhaseSet.Builder;
import java.util.ArrayList;
import java.util.Collections;
import jdocs.ddata.TwoPhaseSet;

public class TwoPhaseSetSerializer extends AbstractSerializationSupport {

  private final ExtendedActorSystem system;

  public TwoPhaseSetSerializer(ExtendedActorSystem system) {
    this.system = system;
  }

  @Override
  public ExtendedActorSystem system() {
    return this.system;
  }

  @Override
  public boolean includeManifest() {
    return false;
  }

  @Override
  public int identifier() {
    return 99998;
  }

  @Override
  public byte[] toBinary(Object obj) {
    if (obj instanceof TwoPhaseSet) {
      return twoPhaseSetToProto((TwoPhaseSet) obj).toByteArray();
    } else {
      throw new IllegalArgumentException("Can't serialize object of type " + obj.getClass());
    }
  }

  @Override
  public Object fromBinaryJava(byte[] bytes, Class<?> manifest) {
    return twoPhaseSetFromBinary(bytes);
  }

  protected TwoPhaseSetMessages.TwoPhaseSet twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {
    Builder b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder();
    ArrayList<String> adds = new ArrayList<>(twoPhaseSet.adds.getElements());
    if (!adds.isEmpty()) {
      Collections.sort(adds);
      b.addAllAdds(adds);
    }
    ArrayList<String> removals = new ArrayList<>(twoPhaseSet.removals.getElements());
    if (!removals.isEmpty()) {
      Collections.sort(removals);
      b.addAllRemovals(removals);
    }
    return b.build();
  }

  protected TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {
    try {
      TwoPhaseSetMessages.TwoPhaseSet msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes);
      GSet<String> adds = GSet.create();
      for (String elem : msg.getAddsList()) {
        adds = adds.add(elem);
      }
      GSet<String> removals = GSet.create();
      for (String elem : msg.getRemovalsList()) {
        removals = removals.add(elem);
      }
      // GSet will accumulate deltas when adding elements,
      // but those are not of interest in the result of the deserialization
      return new TwoPhaseSet(adds.resetDelta(), removals.resetDelta());
    } catch (Exception e) {
      throw new RuntimeException(e.getMessage(), e);
    }
  }
}




Note that the elements of the sets are sorted so the SHA-1 digests are the same for the same elements.


You register the serializer in configuration:




Scala




copy
source
akka.actor {
  serializers {
    two-phase-set = "docs.ddata.protobuf.TwoPhaseSetSerializer"
  }
  serialization-bindings {
    "docs.ddata.TwoPhaseSet" = two-phase-set
  }
}


Java




copy
source
akka.actor {
  serializers {
    twophaseset = "jdocs.ddata.protobuf.TwoPhaseSetSerializer"
  }
  serialization-bindings {
    "jdocs.ddata.TwoPhaseSet" = twophaseset
  }
}




Using compression can sometimes be a good idea to reduce the data size. Gzip compression is provided by the 
akka.cluster.ddata.protobuf.SerializationSupport
 trait
akka.cluster.ddata.protobuf.AbstractSerializationSupport
 interface
:




Scala




copy
source
override def toBinary(obj: AnyRef): Array[Byte] = obj match {
  case m: TwoPhaseSet => compress(twoPhaseSetToProto(m))
  case _              => throw new IllegalArgumentException(s"Can't serialize object of type ${obj.getClass}")
}

override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
  twoPhaseSetFromBinary(decompress(bytes))
}


Java




copy
source
@Override
public byte[] toBinary(Object obj) {
  if (obj instanceof TwoPhaseSet) {
    return compress(twoPhaseSetToProto((TwoPhaseSet) obj));
  } else {
    throw new IllegalArgumentException("Can't serialize object of type " + obj.getClass());
  }
}

@Override
public Object fromBinaryJava(byte[] bytes, Class<?> manifest) {
  return twoPhaseSetFromBinary(decompress(bytes));
}




The two embedded 
GSet
 can be serialized as illustrated above, but in general when composing new data types from the existing built in types it is better to make use of the existing serializer for those types. This can be done by declaring those as bytes fields in protobuf:


copy
source
message TwoPhaseSet2 {
  optional bytes adds = 1;
  optional bytes removals = 2;
}


and use the methods 
otherMessageToProto
 and 
otherMessageFromBinary
 that are provided by the 
SerializationSupport
 trait to serialize and deserialize the 
GSet
 instances. This works with any type that has a registered Akka serializer. This is how such an serializer would look like for the 
TwoPhaseSet
:




Scala




copy
source
import akka.actor.ExtendedActorSystem
import akka.cluster.ddata.GSet
import akka.cluster.ddata.protobuf.SerializationSupport
import akka.serialization.Serializer
import docs.ddata.TwoPhaseSet
import docs.ddata.protobuf.msg.TwoPhaseSetMessages

class TwoPhaseSetSerializer2(val system: ExtendedActorSystem) extends Serializer with SerializationSupport {

  override def includeManifest: Boolean = false

  override def identifier = 99999

  override def toBinary(obj: AnyRef): Array[Byte] = obj match {
    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray
    case _              => throw new IllegalArgumentException(s"Can't serialize object of type ${obj.getClass}")
  }

  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
    twoPhaseSetFromBinary(bytes)
  }

  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet2 = {
    val b = TwoPhaseSetMessages.TwoPhaseSet2.newBuilder()
    if (!twoPhaseSet.adds.isEmpty)
      b.setAdds(otherMessageToProto(twoPhaseSet.adds).toByteString())
    if (!twoPhaseSet.removals.isEmpty)
      b.setRemovals(otherMessageToProto(twoPhaseSet.removals).toByteString())
    b.build()
  }

  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {
    val msg = TwoPhaseSetMessages.TwoPhaseSet2.parseFrom(bytes)
    val adds =
      if (msg.hasAdds)
        otherMessageFromBinary(msg.getAdds.toByteArray).asInstanceOf[GSet[String]]
      else
        GSet.empty[String]
    val removals =
      if (msg.hasRemovals)
        otherMessageFromBinary(msg.getRemovals.toByteArray).asInstanceOf[GSet[String]]
      else
        GSet.empty[String]
    TwoPhaseSet(adds, removals)
  }
}


Java




copy
source
import akka.actor.ExtendedActorSystem;
import akka.cluster.ddata.GSet;
import akka.cluster.ddata.protobuf.AbstractSerializationSupport;
import akka.cluster.ddata.protobuf.ReplicatedDataSerializer;
import docs.ddata.protobuf.msg.TwoPhaseSetMessages;
import docs.ddata.protobuf.msg.TwoPhaseSetMessages.TwoPhaseSet2.Builder;
import jdocs.ddata.TwoPhaseSet;

public class TwoPhaseSetSerializer2 extends AbstractSerializationSupport {

  private final ExtendedActorSystem system;
  private final ReplicatedDataSerializer replicatedDataSerializer;

  public TwoPhaseSetSerializer2(ExtendedActorSystem system) {
    this.system = system;
    this.replicatedDataSerializer = new ReplicatedDataSerializer(system);
  }

  @Override
  public ExtendedActorSystem system() {
    return this.system;
  }

  @Override
  public boolean includeManifest() {
    return false;
  }

  @Override
  public int identifier() {
    return 99998;
  }

  @Override
  public byte[] toBinary(Object obj) {
    if (obj instanceof TwoPhaseSet) {
      return twoPhaseSetToProto((TwoPhaseSet) obj).toByteArray();
    } else {
      throw new IllegalArgumentException("Can't serialize object of type " + obj.getClass());
    }
  }

  @Override
  public Object fromBinaryJava(byte[] bytes, Class<?> manifest) {
    return twoPhaseSetFromBinary(bytes);
  }

  protected TwoPhaseSetMessages.TwoPhaseSet2 twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {
    Builder b = TwoPhaseSetMessages.TwoPhaseSet2.newBuilder();
    if (!twoPhaseSet.adds.isEmpty())
      b.setAdds(otherMessageToProto(twoPhaseSet.adds).toByteString());
    if (!twoPhaseSet.removals.isEmpty())
      b.setRemovals(otherMessageToProto(twoPhaseSet.removals).toByteString());
    return b.build();
  }

  @SuppressWarnings("unchecked")
  protected TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {
    try {
      TwoPhaseSetMessages.TwoPhaseSet2 msg = TwoPhaseSetMessages.TwoPhaseSet2.parseFrom(bytes);

      GSet<String> adds = GSet.create();
      if (msg.hasAdds()) adds = (GSet<String>) otherMessageFromBinary(msg.getAdds().toByteArray());

      GSet<String> removals = GSet.create();
      if (msg.hasRemovals())
        adds = (GSet<String>) otherMessageFromBinary(msg.getRemovals().toByteArray());

      return new TwoPhaseSet(adds, removals);
    } catch (Exception e) {
      throw new RuntimeException(e.getMessage(), e);
    }
  }
}




Durable Storage


By default the data is only kept in memory. It is redundant since it is replicated to other nodes in the cluster, but if you stop all nodes the data is lost, unless you have saved it elsewhere.


Entries can be configured to be durable, i.e. stored on local disk on each node. The stored data will be loaded next time the replicator is started, i.e. when actor system is restarted. This means data will survive as long as at least one node from the old cluster takes part in a new cluster. The keys of the durable entries are configured with:


akka.cluster.distributed-data.durable.keys = ["a", "b", "durable*"]



Prefix matching is supported by using 
*
 at the end of a key.


All entries can be made durable by specifying:


akka.cluster.distributed-data.durable.keys = ["*"]



LMDB
LMDB
 is the default storage implementation. It is possible to replace that with another implementation by implementing the actor protocol described in 
akka.cluster.ddata.DurableStore
 and defining the 
akka.cluster.distributed-data.durable.store-actor-class
 property for the new implementation.
Java 17


When using LMDB with Java 17 you have to add JVM flags 
--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED
.


The location of the files for the data is configured with:




Scala




# Directory of LMDB file. There are two options:
# 1. A relative or absolute path to a directory that ends with 'ddata'
#    the full name of the directory will contain name of the ActorSystem
#    and its remote port.
# 2. Otherwise the path is used as is, as a relative or absolute path to
#    a directory.
akka.cluster.distributed-data.durable.lmdb.dir = "ddata"



Java




# Directory of LMDB file. There are two options:
# 1. A relative or absolute path to a directory that ends with 'ddata'
#    the full name of the directory will contain name of the ActorSystem
#    and its remote port.
# 2. Otherwise the path is used as is, as a relative or absolute path to
#    a directory.
akka.cluster.distributed-data.durable.lmdb.dir = "ddata"





When running in production you may want to configure the directory to a specific path (alt 2), since the default directory contains the remote port of the actor system to make the name unique. If using a dynamically assigned port (0) it will be different each time and the previously stored data will not be loaded.


Making the data durable has a performance cost. By default, each update is flushed to disk before the 
UpdateSuccess
 reply is sent. For better performance, but with the risk of losing the last writes if the JVM crashes, you can enable write behind mode. Changes are then accumulated during a time period before it is written to LMDB and flushed to disk. Enabling write behind is especially efficient when performing many writes to the same key, because it is only the last value for each key that will be serialized and stored. The risk of losing writes if the JVM crashes is small since the data is typically replicated to other nodes immediately according to the given 
WriteConsistency
.


akka.cluster.distributed-data.durable.lmdb.write-behind-interval = 200 ms



Note that you should be prepared to receive 
WriteFailure
 as reply to an 
Update
 of a durable entry if the data could not be stored for some reason. When enabling 
write-behind-interval
 such errors will only be logged and 
UpdateSuccess
 will still be the reply to the 
Update
.


There is one important caveat when it comes pruning of 
CRDT Garbage
 for durable data. If an old data entry that was never pruned is injected and merged with existing data after that the pruning markers have been removed the value will not be correct. The time-to-live of the markers is defined by configuration 
akka.cluster.distributed-data.durable.pruning-marker-time-to-live
 and is in the magnitude of days. This would be possible if a node with durable data didn’t participate in the pruning (e.g. it was shutdown) and later started after this time. A node with durable data should not be stopped for longer time than this duration and if it is joining again after this duration its data should first be manually removed (from the lmdb directory).


Limitations


There are some limitations that you should be aware of.


CRDTs cannot be used for all types of problems, and eventual consistency does not fit all domains. Sometimes you need strong consistency.


It is not intended for 
Big Data
. The number of top level entries should not exceed 100000. When a new node is added to the cluster all these entries are transferred (gossiped) to the new node. The entries are split up in chunks and all existing nodes collaborate in the gossip, but it will take a while (tens of seconds) to transfer all entries and this means that you cannot have too many top level entries. The current recommended limit is 100000. We will be able to improve this if needed, but the design is still not intended for billions of entries.


All data is held in memory, which is another reason why it is not intended for 
Big Data
.


When a data entry is changed the full state of that entry may be replicated to other nodes if it doesn’t support 
delta-CRDT
. The full state is also replicated for delta-CRDTs, for example when new nodes are added to the cluster or when deltas could not be propagated because of network partitions or similar problems. This means that you cannot have too large data entries, because then the remote message size will be too large.


CRDT Garbage


One thing that can be problematic with CRDTs is that some data types accumulate history (garbage). For example a 
GCounter
 keeps track of one counter per node. If a 
GCounter
 has been updated from one node it will associate the identifier of that node forever. That can become a problem for long running systems with many cluster nodes being added and removed. To solve this problem the 
Replicator
 performs pruning of data associated with nodes that have been removed from the cluster. Data types that need pruning have to implement the 
RemovedNodePruning
 trait. See the API documentation of the 
Replicator
 for details.


Learn More about CRDTs




Strong Eventual Consistency and Conflict-free Replicated Data Types (video)
 talk by Mark Shapiro


A comprehensive study of Convergent and Commutative Replicated Data Types
 paper by Mark Shapiro et. al.




Configuration


The 
DistributedData
 extension can be configured with the following properties:


copy
source
# Settings for the DistributedData extension
akka.cluster.distributed-data {
  # Actor name of the Replicator actor, /system/ddataReplicator
  name = ddataReplicator

  # Replicas are running on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # How often the Replicator should send out gossip information
  gossip-interval = 2 s
  
  # How often the subscribers will be notified of changes, if any
  notify-subscribers-interval = 500 ms

  # Logging of data with payload size in bytes larger than
  # this value. Maximum detected size per key is logged once,
  # with an increase threshold of 10%.
  # It can be disabled by setting the property to off.
  log-data-size-exceeding = 10 KiB

  # Maximum number of entries to transfer in one round of gossip exchange when
  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.
  # The actual number of data entries in each Gossip message is dynamically
  # adjusted to not exceed the maximum remote message size (maximum-frame-size).
  max-delta-elements = 500
  
  # The id of the dispatcher to use for Replicator actors.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = "akka.actor.internal-dispatcher"

  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes. If this is set to 'off' the pruning feature will
  # be completely disabled.
  pruning-interval = 120 s
  
  # How long time it takes to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is 
  # unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes even though typical dissemination time
  # is shorter (grows logarithmic with number of nodes). There is no advantage of 
  # setting this too low. Setting it to large value will delay the pruning process.
  max-pruning-dissemination = 300 s
  
  # The markers of that pruning has been performed for a removed node are kept for this
  # time and thereafter removed. If and old data entry that was never pruned is somehow
  # injected and merged with existing data after this time the value will not be correct.
  # This would be possible (although unlikely) in the case of a long network partition.
  # It should be in the magnitude of hours. For durable data it is configured by 
  # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
 pruning-marker-time-to-live = 6 h
  
  # Serialized Write and Read messages are cached when they are sent to 
  # several nodes. If no further activity they are removed from the cache
  # after this duration.
  serializer-cache-time-to-live = 10s

  # Update and Get operations are sent to oldest nodes first.
  # This is useful together with Cluster Singleton, which is running on oldest nodes.
  prefer-oldest = off
  
  # Settings for delta-CRDT
  delta-crdt {
    # enable or disable delta-CRDT replication
    enabled = on
    
    # Some complex deltas grow in size for each update and above this
    # threshold such deltas are discarded and sent as full state instead.
    # This is number of elements or similar size hint, not size in bytes.
    max-delta-size = 50
  }

  # Map of keys and inactivity duration for entries that will automatically be removed
  # without tombstones when they have been inactive for the given duration.
  # Prefix matching is supported by using * at the end of a key.
  # Matching tombstones will also be removed after the expiry duration.
  expire-keys-after-inactivity {
    # Example syntax:
    # "key-1" = 10 minutes
    # "cache-*" = 2 minutes
  }
  
  durable {
    # List of keys that are durable. Prefix matching is supported by using * at the
    # end of a key.  
    keys = []
    
    # The markers of that pruning has been performed for a removed node are kept for this
    # time and thereafter removed. If and old data entry that was never pruned is
    # injected and merged with existing data after this time the value will not be correct.
    # This would be possible if replica with durable data didn't participate in the pruning
    # (e.g. it was shutdown) and later started after this time. A durable replica should not 
    # be stopped for longer time than this duration and if it is joining again after this
    # duration its data should first be manually removed (from the lmdb directory).
    # It should be in the magnitude of days. Note that there is a corresponding setting
    # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
    pruning-marker-time-to-live = 10 d
    
    # Fully qualified class name of the durable store actor. It must be a subclass
    # of akka.actor.Actor and handle the protocol defined in 
    # akka.cluster.ddata.DurableStore. The class must have a constructor with 
    # com.typesafe.config.Config parameter.
    store-actor-class = akka.cluster.ddata.LmdbDurableStore
    
    use-dispatcher = akka.cluster.distributed-data.durable.pinned-store
    
    pinned-store {
      executor = thread-pool-executor
      type = PinnedDispatcher
    }
    
    # Config for the LmdbDurableStore
    lmdb {
      # Directory of LMDB file. There are two options:
      # 1. A relative or absolute path to a directory that ends with 'ddata'
      #    the full name of the directory will contain name of the ActorSystem
      #    and its remote port.
      # 2. Otherwise the path is used as is, as a relative or absolute path to
      #    a directory.
      #
      # When running in production you may want to configure this to a specific
      # path (alt 2), since the default directory contains the remote port of the
      # actor system to make the name unique. If using a dynamically assigned 
      # port (0) it will be different each time and the previously stored data 
      # will not be loaded.
      dir = "ddata"
      
      # Size in bytes of the memory mapped file.
      map-size = 100 MiB
      
      # Accumulate changes before storing improves performance with the
      # risk of losing the last writes if the JVM crashes.
      # The interval is by default set to 'off' to write each update immediately.
      # Enabling write behind by specifying a duration, e.g. 200ms, is especially 
      # efficient when performing many writes to the same key, because it is only 
      # the last value for each key that will be serialized and stored.  
      # write-behind-interval = 200 ms
      write-behind-interval = off
    }
  }
  
}


Example project


Distributed Data example project
 
Distributed Data example project
 is an example project that can be downloaded, and with instructions of how to run.


This project contains several samples illustrating how to use Distributed Data.














 
Phi Accrual Failure Detector






Cluster Singleton 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/multi-jvm-testing.html
Multi JVM Testing • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing




Setup


Running tests


Creating application tests


Changing Defaults


Configuration of the JVM instances


ScalaTest


Multi Node Additions


Example project




Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing




Setup


Running tests


Creating application tests


Changing Defaults


Configuration of the JVM instances


ScalaTest


Multi Node Additions


Example project




Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Multi JVM Testing


Supports running applications (objects with main methods) and ScalaTest tests in multiple JVMs at the same time. Useful for integration testing where multiple systems communicate with each other.


Setup


The multi-JVM testing is an sbt plugin that you can find at 
https://github.com/sbt/sbt-multi-jvm
. To configure it in your project you should do the following steps:






Add it as a plugin by adding the following to your project/plugins.sbt:


addSbtPlugin("com.github.sbt" % "sbt-multi-jvm" % "0.6.0")







Add multi-JVM testing to 
build.sbt
 or 
project/Build.scala
 by enabling 
MultiJvmPlugin
 and setting the 
MultiJvm
 config.


```none
lazy val root = (project in file("."))
  .enablePlugins(MultiJvmPlugin)
  .configs(MultiJvm)
```







Please note
 that by default MultiJvm test sources are located in 
src/multi-jvm/...
, and not in 
src/test/...
.


Running tests


The multi-JVM tasks are similar to the normal tasks: 
test
, 
testOnly
, and 
run
, but are under the 
multi-jvm
 configuration.


So in Akka, to run all the multi-JVM tests in the akka-remote project use (at the sbt prompt):


akka-remote-tests/multi-jvm:test



Or one can change to the 
akka-remote-tests
 project first, and then run the tests:


project akka-remote-tests
multi-jvm:test



To run individual tests use 
testOnly
:


multi-jvm:testOnly akka.remote.RandomRoutedRemoteActor



More than one test name can be listed to run multiple specific tests. Tab-completion in sbt makes it easy to complete the test names.


It’s also possible to specify JVM options with 
testOnly
 by including those options after the test names and 
--
. For example:


multi-jvm:testOnly akka.remote.RandomRoutedRemoteActor -- -Dsome.option=something



Creating application tests


The tests are discovered, and combined, through a naming convention. MultiJvm test sources are located in 
src/multi-jvm/...
. A test is named with the following pattern:


{TestName}MultiJvm{NodeName}



That is, each test has 
MultiJvm
 in the middle of its name. The part before it groups together tests/applications under a single 
TestName
 that will run together. The part after, the 
NodeName
, is a distinguishing name for each forked JVM.


So to create a 3-node test called 
Sample
, you can create three applications like the following:


package sample

object SampleMultiJvmNode1 {
  def main(args: Array[String]) {
    println("Hello from node 1")
  }
}

object SampleMultiJvmNode2 {
  def main(args: Array[String]) {
    println("Hello from node 2")
  }
}

object SampleMultiJvmNode3 {
  def main(args: Array[String]) {
    println("Hello from node 3")
  }
}



When you call 
multi-jvm:run sample.Sample
 at the sbt prompt, three JVMs will be spawned, one for each node. It will look like this:


> multi-jvm:run sample.Sample
...
[info] * sample.Sample
[JVM-1] Hello from node 1
[JVM-2] Hello from node 2
[JVM-3] Hello from node 3
[success] Total time: ...



Changing Defaults


You can specify JVM options for the forked JVMs:


jvmOptions in MultiJvm := Seq("-Xmx256M")



You can change the name of the multi-JVM test source directory by adding the following configuration to your project:


unmanagedSourceDirectories in MultiJvm :=
   Seq(baseDirectory(_ / "src/some_directory_here")).join.value



You can change what the 
MultiJvm
 identifier is. For example, to change it to 
ClusterTest
 use the 
multiJvmMarker
 setting:


multiJvmMarker in MultiJvm := "ClusterTest"



Your tests should now be named 
{TestName}ClusterTest{NodeName}
.


Configuration of the JVM instances


You can define specific JVM options for each of the spawned JVMs. You do that by creating a file named after the node in the test with suffix 
.opts
 and put them in the same directory as the test.


For example, to feed the JVM options 
-Dakka.remote.port=9991
 and 
-Xmx256m
 to the 
SampleMultiJvmNode1
 let’s create three 
*.opts
 files and add the options to them. Separate multiple options with space.


SampleMultiJvmNode1.opts
:


-Dakka.remote.port=9991 -Xmx256m



SampleMultiJvmNode2.opts
:


-Dakka.remote.port=9992 -Xmx256m



SampleMultiJvmNode3.opts
:


-Dakka.remote.port=9993 -Xmx256m



ScalaTest


There is also support for creating ScalaTest tests rather than applications. To do this use the same naming convention as above, but create ScalaTest suites rather than objects with main methods. You need to have ScalaTest on the classpath. Here is a similar example to the one above but using ScalaTest:


package sample

import org.scalatest.wordspec.AnyWordSpec
import org.scalatest.matchers.must.Matchers

class SpecMultiJvmNode1 extends AnyWordSpec with Matchers {
  "A node" should {
    "be able to say hello" in {
      val message = "Hello from node 1"
      message must be("Hello from node 1")
    }
  }
}

class SpecMultiJvmNode2 extends AnyWordSpec with Matchers {
  "A node" should {
    "be able to say hello" in {
      val message = "Hello from node 2"
      message must be("Hello from node 2")
    }
  }
}



To run just these tests you would call 
multi-jvm:testOnly sample.Spec
 at the sbt prompt.


Multi Node Additions


There has also been some additions made to the 
SbtMultiJvm
 plugin to accommodate the 
may change
 module 
multi node testing
, described in that section.


Example project


Akka Cluster Sample with Scala
 is an example project that can be downloaded, and with instructions of how to run.


This project illustrates Cluster features and also includes Multi JVM Testing with the 
sbt-multi-jvm
 plugin.














 
Serialization with Jackson






Multi Node Testing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/security/index.html
Security Announcements • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Security Announcements
Note


Security announcements has moved to a shared page for all Akka projects and can now be found at 
akka.io/security


Receiving Security Advisories


The best way to receive any and all security announcements is to subscribe to the 
Akka security list
.


The mailing list is very low traffic, and receives notifications only after security reports have been managed by the core team and fixes are publicly available.


Reporting Vulnerabilities


We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.


Following best practice, we strongly encourage anyone to report potential security vulnerabilities to 
[email protected]
 before disclosing them in a public forum like the mailing list or as a GitHub issue.


Reports to this email address will be handled by our security team, who will work together with you to ensure that a fix can be provided without delay.


Security Related Documentation




Java Serialization


Remote deployment allow list


Remote Security




Fixed Security Vulnerabilities






Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16


















 
Akka Documentation






Java Serialization, Fixed in Akka 2.4.17 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/actor-interop.html
Actors interop • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop




Dependency


Overview




Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop




Dependency


Overview




Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Actors interop


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Overview


There are various use cases where it might be reasonable to use actors and streams together:




when integrating existing API’s that might be streams- or actors-based.


when there is any mutable state that should be shared across multiple streams.


when there is any mutable state or logic that can be influenced ‘from outside’ while the stream is running.




For piping the elements of a stream as messages to an ordinary actor you can use 
ask
 in a 
mapAsync
 or use 
Sink.actorRefWithBackpressure
.


Messages can be sent to a stream with 
Source.queue
 or via the 
ActorRef
 that is materialized by 
Source.actorRef
.


Additionally you can use 
ActorSource.actorRef
, 
ActorSource.actorRefWithBackpressure
, 
ActorSink.actorRef
 and 
ActorSink.actorRefWithBackpressure
 shown below.


ask
Note


See also: 
Flow.ask operator reference docs
, 
ActorFlow.ask operator reference docs
 for Akka Typed


A nice way to delegate some processing of elements in a stream to an actor is to use 
ask
. The back-pressure of the stream is maintained by the 
Future
CompletionStage
 of the 
ask
 and the mailbox of the actor will not be filled with more messages than the given 
parallelism
 of the 
ask
 operator (similarly to how the 
mapAsync
 operator works).




Scala




copy
source
implicit val askTimeout: Timeout = 5.seconds
val words: Source[String, NotUsed] =
  Source(List("hello", "hi"))

words
  .ask[String](parallelism = 5)(ref)
  // continue processing of the replies from the actor
  .map(_.toLowerCase)
  .runWith(Sink.ignore)


Java




copy
source
Source<String, NotUsed> words = Source.from(Arrays.asList("hello", "hi"));
Timeout askTimeout = Timeout.apply(5, TimeUnit.SECONDS);

words
    .ask(5, ref, String.class, askTimeout)
    // continue processing of the replies from the actor
    .map(elem -> elem.toLowerCase())
    .runWith(Sink.ignore(), system);




Note that the messages received in the actor will be in the same order as the stream elements, i.e. the 
parallelism
 does not change the ordering of the messages. There is a performance advantage of using parallelism > 1 even though the actor will only process one message at a time because then there is already a message in the mailbox when the actor has completed previous message.


The actor must reply to the 
sender()
getSender()
 for each message from the stream. That reply will complete the 
Future
CompletionStage
 of the 
ask
 and it will be the element that is emitted downstream.


In case the target actor is stopped, the operator will fail with an 
AskStageTargetActorTerminatedException




Scala




copy
source
class Translator extends Actor {
  def receive = {
    case word: String =>
      // ... process message
      val reply = word.toUpperCase
      sender() ! reply // reply to the ask
  }
}


Java




copy
source
static class Translator extends AbstractActor {
  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            String.class,
            word -> {
              // ... process message
              String reply = word.toUpperCase();
              // reply to the ask
              getSender().tell(reply, getSelf());
            })
        .build();
  }
}




The stream can be completed with failure by sending 
akka.actor.Status.Failure
 as reply from the actor.


If the 
ask
 fails due to timeout the stream will be completed with 
TimeoutException
 failure. If that is not desired outcome you can use 
recover
 on the 
ask
 
Future
CompletionStage
, or use the other “restart” operators to restart it.


If you don’t care about the reply values and only use them as back-pressure signals you can use 
Sink.ignore
 after the 
ask
 operator and then actor is effectively a sink of the stream.


Note that while you may implement the same concept using 
mapAsync
, that style would not be aware of the actor terminating.


If you are intending to ask multiple actors by using 
Actor routers
, then you should use 
mapAsyncUnordered
 and perform the ask manually in there, as the ordering of the replies is not important, since multiple actors are being asked concurrently to begin with, and no single actor is the one to be watched by the operator.


Sink.actorRefWithBackpressure
Note


See also: 
Sink.actorRefWithBackpressure operator reference docs


The sink sends the elements of the stream to the given 
ActorRef
 that sends back back-pressure signal. First element is always 
onInitMessage
, then stream is waiting for the given acknowledgement message from the given actor which means that it is ready to process elements. It also requires the given acknowledgement message after each stream element to make back-pressure work.


If the target actor terminates the stream will be cancelled. When the stream is completed successfully the given 
onCompleteMessage
 will be sent to the destination actor. When the stream is completed with failure a 
akka.actor.Status.Failure
 message will be sent to the destination actor.




Scala




copy
source
val words: Source[String, NotUsed] =
  Source(List("hello", "hi"))

// sent from actor to stream to "ack" processing of given element
val AckMessage = AckingReceiver.Ack

// sent from stream to actor to indicate start, end or failure of stream:
val InitMessage = AckingReceiver.StreamInitialized
val OnCompleteMessage = AckingReceiver.StreamCompleted
val onErrorMessage = (ex: Throwable) => AckingReceiver.StreamFailure(ex)

val probe = TestProbe()
val receiver = system.actorOf(Props(new AckingReceiver(probe.ref)))
val sink = Sink.actorRefWithBackpressure(
  receiver,
  onInitMessage = InitMessage,
  ackMessage = AckMessage,
  onCompleteMessage = OnCompleteMessage,
  onFailureMessage = onErrorMessage)

words.map(_.toLowerCase).runWith(sink)

probe.expectMsg("Stream initialized!")
probe.expectMsg("hello")
probe.expectMsg("hi")
probe.expectMsg("Stream completed!")


Java




copy
source
Source<String, NotUsed> words = Source.from(Arrays.asList("hello", "hi"));

final TestKit probe = new TestKit(system);

ActorRef receiver = system.actorOf(Props.create(AckingReceiver.class, probe.getRef()));

Sink<String, NotUsed> sink =
    Sink.<String>actorRefWithBackpressure(
        receiver,
        new StreamInitialized(),
        Ack.INSTANCE,
        new StreamCompleted(),
        ex -> new StreamFailure(ex));

words.map(el -> el.toLowerCase()).runWith(sink, system);

probe.expectMsg("Stream initialized");
probe.expectMsg("hello");
probe.expectMsg("hi");
probe.expectMsg("Stream completed");




The receiving actor would then need to be implemented similar to the following:




Scala




copy
source
object AckingReceiver {
  case object Ack

  case object StreamInitialized
  case object StreamCompleted
  case class StreamFailure(ex: Throwable)
}

class AckingReceiver(probe: ActorRef) extends Actor with ActorLogging {
  import AckingReceiver._

  def receive: Receive = {
    case StreamInitialized =>
      log.info("Stream initialized!")
      probe ! "Stream initialized!"
      sender() ! Ack // ack to allow the stream to proceed sending more elements

    case el: String =>
      log.info("Received element: {}", el)
      probe ! el
      sender() ! Ack // ack to allow the stream to proceed sending more elements

    case StreamCompleted =>
      log.info("Stream completed!")
      probe ! "Stream completed!"
    case StreamFailure(ex) =>
      log.error(ex, "Stream failed!")
  }
}


Java




copy
source
enum Ack {
  INSTANCE;
}

static class StreamInitialized {}

static class StreamCompleted {}

static class StreamFailure {
  private final Throwable cause;

  public StreamFailure(Throwable cause) {
    this.cause = cause;
  }

  public Throwable getCause() {
    return cause;
  }
}

static class AckingReceiver extends AbstractLoggingActor {

  private final ActorRef probe;

  public AckingReceiver(ActorRef probe) {
    this.probe = probe;
  }

  @Override
  public Receive createReceive() {
    return receiveBuilder()
        .match(
            StreamInitialized.class,
            init -> {
              log().info("Stream initialized");
              probe.tell("Stream initialized", getSelf());
              sender().tell(Ack.INSTANCE, self());
            })
        .match(
            String.class,
            element -> {
              log().info("Received element: {}", element);
              probe.tell(element, getSelf());
              sender().tell(Ack.INSTANCE, self());
            })
        .match(
            StreamCompleted.class,
            completed -> {
              log().info("Stream completed");
              probe.tell("Stream completed", getSelf());
            })
        .match(
            StreamFailure.class,
            failed -> {
              log().error(failed.getCause(), "Stream failed!");
              probe.tell("Stream failed!", getSelf());
            })
        .build();
  }
}




Note that replying to the sender of the elements (the “stream”) is required as lack of those ack signals would be interpreted as back-pressure (as intended), and no new elements will be sent into the actor until it acknowledges some elements. Handling the other signals while is not required, however is a good practice, to see the state of the stream’s lifecycle in the connected actor as well. Technically it is also possible to use multiple sinks targeting the same actor, however it is not common practice to do so, and one should rather investigate using a 
Merge
 operator for this purpose.
Note


Using 
Sink.actorRef
 or ordinary 
tell
 from a 
map
 or 
foreach
 operator means that there is no back-pressure signal from the destination actor, i.e. if the actor is not consuming the messages fast enough the mailbox of the actor will grow, unless you use a bounded mailbox with zero 
mailbox-push-timeout-time
 or use a rate limiting operator in front. It’s often better to use 
Sink.actorRefWithBackpressure
 or 
ask
 in 
mapAsync
, though.


Source.queue


Source.queue
 is an improvement over 
Sink.actorRef
, since it can provide backpressure. The 
offer
 method returns a 
Future
CompletionStage
, which completes with the result of the enqueue operation.


Source.queue
 can be used for emitting elements to a stream from an actor (or from anything running outside the stream). The elements will be buffered until the stream can process them. You can 
offer
 elements to the queue and they will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received.


Use overflow strategy 
akka.stream.OverflowStrategy.backpressure
 to avoid dropping of elements if the buffer is full, instead the returned 
Future
CompletionStage
 does not complete until there is space in the buffer and 
offer
 should not be called again until it completes.


Using 
Source.queue
 you can push elements to the queue and they will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received. Elements in the buffer will be discarded if downstream is terminated.


You could combine it with the 
throttle
 operator is used to slow down the stream to 
5 element
 per 
3 seconds
 and other patterns.


SourceQueue.offer
 returns 
Future[QueueOfferResult]
CompletionStage<QueueOfferResult>
 which completes with 
QueueOfferResult.Enqueued
 if element was added to buffer or sent downstream. It completes with 
QueueOfferResult.Dropped
 if element was dropped. Can also complete with 
QueueOfferResult.Failure
 - when stream failed or 
QueueOfferResult.QueueClosed
 when downstream is completed.




Scala




copy
source
val bufferSize = 10
val elementsToProcess = 5

val queue = Source
  .queue[Int](bufferSize)
  .throttle(elementsToProcess, 3.second)
  .map(x => x * x)
  .toMat(Sink.foreach(x => println(s"completed $x")))(Keep.left)
  .run()

val source = Source(1 to 10)

source
  .map(x => {
    queue.offer(x).map {
      case QueueOfferResult.Enqueued    => println(s"enqueued $x")
      case QueueOfferResult.Dropped     => println(s"dropped $x")
      case QueueOfferResult.Failure(ex) => println(s"Offer failed ${ex.getMessage}")
      case QueueOfferResult.QueueClosed => println("Source Queue closed")
    }
  })
  .runWith(Sink.ignore)


Java




copy
source
int bufferSize = 10;
int elementsToProcess = 5;

BoundedSourceQueue<Integer> sourceQueue =
    Source.<Integer>queue(bufferSize)
        .throttle(elementsToProcess, Duration.ofSeconds(3))
        .map(x -> x * x)
        .to(Sink.foreach(x -> System.out.println("got: " + x)))
        .run(system);

Source<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));

source.map(x -> sourceQueue.offer(x)).runWith(Sink.ignore(), system);





When used from an actor you typically 
pipe
 the result of the 
Future
CompletionStage
 back to the actor to continue processing.


Source.actorRef


Messages sent to the actor that is materialized by 
Source.actorRef
 will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received.


Depending on the defined 
OverflowStrategy
 it might drop elements if there is no space available in the buffer. The strategy 
OverflowStrategy.backpressure
 is not supported for this Source type, i.e. elements will be dropped if the buffer is filled by sending at a rate that is faster than the stream can consume. You should consider using 
Source.queue
 if you want a backpressured actor interface.


The stream can be completed successfully by sending any message to the actor that is handled by the completion matching function that was provided when the actor reference was created. If the returned completion strategy is 
akka.stream.CompletionStrategy.immediately
 the completion will be signaled immediately. If the completion strategy is 
akka.stream.CompletionStrategy.draining
, already buffered elements will be processed before signaling completion. Any elements that are in the actor’s mailbox and subsequent elements sent to the actor will not be processed.


The stream can be completed with failure by sending any message to the actor that is handled by the failure matching function that was specified when the actor reference was created.


The actor will be stopped when the stream is completed, failed or cancelled from downstream. You can watch it to get notified when that happens.




Scala




copy
source
val bufferSize = 10

val cm: PartialFunction[Any, CompletionStrategy] = {
  case Done =>
    CompletionStrategy.immediately
}

val ref = Source
  .actorRef[Int](
    completionMatcher = cm,
    failureMatcher = PartialFunction.empty[Any, Throwable],
    bufferSize = bufferSize,
    overflowStrategy = OverflowStrategy.fail) // note: backpressure is not supported
  .map(x => x * x)
  .toMat(Sink.foreach((x: Int) => println(s"completed $x")))(Keep.left)
  .run()

ref ! 1
ref ! 2
ref ! 3
ref ! Done


Java




copy
source
int bufferSize = 10;

Source<Integer, ActorRef> source =
    Source.actorRef(
        elem -> {
          // complete stream immediately if we send it Done
          if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());
          else return Optional.empty();
        },
        // never fail the stream because of a message
        elem -> Optional.empty(),
        bufferSize,
        OverflowStrategy.dropHead()); // note: backpressure is not supported
ActorRef actorRef =
    source
        .map(x -> x * x)
        .to(Sink.foreach(x -> System.out.println("got: " + x)))
        .run(system);

actorRef.tell(1, ActorRef.noSender());
actorRef.tell(2, ActorRef.noSender());
actorRef.tell(3, ActorRef.noSender());
actorRef.tell(
    new akka.actor.Status.Success(CompletionStrategy.draining()), ActorRef.noSender());




ActorSource.actorRef


Materialize an 
ActorRef<T>
ActorRef[T]
; sending messages to it will emit them on the stream only if they are of the same type as the stream.
Note


See also: 
ActorSource.actorRef operator reference docs


ActorSource.actorRefWithBackpressure


Materialize an 
ActorRef<T>
ActorRef[T]
; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.
Note


See also: 
ActorSource.actorRefWithBackpressure operator reference docs


ActorSink.actorRef


Sends the elements of the stream to the given 
ActorRef<T>
ActorRef[T]
, without considering backpressure.
Note


See also: 
ActorSink.actorRef operator reference docs


ActorSink.actorRefWithBackpressure


Sends the elements of the stream to the given 
ActorRef<T>
ActorRef[T]
 with backpressure, to be able to signal demand when the actor is ready to receive more elements.
Note


See also: 
ActorSink.actorRefWithBackpressure operator reference docs


Topic.source


A source that will subscribe to a 
Topic
Topic
 and stream messages published to the topic.
Note


See also: 
ActorSink.actorRefWithBackpressure operator reference docs


Topic.sink


A sink that will publish emitted messages to a 
Topic
Topic
.
Note


See also: 
ActorSink.actorRefWithBackpressure operator reference docs














 
Futures interop






Reactive Streams Interop 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/index.html
Getting Started Guide • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Getting Started Guide






Introduction to Akka




How to get started




Why modern systems need a new programming model




The challenge of encapsulation


The illusion of shared memory on modern computer architectures


The illusion of a call stack




How the Actor Model Meets the Needs of Modern, Distributed Systems




Usage of message passing avoids locking and blocking


Actors handle error situations gracefully




Overview of Akka libraries and modules




Actor library


Remoting


Cluster


Cluster Sharding


Cluster Singleton


Persistence


Projections


Distributed Data


Streams


Alpakka


HTTP


gRPC


Example of module use




Introduction to the Example




Prerequisites


IoT example use case


What you will learn in this tutorial




Part 1: Actor Architecture




Dependency


Introduction


The Akka actor hierarchy


Summary




Part 2: Creating the First Actor




Introduction


What’s next?




Part 3: Working with Device Actors




Introduction


Identifying messages for devices


Adding flexibility to device messages


Implementing the device actor and its read protocol


Testing the actor


Adding a write protocol


Actor with read and write messages


What’s Next?




Part 4: Working with Device Groups




Introduction


Device manager hierarchy


The Registration Protocol


Adding registration support to device group actors


Creating device manager actors


What’s next?




Part 5: Querying Device Groups




Introduction


Dealing with possible scenarios


Implementing the query


Adding query capability to the group


Summary


What’s Next?




















 
Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16






Introduction to Akka 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/logging.html
Logging • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging




Dependency


Introduction


How to log


MDC


SLF4J API compatibility


SLF4J backend


Internal logging by Akka


Logging in tests




Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities




EventStream


Logging




Dependency


Introduction


How to log


MDC


SLF4J API compatibility


SLF4J backend


Internal logging by Akka


Logging in tests




Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Logging


You are viewing the documentation for the new actor APIs, to view the Akka Classic documentation, see 
Classic Logging
.


Dependency


To use Logging, you must at least use the Akka actors dependency in your project, and configure logging via the SLF4J backend, such as Logback configuration.


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-actor-typed" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor-typed_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor-typed_${versions.ScalaBinary}"
}


Introduction


SLF4J
 is used for logging and Akka provides access to an 
org.slf4j.Logger
 for a specific actor via the 
ActorContext
ActorContext
. You may also retrieve a 
Logger
 with the ordinary 
org.slf4j.LoggerFactory
.


To ensure that logging has minimal performance impact it’s important that you configure an asynchronous appender for the SLF4J backend. Logging generally means IO and locks, which can slow down the operations of your code if it was performed synchronously.


How to log


The 
ActorContext
ActorContext
 provides access to an 
org.slf4j.Logger
 for a specific actor.




Scala




copy
source
Behaviors.receive[String] { (context, message) =>
  context.log.info("Received message: {}", message)
  Behaviors.same
}


Java




copy
source
public class MyLoggingBehavior extends AbstractBehavior<String> {

  public static Behavior<String> create() {
    return Behaviors.setup(MyLoggingBehavior::new);
  }

  private MyLoggingBehavior(ActorContext<String> context) {
    super(context);
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onMessage(String.class, this::onReceive).build();
  }

  private Behavior<String> onReceive(String message) {
    getContext().getLog().info("Received message: {}", message);
    return this;
  }
}




The 
Logger
 via the 
ActorContext
 will automatically have a name that corresponds to the 
Behavior
Behavior
 of the actor when the log is accessed the first time. The class name when using 
AbstractBehavior
AbstractBehavior
 or the class 
or object
 name where the 
Behavior
 is defined when using the functional style. You can set a custom logger name with the 
setLoggerName
setLoggerName
 of the 
ActorContext
.




Scala




copy
source
Behaviors.setup[String] { context =>
  context.setLoggerName("com.myservice.BackendManager")
  context.log.info("Starting up")

  Behaviors.receiveMessage { message =>
    context.log.debug("Received message: {}", message)
    Behaviors.same
  }
}


Java




copy
source
public class BackendManager extends AbstractBehavior<String> {

  public static Behavior<String> create() {
    return Behaviors.setup(
        context -> {
          context.setLoggerName(BackendManager.class);
          context.getLog().info("Starting up");
          return new BackendManager(context);
        });
  }

  private BackendManager(ActorContext<String> context) {
    super(context);
  }

  @Override
  public Receive<String> createReceive() {
    return newReceiveBuilder().onMessage(String.class, this::onReceive).build();
  }

  private Behavior<String> onReceive(String message) {
    getContext().getLog().debug("Received message: {}", message);
    return this;
  }
}




The convention is to use logger names like fully qualified class names. The parameter to 
setLoggerName
 can be a 
String
 or a 
Class
, where the latter is convenience for the class name.


When logging via the 
ActorContext
 the path of the actor will automatically be added as 
akkaSource
 Mapped Diagnostic Context (MDC) value. MDC is typically implemented with a 
ThreadLocal
 by the SLF4J backend. To reduce performance impact, this MDC value is set when you access the 
log
getLog()
 method so you shouldn’t cache the returned 
Logger
 in your own field. That is handled by 
ActorContext
 and retrieving the 
Logger
 repeatedly with the 
log
getLog
 method has low overhead. The MDC is cleared automatically after processing of current message has finished.
Note


The 
Logger
 is thread-safe but the 
log
getLog
 method in 
ActorContext
 is not thread-safe and should not be accessed from threads other than the ordinary actor message processing thread, such as 
Future
CompletionStage
 callbacks.


It’s also perfectly fine to use a 
Logger
 retrieved via 
org.slf4j.LoggerFactory
, but then the logging events will not include the 
akkaSource
 MDC value. This is the recommended way when logging outside of an actor, including logging from 
Future
CompletionStage
 callbacks.




Scala




copy
source
val log = LoggerFactory.getLogger("com.myservice.BackendTask")

Future {
  // some work
  "result"
}.onComplete {
  case Success(result) => log.info("Task completed: {}", result)
  case Failure(exc)    => log.error("Task failed", exc)
}


Java




copy
source
class BackendTask {
  private final Logger log = LoggerFactory.getLogger(getClass());

  void run() {
    CompletableFuture<String> task =
        CompletableFuture.supplyAsync(
            () -> {
              // some work
              return "result";
            });
    task.whenComplete(
        (result, exc) -> {
          if (exc == null) log.error("Task failed", exc);
          else log.info("Task completed: {}", result);
        });
  }
}




Placeholder arguments


The log message may contain argument placeholders 
{}
, which will be substituted if the log level is enabled. Compared to constructing a full string for the log message this has the advantage of avoiding superfluous string concatenation and object allocations when the log level is disabled. Some logging backends may also use these message templates before argument substitution to group and filter logging events.


It can be good to know that 3 or more arguments will result in the relatively small cost of allocating an array (vararg parameter) also when the log level is disabled. The methods with 1 or 2 arguments don’t allocate the vararg array.


Behaviors.logMessages


If you want very detailed logging of messages and signals you can decorate a 
Behavior
Behavior
 with 
Behaviors.logMessages
Behaviors.logMessages
.




Scala




copy
source
import akka.actor.typed.LogOptions
import org.slf4j.event.Level

Behaviors.logMessages(LogOptions().withLevel(Level.TRACE), BackendManager())


Java




copy
source
import org.slf4j.event.Level;

Behaviors.logMessages(LogOptions.create().withLevel(Level.TRACE), BackendManager.create());




MDC


MDC
 allows for adding additional context dependent attributes to log entries. Out of the box, Akka will place the path of the actor in the the MDC attribute 
akkaSource
.


One or more tags can also be added to the MDC using the 
ActorTags
ActorTags
 props. The tags will be rendered as a comma separated list and be put in the MDC attribute 
akkaTags
. This can be used to categorize log entries from a set of different actors to allow easier filtering of logs:




Scala




copy
source
context.spawn(myBehavior, "MyActor", ActorTags("processing"))


Java




copy
source
context.spawn(myBehavior, "MyActor", ActorTags.create("processing"));




In addition to these two built in MDC attributes you can also decorate a 
Behavior
Behavior
 with 
Behaviors.withMdc
Behaviors.withMdc
 or use the 
org.slf4j.MDC
 API directly.


The 
Behaviors.withMdc
 decorator has two parts. A static 
Map
 of MDC attributes that are not changed, and a dynamic 
Map
 that can be constructed for each message.




Scala




copy
source
val staticMdc = Map("startTime" -> system.startTime.toString)
Behaviors.withMdc[BackendManager.Command](
  staticMdc,
  mdcForMessage =
    (msg: BackendManager.Command) => Map("identifier" -> msg.identifier, "upTime" -> system.uptime.toString)) {
  BackendManager()
}


Java




copy
source
Map<String, String> staticMdc = new HashMap<>();
staticMdc.put("startTime", String.valueOf(system.startTime()));

Behaviors.withMdc(
    BackendManager2.Command.class,
    staticMdc,
    message -> {
      Map<String, String> msgMdc = new HashMap<>();
      msgMdc.put("identifier", message.identifier());
      msgMdc.put("upTime", String.valueOf(system.uptime()));
      return msgMdc;
    },
    BackendManager2.create());




If you use the MDC API directly, be aware that MDC is typically implemented with a 
ThreadLocal
 by the SLF4J backend. Akka clears the MDC if logging is performed via the 
log
getLog()
 of the 
ActorContext
 and it is cleared automatically after processing of current message has finished, but only if you accessed 
log
getLog()
. The entire MDC is cleared, including attributes that you add yourself to the MDC. MDC is not cleared automatically if you use a 
Logger
 via 
LoggerFactory
 or not touch 
log
getLog()
 in the 
ActorContext
.


SLF4J API compatibility


Since Akka 2.10.0, only SLF4j version 
2.0
 is supported. 


It is not possible to mix a logger backend supporting one version with SLF4J API of older versions.


SLF4J backend


To ensure that logging has minimal performance impact it’s important that you configure an asynchronous appender for the SLF4J backend. Logging generally means IO and locks, which can slow down the operations of your code if it was performed synchronously.
Warning


For production the SLF4J backend should be configured with an asynchronous appender as described here. Otherwise, there is a risk of reduced performance and thread starvation problems of the dispatchers that are running actors and other tasks.


Logback


akka-actor-typed
 includes a dependency to the 
slf4j-api
. In your runtime, you also need a SLF4J backend. We recommend 
Logback
:
sbt
libraryDependencies += "ch.qos.logback" % "logback-classic" % "1.5.15"
Maven
<dependencies>
  <dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-classic</artifactId>
    <version>1.5.15</version>
  </dependency>
</dependencies>
Gradle
dependencies {
  implementation "ch.qos.logback:logback-classic:1.5.15"
}


Logback has flexible configuration options and details can be found in the 
Logback manual
 and other external resources.


One part that is important to highlight is the importance of configuring an 
AsyncAppender
, because it offloads rendering of logging events to a background thread, increasing performance. It doesn’t block the threads of the 
ActorSystem
ActorSystem
 while the underlying infrastructure writes the log messages to disk or other configured destination. It also contains a feature which will drop 
INFO
 and 
DEBUG
 messages if the logging load is high.


A starting point for configuration of 
logback.xml
 for production:


copy
source
<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>myapp.log</file>
        <immediateFlush>false</immediateFlush>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>myapp_%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
        </encoder>
    </appender>

    <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>8192</queueSize>
        <neverBlock>true</neverBlock>
        <appender-ref ref="FILE" />
    </appender>

    <root level="INFO">
        <appender-ref ref="ASYNC"/>
    </root>
</configuration>


Note that the 
AsyncAppender
 may drop log events if the queue becomes full, which may happen if the logging backend can’t keep up with the throughput of produced log events. Dropping log events is necessary if you want to gracefully degrade your application if only your logging backend or filesystem is experiencing issues. 


An alternative of the Logback 
AsyncAppender
 with better performance is the 
Logstash async appender
.


The ELK-stack is commonly used as logging infrastructure for production:




Logstash Logback encoder


Logstash


Elasticsearch


Kibana




For development you might want to log to standard out, but also have all debug level logging to file, like in this example:


copy
source
<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <encoder>
            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
        </encoder>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.FileAppender">
        <file>target/myapp-dev.log</file>
        <encoder>
            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
        </encoder>
    </appender>

    <root level="DEBUG">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>


Place the 
logback.xml
 file in 
src/main/resources/logback.xml
. For tests you can define different logging configuration in 
src/test/resources/logback-test.xml
.


MDC values


When logging via the 
log
getLog()
 of the 
ActorContext
, as described in 
How to log
, Akka includes a few MDC properties:




akkaSource
: the actor’s path


akkaAddress
: the full address of the ActorSystem, including hostname and port if Cluster is enabled


akkaTags
: tags defined in the 
Props
Props
 of the actor


sourceActorSystem
: the name of the ActorSystem




These MDC properties can be included in the Logback output with for example 
%X{akkaSource}
 specifier within the 
pattern layout configuration
:


  <encoder>
    <pattern>%date{ISO8601} %-5level %logger{36} %X{akkaSource} - %msg%n</pattern>
  </encoder>



All MDC properties as key-value entries can be included with 
%mdc
:


  <encoder>
    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>
  </encoder>



Internal logging by Akka


Event bus


For historical reasons logging by the Akka internals and by classic actors are performed asynchronously through an event bus. Such log events are processed by an event handler actor, which then emits them to SLF4J or directly to standard out.


When 
akka-actor-typed
 and 
akka-slf4j
 are on the classpath this event handler actor will emit the events to SLF4J. The 
akka.event.slf4j.Slf4jLogger
akka.event.slf4j.Slf4jLogger
 and 
akka.event.slf4j.Slf4jLoggingFilter
akka.event.slf4j.Slf4jLoggingFilter
 are enabled automatically without additional configuration. This can be disabled by 
akka.use-slf4j=off
 configuration property.


In other words, you don’t have to do anything for the Akka internal logging to end up in your configured SLF4J backend.


Log level


Ultimately the log level defined in the SLF4J backend is used. For the Akka internal logging it will also check the level defined by the SLF4J backend before constructing the final log message and emitting it to the event bus.


However, there is an additional 
akka.loglevel
 configuration property that defines if logging events with lower log level should be discarded immediately without consulting the SLF4J backend. By default this is at 
INFO
 level, which means that 
DEBUG
 level logging from the Akka internals will not reach the SLF4J backend even if 
DEBUG
 is enabled in the backend.


You can enable 
DEBUG
 level for 
akka.loglevel
 and control the actual level in the SLF4j backend without any significant overhead, also for production.


akka.loglevel = "DEBUG"



To turn off all Akka internal logging (not recommended) you can configure the log levels to be 
OFF
 like this.


akka {
  stdout-loglevel = "OFF"
  loglevel = "OFF"
}



The 
stdout-loglevel
 is only in effect during system startup and shutdown, and setting it to 
OFF
 as well, ensures that nothing gets logged during system startup or shutdown.


See 
Logger names
 for configuration of log level in SLF4J backend for certain modules of Akka.


Logging to stdout during startup and shutdown


When the actor system is starting up and shutting down the configured 
loggers
 are not used. Instead log messages are printed to stdout (System.out). The default log level for this stdout logger is 
WARNING
 and it can be silenced completely by setting 
akka.stdout-loglevel=OFF
.


Logging of Dead Letters


By default messages sent to dead letters are logged at info level. Existence of dead letters does not necessarily indicate a problem, but they are logged by default for the sake of caution. After a few messages this logging is turned off, to avoid flooding the logs. You can disable this logging completely or adjust how many dead letters are logged. During system shutdown it is likely that you see dead letters, since pending messages in the actor mailboxes are sent to dead letters. You can also disable logging of dead letters during shutdown.


akka {
  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
}



To customize the logging further or take other actions for dead letters you can subscribe to the 
Event Stream
.


Auxiliary logging options


Akka has a few configuration options for very low level debugging. These make more sense in development than in production.


You almost definitely need to have logging set to DEBUG to use any of the options below:


akka {
  loglevel = "DEBUG"
}



This config option is very good if you want to know what config settings are loaded by Akka:


akka {
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = on
}



If you want unhandled messages logged at DEBUG:


akka {
  actor {
    debug {
      # enable DEBUG logging of unhandled messages
      unhandled = on
    }
  }
}



If you want to monitor subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream:


akka {
  actor {
    debug {
      # enable DEBUG logging of subscription changes on the eventStream
      event-stream = on
    }
  }
}





Auxiliary remote logging options


If you want to see all messages that are sent through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are sent by the transport layer, not by an actor.


akka.remote.artery {
  # If this is "on", Akka will log all outbound messages at DEBUG level,
  # if off then they are not logged
  log-sent-messages = on
}



If you want to see all messages that are received through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are received by the transport layer, not by an actor.


akka.remote.artery {
  # If this is "on", Akka will log all inbound messages at DEBUG level,
  # if off then they are not logged
  log-received-messages = on
}



Logging of message types with payload size in bytes larger than the configured 
log-frame-size-exceeding
.


akka.remote.artery {
  log-frame-size-exceeding = 10000b
}



MDC values from Akka internal logging


Since the logging is done asynchronously, the thread in which the logging was performed is captured in MDC with attribute name 
sourceThread
.


The path of the actor in which the logging was performed is available in the MDC with attribute name 
akkaSource
.


The actor system name in which the logging was performed is available in the MDC with attribute name 
sourceActorSystem
, but that is typically also included in the 
akkaSource
 attribute.


The address of the actor system, containing host and port if the system is using cluster, is available through 
akkaAddress
.


For typed actors the log event timestamp is taken when the log call was made but for Akka’s 
internal
 logging as well as the classic actor logging is asynchronous which means that the timestamp of a log entry is taken from when the underlying logger implementation is called, which can be surprising at first. If you want to more accurately output the timestamp for such loggers, use the MDC attribute 
akkaTimestamp
. Note that the MDC key will not have any value for a typed actor.


Markers


Akka is logging some events with markers. Some of these events also include structured MDC properties. 




The “SECURITY” marker is used for highlighting security related events or incidents.


Akka Actor is using the markers defined in 
ActorLogMarker
ActorLogMarker
.


Akka Cluster is using the markers defined in 
ClusterLogMarker
ClusterLogMarker
.


Akka Remoting is using the markers defined in 
RemoteLogMarker
RemoteLogMarker
.


Akka Cluster Sharding is using the markers defined in 
ShardingLogMarker
ShardingLogMarker
.




Markers and MDC properties are automatically picked up by the 
Logstash Logback encoder
.


The marker can be included in the Logback output with 
%marker
 and all MDC properties as key-value entries with 
%mdc
.


  <encoder>
    <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>
  </encoder>



Logger names


It can be useful to enable debug level or other SLF4J backend configuration for certain modules of Akka when troubleshooting. Those logger names are typically prefixed with the package name of the classes in that module. For example, in Logback the configuration may look like this to enable debug logging for Cluster Sharding: 


   <logger name="akka.cluster.sharding" level="DEBUG" />

    <root level="INFO">
        <appender-ref ref="ASYNC"/>
    </root>



Other examples of logger names or prefixes:


akka.cluster
akka.cluster.Cluster
akka.cluster.ClusterHeartbeat
akka.cluster.ClusterGossip
akka.cluster.ddata
akka.cluster.pubsub
akka.cluster.singleton
akka.cluster.sharding
akka.coordination.lease
akka.discovery
akka.persistence
akka.remote



Logging in tests


Testing utilities are described in 
Testing
.














 
EventStream






Circuit Breaker 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index.html
Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Akka Documentation






Security Announcements




Receiving Security Advisories


Reporting Vulnerabilities


Security Related Documentation


Fixed Security Vulnerabilities


Java Serialization, Fixed in Akka 2.4.17


Camel Dependency, Fixed in Akka 2.5.4


Broken random number generators AES128CounterSecureRNG / AES256CounterSecureRNG, Fixed in Akka 2.5.16




Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems


Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors




Introduction to Actors


Actor lifecycle


Interaction Patterns


Fault Tolerance


Actor discovery


Routers


Stash


Behaviors as finite state machines


Coordinated Shutdown


Dispatchers


Mailboxes


Testing


Coexistence


Style guide


Learning Akka Typed from Classic




Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)




Durable State


Style Guide


CQRS


Persistence Query


Building a storage backend for Durable State




Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities




EventStream


Logging


Circuit Breaker


Futures patterns


Extending Akka




Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run




Packaging


Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities
























Security Announcements 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-error.html
Error Handling in Streams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams




Dependency


Introduction


Logging errors


Recover


Recover with retries


Delayed restarts with a backoff operator


Supervision Strategies




Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams




Dependency


Introduction


Logging errors


Recover


Recover with retries


Delayed restarts with a backoff operator


Supervision Strategies




Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Error Handling in Streams


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


When an operator in a stream fails this will normally lead to the entire stream being torn down. Each of the operators downstream gets informed about the failure and each upstream operator sees a cancellation.


In many cases you may want to avoid complete stream failure, this can be done in a few different ways:




recover
recover
 to emit a final element then complete the stream normally on upstream failure


recoverWithRetries
recoverWithRetries
 to create a new upstream and start consuming from that on failure


Restarting sections of the stream after a backoff


Using a supervision strategy for operators that support it




In addition to these built in tools for error handling, a common pattern is to wrap the stream inside an actor, and have the actor restart the entire stream on failure.


Logging errors


log()
log()
 enables logging of a stream, which is typically useful for error logging. The below stream fails with 
ArithmeticException
 when the element 
0
 goes through the 
map
map
 operator, 




Scala




copy
source
Source(-5 to 5)
  .map(1 / _) //throwing ArithmeticException: / by zero
  .log("error logging")
  .runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(-1, 0, 1))
    .map(x -> 1 / x) // throwing ArithmeticException: / by zero
    .log("error logging")
    .runWith(Sink.ignore(), system);




and error messages like below will be logged. 


[error logging] Upstream failed.
java.lang.ArithmeticException: / by zero



If you want to control logging levels on each element, completion, and failure, you can find more details in 
Logging in streams
.


Recover


recover
recover
 allows you to emit a final element and then complete the stream on an upstream failure. Deciding which exceptions should be recovered is done through a 
PartialFunction
. If an exception does not have a 
matching case
 
match defined
 the stream is failed. 


Recovering can be useful if you want to gracefully complete a stream on failure while letting downstream know that there was a failure.


Throwing an exception inside 
recover
 
will
 be logged on ERROR level automatically.


More details in 
recover




Scala




copy
source
Source(0 to 6)
  .map(
    n =>
      // assuming `4` and `5` are unexpected values that could throw exception
      if (List(4, 5).contains(n)) throw new RuntimeException(s"Boom! Bad value found: $n")
      else n.toString)
  .recover {
    case e: RuntimeException => e.getMessage
  }
  .runForeach(println)


Java




copy
source
Source.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))
    .map(
        n -> {
          // assuming `4` and `5` are unexpected values that could throw exception
          if (Arrays.asList(4, 5).contains(n))
            throw new RuntimeException(String.format("Boom! Bad value found: %s", n));
          else return n.toString();
        })
    .recover(
        new PFBuilder<Throwable, String>()
            .match(RuntimeException.class, Throwable::getMessage)
            .build())
    .runForeach(System.out::println, system);




This will output:




Scala




copy
source
0
1
2
3                         // last element before failure
Boom! Bad value found: 4  // first element on failure


Java




copy
source
0
1
2
3                         // last element before failure
Boom! Bad value found: 4  // first element on failure




Recover with retries


recoverWithRetries
recoverWithRetries
 allows you to put a new upstream in place of the failed one, recovering stream failures up to a specified maximum number of times. 


Deciding which exceptions should be recovered is done through a 
PartialFunction
. If an exception does not have a 
matching case
 
match defined
 the stream is failed.




Scala




copy
source
val planB = Source(List("five", "six", "seven", "eight"))

Source(0 to 10)
  .map(n =>
    if (n < 5) n.toString
    else throw new RuntimeException("Boom!"))
  .recoverWithRetries(attempts = 1, {
    case _: RuntimeException => planB
  })
  .runForeach(println)


Java




copy
source
Source<String, NotUsed> planB = Source.from(Arrays.asList("five", "six", "seven", "eight"));

Source.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))
    .map(
        n -> {
          if (n < 5) return n.toString();
          else throw new RuntimeException("Boom!");
        })
    .recoverWithRetries(
        1, // max attempts
        new PFBuilder<Throwable, Source<String, NotUsed>>()
            .match(RuntimeException.class, ex -> planB)
            .build())
    .runForeach(System.out::println, system);




This will output:




Scala




copy
source
0
1
2
3
4
five
six
seven
eight


Java




copy
source
0
1
2
3
4
five
six
seven
eight






Delayed restarts with a backoff operator


Akka streams provides a 
RestartSource
RestartSource
, 
RestartSink
RestartSink
 and 
RestartFlow
RestartFlow
 for implementing the so-called 
exponential backoff supervision strategy
, starting an operator again when it fails or completes, each time with a growing time delay between restarts.


This pattern is useful when the operator fails or completes because some external resource is not available and we need to give it some time to start-up again. One of the prime examples when this is useful is when a WebSocket connection fails due to the HTTP server it’s running on going down, perhaps because it is overloaded. By using an exponential backoff, we avoid going into a tight reconnect loop, which both gives the HTTP server some time to recover, and it avoids using needless resources on the client side.


The various restart shapes mentioned all expect an 
RestartSettings
RestartSettings
 which configures the restart behaviour. Configurable parameters are:




minBackoff
 is the initial duration until the underlying stream is restarted


maxBackoff
 caps the exponential backoff


randomFactor
 allows addition of a random delay following backoff calculation


maxRestarts
 caps the total number of restarts


maxRestartsWithin
 sets a timeframe during which restarts are counted towards the same total for 
maxRestarts




The following snippet shows how to create a backoff supervisor using 
RestartSource
RestartSource
 which will supervise the given 
Source
Source
. The 
Source
 in this case is a stream of Server Sent Events, produced by akka-http. If the stream fails or completes at any point, the request will be made again, in increasing intervals of 3, 6, 12, 24 and finally 30 seconds (at which point it will remain capped due to the 
maxBackoff
 parameter):




Scala




copy
source
val settings = RestartSettings(
  minBackoff = 3.seconds,
  maxBackoff = 30.seconds,
  randomFactor = 0.2 // adds 20% "noise" to vary the intervals slightly
).withMaxRestarts(20, 5.minutes) // limits the amount of restarts to 20 within 5 minutes

val restartSource = RestartSource.withBackoff(settings) { () =>
  // Create a source from a future of a source
  Source.futureSource {
    // Make a single request with akka-http
    Http()
      .singleRequest(HttpRequest(uri = "http://example.com/eventstream"))
      // Unmarshall it as a source of server sent events
      .flatMap(Unmarshal(_).to[Source[ServerSentEvent, NotUsed]])
  }
}


Java




copy
source
RestartSettings settings =
    RestartSettings.create(
            Duration.ofSeconds(3), // min backoff
            Duration.ofSeconds(30), // max backoff
            0.2 // adds 20% "noise" to vary the intervals slightly
            )
        .withMaxRestarts(
            20, Duration.ofMinutes(5)); // limits the amount of restarts to 20 within 5 minutes

Source<ServerSentEvent, NotUsed> eventStream =
    RestartSource.withBackoff(
        settings,
        () ->
            // Create a source from a future of a source
            Source.completionStageSource(
                // Issue a GET request on the event stream
                Http.get(system)
                    .singleRequest(HttpRequest.create("http://example.com/eventstream"))
                    .thenCompose(
                        response ->
                            // Unmarshall it to a stream of ServerSentEvents
                            EventStreamUnmarshalling.fromEventStream()
                                .unmarshall(response, materializer))));




Using a 
randomFactor
 to add a little bit of additional variance to the backoff intervals is highly recommended, in order to avoid multiple streams re-start at the exact same point in time, for example because they were stopped due to a shared resource such as the same server going down and re-starting after the same configured interval. By adding additional randomness to the re-start intervals the streams will start in slightly different points in time, thus avoiding large spikes of traffic hitting the recovering server or other resource that they all need to contact.


The above 
RestartSource
 will never terminate unless the 
Sink
Sink
 it’s fed into cancels. It will often be handy to use it in combination with a 
KillSwitch
, so that you can terminate it when needed:




Scala




copy
source
val killSwitch = restartSource
  .viaMat(KillSwitches.single)(Keep.right)
  .toMat(Sink.foreach(event => println(s"Got event: $event")))(Keep.left)
  .run()

doSomethingElse()

killSwitch.shutdown()


Java




copy
source
KillSwitch killSwitch =
    eventStream
        .viaMat(KillSwitches.single(), Keep.right())
        .toMat(Sink.foreach(event -> System.out.println("Got event: " + event)), Keep.left())
        .run(materializer);

doSomethingElse();

killSwitch.shutdown();




Sinks and flows can also be supervised, using 
RestartSink
RestartSink
 and 
RestartFlow
RestartFlow
. The 
RestartSink
 is restarted when it cancels, while the 
RestartFlow
 is restarted when either the in port cancels, the out port completes, or the out  port sends an error.
Note


Care should be taken when using 
GraphStage
s
 that conditionally propagate termination signals inside a 
RestartSource
RestartSource
, 
RestartSink
RestartSink
 or 
RestartFlow
RestartFlow
. 


An example is a 
Broadcast
 operator with the default 
eagerCancel = false
 where some of the outlets are for side-effecting branches (that do not re-join e.g. via a 
Merge
). A failure on a side branch will not terminate the supervised stream which will not be restarted. Conversely, a failure on the main branch can trigger a restart but leave behind old running instances of side branches.


In this example 
eagerCancel
 should probably be set to 
true
, or, when only a single side branch is used, 
alsoTo
 or 
divertTo
 should be considered as alternatives.


Supervision Strategies
Note


The operators that support supervision strategies are explicitly documented to do so, if there is nothing in the documentation of an operator saying that it adheres to the supervision strategy it means it fails rather than applies supervision.


The error handling strategies are inspired by actor supervision strategies, but the semantics have been adapted to the domain of stream processing. The most important difference is that supervision is not automatically applied to stream operators but instead something that each operator has to implement explicitly. 


For many operators it may not even make sense to implement support for supervision strategies, this is especially true for operators connecting to external technologies where for example a failed connection will likely still fail if a new connection is tried immediately (see 
Restart with back off
 for such scenarios). 


For operators that do implement supervision, the strategies for how to handle exceptions from processing stream elements can be selected when materializing the stream through use of an attribute. 


There are three ways to handle exceptions from application code:




Stop
Supervision.stop()
 - The stream is completed with failure.


Resume
Supervision.resume()
 - The element is dropped and the stream continues.


Restart
Supervision.restart()
 - The element is dropped and the stream continues after restarting the operator. Restarting an operator means that any accumulated state is cleared. This is typically performed by creating a new instance of the operator.




By default the stopping strategy is used for all exceptions, i.e. the stream will be completed with failure when an exception is thrown.




Scala




copy
source
val source = Source(0 to 5).map(100 / _)
val result = source.runWith(Sink.fold(0)(_ + _))
// division by zero will fail the stream and the
// result here will be a Future completed with Failure(ArithmeticException)


Java




copy
source
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(0, 1, 2, 3, 4, 5)).map(elem -> 100 / elem);
final Sink<Integer, CompletionStage<Integer>> fold =
    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);
final CompletionStage<Integer> result = source.runWith(fold, system);
// division by zero will fail the stream and the
// result here will be a CompletionStage failed with ArithmeticException




The default supervision strategy for a stream can be defined on the complete 
RunnableGraph
RunnableGraph
.




Scala




copy
source
val decider: Supervision.Decider = {
  case _: ArithmeticException => Supervision.Resume
  case _                      => Supervision.Stop
}
val source = Source(0 to 5).map(100 / _)
val runnableGraph =
  source.toMat(Sink.fold(0)(_ + _))(Keep.right)

val withCustomSupervision = runnableGraph.withAttributes(ActorAttributes.supervisionStrategy(decider))

val result = withCustomSupervision.run()
// the element causing division by zero will be dropped
// result here will be a Future completed with Success(228)


Java




copy
source
final Function<Throwable, Supervision.Directive> decider =
    exc -> {
      if (exc instanceof ArithmeticException) return Supervision.resume();
      else return Supervision.stop();
    };
final Source<Integer, NotUsed> source =
    Source.from(Arrays.asList(0, 1, 2, 3, 4, 5))
        .map(elem -> 100 / elem)
        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));
final Sink<Integer, CompletionStage<Integer>> fold = Sink.fold(0, (acc, elem) -> acc + elem);

final RunnableGraph<CompletionStage<Integer>> runnableGraph = source.toMat(fold, Keep.right());

final RunnableGraph<CompletionStage<Integer>> withCustomSupervision =
    runnableGraph.withAttributes(ActorAttributes.withSupervisionStrategy(decider));

final CompletionStage<Integer> result = withCustomSupervision.run(system);
// the element causing division by zero will be dropped
// result here will be a CompletionStage completed with 228




Here you can see that all 
ArithmeticException
 will resume the processing, i.e. the elements that cause the division by zero are effectively dropped.
Note


Be aware that dropping elements may result in deadlocks in graphs with cycles, as explained in 
Graph cycles, liveness and deadlocks
.


The supervision strategy can also be defined for all operators of a flow.




Scala




copy
source
val decider: Supervision.Decider = {
  case _: ArithmeticException => Supervision.Resume
  case _                      => Supervision.Stop
}
val flow = Flow[Int]
  .filter(100 / _ < 50)
  .map(elem => 100 / (5 - elem))
  .withAttributes(ActorAttributes.supervisionStrategy(decider))
val source = Source(0 to 5).via(flow)

val result = source.runWith(Sink.fold(0)(_ + _))
// the elements causing division by zero will be dropped
// result here will be a Future completed with Success(150)


Java




copy
source
final Function<Throwable, Supervision.Directive> decider =
    exc -> {
      if (exc instanceof ArithmeticException) return Supervision.resume();
      else return Supervision.stop();
    };
final Flow<Integer, Integer, NotUsed> flow =
    Flow.of(Integer.class)
        .filter(elem -> 100 / elem < 50)
        .map(elem -> 100 / (5 - elem))
        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));
final Source<Integer, NotUsed> source = Source.from(Arrays.asList(0, 1, 2, 3, 4, 5)).via(flow);
final Sink<Integer, CompletionStage<Integer>> fold =
    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);
final CompletionStage<Integer> result = source.runWith(fold, system);
// the elements causing division by zero will be dropped
// result here will be a Future completed with 150




Restart
Supervision.restart()
 works in a similar way as 
Resume
Supervision.resume()
 with the addition that accumulated state, if any, of the failing processing operator will be reset.




Scala




copy
source
val decider: Supervision.Decider = {
  case _: IllegalArgumentException => Supervision.Restart
  case _                           => Supervision.Stop
}
val flow = Flow[Int]
  .scan(0) { (acc, elem) =>
    if (elem < 0) throw new IllegalArgumentException("negative not allowed")
    else acc + elem
  }
  .withAttributes(ActorAttributes.supervisionStrategy(decider))
val source = Source(List(1, 3, -1, 5, 7)).via(flow)
val result = source.limit(1000).runWith(Sink.seq)
// the negative element cause the scan stage to be restarted,
// i.e. start from 0 again
// result here will be a Future completed with Success(Vector(0, 1, 4, 0, 5, 12))


Java




copy
source
final Function<Throwable, Supervision.Directive> decider =
    exc -> {
      if (exc instanceof IllegalArgumentException) return Supervision.restart();
      else return Supervision.stop();
    };
final Flow<Integer, Integer, NotUsed> flow =
    Flow.of(Integer.class)
        .scan(
            0,
            (acc, elem) -> {
              if (elem < 0) throw new IllegalArgumentException("negative not allowed");
              else return acc + elem;
            })
        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));
final Source<Integer, NotUsed> source = Source.from(Arrays.asList(1, 3, -1, 5, 7)).via(flow);
final CompletionStage<List<Integer>> result =
    source.grouped(1000).runWith(Sink.<List<Integer>>head(), system);
// the negative element cause the scan stage to be restarted,
// i.e. start from 0 again
// result here will be a Future completed with List(0, 1, 4, 0, 5, 12)




Errors from mapAsync


Stream supervision can also be applied to the futures of 
mapAsync
mapAsync
 and 
mapAsyncUnordered
mapAsyncUnordered
 even if such failures happen in the future rather than inside the operator itself.


Let’s say that we use an external service to lookup email addresses and we would like to discard those that cannot be found.


We start with the tweet stream of authors:




Scala




copy
source
val authors: Source[Author, NotUsed] =
  tweets.filter(_.hashtags.contains(akkaTag)).map(_.author)


Java




copy
source
final Source<Author, NotUsed> authors =
    tweets.filter(t -> t.hashtags().contains(AKKA)).map(t -> t.author);





Assume that we can lookup their email address using:




Scala




copy
source
def lookupEmail(handle: String): Future[String] =


Java




copy
source
public CompletionStage<String> lookupEmail(String handle)




The 
Future
 
CompletionStage
 is completed 
with 
Failure
 
normally
 if the email is not found.


Transforming the stream of authors to a stream of email addresses by using the 
lookupEmail
 service can be done with 
mapAsync
mapAsync
 and we use 
Supervision.resumingDecider
 
Supervision.getResumingDecider()
 to drop unknown email addresses:




Scala




copy
source
import ActorAttributes.supervisionStrategy
import Supervision.resumingDecider

val emailAddresses: Source[String, NotUsed] =
  authors.via(
    Flow[Author]
      .mapAsync(4)(author => addressSystem.lookupEmail(author.handle))
      .withAttributes(supervisionStrategy(resumingDecider)))


Java




copy
source
final Attributes resumeAttrib =
    ActorAttributes.withSupervisionStrategy(Supervision.getResumingDecider());
final Flow<Author, String, NotUsed> lookupEmail =
    Flow.of(Author.class)
        .mapAsync(4, author -> addressSystem.lookupEmail(author.handle))
        .withAttributes(resumeAttrib);
final Source<String, NotUsed> emailAddresses = authors.via(lookupEmail);





If we would not use 
Resume
Supervision.resume()
 the default stopping strategy would complete the stream with failure on the first 
Future
 
CompletionStage
 that was completed 
with 
Failure
exceptionally
.














 
Reactive Streams Interop






Working with streaming IO 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/rolling-update.html
Rolling Updates and Versions • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions




Akka upgrades


Change log




Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions




Akka upgrades


Change log




Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects


Project




Akka Classic




















Rolling Updates and Versions


Akka upgrades


Akka supports rolling updates between two consecutive patch versions unless an exception is mentioned on this page. For example updating Akka version from 2.5.15 to 2.5.16. Many times it is also possible to skip several versions and exceptions to that are also described here. For example it’s possible to update from 2.5.14 to 2.5.16 without intermediate 2.5.15.


It’s not supported to have a cluster with more than two different versions. Roll out the first update completely before starting next update.
Note


Rolling update from classic remoting to Artery
 is not supported since the protocol is completely different. It will require a full cluster shutdown and new startup.


Change log


2.5.0 Several changes in minor release


See 
migration guide
 when updating from 2.4.x to 2.5.x.


2.5.10 Joining regression


Issue: 
#24622


Incompatible change was introduced in 2.5.10 and fixed in 2.5.11.


This means that you can’t do a rolling update from 2.5.9 to 2.5.10 and must instead do update from 2.5.9 to 2.5.11.


2.5.10 Joining old versions


Issue: 
#25491


Incompatibility was introduced in in 2.5.10 and fixed in 2.5.15.


That means that you should do rolling update from 2.5.9 directly to 2.5.15 if you need to be able to join 2.5.9 nodes during the update phase.


2.5.14 Distributed Data serializer for 
ORSet[ActorRef]


Issue: 
#23703


Intentional change was done in 2.5.14.


This change required a two phase update where the data was duplicated to be compatible with both old and new nodes.




2.5.13 - old format, before the change. Can communicate with intermediate format and with old format.


2.5.14, 2.5.15, 2.5.16 - intermediate format. Can communicate with old format and with new format.


2.5.17 - new format. Can communicate with intermediate format and with new format.




This means that you can’t update from 2.5.13 directly to 2.5.17. You must first update to one of the intermediate versions 2.5.14, 2.5.15, or 2.5.16.


2.5.22 ClusterSharding serializer for 
ShardRegionStats


Issue: 
#25348


Intentional change was done in 2.5.22.


Changed serializer for classes: 
GetShardRegionStats
, 
ShardRegionStats
, 
GetShardStats
, 
ShardStats


This change required a two phase update where new serializer was introduced but not enabled in an earlier version.




2.5.18 - serializer was added but not enabled, 
JavaSerializer
 still used


2.5.22 - 
ClusterShardingMessageSerializer
 was enabled for these classes




This means that you can’t update from 2.5.17 directly to 2.5.22. You must first update to one of the intermediate versions 2.5.18, 2.5.19, 2.5.20 or 2.5.21.


2.6.0 Several changes in minor release


See 
migration guide
 when updating from 2.5.x to 2.6.x.


2.6.2 ClusterMessageSerializer manifests change


Issue: 
#23654


In preparation of switching away from class based manifests to more efficient letter codes the 
ClusterMessageSerializer
 has been prepared to accept those shorter forms but still emits the old long manifests.




2.6.2 - shorter manifests accepted


2.6.5 - shorter manifests emitted




This means that a rolling update will have to go through at least one of 2.6.2, 2.6.3 or 2.6.4 when upgrading to 2.6.5 or higher or else cluster nodes will not be able to communicate during the rolling update.


2.6.5 JacksonCborSerializer


Issue: 
#28918
. JacksonCborSerializer was using plain JSON format instead of CBOR.


If you have 
jackson-cbor
 in your 
serialization-bindings
 a rolling update will have to go through 2.6.5 when upgrading to 2.6.5 or higher.


In Akka 2.6.5 the 
jackson-cbor
 binding will still serialize to JSON format to support rolling update from 2.6.4. It also adds a new binding to be able to deserialize CBOR format when rolling update from 2.6.5 to 2.6.6. In Akka 2.6.6 the 
jackson-cbor
 binding will serialize to CBOR and that can be deserialized by 2.6.5. Old data, such as persistent events, can still be deserialized.


You can start using CBOR format already with Akka 2.6.5 without waiting for the 2.6.6 release. First, perform a rolling update to Akka 2.6.5 using default configuration. Then change the configuration to:


akka.actor {
  serializers {
    jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
  }
  serialization-identifiers {
    jackson-cbor = 33
  }
}



Perform a second rolling update with the new configuration.














 
Older Migration Guides






Issue Tracking 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/persistence-style.html
Style Guide • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide




Event handlers in the state


Command handlers in the state


Optional initial state


Mutable state


Leveraging Java 21 features




Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide




Event handlers in the state


Command handlers in the state


Optional initial state


Mutable state


Leveraging Java 21 features




Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Style Guide


Event handlers in the state


The section about 
Changing Behavior
 described how commands and events can be handled differently depending on the state. One can take that one step further and define the event handler inside the state classes. 
In 
the next section
 the command handlers are also defined in the state.


The state can be seen as your domain object and it should contain the core business logic. Then it’s a matter of taste if event handlers and command handlers should be defined in the state or be kept outside of it.


Here we are using a bank account as the example domain. It has 3 state classes that are representing the lifecycle of the account; 
EmptyAccount
, 
OpenedAccount
, and 
ClosedAccount
.




Scala




copy
source
object AccountEntity {
  // Command
  sealed trait Command extends CborSerializable
  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command
  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command

  // Reply
  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable

  // Event
  sealed trait Event extends CborSerializable
  case object AccountCreated extends Event
  case class Deposited(amount: BigDecimal) extends Event
  case class Withdrawn(amount: BigDecimal) extends Event
  case object AccountClosed extends Event

  val Zero = BigDecimal(0)

  // State
  sealed trait Account extends CborSerializable {
    def applyEvent(event: Event): Account
  }
  case object EmptyAccount extends Account {
    override def applyEvent(event: Event): Account = event match {
      case AccountCreated => OpenedAccount(Zero)
      case _              => throw new IllegalStateException(s"unexpected event [$event] in state [EmptyAccount]")
    }
  }
  case class OpenedAccount(balance: BigDecimal) extends Account {
    require(balance >= Zero, "Account balance can't be negative")

    override def applyEvent(event: Event): Account =
      event match {
        case Deposited(amount) => copy(balance = balance + amount)
        case Withdrawn(amount) => copy(balance = balance - amount)
        case AccountClosed     => ClosedAccount
        case AccountCreated    => throw new IllegalStateException(s"unexpected event [$event] in state [OpenedAccount]")
      }

    def canWithdraw(amount: BigDecimal): Boolean = {
      balance - amount >= Zero
    }

  }
  case object ClosedAccount extends Account {
    override def applyEvent(event: Event): Account =
      throw new IllegalStateException(s"unexpected event [$event] in state [ClosedAccount]")
  }

  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:
  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("Account")

  // Note that after defining command, event and state classes you would probably start here when writing this.
  // When filling in the parameters of EventSourcedBehavior.apply you can use IntelliJ alt+Enter > createValue
  // to generate the stub with types for the command and event handlers.

  def apply(accountNumber: String, persistenceId: PersistenceId): Behavior[Command] = {
    EventSourcedBehavior.withEnforcedReplies(persistenceId, EmptyAccount, commandHandler(accountNumber), eventHandler)
  }

  private def commandHandler(accountNumber: String): (Account, Command) => ReplyEffect[Event, Account] = {
    (state, cmd) =>
      state match {
        case EmptyAccount =>
          cmd match {
            case c: CreateAccount => createAccount(c)
            case _                => Effect.unhandled.thenNoReply() // CreateAccount before handling any other commands
          }

        case acc @ OpenedAccount(_) =>
          cmd match {
            case c: Deposit      => deposit(c)
            case c: Withdraw     => withdraw(acc, c)
            case c: GetBalance   => getBalance(acc, c)
            case c: CloseAccount => closeAccount(acc, c)
            case c: CreateAccount =>
              Effect.reply(c.replyTo)(StatusReply.Error(s"Account $accountNumber is already created"))
          }

        case ClosedAccount =>
          cmd match {
            case c: Deposit =>
              replyClosed(accountNumber, c.replyTo)
            case c: Withdraw =>
              replyClosed(accountNumber, c.replyTo)
            case GetBalance(replyTo) =>
              Effect.reply(replyTo)(CurrentBalance(Zero))
            case CloseAccount(replyTo) =>
              replyClosed(accountNumber, replyTo)
            case CreateAccount(replyTo) =>
              replyClosed(accountNumber, replyTo)
          }
      }
  }

  private def replyClosed(
      accountNumber: String,
      replyTo: ActorRef[StatusReply[Done]]): ReplyEffect[Event, Account] = {
    Effect.reply(replyTo)(StatusReply.Error(s"Account $accountNumber is closed"))
  }

  private val eventHandler: (Account, Event) => Account = { (state, event) =>
    state.applyEvent(event)
  }

  private def createAccount(cmd: CreateAccount): ReplyEffect[Event, Account] = {
    Effect.persist(AccountCreated).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
  }

  private def deposit(cmd: Deposit): ReplyEffect[Event, Account] = {
    Effect.persist(Deposited(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
  }

  private def withdraw(acc: OpenedAccount, cmd: Withdraw): ReplyEffect[Event, Account] = {
    if (acc.canWithdraw(cmd.amount))
      Effect.persist(Withdrawn(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
    else
      Effect.reply(cmd.replyTo)(
        StatusReply.Error(s"Insufficient balance ${acc.balance} to be able to withdraw ${cmd.amount}"))
  }

  private def getBalance(acc: OpenedAccount, cmd: GetBalance): ReplyEffect[Event, Account] = {
    Effect.reply(cmd.replyTo)(CurrentBalance(acc.balance))
  }

  private def closeAccount(acc: OpenedAccount, cmd: CloseAccount): ReplyEffect[Event, Account] = {
    if (acc.balance == Zero)
      Effect.persist(AccountClosed).thenReply(cmd.replyTo)(_ => StatusReply.Ack)
    else
      Effect.reply(cmd.replyTo)(StatusReply.Error("Can't close account with non-zero balance"))
  }

}


Java




copy
source
public class AccountEntity
    extends EventSourcedBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {

  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =
      EntityTypeKey.create(Command.class, "Account");

  // Command
  interface Command extends CborSerializable {}

  public static class CreateAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Deposit implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
      this.amount = amount;
    }
  }

  public static class Withdraw implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.amount = amount;
      this.replyTo = replyTo;
    }
  }

  public static class GetBalance implements Command {
    public final ActorRef<CurrentBalance> replyTo;

    @JsonCreator
    public GetBalance(ActorRef<CurrentBalance> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class CloseAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  // Reply
  public static class CurrentBalance implements CborSerializable {
    public final BigDecimal balance;

    @JsonCreator
    public CurrentBalance(BigDecimal balance) {
      this.balance = balance;
    }
  }

  // Event
  interface Event extends CborSerializable {}

  public enum AccountCreated implements Event {
    INSTANCE
  }

  public static class Deposited implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Deposited(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class Withdrawn implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Withdrawn(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class AccountClosed implements Event {}

  // State
  interface Account extends CborSerializable {}

  public static class EmptyAccount implements Account {
    OpenedAccount openedAccount() {
      return new OpenedAccount(BigDecimal.ZERO);
    }
  }

  public static class OpenedAccount implements Account {
    final BigDecimal balance;

    @JsonCreator
    public OpenedAccount(BigDecimal balance) {
      this.balance = balance;
    }

    OpenedAccount makeDeposit(BigDecimal amount) {
      return new OpenedAccount(balance.add(amount));
    }

    boolean canWithdraw(BigDecimal amount) {
      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);
    }

    OpenedAccount makeWithdraw(BigDecimal amount) {
      if (!canWithdraw(amount))
        throw new IllegalStateException("Account balance can't be negative");
      return new OpenedAccount(balance.subtract(amount));
    }

    ClosedAccount closedAccount() {
      return new ClosedAccount();
    }
  }

  public static class ClosedAccount implements Account {}

  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {
    return new AccountEntity(accountNumber, persistenceId);
  }

  private final String accountNumber;

  private AccountEntity(String accountNumber, PersistenceId persistenceId) {
    super(persistenceId);
    this.accountNumber = accountNumber;
  }

  @Override
  public Account emptyState() {
    return new EmptyAccount();
  }

  @Override
  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {
    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =
        newCommandHandlerWithReplyBuilder();

    builder.forStateType(EmptyAccount.class).onCommand(CreateAccount.class, this::createAccount);

    builder
        .forStateType(OpenedAccount.class)
        .onCommand(Deposit.class, this::deposit)
        .onCommand(Withdraw.class, this::withdraw)
        .onCommand(GetBalance.class, this::getBalance)
        .onCommand(CloseAccount.class, this::closeAccount);

    builder
        .forStateType(ClosedAccount.class)
        .onAnyCommand(() -> Effect().unhandled().thenNoReply());

    return builder.build();
  }

  private ReplyEffect<Event, Account> createAccount(EmptyAccount account, CreateAccount command) {
    return Effect()
        .persist(AccountCreated.INSTANCE)
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {
    return Effect()
        .persist(new Deposited(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {
    if (!account.canWithdraw(command.amount)) {
      return Effect()
          .reply(
              command.replyTo,
              StatusReply.error("not enough funds to withdraw " + command.amount));
    } else {
      return Effect()
          .persist(new Withdrawn(command.amount))
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    }
  }

  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {
    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));
  }

  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {
    if (account.balance.equals(BigDecimal.ZERO)) {
      return Effect()
          .persist(new AccountClosed())
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    } else {
      return Effect()
          .reply(command.replyTo, StatusReply.error("balance must be zero for closing account"));
    }
  }

  @Override
  public EventHandler<Account, Event> eventHandler() {
    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();

    builder
        .forStateType(EmptyAccount.class)
        .onEvent(AccountCreated.class, (account, created) -> account.openedAccount());

    builder
        .forStateType(OpenedAccount.class)
        .onEvent(Deposited.class, (account, deposited) -> account.makeDeposit(deposited.amount))
        .onEvent(Withdrawn.class, (account, withdrawn) -> account.makeWithdraw(withdrawn.amount))
        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());

    return builder.build();
  }
}





Notice how the 
eventHandler
 delegates to the 
applyEvent
 in the 
Account
 (state), which is implemented in the concrete 
EmptyAccount
, 
OpenedAccount
, and 
ClosedAccount
.
 
Notice how the 
eventHandler
 delegates to methods in the concrete 
Account
 (state) classes; 
EmptyAccount
, 
OpenedAccount
, and 
ClosedAccount
.


Command handlers in the state


We can take the previous bank account example one step further by handling the commands in the state too.




Scala




copy
source
object AccountEntity {
  // Command
  sealed trait Command extends CborSerializable
  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command
  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command

  // Reply
  final case class CurrentBalance(balance: BigDecimal)

  // Event
  sealed trait Event extends CborSerializable
  case object AccountCreated extends Event
  case class Deposited(amount: BigDecimal) extends Event
  case class Withdrawn(amount: BigDecimal) extends Event
  case object AccountClosed extends Event

  val Zero = BigDecimal(0)

  // type alias to reduce boilerplate
  type ReplyEffect = akka.persistence.typed.scaladsl.ReplyEffect[Event, Account]

  // State
  sealed trait Account extends CborSerializable {
    def applyCommand(cmd: Command): ReplyEffect
    def applyEvent(event: Event): Account
  }
  case object EmptyAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case CreateAccount(replyTo) =>
          Effect.persist(AccountCreated).thenReply(replyTo)(_ => StatusReply.Ack)
        case _ =>
          // CreateAccount before handling any other commands
          Effect.unhandled.thenNoReply()
      }

    override def applyEvent(event: Event): Account =
      event match {
        case AccountCreated => OpenedAccount(Zero)
        case _              => throw new IllegalStateException(s"unexpected event [$event] in state [EmptyAccount]")
      }
  }
  case class OpenedAccount(balance: BigDecimal) extends Account {
    require(balance >= Zero, "Account balance can't be negative")

    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case Deposit(amount, replyTo) =>
          Effect.persist(Deposited(amount)).thenReply(replyTo)(_ => StatusReply.Ack)

        case Withdraw(amount, replyTo) =>
          if (canWithdraw(amount))
            Effect.persist(Withdrawn(amount)).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error(s"Insufficient balance $balance to be able to withdraw $amount"))

        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(balance))

        case CloseAccount(replyTo) =>
          if (balance == Zero)
            Effect.persist(AccountClosed).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error("Can't close account with non-zero balance"))

        case CreateAccount(replyTo) =>
          Effect.reply(replyTo)(StatusReply.Error("Account is already created"))

      }

    override def applyEvent(event: Event): Account =
      event match {
        case Deposited(amount) => copy(balance = balance + amount)
        case Withdrawn(amount) => copy(balance = balance - amount)
        case AccountClosed     => ClosedAccount
        case AccountCreated    => throw new IllegalStateException(s"unexpected event [$event] in state [OpenedAccount]")
      }

    def canWithdraw(amount: BigDecimal): Boolean = {
      balance - amount >= Zero
    }

  }
  case object ClosedAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case c: Deposit =>
          replyClosed(c.replyTo)
        case c: Withdraw =>
          replyClosed(c.replyTo)
        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(Zero))
        case CloseAccount(replyTo) =>
          replyClosed(replyTo)
        case CreateAccount(replyTo) =>
          replyClosed(replyTo)
      }

    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =
      Effect.reply(replyTo)(StatusReply.Error(s"Account is closed"))

    override def applyEvent(event: Event): Account =
      throw new IllegalStateException(s"unexpected event [$event] in state [ClosedAccount]")
  }

  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:
  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("Account")

  def apply(persistenceId: PersistenceId): Behavior[Command] = {
    EventSourcedBehavior.withEnforcedReplies[Command, Event, Account](
      persistenceId,
      EmptyAccount,
      (state, cmd) => state.applyCommand(cmd),
      (state, event) => state.applyEvent(event))
  }

}




Notice how the command handler is delegating to 
applyCommand
 in the 
Account
 (state), which is implemented in the concrete 
EmptyAccount
, 
OpenedAccount
, and 
ClosedAccount
.


Optional initial state


Sometimes it’s not desirable to use a separate state class for the empty initial state, but rather treat that as there is no state yet. 
null
 can then be used as the 
emptyState
, but be aware of that the 
state
 parameter will then be 
null
 for the first commands and events until the first event has be persisted to create the non-null state. It’s possible to use 
Optional
 instead of 
null
 but that results in rather much boilerplate to unwrap the 
Optional
 state parameter and therefore 
null
 is probably preferred. The following example illustrates using 
null
 as the 
emptyState
.
 
Option[State]
 can be used as the state type and 
None
 as the 
emptyState
. Pattern matching is then used in command and event handlers at the outer layer before delegating to the state or other methods.




Scala




copy
source
object AccountEntity {
  // Command
  sealed trait Command extends CborSerializable
  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command
  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command
  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command

  // Reply
  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable

  // Event
  sealed trait Event extends CborSerializable
  case object AccountCreated extends Event
  case class Deposited(amount: BigDecimal) extends Event
  case class Withdrawn(amount: BigDecimal) extends Event
  case object AccountClosed extends Event

  val Zero = BigDecimal(0)

  // type alias to reduce boilerplate
  type ReplyEffect = akka.persistence.typed.scaladsl.ReplyEffect[Event, Option[Account]]

  // State
  sealed trait Account extends CborSerializable {
    def applyCommand(cmd: Command): ReplyEffect
    def applyEvent(event: Event): Account
  }
  case class OpenedAccount(balance: BigDecimal) extends Account {
    require(balance >= Zero, "Account balance can't be negative")

    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case Deposit(amount, replyTo) =>
          Effect.persist(Deposited(amount)).thenReply(replyTo)(_ => StatusReply.Ack)

        case Withdraw(amount, replyTo) =>
          if (canWithdraw(amount))
            Effect.persist(Withdrawn(amount)).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error(s"Insufficient balance $balance to be able to withdraw $amount"))

        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(balance))

        case CloseAccount(replyTo) =>
          if (balance == Zero)
            Effect.persist(AccountClosed).thenReply(replyTo)(_ => StatusReply.Ack)
          else
            Effect.reply(replyTo)(StatusReply.Error("Can't close account with non-zero balance"))

        case CreateAccount(replyTo) =>
          Effect.reply(replyTo)(StatusReply.Error("Account is already created"))

      }

    override def applyEvent(event: Event): Account =
      event match {
        case Deposited(amount) => copy(balance = balance + amount)
        case Withdrawn(amount) => copy(balance = balance - amount)
        case AccountClosed     => ClosedAccount
        case AccountCreated    => throw new IllegalStateException(s"unexpected event [$event] in state [OpenedAccount]")
      }

    def canWithdraw(amount: BigDecimal): Boolean = {
      balance - amount >= Zero
    }

  }
  case object ClosedAccount extends Account {
    override def applyCommand(cmd: Command): ReplyEffect =
      cmd match {
        case c: Deposit =>
          replyClosed(c.replyTo)
        case c: Withdraw =>
          replyClosed(c.replyTo)
        case GetBalance(replyTo) =>
          Effect.reply(replyTo)(CurrentBalance(Zero))
        case CloseAccount(replyTo) =>
          replyClosed(replyTo)
        case CreateAccount(replyTo) =>
          replyClosed(replyTo)
      }

    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =
      Effect.reply(replyTo)(StatusReply.Error(s"Account is closed"))

    override def applyEvent(event: Event): Account =
      throw new IllegalStateException(s"unexpected event [$event] in state [ClosedAccount]")
  }

  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:
  val TypeKey: EntityTypeKey[Command] =
    EntityTypeKey[Command]("Account")

  def apply(persistenceId: PersistenceId): Behavior[Command] = {
    EventSourcedBehavior.withEnforcedReplies[Command, Event, Option[Account]](
      persistenceId,
      None,
      (state, cmd) =>
        state match {
          case None          => onFirstCommand(cmd)
          case Some(account) => account.applyCommand(cmd)
        },
      (state, event) =>
        state match {
          case None          => Some(onFirstEvent(event))
          case Some(account) => Some(account.applyEvent(event))
        })
  }

  def onFirstCommand(cmd: Command): ReplyEffect = {
    cmd match {
      case CreateAccount(replyTo) =>
        Effect.persist(AccountCreated).thenReply(replyTo)(_ => StatusReply.Ack)
      case _ =>
        // CreateAccount before handling any other commands
        Effect.unhandled.thenNoReply()
    }
  }

  def onFirstEvent(event: Event): Account = {
    event match {
      case AccountCreated => OpenedAccount(Zero)
      case _              => throw new IllegalStateException(s"unexpected event [$event] in state [EmptyAccount]")
    }
  }

}


Java




copy
source
public class AccountEntity
    extends EventSourcedBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {

  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =
      EntityTypeKey.create(Command.class, "Account");

  // Command
  interface Command extends CborSerializable {}

  public static class CreateAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Deposit implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
      this.amount = amount;
    }
  }

  public static class Withdraw implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.amount = amount;
      this.replyTo = replyTo;
    }
  }

  public static class GetBalance implements Command {
    public final ActorRef<CurrentBalance> replyTo;

    @JsonCreator
    public GetBalance(ActorRef<CurrentBalance> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class CloseAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  // Reply
  public static class CurrentBalance implements CborSerializable {
    public final BigDecimal balance;

    @JsonCreator
    public CurrentBalance(BigDecimal balance) {
      this.balance = balance;
    }
  }

  // Event
  interface Event extends CborSerializable {}

  public enum AccountCreated implements Event {
    INSTANCE
  }

  public static class Deposited implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Deposited(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class Withdrawn implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Withdrawn(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class AccountClosed implements Event {}

  // State
  interface Account extends CborSerializable {}

  public static class OpenedAccount implements Account {
    public final BigDecimal balance;

    public OpenedAccount() {
      this.balance = BigDecimal.ZERO;
    }

    @JsonCreator
    public OpenedAccount(BigDecimal balance) {
      this.balance = balance;
    }

    OpenedAccount makeDeposit(BigDecimal amount) {
      return new OpenedAccount(balance.add(amount));
    }

    boolean canWithdraw(BigDecimal amount) {
      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);
    }

    OpenedAccount makeWithdraw(BigDecimal amount) {
      if (!canWithdraw(amount))
        throw new IllegalStateException("Account balance can't be negative");
      return new OpenedAccount(balance.subtract(amount));
    }

    ClosedAccount closedAccount() {
      return new ClosedAccount();
    }
  }

  public static class ClosedAccount implements Account {}

  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {
    return new AccountEntity(accountNumber, persistenceId);
  }

  private final String accountNumber;

  private AccountEntity(String accountNumber, PersistenceId persistenceId) {
    super(persistenceId);
    this.accountNumber = accountNumber;
  }

  @Override
  public Account emptyState() {
    return null;
  }

  @Override
  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {
    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =
        newCommandHandlerWithReplyBuilder();

    builder.forNullState().onCommand(CreateAccount.class, this::createAccount);

    builder
        .forStateType(OpenedAccount.class)
        .onCommand(Deposit.class, this::deposit)
        .onCommand(Withdraw.class, this::withdraw)
        .onCommand(GetBalance.class, this::getBalance)
        .onCommand(CloseAccount.class, this::closeAccount);

    builder
        .forStateType(ClosedAccount.class)
        .onAnyCommand(() -> Effect().unhandled().thenNoReply());

    return builder.build();
  }

  private ReplyEffect<Event, Account> createAccount(CreateAccount command) {
    return Effect()
        .persist(AccountCreated.INSTANCE)
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {
    return Effect()
        .persist(new Deposited(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {
    if (!account.canWithdraw(command.amount)) {
      return Effect()
          .reply(
              command.replyTo,
              StatusReply.error("not enough funds to withdraw " + command.amount));
    } else {
      return Effect()
          .persist(new Withdrawn(command.amount))
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    }
  }

  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {
    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));
  }

  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {
    if (account.balance.equals(BigDecimal.ZERO)) {
      return Effect()
          .persist(new AccountClosed())
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    } else {
      return Effect()
          .reply(command.replyTo, StatusReply.error("balance must be zero for closing account"));
    }
  }

  @Override
  public EventHandler<Account, Event> eventHandler() {
    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();

    builder.forNullState().onEvent(AccountCreated.class, () -> new OpenedAccount());

    builder
        .forStateType(OpenedAccount.class)
        .onEvent(Deposited.class, (account, deposited) -> account.makeDeposit(deposited.amount))
        .onEvent(Withdrawn.class, (account, withdrawn) -> account.makeWithdraw(withdrawn.amount))
        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());

    return builder.build();
  }
}





Mutable state


The state can be mutable or immutable. When it is immutable the event handler returns a new instance of the state for each change.


When using mutable state it’s important to not send the full state instance as a message to another actor, e.g. as a reply to a command. Messages must be immutable to avoid concurrency problems.


The above examples are using immutable state classes and below is corresponding example with mutable state.




Java




copy
source
public class AccountEntity
    extends EventSourcedBehaviorWithEnforcedReplies<
        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {

  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =
      EntityTypeKey.create(Command.class, "Account");

  // Command
  interface Command extends CborSerializable {}

  public static class CreateAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class Deposit implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
      this.amount = amount;
    }
  }

  public static class Withdraw implements Command {
    public final BigDecimal amount;
    public final ActorRef<StatusReply<Done>> replyTo;

    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {
      this.amount = amount;
      this.replyTo = replyTo;
    }
  }

  public static class GetBalance implements Command {
    public final ActorRef<CurrentBalance> replyTo;

    @JsonCreator
    public GetBalance(ActorRef<CurrentBalance> replyTo) {
      this.replyTo = replyTo;
    }
  }

  public static class CloseAccount implements Command {
    public final ActorRef<StatusReply<Done>> replyTo;

    @JsonCreator
    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {
      this.replyTo = replyTo;
    }
  }

  // Reply
  public static class CurrentBalance implements CborSerializable {
    public final BigDecimal balance;

    @JsonCreator
    public CurrentBalance(BigDecimal balance) {
      this.balance = balance;
    }
  }

  // Event
  interface Event extends CborSerializable {}

  public enum AccountCreated implements Event {
    INSTANCE
  }

  public static class Deposited implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Deposited(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class Withdrawn implements Event {
    public final BigDecimal amount;

    @JsonCreator
    Withdrawn(BigDecimal amount) {
      this.amount = amount;
    }
  }

  public static class AccountClosed implements Event {}

  // State
  interface Account extends CborSerializable {}

  public static class EmptyAccount implements Account {
    OpenedAccount openedAccount() {
      return new OpenedAccount();
    }
  }

  public static class OpenedAccount implements Account {
    private BigDecimal balance = BigDecimal.ZERO;

    public BigDecimal getBalance() {
      return balance;
    }

    void makeDeposit(BigDecimal amount) {
      balance = balance.add(amount);
    }

    boolean canWithdraw(BigDecimal amount) {
      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);
    }

    void makeWithdraw(BigDecimal amount) {
      if (!canWithdraw(amount))
        throw new IllegalStateException("Account balance can't be negative");
      balance = balance.subtract(amount);
    }

    ClosedAccount closedAccount() {
      return new ClosedAccount();
    }
  }

  public static class ClosedAccount implements Account {}

  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {
    return new AccountEntity(accountNumber, persistenceId);
  }

  private final String accountNumber;

  private AccountEntity(String accountNumber, PersistenceId persistenceId) {
    super(persistenceId);
    this.accountNumber = accountNumber;
  }

  @Override
  public Account emptyState() {
    return new EmptyAccount();
  }

  @Override
  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {
    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =
        newCommandHandlerWithReplyBuilder();

    builder.forStateType(EmptyAccount.class).onCommand(CreateAccount.class, this::createAccount);

    builder
        .forStateType(OpenedAccount.class)
        .onCommand(Deposit.class, this::deposit)
        .onCommand(Withdraw.class, this::withdraw)
        .onCommand(GetBalance.class, this::getBalance)
        .onCommand(CloseAccount.class, this::closeAccount);

    builder
        .forStateType(ClosedAccount.class)
        .onAnyCommand(() -> Effect().unhandled().thenNoReply());

    return builder.build();
  }

  private ReplyEffect<Event, Account> createAccount(EmptyAccount account, CreateAccount command) {
    return Effect()
        .persist(AccountCreated.INSTANCE)
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {
    return Effect()
        .persist(new Deposited(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {
    if (!account.canWithdraw(command.amount)) {
      return Effect()
          .reply(
              command.replyTo,
              StatusReply.error("not enough funds to withdraw " + command.amount));
    } else {
      return Effect()
          .persist(new Withdrawn(command.amount))
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    }
  }

  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {
    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));
  }

  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {
    if (account.getBalance().equals(BigDecimal.ZERO)) {
      return Effect()
          .persist(new AccountClosed())
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    } else {
      return Effect()
          .reply(command.replyTo, StatusReply.error("balance must be zero for closing account"));
    }
  }

  @Override
  public EventHandler<Account, Event> eventHandler() {
    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();

    builder
        .forStateType(EmptyAccount.class)
        .onEvent(AccountCreated.class, (account, event) -> account.openedAccount());

    builder
        .forStateType(OpenedAccount.class)
        .onEvent(
            Deposited.class,
            (account, deposited) -> {
              account.makeDeposit(deposited.amount);
              return account;
            })
        .onEvent(
            Withdrawn.class,
            (account, withdrawn) -> {
              account.makeWithdraw(withdrawn.amount);
              return account;
            })
        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());

    return builder.build();
  }
}





Leveraging Java 21 features


When building event sourced entities in a project using Java 21 or newer, the 
EventSourcedOnCommandBehavior
 base class provides an API that let you leverage the switch pattern match feature. When combined with 
sealed
 command and event top types this gives you a more direct handling of commands and events as well as a compile time completeness check that you have handled all kinds of commands and events in your event sourced behavior handler methods:




Java




copy
source
public class AccountBehavior
    extends EventSourcedOnCommandBehavior<
        AccountBehavior.Command, AccountBehavior.Event, AccountBehavior.Account> {

  public sealed interface Command extends CborSerializable {
  }

  public record CreateAccount(ActorRef<StatusReply<Done>> replyTo) implements Command {
  }

  public record Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) implements Command {
  }

  public record Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) implements Command { }

  public record GetBalance(ActorRef<CurrentBalance> replyTo) implements Command {
  }

  public record CloseAccount(ActorRef<StatusReply<Done>> replyTo) implements Command {
  }

  // Reply
  public record CurrentBalance(BigDecimal balance) implements CborSerializable {
  }


  public sealed interface Event extends CborSerializable {
  }

  public record AccountCreated() implements Event {
  }

  public record Deposited(BigDecimal amount) implements Event {
  }

  public record Withdrawn(BigDecimal amount) implements Event {
  }

  public record AccountClosed() implements Event {
  }

  // State
  public sealed interface Account extends CborSerializable {}

  public record OpenedAccount(BigDecimal balance) implements Account {
    OpenedAccount makeDeposit(BigDecimal amount) {
      return new OpenedAccount(balance.add(amount));
    }

    boolean canWithdraw(BigDecimal amount) {
      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);
    }

    OpenedAccount makeWithdraw(BigDecimal amount) {
      if (!canWithdraw(amount))
        throw new IllegalStateException("Account balance can't be negative");
      return new OpenedAccount(balance.subtract(amount));
    }

    ClosedAccount closedAccount() {
      return new ClosedAccount();
    }
  }

  public record ClosedAccount() implements Account {
  }

  public static AccountBehavior create(String accountNumber, PersistenceId persistenceId) {
    return new AccountBehavior(accountNumber, persistenceId);
  }

  private final String accountNumber;

  private AccountBehavior(String accountNumber, PersistenceId persistenceId) {
    super(persistenceId);
    this.accountNumber = accountNumber;
  }

  @Override
  public Account emptyState() {
    return null;
  }

  @Override
  public Effect<Event, Account> onCommand(Account account, Command command) {
    return switch (account) {
      case null -> switch (command) {
        case CreateAccount create -> onCreateAccount(create);
        default -> Effect().unhandled();
      };
      case OpenedAccount opened -> switch (command) {
        case Deposit deposit -> onDeposit(opened, deposit);
        case Withdraw withdraw -> onWithdraw(opened, withdraw);
        case GetBalance getBalance -> onGetBalance(opened, getBalance);
        case CloseAccount closeAccount -> onCloseAccount(opened, closeAccount);
        case CreateAccount createAccount ->
            Effect().reply(createAccount.replyTo, StatusReply.error("Account already opened"));
      };
      case ClosedAccount ignore -> Effect().unhandled();
    };
  }

  private ReplyEffect<Event, Account> onCreateAccount(CreateAccount command) {
    return Effect()
        .persist(new AccountCreated())
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> onDeposit(OpenedAccount account, Deposit command) {
    return Effect()
        .persist(new Deposited(command.amount))
        .thenReply(command.replyTo, account2 -> StatusReply.ack());
  }

  private ReplyEffect<Event, Account> onWithdraw(OpenedAccount account, Withdraw command) {
    if (!account.canWithdraw(command.amount)) {
      return Effect()
          .reply(
              command.replyTo,
              StatusReply.error("not enough funds to withdraw " + command.amount));
    } else {
      return Effect()
          .persist(new Withdrawn(command.amount))
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    }
  }

  private ReplyEffect<Event, Account> onGetBalance(OpenedAccount account, GetBalance command) {
    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));
  }

  private ReplyEffect<Event, Account> onCloseAccount(OpenedAccount account, CloseAccount command) {
    if (account.balance.equals(BigDecimal.ZERO)) {
      return Effect()
          .persist(new AccountClosed())
          .thenReply(command.replyTo, account2 -> StatusReply.ack());
    } else {
      return Effect()
          .reply(command.replyTo, StatusReply.error("balance must be zero for closing account"));
    }
  }


  @Override
  public Account onEvent(Account state, Event event) {
    return switch (state) {
      case null -> switch (event) {
        case AccountCreated ignored -> new OpenedAccount(BigDecimal.ZERO);
        default -> throw new IllegalStateException("Unexpected event for null account " + event);
      };
      case OpenedAccount account -> switch (event) {
        case Deposited deposited -> account.makeDeposit(deposited.amount);
        case Withdrawn withdrawn -> account.makeWithdraw(withdrawn.amount);
        case AccountClosed ignored -> account.closedAccount();
        case AccountCreated ignored ->
            throw new IllegalStateException("AccountCreated event for already open account");
      };
      case ClosedAccount ignored ->
          throw new IllegalStateException("ClosedAccount does not accept any events but saw event " + event);
    };
  }
}
















 
CQRS






Snapshotting 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/operators/index.html
Operators • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Source operators


Sink operators


Additional Sink and Source converters


File IO Sinks and Sources


Simple operators


Flow operators composed of Sinks and Sources


Asynchronous operators


Timer driven operators


Backpressure aware operators


Nesting and flattening operators


Time aware operators


Fan-in operators


Fan-out operators


Watching status operators


Actor interop operators


Compression operators


Error handling


Source.actorRef


Sink.actorRef


ActorSource.actorRef


ActorSink.actorRef


Source.actorRefWithBackpressure


Sink.actorRefWithBackpressure


ActorSource.actorRefWithBackpressure


ActorSink.actorRefWithBackpressure


aggregateWithBoundary


alsoTo


alsoToAll


Flow.asFlowWithContext


StreamConverters.asInputStream


StreamConverters.asJavaStream


ask


ActorFlow.ask


ActorFlow.askWithContext


ActorFlow.askWithStatus


ActorFlow.askWithContext


StreamConverters.asOutputStream


Sink.asPublisher


Source.asSourceWithContext


Source.asSubscriber


backpressureTimeout


Balance


batch


batchWeighted


Broadcast


buffer


Sink.cancelled


collect


Sink.collect


Sink.collection


collectType


Source.combine


Sink.combine


Source.completionStage


Flow.completionStageFlow


Sink.completionStageSink


Source.completionStageSource


completionTimeout


concat


concatAllLazy


concatLazy


conflate


conflateWithSeed


contramap


Source.cycle


Compression.deflate


delay


delayWith


detach


divertTo


drop


dropWhile


dropWithin


Source.empty


expand


extrapolate


Source.failed


filter


filterNot


flatMapConcat


flatMapMerge


flatMapPrefix


Flow.flattenOptional


fold


Sink.fold


foldAsync


Sink.foreach


Sink.foreachAsync


Source.apply
Source.from


Source.fromCompletionStage


FileIO.fromFile


Source.fromFuture


Source.fromFutureSource


StreamConverters.fromInputStream


Source.fromIterator


fromJavaStream


StreamConverters.fromJavaStream


fromMaterializer


Sink.fromMaterializer


StreamConverters.fromOutputStream


FileIO.fromPath


Source.fromPublisher


Flow.fromSinkAndSource


Flow.fromSinkAndSourceCoupled


Source.fromSourceCompletionStage


Sink.fromSubscriber


Source.future


Flow.futureFlow


Sink.futureSink


Source.futureSource


groupBy


grouped


groupedWeighted


groupedWeightedWithin


groupedWithin


Compression.gunzip


Compression.gzip


Sink.head


Sink.headOption


idleTimeout


Sink.ignore


Compression.inflate


initialDelay


initialTimeout


interleave


interleaveAll


intersperse


StreamConverters.javaCollector


StreamConverters.javaCollectorParallelUnordered


keepAlive


Sink.last


Sink.lastOption


Source.lazily


Source.lazilyAsync


Source.lazyCompletionStage


Flow.lazyCompletionStageFlow


Sink.lazyCompletionStageSink


Source.lazyCompletionStageSource


Flow.lazyFlow


Source.lazyFuture


Flow.lazyFutureFlow


Sink.lazyFutureSink


Source.lazyFutureSource


Flow.lazyInitAsync


Sink.lazyInitAsync


Source.lazySingle


Sink.lazySink


Source.lazySource


limit


limitWeighted


log


logWithMarker


map


mapAsync


mapAsyncPartitioned


mapAsyncUnordered


mapConcat


mapError


mapWithResource


Source.maybe


merge


mergeAll


mergeLatest


mergePreferred


mergePrioritized


mergePrioritizedN


MergeSequence


mergeSorted


monitor


never


Sink.never


Sink.onComplete


onErrorComplete


RestartSource.onFailuresWithBackoff


RestartFlow.onFailuresWithBackoff


orElse


Partition


prefixAndTail


preMaterialize


Sink.preMaterialize


prepend


prependLazy


Source.queue


Sink.queue


Source.range


recover


recoverWith


recoverWithRetries


reduce


Sink.reduce


Source.repeat


scan


scanAsync


Sink.seq


setup


Sink.setup


Source.single


PubSub.sink


sliding


PubSub.source


splitAfter


splitWhen


statefulMap


statefulMapConcat


take


Sink.takeLast


takeWhile


takeWithin


throttle


Source.tick


FileIO.toFile


FileIO.toPath


Source.unfold


Source.unfoldAsync


Source.unfoldResource


Source.unfoldResourceAsync


Unzip


UnzipWith


watch


watchTermination


wireTap


RestartSource.withBackoff


RestartFlow.withBackoff


RestartSink.withBackoff


RetryFlow.withBackoff


RetryFlow.withBackoffAndContext


zip


zipAll


zipLatest


zipLatestWith


Source.zipN


zipWith


zipWithIndex


Source.zipWithN






Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Source operators


Sink operators


Additional Sink and Source converters


File IO Sinks and Sources


Simple operators


Flow operators composed of Sinks and Sources


Asynchronous operators


Timer driven operators


Backpressure aware operators


Nesting and flattening operators


Time aware operators


Fan-in operators


Fan-out operators


Watching status operators


Actor interop operators


Compression operators


Error handling


Source.actorRef


Sink.actorRef


ActorSource.actorRef


ActorSink.actorRef


Source.actorRefWithBackpressure


Sink.actorRefWithBackpressure


ActorSource.actorRefWithBackpressure


ActorSink.actorRefWithBackpressure


aggregateWithBoundary


alsoTo


alsoToAll


Flow.asFlowWithContext


StreamConverters.asInputStream


StreamConverters.asJavaStream


ask


ActorFlow.ask


ActorFlow.askWithContext


ActorFlow.askWithStatus


ActorFlow.askWithContext


StreamConverters.asOutputStream


Sink.asPublisher


Source.asSourceWithContext


Source.asSubscriber


backpressureTimeout


Balance


batch


batchWeighted


Broadcast


buffer


Sink.cancelled


collect


Sink.collect


Sink.collection


collectType


Source.combine


Sink.combine


Source.completionStage


Flow.completionStageFlow


Sink.completionStageSink


Source.completionStageSource


completionTimeout


concat


concatAllLazy


concatLazy


conflate


conflateWithSeed


contramap


Source.cycle


Compression.deflate


delay


delayWith


detach


divertTo


drop


dropWhile


dropWithin


Source.empty


expand


extrapolate


Source.failed


filter


filterNot


flatMapConcat


flatMapMerge


flatMapPrefix


Flow.flattenOptional


fold


Sink.fold


foldAsync


Sink.foreach


Sink.foreachAsync


Source.apply
Source.from


Source.fromCompletionStage


FileIO.fromFile


Source.fromFuture


Source.fromFutureSource


StreamConverters.fromInputStream


Source.fromIterator


fromJavaStream


StreamConverters.fromJavaStream


fromMaterializer


Sink.fromMaterializer


StreamConverters.fromOutputStream


FileIO.fromPath


Source.fromPublisher


Flow.fromSinkAndSource


Flow.fromSinkAndSourceCoupled


Source.fromSourceCompletionStage


Sink.fromSubscriber


Source.future


Flow.futureFlow


Sink.futureSink


Source.futureSource


groupBy


grouped


groupedWeighted


groupedWeightedWithin


groupedWithin


Compression.gunzip


Compression.gzip


Sink.head


Sink.headOption


idleTimeout


Sink.ignore


Compression.inflate


initialDelay


initialTimeout


interleave


interleaveAll


intersperse


StreamConverters.javaCollector


StreamConverters.javaCollectorParallelUnordered


keepAlive


Sink.last


Sink.lastOption


Source.lazily


Source.lazilyAsync


Source.lazyCompletionStage


Flow.lazyCompletionStageFlow


Sink.lazyCompletionStageSink


Source.lazyCompletionStageSource


Flow.lazyFlow


Source.lazyFuture


Flow.lazyFutureFlow


Sink.lazyFutureSink


Source.lazyFutureSource


Flow.lazyInitAsync


Sink.lazyInitAsync


Source.lazySingle


Sink.lazySink


Source.lazySource


limit


limitWeighted


log


logWithMarker


map


mapAsync


mapAsyncPartitioned


mapAsyncUnordered


mapConcat


mapError


mapWithResource


Source.maybe


merge


mergeAll


mergeLatest


mergePreferred


mergePrioritized


mergePrioritizedN


MergeSequence


mergeSorted


monitor


never


Sink.never


Sink.onComplete


onErrorComplete


RestartSource.onFailuresWithBackoff


RestartFlow.onFailuresWithBackoff


orElse


Partition


prefixAndTail


preMaterialize


Sink.preMaterialize


prepend


prependLazy


Source.queue


Sink.queue


Source.range


recover


recoverWith


recoverWithRetries


reduce


Sink.reduce


Source.repeat


scan


scanAsync


Sink.seq


setup


Sink.setup


Source.single


PubSub.sink


sliding


PubSub.source


splitAfter


splitWhen


statefulMap


statefulMapConcat


take


Sink.takeLast


takeWhile


takeWithin


throttle


Source.tick


FileIO.toFile


FileIO.toPath


Source.unfold


Source.unfoldAsync


Source.unfoldResource


Source.unfoldResourceAsync


Unzip


UnzipWith


watch


watchTermination


wireTap


RestartSource.withBackoff


RestartFlow.withBackoff


RestartSink.withBackoff


RetryFlow.withBackoff


RetryFlow.withBackoffAndContext


zip


zipAll


zipLatest


zipLatestWith


Source.zipN


zipWith


zipWithIndex


Source.zipWithN






Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic






















Operators


Source operators


These built-in sources are available from 
akka.stream.scaladsl.Source
 
akka.stream.javadsl.Source
:








 


Operator


Description










Source


asSourceWithContext


Extracts context data from the elements of a 
Source
 so that it can be turned into a 
SourceWithContext
 which can propagate that context per element along a stream.






Source


asSubscriber


Integration with Reactive Streams, materializes into a 
Subscriber
.






Source


combine


Combine several sources, using a given strategy such as merge or concat, into one source.






Source


completionStage


Send the single value of the 
CompletionStage
 when it completes and there is demand.






Source


completionStageSource


Streams the elements of an asynchronous source once its given 
completion
 operator completes.






Source


cycle


Stream iterator in cycled manner.






Source


empty


Complete right away without ever emitting any elements.






Source


failed


Fail directly with a user specified exception.






Source


apply
from


Stream the values of an 
immutable.Seq
Iterable
.






Source


fromCompletionStage


Deprecated by 
Source.completionStage
.






Source


fromFuture


Deprecated by 
Source.future
.






Source


fromFutureSource


Deprecated by 
Source.futureSource
.






Source


fromIterator


Stream the values from an 
Iterator
, requesting the next value when there is demand.






Source


fromJavaStream


Stream the values from a Java 8 
Stream
, requesting the next value when there is demand.






Source


fromPublisher


Integration with Reactive Streams, subscribes to a 
Publisher
.






Source


fromSourceCompletionStage


Deprecated by 
Source.completionStageSource
.






Source


future


Send the single value of the 
Future
 when it completes and there is demand.






Source


futureSource


Streams the elements of the given future source once it successfully completes.






Source


lazily


Deprecated by 
Source.lazySource
.






Source


lazilyAsync


Deprecated by 
Source.lazyFutureSource
.






Source


lazyCompletionStage


Defers creation of a future of a single element source until there is demand.






Source


lazyCompletionStageSource


Defers creation of a future source until there is demand.






Source


lazyFuture


Defers creation of a future of a single element source until there is demand.






Source


lazyFutureSource


Defers creation and materialization of a 
Source
 until there is demand.






Source


lazySingle


Defers creation of a single element source until there is demand.






Source


lazySource


Defers creation and materialization of a 
Source
 until there is demand.






Source


maybe


Create a source that emits once the materialized 
Promise
 
CompletableFuture
 is completed with a value.






Source


never


Never emit any elements, never complete and never fail.






Source


queue


Materialize a 
BoundedSourceQueue
 or 
SourceQueue
 onto which elements can be pushed for emitting from the source.






Source


range


Emit each integer in a range, with an option to take bigger steps than 1.






Source


repeat


Stream a single object repeatedly.






Source


single


Stream a single object once.






Source


tick


A periodical repetition of an arbitrary object.






Source


unfold


Stream the result of a function as long as it returns a 
Some
 
non empty 
Optional
.






Source


unfoldAsync


Just like 
unfold
 but the fold function returns a 
Future
 
CompletionStage
.






Source


unfoldResource


Wrap any resource that can be opened, queried for next element (in a blocking way) and closed using three distinct functions into a source.






Source


unfoldResourceAsync


Wrap any resource that can be opened, queried for next element and closed in an asynchronous way.






Source


zipN


Combine the elements of multiple sources into a source of sequences of value.






Source


zipWithN


Combine the elements of multiple streams into a stream of sequences using a combiner function.








Sink operators


These built-in sinks are available from 
akka.stream.scaladsl.Sink
 
akka.stream.javadsl.Sink
:








 


Operator


Description










Sink


asPublisher


Integration with Reactive Streams, materializes into a 
org.reactivestreams.Publisher
.






Sink


cancelled


Immediately cancel the stream






Sink


collect


Collect all input elements using a Java 
Collector
.






Sink


collection


Collect all values emitted from the stream into a collection.
Operator only available in the Scala API. The closest operator in the Java API is 
Sink.seq
.






Sink


combine


Combine several sinks into one using a user specified strategy






Sink


completionStageSink


Streams the elements to the given future sink once it successfully completes. 






Sink


fold


Fold over emitted element with a function, where each invocation will get the new element and the result from the previous fold invocation.






Sink


foreach


Invoke a given procedure for each element received.






Sink


foreachAsync


Invoke a given procedure asynchronously for each element received.






Sink


fromMaterializer


Defer the creation of a 
Sink
 until materialization and access 
Materializer
 and 
Attributes






Sink


fromSubscriber


Integration with Reactive Streams, wraps a 
org.reactivestreams.Subscriber
 as a sink.






Sink


futureSink


Streams the elements to the given future sink once it successfully completes. 






Sink


head


Materializes into a 
Future
 
CompletionStage
 which completes with the first value arriving, after this the stream is canceled.






Sink


headOption


Materializes into a 
Future[Option[T]]
 
CompletionStage<Optional<T>>
 which completes with the first value arriving wrapped in 
Some
 
Optional
, or 
a 
None
 
an empty Optional
 if the stream completes without any elements emitted.






Sink


ignore


Consume all elements but discards them.






Sink


last


Materializes into a 
Future
 
CompletionStage
 which will complete with the last value emitted when the stream completes.






Sink


lastOption


Materialize a 
Future[Option[T]]
 
CompletionStage<Optional<T>>
 which completes with the last value emitted wrapped in an 
Some
 
Optional
 when the stream completes.






Sink


lazyCompletionStageSink


Defers creation and materialization of a 
Sink
 until there is a first element.






Sink


lazyFutureSink


Defers creation and materialization of a 
Sink
 until there is a first element.






Sink


lazyInitAsync


Deprecated by 
Sink.lazyFutureSink
.






Sink


lazySink


Defers creation and materialization of a 
Sink
 until there is a first element.






Sink


never


Always backpressure never cancel and never consume any elements from the stream.






Sink


onComplete


Invoke a callback when the stream has completed or failed.






Sink


preMaterialize


Materializes this Sink, immediately returning (1) its materialized value, and (2) a new Sink that can be consume elements ‘into’ the pre-materialized one.






Sink


queue


Materialize a 
SinkQueue
 that can be pulled to trigger demand through the sink.






Sink


reduce


Apply a reduction function on the incoming elements and pass the result to the next invocation.






Sink


seq


Collect values emitted from the stream into a collection.






Sink


setup


Defer the creation of a 
Sink
 until materialization and access 
ActorMaterializer
 and 
Attributes






Sink


takeLast


Collect the last 
n
 values emitted from the stream into a collection.








Additional Sink and Source converters


Sources and sinks for integrating with 
java.io.InputStream
 and 
java.io.OutputStream
 can be found on 
StreamConverters
. As they are blocking APIs the implementations of these operators are run on a separate dispatcher configured through the 
akka.stream.blocking-io-dispatcher
.
Warning


Be aware that 
asInputStream
 and 
asOutputStream
 materialize 
InputStream
 and 
OutputStream
 respectively as blocking API implementation. They will block the thread until data will be available from upstream. Because of blocking nature these objects cannot be used in 
mapMaterializeValue
 section as it causes deadlock of the stream materialization process. For example, following snippet will fall with timeout exception:


...
.toMat(StreamConverters.asInputStream().mapMaterializedValue { inputStream =>
        inputStream.read()  // this could block forever
        ...
}).run()









 


Operator


Description










StreamConverters


asInputStream


Create a sink which materializes into an 
InputStream
 that can be read to trigger demand through the sink.






StreamConverters


asJavaStream


Create a sink which materializes into Java 8 
Stream
 that can be run to trigger demand through the sink.






StreamConverters


asOutputStream


Create a source that materializes into an 
OutputStream
.






StreamConverters


fromInputStream


Create a source that wraps an 
InputStream
.






StreamConverters


fromJavaStream


Create a source that wraps a Java 8 
java.util.stream.Stream
.






StreamConverters


fromOutputStream


Create a sink that wraps an 
OutputStream
.






StreamConverters


javaCollector


Create a sink which materializes into a 
Future
 
CompletionStage
 which will be completed with a result of the Java 8 
Collector
 transformation and reduction operations.






StreamConverters


javaCollectorParallelUnordered


Create a sink which materializes into a 
Future
 
CompletionStage
 which will be completed with a result of the Java 8 
Collector
 transformation and reduction operations.








File IO Sinks and Sources


Sources and sinks for reading and writing files can be found on 
FileIO
.








 


Operator


Description










FileIO


fromFile


Emits the contents of a file.






FileIO


fromPath


Emits the contents of a file from the given path.






FileIO


toFile


Create a sink which will write incoming 
ByteString
 s to a given file.






FileIO


toPath


Create a sink which will write incoming 
ByteString
 s to a given file path.








Simple operators


These operators can transform the rate of incoming elements since there are operators that emit multiple elements for a single input (e.g. 
mapConcat
) or consume multiple elements before emitting one output (e.g. 
filter
). However, these rate transformations are data-driven, i.e. it is the incoming elements that define how the rate is affected. This is in contrast with 
detached operators
 which can change their processing behavior depending on being backpressured by downstream or not.








 


Operator


Description










Flow


asFlowWithContext


Extracts context data from the elements of a 
Flow
 so that it can be turned into a 
FlowWithContext
 which can propagate that context per element along a stream.






Source/Flow


collect


Apply a partial function to each incoming element, if the partial function is defined for a value the returned value is passed downstream.






Source/Flow


collectType


Transform this stream by testing the type of each of the elements on which the element is an instance of the provided type as they pass through this processing step.






Flow


completionStageFlow


Streams the elements through the given future flow once it successfully completes.






Flow


contramap


Transform this Flow by applying a function to each 
incoming
 upstream element before it is passed to the Flow.






Source/Flow


detach


Detach upstream demand from downstream demand without detaching the stream rates.






Source/Flow


drop


Drop 
n
 elements and then pass any subsequent element downstream.






Source/Flow


dropWhile


Drop elements as long as a predicate function return true for the element






Source/Flow


filter


Filter the incoming elements using a predicate.






Source/Flow


filterNot


Filter the incoming elements using a predicate.






Flow


flattenOptional


Collect the value of 
Optional
 from all the elements passing through this flow , empty 
Optional
 is filtered out.






Source/Flow


fold


Start with current value 
zero
 and then apply the current and next value to the given function. When upstream completes, the current value is emitted downstream.






Source/Flow


foldAsync


Just like 
fold
 but receives a function that results in a 
Future
 
CompletionStage
 to the next value.






Source/Flow


fromMaterializer


Defer the creation of a 
Source/Flow
 until materialization and access 
Materializer
 and 
Attributes






Flow


futureFlow


Streams the elements through the given future flow once it successfully completes.






Source/Flow


grouped


Accumulate incoming events until the specified number of elements have been accumulated and then pass the collection of elements downstream.






Source/Flow


groupedWeighted


Accumulate incoming events until the combined weight of elements is greater than or equal to the minimum weight and then pass the collection of elements downstream.






Source/Flow


intersperse


Intersperse stream with provided element similar to 
List.mkString
.






Flow


lazyCompletionStageFlow


Defers creation and materialization of a 
Flow
 until there is a first element.






Flow


lazyFlow


Defers creation and materialization of a 
Flow
 until there is a first element.






Flow


lazyFutureFlow


Defers creation and materialization of a 
Flow
 until there is a first element.






Flow


lazyInitAsync


Deprecated by 
Flow.lazyFutureFlow
 in combination with 
prefixAndTail
.






Source/Flow


limit


Limit number of element from upstream to given 
max
 number.






Source/Flow


limitWeighted


Limit the total weight of incoming elements






Source/Flow


log


Log elements flowing through the stream as well as completion and erroring.






Source/Flow


logWithMarker


Log elements flowing through the stream as well as completion and erroring.






Source/Flow


map


Transform each element in the stream by calling a mapping function with it and passing the returned value downstream.






Source/Flow


mapConcat


Transform each element into zero or more elements that are individually passed downstream.






Source/Flow


mapWithResource


Map elements with the help of a resource that can be opened, transform each element (in a blocking way) and closed.






Source/Flow


preMaterialize


Materializes this Graph, immediately returning (1) its materialized value, and (2) a new pre-materialized Graph.






Source/Flow


reduce


Start with first element and then apply the current and next value to the given function, when upstream complete the current value is emitted downstream.






Source/Flow


scan


Emit its current value, which starts at 
zero
, and then apply the current and next value to the given function, emitting the next current value.






Source/Flow


scanAsync


Just like 
scan
 but receives a function that results in a 
Future
 
CompletionStage
 to the next value.






Source/Flow


setup


Defer the creation of a 
Source/Flow
 until materialization and access 
Materializer
 and 
Attributes






Source/Flow


sliding


Provide a sliding window over the incoming stream and pass the windows as groups of elements downstream.






Source/Flow


statefulMap


Transform each stream element with the help of a state.






Source/Flow


statefulMapConcat


Transform each element into zero or more elements that are individually passed downstream.






Source/Flow


take


Pass 
n
 incoming elements downstream and then complete






Source/Flow


takeWhile


Pass elements downstream as long as a predicate function returns true and then complete. 






Source/Flow


throttle


Limit the throughput to a specific number of elements per time unit, or a specific total cost per time unit, where a function has to be provided to calculate the individual cost of each element.








Flow operators composed of Sinks and Sources








 


Operator


Description










Flow


fromSinkAndSource


Creates a 
Flow
 from a 
Sink
 and a 
Source
 where the Flow’s input will be sent to the 
Sink
 and the 
Flow
 ’s output will come from the Source.






Flow


fromSinkAndSourceCoupled


Allows coupling termination (cancellation, completion, erroring) of Sinks and Sources while creating a Flow between them.








Asynchronous operators


These operators encapsulate an asynchronous computation, properly handling backpressure while taking care of the asynchronous operation at the same time (usually handling the completion of a 
Future
 
CompletionStage
).








 


Operator


Description










Source/Flow


mapAsync


Pass incoming elements to a function that return a 
Future
 
CompletionStage
 result.






Source/Flow


mapAsyncPartitioned


Pass incoming elements to a function that extracts a partitioning key from the element, then to a function that returns a 
Future
 
CompletionStage
 result, bounding the number of incomplete 
Futures
 
CompletionStages
 per partitioning key.






Source/Flow


mapAsyncUnordered


Like 
mapAsync
 but 
Future
 
CompletionStage
 results are passed downstream as they arrive regardless of the order of the elements that triggered them.








Timer driven operators


These operators process elements using timers, delaying, dropping or grouping elements for certain time durations.








 


Operator


Description










Source/Flow


delay


Delay every element passed through with a specific duration.






Source/Flow


delayWith


Delay every element passed through with a duration that can be controlled dynamically.






Source/Flow


dropWithin


Drop elements until a timeout has fired






Source/Flow


groupedWeightedWithin


Chunk up this stream into groups of elements received within a time window, or limited by the weight of the elements, whatever happens first.






Source/Flow


groupedWithin


Chunk up this stream into groups of elements received within a time window, or limited by the number of the elements, whatever happens first.






Source/Flow


initialDelay


Delays the initial element by the specified duration.






Source/Flow


takeWithin


Pass elements downstream within a timeout and then complete.








Backpressure aware operators


These operators are aware of the backpressure provided by their downstreams and able to adapt their behavior to that signal.








 


Operator


Description










Source/Flow


aggregateWithBoundary


Aggregate and emit until custom boundary condition met.






Source/Flow


batch


Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum number of batched elements is not yet reached.






Source/Flow


batchWeighted


Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum weight batched elements is not yet reached.






Source/Flow


buffer


Allow for a temporarily faster upstream events by buffering 
size
 elements.






Source/Flow


conflate


Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure.






Source/Flow


conflateWithSeed


Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure.






Source/Flow


expand


Like 
extrapolate
, but does not have the 
initial
 argument, and the 
Iterator
 is also used in lieu of the original element, allowing for it to be rewritten and/or filtered.






Source/Flow


extrapolate


Allow for a faster downstream by expanding the last emitted element to an 
Iterator
.








Nesting and flattening operators


These operators either take a stream and turn it into a stream of streams (nesting) or they take a stream that contains nested streams and turn them into a stream of elements instead (flattening).


See the 
Substreams
 page for more detail and code samples.








 


Operator


Description










Source/Flow


flatMapConcat


Transform each input element into a 
Source
 whose elements are then flattened into the output stream through concatenation.






Source/Flow


flatMapMerge


Transform each input element into a 
Source
 whose elements are then flattened into the output stream through merging.






Source/Flow


flatMapPrefix


Use the first 
n
 elements from the stream to determine how to process the rest.






Source/Flow


groupBy


Demultiplex the incoming stream into separate output streams.






Source/Flow


prefixAndTail


Take up to 
n
 elements from the stream (less than 
n
 only if the upstream completes before emitting 
n
 elements) and returns a pair containing a strict sequence of the taken element and a stream representing the remaining elements.






Source/Flow


splitAfter


End the current substream whenever a predicate returns 
true
, starting a new substream for the next element.






Source/Flow


splitWhen


Split off elements into a new substream whenever a predicate function return 
true
.








Time aware operators


Those operators operate taking time into consideration.








 


Operator


Description










Source/Flow


backpressureTimeout


If the time between the emission of an element and the following downstream demand exceeds the provided timeout, the stream is failed with a 
TimeoutException
.






Source/Flow


completionTimeout


If the completion of the stream does not happen until the provided timeout, the stream is failed with a 
TimeoutException
.






Source/Flow


idleTimeout


If the time between two processed elements exceeds the provided timeout, the stream is failed with a 
TimeoutException
.






Source/Flow


initialTimeout


If the first element has not passed through this operators before the provided timeout, the stream is failed with a 
TimeoutException
.






Source/Flow


keepAlive


Injects additional (configured) elements if upstream does not emit for a configured amount of time.








Fan-in operators


These operators take multiple streams as their input and provide a single output combining the elements from all of the inputs in different ways.








 


Operator


Description










 


MergeSequence


Merge a linear sequence partitioned across multiple sources.






Source/Flow


concat


After completion of the original upstream the elements of the given source will be emitted.






Source/Flow


concatAllLazy


After completion of the original upstream the elements of the given sources will be emitted sequentially.






Source/Flow


concatLazy


After completion of the original upstream the elements of the given source will be emitted.






Source/Flow


interleave


Emits a specifiable number of elements from the original source, then from the provided source and repeats.






Source/Flow


interleaveAll


Emits a specifiable number of elements from the original source, then from the provided sources and repeats.






Source/Flow


merge


Merge multiple sources.






Source/Flow


mergeAll


Merge multiple sources.






Source/Flow


mergeLatest


Merge multiple sources.






Source/Flow


mergePreferred


Merge multiple sources.






Source/Flow


mergePrioritized


Merge multiple sources.






Source


mergePrioritizedN


Merge multiple sources with priorities.






Source/Flow


mergeSorted


Merge multiple sources.






Source/Flow


orElse


If the primary source completes without emitting any elements, the elements from the secondary source are emitted.






Source/Flow


prepend


Prepends the given source to the flow, consuming it until completion before the original source is consumed.






Source/Flow


prependLazy


Prepends the given source to the flow, consuming it until completion before the original source is consumed.






Source/Flow


zip


Combines elements from each of multiple sources into 
tuples
 
Pair
 and passes the 
tuples
 
pairs
 downstream.






Source/Flow


zipAll


Combines elements from two sources into 
tuples
 
Pair
 handling early completion of either source.






Source/Flow


zipLatest


Combines elements from each of multiple sources into 
tuples
 
Pair
 and passes the 
tuples
 
pairs
 downstream, picking always the latest element of each.






Source/Flow


zipLatestWith


Combines elements from multiple sources through a 
combine
 function and passes the returned value downstream, picking always the latest element of each.






Source/Flow


zipWith


Combines elements from multiple sources through a 
combine
 function and passes the returned value downstream.






Source/Flow


zipWithIndex


Zips elements of current flow with its indices.








Fan-out operators


These have one input and multiple outputs. They might route the elements between different outputs, or emit elements on multiple outputs at the same time.


There is a number of fan-out operators for which currently no ‘fluent’ is API available. To use those you will have to use the 
Graph DSL
.








 


Operator


Description










 


Balance


Fan-out the stream to several streams.






 


Broadcast


Emit each incoming element each of 
n
 outputs.






 


Partition


Fan-out the stream to several streams.






 


Unzip


Takes a stream of two element tuples and unzips the two elements ino two different downstreams.






 


UnzipWith


Splits each element of input into multiple downstreams using a function






Source/Flow


alsoTo


Attaches the given 
Sink
 to this 
Flow
, meaning that elements that pass through this 
Flow
 will also be sent to the 
Sink
.






Source/Flow


alsoToAll


Attaches the given 
Source
Source
s to this 
Flow
Flow
, meaning that elements that pass through this 
Flow
Flow
 will also be sent to all those 
Sink
Sink
s.






Source/Flow


divertTo


Each upstream element will either be diverted to the given sink, or the downstream consumer according to the predicate function applied to the element.






Source/Flow


wireTap


Attaches the given 
Sink
 to this 
Flow
 as a wire tap, meaning that elements that pass through will also be sent to the wire-tap 
Sink
, without the latter affecting the mainline flow.








Watching status operators








 


Operator


Description










Source/Flow


monitor


Materializes to a 
FlowMonitor
 that monitors messages flowing through or completion of the operators.






Source/Flow


watchTermination


Materializes to a 
Future
 
CompletionStage
 that will be completed with Done or failed depending whether the upstream of the operators has been completed or failed.








Actor interop operators


Operators meant for inter-operating between Akka Streams and Actors:








 


Operator


Description










Source


actorRef


Materialize an 
ActorRef
 of the classic actors API; sending messages to it will emit them on the stream.






Sink


actorRef


Send the elements from the stream to an 
ActorRef
 of the classic actors API.






ActorSource


actorRef


Materialize an 
ActorRef<T>
ActorRef[T]
 of the new actors API; sending messages to it will emit them on the stream only if they are of the same type as the stream.






ActorSink


actorRef


Sends the elements of the stream to the given 
ActorRef<T>
ActorRef[T]
 of the new actors API, without considering backpressure.






Source


actorRefWithBackpressure


Materialize an 
ActorRef
 of the classic actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.






Sink


actorRefWithBackpressure


Send the elements from the stream to an 
ActorRef
 (of the classic actors API) which must then acknowledge reception after completing a message, to provide back pressure onto the sink.






ActorSource


actorRefWithBackpressure


Materialize an 
ActorRef<T>
ActorRef[T]
 of the new actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.






ActorSink


actorRefWithBackpressure


Sends the elements of the stream to the given 
ActorRef<T>
ActorRef[T]
 of the new actors API with backpressure, to be able to signal demand when the actor is ready to receive more elements.






Source/Flow


ask


Use the “Ask Pattern” to send a request-reply message to the target 
ref
 actor (of the classic actors API).






ActorFlow


ask


Use the “Ask Pattern” to send each stream element as an 
ask
 to the target actor (of the new actors API), and expect a reply that will be emitted downstream.






ActorFlow


askWithContext


Use the “Ask Pattern” to send each stream element (without the context) as an 
ask
 to the target actor (of the new actors API), and expect a reply that will be emitted downstream.






ActorFlow


askWithStatus


Use the “Ask Pattern” to send each stream element as an 
ask
 to the target actor (of the new actors API), and expect a reply of Type 
StatusReply[T]
StatusReply<T>
 where the T will be unwrapped and emitted downstream.






ActorFlow


askWithStatusAndContext


Use the “Ask Pattern” to send each stream element (without the context) as an 
ask
 to the target actor (of the new actors API), and expect a reply of Type 
StatusReply[T]
StatusReply<T>
 where the T will be unwrapped and emitted downstream.






PubSub


sink


A sink that will publish emitted messages to a 
Topic
Topic
.






PubSub


source


A source that will subscribe to a 
Topic
Topic
 and stream messages published to the topic. 






Source/Flow


watch


Watch a specific 
ActorRef
 and signal a failure downstream once the actor terminates.








Compression operators


Flow operators to (de)compress.








 


Operator


Description










Compression


deflate


Creates a flow that deflate-compresses a stream of ByteStrings. 






Compression


gunzip


Creates a flow that gzip-decompresses a stream of ByteStrings. 






Compression


gzip


Creates a flow that gzip-compresses a stream of ByteStrings. 






Compression


inflate


Creates a flow that deflate-decompresses a stream of ByteStrings. 








Error handling


For more background see the 
Error Handling in Streams
 section.








 


Operator


Description










Source/Flow


mapError


While similar to 
recover
 this operators can be used to transform an error signal to a different one 
without
 logging it as an error in the process.






Source/Flow


onErrorComplete


Allows completing the stream when an upstream error occurs.






RestartSource


onFailuresWithBackoff


Wrap the given 
Source
Source
 with a 
Source
Source
 that will restart it when it fails using an exponential backoff. Notice that this 
Source
Source
 will not restart on completion of the wrapped flow.






RestartFlow


onFailuresWithBackoff


Wrap the given 
Flow
Flow
 with a 
Flow
Flow
 that will restart it when it fails using an exponential backoff. Notice that this 
Flow
Flow
 will not restart on completion of the wrapped flow.






Source/Flow


recover


Allow sending of one last element downstream when a failure has happened upstream.






Source/Flow


recoverWith


Allow switching to alternative Source when a failure has happened upstream.






Source/Flow


recoverWithRetries


RecoverWithRetries allows to switch to alternative Source on flow failure.






RestartSource


withBackoff


Wrap the given 
Source
Source
 with a 
Source
Source
 that will restart it when it fails or completes using an exponential backoff.






RestartFlow


withBackoff


Wrap the given 
Flow
Flow
 with a 
Flow
Flow
 that will restart it when it fails or complete using an exponential backoff.






RestartSink


withBackoff


Wrap the given 
Sink
Sink
 with a 
Sink
Sink
 that will restart it when it fails or complete using an exponential backoff.






RetryFlow


withBackoff


Wrap the given 
Flow
Flow
 and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try.






RetryFlow


withBackoffAndContext


Wrap the given 
FlowWithContext
FlowWithContext
 and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try.




















 
Configuration






Source.actorRef 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/legal/terms-of-use
Terms of Use


























































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation



































































          Akka Terms of Use
        






Lightbend Inc, d.b.a Akka
October 22, 2024






































Introduction


This policy defines the terms and conditions of use for users of Akka's website and web properties.


Scope


This policy applies to all web sites owned by Akka.


Definitions


Please see the Policy Definitions document distributed with this policy for the definition of terms and abbreviations


Referenced Policies




Privacy Policy




1. Terms of Use


1.1 Terms and Conditions of Use


Please read these terms and conditions of use carefully. These terms and conditions may have changed since your last visit to this web site. By using this web site, you indicate your acceptance of these terms and conditions. If you do not accept these terms and conditions, then do not use this web site.


Lightbend, Inc. (d.b.a "Akka") maintains this web site as a service to its customers, potential customers, and other interested parties. Please visit us frequently, browse our pages, and download documents, subject to the terms and conditions set out below.


1.2 Copyright and Trademark Information


Copyright © 2016 Lightbend, Inc. All rights reserved. This web site, and the information which it contains, is the property of Akka and its affiliates and licensors, and is protected from unauthorized copying and dissemination by copyright laws, trademark laws, international conventions and other intellectual property laws. All other product names that are not owned by Akka are trademarks or registered trademarks of their respective owners.


1.3 Use of Akka Web Site and Its Content


The information contained in this web site is for general guidance on topics selected by Akka. Such information is provided on a blind-basis, without any knowledge as to your industry, identity or specific circumstances. The application and impact of relevant laws will vary from jurisdiction to jurisdiction. There may also be delays, omissions, or inaccuracies in information contained in this web site.


Akka may alter, suspend, or discontinue this web site at any time for any reason, without notice or cost. The web site may become unavailable due to maintenance or malfunction of computer equipment or other reasons.


By using this web site, you agree to indemnify, hold harmless and defend Akka from any claims, damages, losses, liabilities, and all costs and expenses of defense, including but not limited to, attorneys' fees, resulting directly or indirectly from a claim by a third party that is based on your use of this web site in violation of these terms.


No part of this web site may be reproduced or transmitted in any form, by any means, electronic or mechanical, including photocopying and recording, except that Akka authorizes you to view, copy, download, and print Akka documents (such as white papers, press releases, data sheets, and FAQs) that are available on this web site, subject to the following conditions:




The documents may be used solely for noncommercial, informational purposes.


The documents may not be modified.


Copyright, trademark, and other proprietary notices may not be removed.




Nothing contained on this web site should be construed as granting, by implication, estoppel, or otherwise, any license or right to use this web site or any documents displayed on this web site, through the use of framing or otherwise, except: (a) as expressly permitted by these terms and conditions; or (b) with the prior written permission of Akka or such third party that may own the trademark or copyright of material displayed on this web site.


Notwithstanding anything to the contrary above, copy or use of any software made available for download from this website shall be subject to the license agreement associated with such software.


1.4 Accuracy of Content and Future Modifications to Web Site


The information on this web site is believed to be complete and reliable; however, the information may contain technical inaccuracies or typographical errors. Akka reserves the right to make changes to document names and content, product specifications, or other information without obligation to notify any person of such changes.


1.5 Standards of User Conduct


You may not use this web site and its related services (the "Site") to:




transmit via or through the Site any information, data, text, images, files, links, software, chat, communication or other materials ("Content") that is or which Akka considers in its sole discretion to be unlawful, harmful, threatening, abusive, harassing, defamatory, vulgar, offensive, obscene, pornographic, hateful or threatening to any group defined by race, religion, gender, national origin or sexual orientation, or otherwise objectionable, including without limitation blatant expressions of bigotry, prejudice, racism, hatred or excessive profanity or post any obscene, lewd, lascivious, excessively violent, harassing or otherwise objectionable Content;


sell or promote any products or services that are unlawful in the location at which the Content is posted or received;


sell or promote controlled pharmaceutical substances, tobacco, firearms, or alcoholic beverages;




introduce viruses, worms, Trojan horses and/or harmful code on the Internet; display material that exploits, or otherwise exploit, children under 18 years of age;




post any Content or otherwise infringe in any way or violate any copyright, patent, trademark, service mark, trade name, trade secret or other intellectual property right of any third party;


promote, solicit or participate in multi-level marketing or pyramid schemes;


harass, embarrass or cause distress or discomfort upon another participant, user, or other individual or entity;


impersonate any other person, including but not limited to, an Akka official, expert or bulletin board leader, guide or host;


post or disclose any personally identifying information or private information about children or any third parties without their consent (or their parent's consent in case of a child under 13 years of age);


post or transmit any unsolicited advertising, promotional materials, or any other forms of solicitation on our bulletin boards, including with limitation solicitations of credit card numbers, solicitations for sponsors, or promotion of raffles or contests;


intentionally or unintentionally violate any applicable local, state, national or international law, including but not limited to any regulations having the force of law while using or accessing the Site or in connection with your use of the Site, in any manner; or invade the privacy or violate any personal or proprietary right (including intellectual property rights) of any person or entity.


These are some, though not all, of the activities that may result in removal of Content which you post to the Site and/or the termination of your access to the Site.




1.6 No Warranties


Information and documents, including software and product specifications, provided on this web site are provided "as is." specifically, but without limitation, Akka does not warrant that: (i) the information on this web site is correct, accurate, reliable or complete; (ii) the functions contained on this web site will be uninterrupted or error-free; (iii) defects will be corrected, or (iv) this web site or the server(s) that makes it available are free of viruses or other harmful components.


Product descriptions and specifications are subject to change. Akka periodically adds or updates the information and documents on this web site without notice.


It is the user's responsibility to ascertain whether any information downloaded from this web site is free of viruses, worms, trojan horses, or other items of a potentially destructive nature.


1.7 Availability of Products and Services Mentioned


Information that Akka publishes on this web site may contain references or cross references to products or services that are not available or approved by the appropriate regulatory authorities in your country. Such references do not imply that Akka intends to announce or make available such products or services to the general public, or in your country. Consult your Akka account representative to determine which products and services may be available to you.


1.8 Limitation of Liability


Under no circumstances shall Akka be liable for any incidental, special, consequential, exemplary, multiple or other indirect damages that result from the use of, or the inability to use, this web site or the information contained on this web site, even if Akka has been advised of the possibility of such damages. In no event shall Akka 's total liability to you for all damages, losses, and causes of action resulting from your use of this web site, whether in contract, tort (including, but not limited to, negligence) or otherwise, exceed the amounts you paid to Akka during the most recent three-month period in connection with amounts which you paid for using this web site.


1.9 Links to Third-Party Web Sites


This web site may contain links to non-Akka web sites. These links are provided to you as a convenience, and Akka is not responsible for the content of any linked web site. Any outside web site accessed from the Akka web site is independent from Akka, and Akka has no control over the content of that web site. In addition, a link to any non-Akka web site does not imply that Akka endorses or accepts any responsibility for the content or use of such a website.


1.10 No Implied Endorsements


In no event shall any reference to any third party or third party product or service be construed as an approval or endorsement by Akka of that third party or of any product or service provided by a third party.


1.11 Jurisdictional Issues


Akka makes no representation that information on this web site is appropriate or available for use outside the United States. Those who choose to access this web site from outside the United States do so on their own initiative and are responsible for compliance with local laws, if and to the extent local laws are applicable.


1.12 Submissions to Akka and Affiliated Servers


Any information, including but not limited to remarks, suggestions, ideas, graphics, or other submissions, communicated to Akka through this web site is the exclusive property of Akka. Akka is entitled to use any information submitted for any purpose, without restriction (except as stated in Akka’s Privacy Policy) or compensation to the person sending the submission. The user acknowledges the originality of any submission communicated to Akka and accepts responsibility for its accuracy, appropriateness, and legality.


1.13 Enforcement of Terms and Conditions


These Terms and conditions are governed and interpreted pursuant to the laws of the Commonwealth of Delaware, United States of America, notwithstanding any principles of conflicts of law.


All disputes arising out of or relating to these Terms and Conditions shall be finally resolved by arbitration conducted in the English language in Delaware, U.S.A. before a single arbitrator under the commercial arbitration rules of the American Arbitration Association. The parties shall bear equally the cost of the arbitration (except that the prevailing party shall be entitled to an award of reasonable attorneys' fees incurred in connection with the arbitration in such an amount as may be determined by the arbitrator). All decisions of the arbitrator shall be final and binding on both parties and enforceable in any court of competent jurisdiction. Notwithstanding this, application may be made to any court for a judicial acceptance of the award or order of enforcement. Notwithstanding the foregoing, Akka shall be entitled to seek injunctive relief, security, or other equitable remedies from the United States District Court for the District of Delaware or any other court of competent jurisdiction.


If any part of these terms is unlawful, void, or unenforceable, that part will be deemed severable and will not affect the validity and enforceability of the remaining provisions. Akka may, at its sole discretion and without notice, revise these terms at any time by updating this posting.


1.14 Entire Agreement


This is the entire Agreement between the parties relating to the subject matter herein and shall not be modified except in writing signed by both parties or by a new posting by Akka, as described above.


2. Trademarks


2.1 Trademarks Rationale


Trademarks, service marks, and graphics marks of Akka are symbols of the quality and community support that people have come to associate with the Akka, Kalix and Scala projects and other projects of Lightbend, Inc. (d.b.a. “Akka”).


To ensure that the use of Akka marks will not lead to confusion about our software, we must control their use in association with software and related services by other companies. Also, we have a legal responsibility and the authority to set guidelines for the use of our marks.


Akka and its software must be clearly distinguishable from any software that competes with Akka software.


Our marks must not be used to disparage Akka, our projects, or communities, nor be used in any way to imply ownership, endorsement, or sponsorship of any related project or initiative of any kind.


2.2 Description of Key Trademark Principles


This document is not intended to summarize the complex law of trademarks. It will be useful, however, to understand the following key principles:


What is a trademark?


A trademark is a word, phrase, symbol or design, or a combination of words, phrases, symbols or designs, that identifies and distinguishes the source of the goods of one party from those of others. A service mark is the same as a trademark, except that it identifies and distinguishes the source of a service rather than a product. Throughout this policy document, the terms "trademark" and "mark" refer to both trademarks and service marks.


These rules are generalized to describe Akka software associated with the trademark "Akka", when it is understood to refer to this specific Akka software.


Akka's trademarks are either words (e.g., "Akka") or graphic logos that are intended to serve as trademarks for that Akka software.


What is nominative use?


Anyone can use Akka trademarks if that use of the trademark is nominative. The "nominative use" (or "nominative fair use") defense to trademark infringement is a legal doctrine that authorizes everyone (even commercial companies) to use another person's trademark as long as three requirements are met:


The product or service in question must be one not readily identifiable without use of the trademark; (for example, it is not easy to identify Akka software without using the trademark "Akka")


Only so much of the mark or marks may be used as is reasonably necessary to identify the product or service; and


The organization using the mark must do nothing that would, in conjunction with the mark, suggest sponsorship or endorsement by the trademark holder.


The trademark nominative fair use defense is intended to encourage people to refer to trademarked goods and services by using the trademark itself. This trademark defense has nothing to do with copyright fair use and should not be confused with those rules.


What is the "confusing similarity" or "likelihood of confusion" test?


Some uses of another person's trademark are nominative fair use, but some uses are simply infringing. Indeed, if a trademark is used in such a way that the relevant consuming public will likely be confused or mistaken about the source of a product or service sold or provided using the mark in question, then likelihood of confusion exists and the mark has been infringed.


Note that, even if there is no likelihood of confusion, you may still be liable for using another company's trademark if you are blurring or tarnishing their mark under the state and/or federal dilution laws.


To avoid infringing Akka's marks, you should verify that your use of our marks is nominative and that you are not likely to confuse software consumers that your software is the same as Akka's software or is endorsed by Akka. This policy is already summarized in section 6 of the Apache Software License, and so it is a condition for your use of Akka software:


This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.


2.3 Specific Guidelines


The following Specific Guidelines apply to the "Akka" word trademark, as well as the other trademarks..


Examples of permitted nominative fair use:


" Free copies of Akka software under the Apache License and support services for Akka software are available at my own company website. "


" Derivative works of Akka software and support services for those derivative works are available under my own trademarks at my website. "


Please remember that, under trademark law, you may not apply trademarks to your derivative works of Akka software that are confusingly similar to "Akka".


" Akka software is faster (or slower) than Myco software. "


" I recommend (or don't recommend) Akka software for your business. "


Using Akka trademarks in book and article titles:


You may write about Akka software, and use our trademarks in book or article titles. You needn't ask us for permission to refer to Akka, as in "Akka for Dummies", or "Explaining Akka", or " Scala Simplified", or "O'Reilly Guide to Akka", or even "Avoiding Akka".


Using Akka trademarks on merchandise:


You must obtain prior written approval from the VP, Marketing to apply the "Akka" trademarks to any merchandise that is intended to be associated in people's minds with any Akka software.


Using Akka trademarks in domain names.


You may not use Akka trademarks such as "Akka" in your own domain names if that use would be likely to confuse a relevant consumer about the source of software or services provided through your website, without written approval of the VP, Marketing. You should apply the "likelihood of confusion" test described above, and please realize that the use of Akka trademarks in your domain names is generally not "nominative fair use."


2.4 Important Note


Nothing in this Akka policy statement shall be interpreted to allow any third party to claim any association with Akka or any of its projects or to imply any approval or support by Akka for any third party products or services.


3. Cookies


3.1 Cookie Introduction


Our Site uses technologies that enhance user friendliness and engagement, to keep the website operating as smoothly as possible and to provide web services and functionalities for each visitor. Examples of these technologies are cookies, pixel tags, local storage and scripts (hereinafter collectively referred to as "cookies").


Cookies may be either "persistent" cookies or "session" cookies. A persistent cookie consists of a text file sent by a web server to a web browser, which will be stored by the browser and will remain valid until its set expiry date (unless deleted by the visitor before the expiry date). A session cookie, on the other hand, will expire at the end of the visitor session, when the web browser is closed.


This policy describes the options available to you how to manage your cookies, which cookies we use and for what purposes we use them.


3.2 How to Manage Cookies


If you do not want our websites to store cookies on your computer or mobile device, you can change your browser settings so that you receive a warning before certain cookies are stored. You can also adjust your settings so that your browser refuses most of our cookies or only certain cookies. You can also withdraw your consent to cookies by deleting the cookies that have already been stored.


Please be aware that if you do not want to accept any cookies, we cannot guarantee that our website will function properly. It may be that several functions will be unavailable to you or that you will even be unable to view certain parts of the website


Please note that you will have to change your settings for each browser and device you use. The procedures for changing your settings and cookies differ from browser to browser if necessary, use the help function on your browser or click on one of the links below to go directly to the use manual for your browser:




Internet Explorer


Mozilla Firefox


Google Chrome


Safari


Opera


Adobe (plug-in for flash cookies)




There are also software products available that can manage cookies for you.


To find out more about cookies, including how to see what cookies have been set and how to manage or delete them, visit 
All About Cookies.org
.


Because we respect your right to privacy, you can choose not to allow some types of cookies. Below is a list of the cookies that get set on the lightbend.com domain. Blocking some types of cookies may impact your experience of the site and the services we are able to offer.


3.3 Cookie List




 
















 Legal










Privacy Policy 


Terms of Use 


 Business Source License 1.1 










































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/discovery/index.html#discovery-method-dns
Discovery • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery




Module info


How it works


Discovery Method: DNS


Discovery Method: Configuration


Discovery Method: Aggregate multiple discovery methods


Migrating from Akka Management Discovery (before 1.0.0)




Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Discovery


The Akka Discovery API enables 
service discovery
 to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files. 


Implementations provided by the Akka Discovery module are 




Configuration
 (HOCON)


DNS
 (SRV records)


Aggregate
 multiple discovery methods




In addition the 
Akka Management
 toolbox contains Akka Discovery implementations for




Kubernetes API


AWS API: EC2 Tag-Based Discovery


AWS API: ECS Discovery


Consul


Note


Discovery used to be part of Akka Management but has become an Akka module as of 
2.5.19
 of Akka and version 
1.0.0
 of Akka Management. If you’re also using Akka Management for other service discovery methods or bootstrap make sure you are using at least version 
1.0.0
 of Akka Management.


See 
Migration hints


Module info


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



Additionally, add the dependency as below.
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-discovery" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-discovery_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-discovery_${versions.ScalaBinary}"
}




Project Info: Akka Discovery


Artifact
com.typesafe.akka


akka-discovery


2.10.1+5-4c570f91-SNAPSHOT


Snapshots are available




JDK versions
Eclipse Temurin JDK 11
Eclipse Temurin JDK 17
Eclipse Temurin JDK 21


Scala versions
2.13.15, 3.3.4


JPMS module name
akka.discovery


License
BUSL-1.1




Readiness level
Supported
, support is available from 
Lightbend


Since 2.5.19, 2018-12-07




Home page
https://akka.io/


API documentation


API (Scaladoc)


API (Javadoc)




Forums


Akka Discuss




Release notes
Akka release notes


Issues
Github issues


Sources
https://github.com/akka/akka




How it works


Loading the extension:




Scala




copy
source
import akka.discovery.Discovery

val system = ActorSystem()
val serviceDiscovery = Discovery(system).discovery


Java




copy
source
ActorSystem as = ActorSystem.create();
ServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();




A 
Lookup
 contains a mandatory 
serviceName
 and an optional 
portName
 and 
protocol
. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:




Scala




copy
source
import akka.discovery.Lookup

serviceDiscovery.lookup(Lookup("akka.io"), 1.second)
// Convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", 1.second)


Java




copy
source
serviceDiscovery.lookup(Lookup.create("akka.io"), Duration.ofSeconds(1));
// convenience for a Lookup with only a serviceName
serviceDiscovery.lookup("akka.io", Duration.ofSeconds(1));




portName
 and 
protocol
 are optional and their meaning is interpreted by the method.




Scala




copy
source
import akka.discovery.Lookup
import akka.discovery.ServiceDiscovery.Resolved

val lookup: Future[Resolved] =
  serviceDiscovery.lookup(Lookup("akka.io").withPortName("remoting").withProtocol("tcp"), 1.second)


Java




copy
source
CompletionStage<ServiceDiscovery.Resolved> lookup =
    serviceDiscovery.lookup(
        Lookup.create("akka.io").withPortName("remoting").withProtocol("tcp"),
        Duration.ofSeconds(1));




Port can be used when a service opens multiple ports e.g. a HTTP port and an Akka remoting port.


Discovery Method: DNS
Async DNS


Akka Discovery with DNS does always use the 
Akka-native “async-dns” implementation
 (it is independent of the 
akka.io.dns.resolver
 setting).


DNS discovery maps 
Lookup
 queries as follows:




serviceName
, 
portName
 and 
protocol
 set: SRV query in the form: 
_port._protocol.name
 Where the 
_
s are added.


Any query missing any of the fields is mapped to a A/AAAA query for the 
serviceName




The mapping between Akka service discovery terminology and SRV terminology:




SRV service = port


SRV name = serviceName


SRV protocol = protocol




Configure 
akka-dns
 to be used as the discovery implementation in your 
application.conf
:


copy
source
akka {
  discovery {
    method = akka-dns
  }
}


From there on, you can use the generic API that hides the fact which discovery method is being used by calling:




Scala




copy
source
import akka.discovery.Discovery
import akka.discovery.ServiceDiscovery

val discovery: ServiceDiscovery = Discovery(system).discovery
// ...
val result: Future[ServiceDiscovery.Resolved] = discovery.lookup("akka.io", resolveTimeout = 3.seconds)


Java




copy
source
import akka.discovery.Discovery;
import akka.discovery.ServiceDiscovery;

ServiceDiscovery discovery = Discovery.get(system).discovery();
// ...
CompletionStage<ServiceDiscovery.Resolved> result =
    discovery.lookup("foo", Duration.ofSeconds(3));




DNS records used


DNS discovery will use either A/AAAA records or SRV records depending on whether a 
Simple
 or 
Full
 lookup is issued. The advantage of SRV records is that they can include a port.


SRV records


Lookups with all the fields set become SRV queries. For example:


dig srv _service._tcp.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)
;; QUESTION SECTION:
;service.tcp.akka.test.         IN      SRV

;; ANSWER SECTION:
_service._tcp.akka.test.  86400   IN      SRV     10 60 5060 a-single.akka.test.
_service._tcp.akka.test.  86400   IN      SRV     10 40 5070 a-double.akka.test.




In this case 
service.tcp.akka.test
 resolves to 
a-single.akka.test
 on port 
5060
 and 
a-double.akka.test
 on port 
5070
. Currently discovery does not support the weightings.


A/AAAA records


Lookups with any fields missing become A/AAAA record queries. For example:


dig a-double.akka.test

; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.akka.test
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)
;; QUESTION SECTION:
;a-double.akka.test.            IN      A

;; ANSWER SECTION:
a-double.akka.test.     86400   IN      A       192.168.1.21
a-double.akka.test.     86400   IN      A       192.168.1.22




In this case 
a-double.akka.test
 would resolve to 
192.168.1.21
 and 
192.168.1.22
.


Discovery Method: Configuration


Configuration currently ignores all fields apart from service name.


For simple use cases configuration can be used for service discovery. The advantage of using Akka Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.


Configure it to be used as discovery method in your 
application.conf


akka {
  discovery.method = config
}



By default the services discoverable are defined in 
akka.discovery.config.services
 and have the following format:


akka.discovery.config.services = {
  service1 = {
    endpoints = [
      {
        host = "cat"
        port = 1233
      },
      {
        host = "dog"
        port = 1234
      }
    ]
  },
  service2 = {
    endpoints = []
  }
}



Where the above block defines two services, 
service1
 and 
service2
. Each service can have multiple endpoints.


Discovery Method: Aggregate multiple discovery methods


Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.


To use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.


Configure 
aggregate
 as 
akka.discovery.method
 and which discovery methods are tried and in which order.


akka {
  discovery {
    method = aggregate
    aggregate {
      discovery-methods = ["akka-dns", "config"]
    }
    config {
      services {
        service1 {
          endpoints = [
            {
              host = "host1"
              port = 1233
            },
            {
              host = "host2"
              port = 1234
            }
          ]
        }
      }
    }
  }
}




The above configuration will result in 
akka-dns
 first being checked and if it fails or returns no targets for the given service name then 
config
 is queried which i configured with one service called 
service1
 which two hosts 
host1
 and 
host2
.


Migrating from Akka Management Discovery (before 1.0.0)


Akka Discovery started out as a submodule of Akka Management, before 1.0.0 of Akka Management. Akka Discovery is not compatible with those versions of Akka Management Discovery.


At least version 
1.0.0
 of any Akka Management module should be used if also using Akka Discovery.


Migration steps:




Any custom discovery method should now implement 
akka.discovery.ServiceDiscovery


discovery-method
 now has to be a configuration location under 
akka.discovery
 with at minimum a property 
class
 specifying the fully qualified name of the implementation of 
akka.discovery.ServiceDiscovery
.  Previous versions allowed this to be a class name or a fully qualified config location e.g. 
akka.discovery.kubernetes-api
 rather than just 
kubernetes-api
















 
Source.zipWithN






Utilities 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://portal.akka.io
Log in - Lightbend Customer Portal




























 
 


|
Customer Portal






















Customer Portal


Log in
















Email














Password












Log in




Forgot password?






















© 2011 - 2024, Lightbend, Inc. dba Akka. All rights reserved. | 
Terms
 | 
Privacy Policy
 | 
Cookie Settings
 | 
RSS

URL: https://doc.akka.io/libraries/akka/snapshot/additional/operations.html
Operating a Cluster • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster




Starting


Stopping


Cluster Management


Monitoring and Observability




Deploying


Rolling Updates


Building Native Images




Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging


Operating a Cluster




Starting


Stopping


Cluster Management


Monitoring and Observability




Deploying


Rolling Updates


Building Native Images




Project Information


Akka Classic




















Operating a Cluster


This documentation discusses how to operate a cluster. For related, more specific guides see 
Packaging
, 
Deploying
 and 
Rolling Updates
.


Starting


Cluster Bootstrap


When starting clusters on cloud systems such as Kubernetes, AWS, Google Cloud, Azure, Mesos or others, you may want to automate the discovery of nodes for the cluster joining process, using your cloud providers, cluster orchestrator, or other form of service discovery (such as managed DNS).


The Akka Management library includes the 
Cluster Bootstrap
 module which handles just that. Please refer to its documentation for more details.
Note


If you are running Akka in a Docker container or the nodes for some other reason have separate internal and external ip addresses you must configure remoting according to 
Akka behind NAT or in a Docker container


Stopping


See 
Rolling Updates, Cluster Shutdown and Coordinated Shutdown
.


Cluster Management


There are several management tools for the cluster. Complete information on running and managing Akka applications can be found in the  project documentation.




HTTP


Information and management of the cluster is available with a HTTP API. See documentation of 
Akka Management
.




JMX


Information and management of the cluster is available as JMX MBeans with the root name 
akka.Cluster
. The JMX information can be displayed with an ordinary JMX console such as JConsole or JVisualVM.


From JMX you can:




see what members that are part of the cluster


see status of this node


see roles of each member


join this node to another node in cluster


mark any node in the cluster as down


tell any node in the cluster to leave




Member nodes are identified by their address, in format 
akka://actor-system-name@hostname:port
.


Monitoring and Observability


Aside from log monitoring and the monitoring provided by your APM or platform provider, 
Akka Insights
, available through a 
Akka Subscription
, can provide additional insights in the run-time characteristics of your application, including metrics, events, and distributed tracing for Akka Actors, Cluster, HTTP, and more.














 
Packaging






Deploying 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka-core/current/typed/index-persistence.html
Persistence (Event Sourcing) • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)




Event Sourcing


Replicated Event Sourcing


CQRS


Style Guide


Snapshotting


Testing


Schema Evolution for Event Sourced Actors


Persistence Query


Persistence Query for LevelDB


Persistence Plugins


Persistence - Building a storage backend


Replicated Event Sourcing replication via direct access to replica databases


Replicated Event Sourcing Examples




Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Persistence (Event Sourcing)






Event Sourcing




Module info


Introduction


Example and core API


Effects and Side Effects


Cluster Sharding and EventSourcedBehavior


Accessing the ActorContext


Changing Behavior


Replies


Serialization


Recovery


Tagging


Event adapters


Wrapping EventSourcedBehavior


Journal failures


Stash


Scaling out


Configuration


Example project




Replicated Event Sourcing




Relaxing the single-writer principle for availability


API


Resolving conflicting updates


Side effects


How it works


Running projections


Examples


Journal Support


Migrating from non-replicated




CQRS


Style Guide




Event handlers in the state


Command handlers in the state


Optional initial state


Mutable state


Leveraging Java 21 features




Snapshotting




Snapshots


Snapshot failures


Snapshot deletion


Event deletion




Testing




Module info


Unit testing with the BehaviorTestKit


Unit testing with the the ActorTestKit and EventSourcedBehaviorTestKit


Persistence TestKit


Integration testing




Schema Evolution for Event Sourced Actors




Dependency


Introduction


Schema evolution in event-sourced systems


Picking the right serialization format


Schema evolution in action




Persistence Query




Dependency


Introduction


Design overview


Read Journals


Performance and denormalization


Query plugins


Scaling out


Example project




Persistence Query for LevelDB




Dependency


Introduction


How to get the ReadJournal


Supported Queries


Configuration




Persistence Plugins




R2DBC plugin


Cassandra plugin


AWS DynamoDB plugin


JDBC plugin


Feature limitations


Enabling a plugin


Eager initialization of persistence plugin


Pre-packaged plugins




Persistence - Building a storage backend




Journal plugin API


Snapshot store plugin API


Plugin TCK


Corrupt event logs




Replicated Event Sourcing replication via direct access to replica databases




Sharded Replicated Event Sourced entities


Direct Replication of Events


Hot Standby


Examples


Journal Support




Replicated Event Sourcing Examples




Additional samples


Auction example


Shopping cart example




















 
Choosing Akka Cluster






Event Sourcing 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/index.html
Streams • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate


Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Streams






Introduction




Motivation


How to read these docs


Module info




Streams Quickstart Guide




Dependency


First steps


Reusable Pieces


Time-Based Processing


Reactive Tweets




Design Principles behind Akka Streams




What shall users of Akka Streams expect?


Interoperation with other Reactive Streams implementations


What shall users of streaming libraries expect?


The difference between Error and Failure




Basics and working with Flows




Dependency


Introduction


Core concepts


Defining and running streams


Back-pressure explained


Stream Materialization


Stream ordering


Actor Materializer Lifecycle




Working with Graphs




Dependency


Introduction


Constructing Graphs


Constructing and combining Partial Graphs


Constructing Sources, Sinks and Flows from Partial Graphs


Combining Sources and Sinks with simplified API


Building reusable Graph components


Predefined shapes


Bidirectional Flows


Accessing the materialized value inside the Graph


Graph cycles, liveness and deadlocks




Modularity, Composition and Hierarchy




Dependency


Introduction


Basics of composition and modularity


Composing complex systems


Materialized values


Attributes




Buffers and working with rate




Dependency


Introduction


Buffers for asynchronous operators


Buffers in Akka Streams


Rate transformation




Context Propagation




Restrictions


Creation


Composition




Dynamic stream handling




Dependency


Introduction


Controlling stream completion with KillSwitch


Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub




Custom stream processing




Dependency


Introduction


Custom processing with GraphStage


Thread safety of custom operators


Resources and the operator lifecycle


Extending Flow Operators with Custom Operators




Futures interop




Dependency


Overview




Actors interop




Dependency


Overview




Reactive Streams Interop




Dependency


Overview


Other implementations




Error Handling in Streams




Dependency


Introduction


Logging errors


Recover


Recover with retries


Delayed restarts with a backoff operator


Supervision Strategies




Working with streaming IO




Dependency


Introduction


Streaming TCP


Streaming File IO




StreamRefs - Reactive Streams over the network




Dependency


Introduction


Stream References


Bulk Stream References


Serialization of SourceRef and SinkRef


Configuration




Pipelining and Parallelism




Dependency


Introduction


Pipelining


Parallel processing


Combining pipelining and parallel processing




Testing streams




Dependency


Introduction


Built-in sources, sinks and operators


TestKit


Streams TestKit


Fuzzing Mode




Substreams




Dependency


Introduction


Nesting operators


Flattening operators




Streams Cookbook




Dependency


Introduction


Working with Flows


Working with Operators


Working with rate


Working with IO




Configuration


Operators




Source operators


Sink operators


Additional Sink and Source converters


File IO Sinks and Sources


Simple operators


Flow operators composed of Sinks and Sources


Asynchronous operators


Timer driven operators


Backpressure aware operators


Nesting and flattening operators


Time aware operators


Fan-in operators


Fan-out operators


Watching status operators


Actor interop operators


Compression operators


Error handling


Source.actorRef


Sink.actorRef


ActorSource.actorRef


ActorSink.actorRef


Source.actorRefWithBackpressure


Sink.actorRefWithBackpressure


ActorSource.actorRefWithBackpressure


ActorSink.actorRefWithBackpressure


aggregateWithBoundary


alsoTo


alsoToAll


Flow.asFlowWithContext


StreamConverters.asInputStream


StreamConverters.asJavaStream


ask


ActorFlow.ask


ActorFlow.askWithContext


ActorFlow.askWithStatus


ActorFlow.askWithContext


StreamConverters.asOutputStream


Sink.asPublisher


Source.asSourceWithContext


Source.asSubscriber


backpressureTimeout


Balance


batch


batchWeighted


Broadcast


buffer


Sink.cancelled


collect


Sink.collect


Sink.collection


collectType


Source.combine


Sink.combine


Source.completionStage


Flow.completionStageFlow


Sink.completionStageSink


Source.completionStageSource


completionTimeout


concat


concatAllLazy


concatLazy


conflate


conflateWithSeed


contramap


Source.cycle


Compression.deflate


delay


delayWith


detach


divertTo


drop


dropWhile


dropWithin


Source.empty


expand


extrapolate


Source.failed


filter


filterNot


flatMapConcat


flatMapMerge


flatMapPrefix


Flow.flattenOptional


fold


Sink.fold


foldAsync


Sink.foreach


Sink.foreachAsync


Source.apply
Source.from


Source.fromCompletionStage


FileIO.fromFile


Source.fromFuture


Source.fromFutureSource


StreamConverters.fromInputStream


Source.fromIterator


fromJavaStream


StreamConverters.fromJavaStream


fromMaterializer


Sink.fromMaterializer


StreamConverters.fromOutputStream


FileIO.fromPath


Source.fromPublisher


Flow.fromSinkAndSource


Flow.fromSinkAndSourceCoupled


Source.fromSourceCompletionStage


Sink.fromSubscriber


Source.future


Flow.futureFlow


Sink.futureSink


Source.futureSource


groupBy


grouped


groupedWeighted


groupedWeightedWithin


groupedWithin


Compression.gunzip


Compression.gzip


Sink.head


Sink.headOption


idleTimeout


Sink.ignore


Compression.inflate


initialDelay


initialTimeout


interleave


interleaveAll


intersperse


StreamConverters.javaCollector


StreamConverters.javaCollectorParallelUnordered


keepAlive


Sink.last


Sink.lastOption


Source.lazily


Source.lazilyAsync


Source.lazyCompletionStage


Flow.lazyCompletionStageFlow


Sink.lazyCompletionStageSink


Source.lazyCompletionStageSource


Flow.lazyFlow


Source.lazyFuture


Flow.lazyFutureFlow


Sink.lazyFutureSink


Source.lazyFutureSource


Flow.lazyInitAsync


Sink.lazyInitAsync


Source.lazySingle


Sink.lazySink


Source.lazySource


limit


limitWeighted


log


logWithMarker


map


mapAsync


mapAsyncPartitioned


mapAsyncUnordered


mapConcat


mapError


mapWithResource


Source.maybe


merge


mergeAll


mergeLatest


mergePreferred


mergePrioritized


mergePrioritizedN


MergeSequence


mergeSorted


monitor


never


Sink.never


Sink.onComplete


onErrorComplete


RestartSource.onFailuresWithBackoff


RestartFlow.onFailuresWithBackoff


orElse


Partition


prefixAndTail


preMaterialize


Sink.preMaterialize


prepend


prependLazy


Source.queue


Sink.queue


Source.range


recover


recoverWith


recoverWithRetries


reduce


Sink.reduce


Source.repeat


scan


scanAsync


Sink.seq


setup


Sink.setup


Source.single


PubSub.sink


sliding


PubSub.source


splitAfter


splitWhen


statefulMap


statefulMapConcat


take


Sink.takeLast


takeWhile


takeWithin


throttle


Source.tick


FileIO.toFile


FileIO.toPath


Source.unfold


Source.unfoldAsync


Source.unfoldResource


Source.unfoldResourceAsync


Unzip


UnzipWith


watch


watchTermination


wireTap


RestartSource.withBackoff


RestartFlow.withBackoff


RestartSink.withBackoff


RetryFlow.withBackoff


RetryFlow.withBackoffAndContext


zip


zipAll


zipLatest


zipLatestWith


Source.zipN


zipWith


zipWithIndex


Source.zipWithN




















 
Building a storage backend for Durable State






Introduction 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://akka.io/blog
Blog
































































































































Akka Logo


















Product








Developers








Stories


Blog


Pricing


Company




About


Customer Portal


Trust Center


Professional Services


Partners


Support
































































Contact Us


Sign In


GET STARTED































            Overview
          










What
 
 Platform to build and run responsive apps.










Why
 
 React to change with elasticity, agility and resilience.










How
 
 Distribute data and logic together.









              Get Started 
 



















            Apps you build
          










Transactional


Event-sourced 


AI Inference


Digital Twin


Durable Execution


Analytics


Streaming


Edge



















            Stories
          














Icon Solutions leverages Akka to deliver unique an..












eero disrupts consumer WiFi with highly reliable s..












Ramen builds on Akka to provide network-as-a-servi..





























            Resources
          










Docs


Discussion Forum


Release Notes


Support



















            Architecture
          










Reactive Principles


Reactive Manifesto


Free Training



















            Latest Blogs
          










Saga patterns in Akka 3 (part 1) - event choreography






99.9999% app availability






Multi-region replicated apps



















            Benchmarks
          








1.4M TPS


9 ms latency


$11.77 / month / 1K TPS


see all

















            SDK
          








entities


views


endpoints


streaming


timers


workflows

















            Libraries
          








actors


http


streams


cluster


event sourcing


grpc


durable state


projections


persistence


edge


integrations


observability



























               Try Akka for free
            


Deploy and scale a multi-region app
No credit card required











               Develop your own Akka app
            


Akka account not required
Free and open SDK for offline dev











               Request demo
            


Personalized demo
Akka app design consultation























































          Blog
        





        All the latest 
info and insights
 from the Akka team
      


















Featured


























99.9999% app availability





                  Application resilience is key to business resilience. Akka reduces the cost and complexity to achieve up to 99.9999% availability without sacrificing performance and scale. Read the blog to learn more.
                






Read more




























































Multi-region replicated apps





                  With Akka, create multi-region stateful applications that ensure low latency, no-downtime disaster recovery, seamless upgrades, and effortless cross-cloud migrations.
                






Read more






























































Webinar: introducing Akka 3





                  Akka leaders share and demonstrate game-changing innovation for distributed applications.
                






Read more






























































All posts














All







                  All
                







                  Akka Edge
                







                  Press Releases
                







                  Webinars
                







                  Akka Projections
                







                  Cloud Native
                







                  Akka Cluster
                







                  Database Sharding
                







                  Microservices
                







                  Active-Active
                







                  Zero Trust
                







                  Akka Streams
                







                  Kubernetes
                







                  Benchmarking
                







                  Distributed Architecture
                







                  Event-Driven
                







                  Workflows
                











































                    Saga patterns in Akka 3 (part 1) - event choreography
                  










Andrzej Ludwikowski





















                    99.9999% app availability
                  










Tyler Jewell





















                    Multi-region replicated apps
                  










Tyler Jewell





















                    Announcing Akka 3
                  










Jonas Bonér





















                    Workflows with Akka
                  










Andrzej Ludwikowski






Workflows

















                    Lightbend is now Akka
                  










Tyler Jewell





















                    Akka 3 - FAQ
                  










Team Akka





















                    Webinar: introducing Akka 3
                  










Team Akka






Webinars

















                    Lightbend launches Akka 3 to make it easy to build and run apps that react to change; rebrands company as Akka
                  










Team Akka






Press Releases

















                    InfoQ webinar: the architect's guide to elasticity
                  










Jonas Bonér






Webinars

















                    Lightbend and Scalac partner to enable enterprises to leverage the power of Akka
                  










Team Akka






Press Releases

















                    Akka license keys and a no SPAM promise
                  










Tyler Jewell





















                    O’Reilly webinar: transcending the barriers to elasticity
                  










Jonas Bonér






Webinars

















                    Akka innovations: multi-cloud, edge, and security enhancements
                  










Jonas Bonér






Akka Projections

















                    Benchmarking database sharding in Akka
                  










Andrzej Ludwikowski






Database Sharding

















                    Not all CVE fixes are created equal
                  










Michael Nash





















                    Akka 24.05 - fully replicated entities across regions, edge, and clouds
                  










Patrik Nordwall






Akka Edge

















                    Building Zero Trust security into your Akka applications
                  










Jonas Bonér






Zero Trust

















                    Akka 24.05 — the edge unleashed
                  










Jonas Bonér






Akka Edge

















                    Akka with Java 21: less is more
                  










Eduardo Pinto





















                    The database is the bottleneck!
                  










Patrik Nordwall






Akka Cluster

















                    Is community-backed open source software worth the risk?
                  










Tyler Jewell





















                    Webinar: Akka 24.05 release highlights
                  










Team Akka






Webinars

















                    Lightbend releases Akka 24.05 with a focus on security, performance, and edge efficiency
                  










Team Akka






Press Releases

















                    Akka 24.05: more security. More performance. More efficiency.
                  










Jonas Bonér






Akka Edge

















                    Open source is at a crossroads
                  










Tyler Jewell





















                    Lightbend aims to democratize distributed systems for developers
                  










Tyler Jewell





















                    Cloud architecture will fail you. Distributed application architecture will not.
                  










Tyler Jewell






Distributed Architecture

















                    Akka edging further
                  










Christopher Hunt






Akka Edge

















                    2x improvement in latency in Swiggy data science platform
                  










Team Akka





















                    Boundaries between cloud and edge computing will continue to blur, predicts Jonas Bonér of Akka
                  










Jonas Bonér





















                    Akka closes FY24 with record growth
                  










Jonas Bonér





















                    Akka Edge: shaping the future of industry with edge computing
                  










Aakash Sharan






Akka Edge

















                    2024 predictions for the cloud native market
                  










Jonas Bonér






Akka Edge

















                    Akka Edge: exploring scale to zero
                  










Peter Vlugter






Akka Edge

















                    Deep dive of event-driven communication between edge and cloud
                  










Patrik Nordwall






Akka Projections

















                    What is Akka Edge?
                  










Patrik Nordwall






Akka Edge

















                    Webinar: Akka Edge sample project overview
                  










Johan Andrén






Akka Edge

















                    Lightbend announces the industry’s first programming model enabling application development for both cloud and edge
                  










Team Akka






Press Releases

















                    Akka Edge: unifying the cloud and edge
                  










Jonas Bonér






Akka Edge

















                    Reactive programming vs. reactive systems
                  










Jonas Bonér





















                    Microservices unleashed: evolving mission-critical applications in the cloud at lower cost, risk, and complexity
                  










Team Akka





















                    5 reactive microservices use cases that power business innovation
                  










Team Akka





















                    Fast, lean, and unbreakable: building cloud apps with Akka and Kubernetes
                  










Jonas Bonér






Kubernetes

















                    Get productive with Akka Streams
                  










Janik Dotzel






Akka Streams

















                    Celebrating a milestone: Akka surpasses 1 billion downloads
                  










James Townley





















                    The easy button for durable state queries
                  










Patrik Nordwall





















                    Adaptive stream parallelization for fun and throughput
                  










Levi Ramsey






Akka Streams

















                    Active-Active
                  










Johan Andrén






Active-Active

















                    Faster and smoother rolling updates for Akka Clusters in Kubernetes
                  










Eduardo Pinto






Akka Cluster

















                    Lightbend launches Akka Distributed Cluster for next-gen edge computing across multiple data centers
                  










Team Akka






Press Releases

















                    Akka: enabling the cloud to edge continuum
                  










Jonas Bonér






Akka Edge

















                    Benchmarking Kafka vs. Akka brokerless pub/sub
                  










Patrik Nordwall





















                    Lightbend achieves SOC 2 compliance
                  










Michael Nash





















                    Ditch the message broker, go faster
                  










Patrik Nordwall






Akka Projections

















                    Akka 22.10 released
                  










Jonas Bonér






Akka Projections

















                    Lightbend changes its software licensing model for Akka technology
                  










Team Akka






Press Releases

















                    Why we are changing the license for Akka
                  










Jonas Bonér





















                    Akka gRPC update delivers 1200% performance improvement (so what happened?)
                  










Johannes Rudolph





















                    Webinar: Putting the ‘I’ in IoT – building digital twins with Akka microservices
                  










Hugh McKee






Akka Projections

















                    Design techniques for building stateful, cloud-native applications: Part 4 – message delivery & reliability
                  










Hugh McKee






Cloud Native

















                    Design techniques for building stateful, cloud-native applications: Part 3 – messages, CQRS, and event sourcing
                  










Team Akka






Cloud Native

















                    Design techniques for building stateful cloud-native applications: Part 2 – distributed state
                  










Sean Walsh






Cloud Native

















                    Design techniques for building stateful, cloud-native applications: Part 1 - resiliency, recoverability, and scalability
                  










Hugh McKee






Cloud Native















          No blogs found. Please check back later.
        






















Stay Responsive 
to Change.










GET STARTED










































REQUEST A DEMO




























































































































































































Akka










Documentation


Akka Newsletter


Akkademy


Source Code


License Key














 










Support


Pekko


Security Announcements


Trust Center
















Follow Us





































































































            ©2025 Lightbend, Inc. dba Akka. All rights reserved.
          







            Terms
          







             Privacy Policy 
          








Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/common/other-modules.html#akka-diagnostics
Other Akka modules • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules




Akka HTTP


Akka gRPC


Alpakka


Alpakka Kafka Connector


Akka Projections


Cassandra Plugin for Akka Persistence


JDBC Plugin for Akka Persistence


R2DBC Plugin for Akka Persistence


Akka Management


Akka Diagnostics


Akka Insights


Community Projects




Package, Deploy and Run


Project Information


Akka Classic




















Other Akka modules


This page describes modules that compliment libraries from the Akka core. See 
this overview
 instead for a guide on the core modules.


Akka HTTP


A full server- and client-side HTTP stack on top of akka-actor and akka-stream.


Akka gRPC


Akka gRPC provides support for building streaming gRPC servers and clients on top of Akka Streams.


Alpakka


Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka.


Alpakka Kafka Connector


The Alpakka Kafka Connector connects Apache Kafka with Akka Streams.


Akka Projections


Akka Projections let you process a stream of events or records from a source to a projected model or external system.


Cassandra Plugin for Akka Persistence


An Akka Persistence journal and snapshot store backed by Apache Cassandra.


JDBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on 
Slick
.


R2DBC Plugin for Akka Persistence


An Akka Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on 
R2DBC
.


Akka Management




Akka Management
 provides a central HTTP endpoint for Akka management extensions.


Akka Cluster Bootstrap
 helps bootstrapping an Akka cluster using Akka Discovery.


Akka Management Kubernetes Rolling Updates
 for smooth rolling updates.


Akka Management Cluster HTTP
 provides HTTP endpoints for introspecting and managing Akka clusters.


Akka Discovery for Kubernetes, Consul, Marathon, and AWS


Kubernetes Lease




Akka Diagnostics




Akka Thread Starvation Detector


Akka Configuration Checker




Akka Insights


Intelligent monitoring and observability purpose-built for Akka: 
Lightbend Telemetry


Community Projects


Akka has a vibrant and passionate user community, the members of which have created many independent projects using Akka as well as extensions to it. See 
Community Projects
.














 
Extending Akka






Package, Deploy and Run 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/project/examples.html
Example projects • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects




Quickstart


FSM


Cluster


Distributed Data


Cluster Sharding


Persistence and CQRS


Replicated Event Sourcing


Kafka to Cluster Sharding




Project




Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information




Binary Compatibility Rules


Downstream upgrade strategy


Modules marked “May Change”


IDE Tips


Immutability using Lombok


Migration Guides


Rolling Updates and Versions


Issue Tracking


Licenses


Frequently Asked Questions


Books and Videos


Example projects




Quickstart


FSM


Cluster


Distributed Data


Cluster Sharding


Persistence and CQRS


Replicated Event Sourcing


Kafka to Cluster Sharding




Project




Akka Classic




















Example projects


The following example projects can be downloaded. They contain build files and have instructions of how to run.


Quickstart




Scala 
akka-quickstart-scala.zip


Java 
akka-quickstart-java.zip




The 
Quickstart
 sample is described in 
Introduction to Actors
 and walks you through example code that introduces how to define actor systems, actors, and messages.


FSM




Scala 
Dining hackers with FSM
 (
akka-sample-fsm-scala.zip
)


Java 
Dining hackers with FSM
 (
akka-sample-fsm-java.zip
)




This project contains a Dining Hakkers sample illustrating how to model a Finite State Machine (FSM) with actors.


Cluster




Scala 
Cluster example project
 (
akka-sample-cluster-scala.zip
)


Java 
Cluster example project
 (
akka-sample-cluster-java.zip
)




This project contains samples illustrating different Cluster features, such as subscribing to cluster membership events, and sending messages to actors running on nodes in the cluster with Cluster aware routers.


It also includes Multi JVM Testing with the 
sbt-multi-jvm
 plugin.


Distributed Data




Scala 
Distributed data example project
 (
akka-sample-distributed-data-scala.zip
)


Java 
Distributed data example project
 (
akka-sample-distributed-data-java.zip
)




This project contains several samples illustrating how to use Distributed Data.


Cluster Sharding




Scala 
Cluster Sharding example
 (
akka-sample-sharding-scala.zip
)


Java 
Cluster Sharding example
 (
akka-sample-sharding-java.zip
)




This project contains a KillrWeather sample illustrating how to use Cluster Sharding.


Persistence and CQRS


The 
Microservices with Akka tutorial
 contains a Shopping Cart sample illustrating how to use Event Sourcing and Projections together. The events are consumed by even processors to build other representations from the events, or publish the events to other services.


Replicated Event Sourcing


The 
Akka Distributed Cluster Guide
 illustrates how to use 
Replicated Event Sourcing
 that supports active-active persistent entities.


Kafka to Cluster Sharding




Scala 
akka-sample-kafka-to-sharding-scala.zip




This project demonstrates how to use the External Shard Allocation strategy to co-locate the consumption of Kafka partitions with the shard that processes the messages.














 
Books and Videos






Project 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/remote-security.html
Remote Security • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security




Configuring SSL/TLS for Akka Remoting


mTLS with rotated certificates in Kubernetes


Untrusted Mode




Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security




Configuring SSL/TLS for Akka Remoting


mTLS with rotated certificates in Kubernetes


Untrusted Mode




Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Remote Security


An 
ActorSystem
ActorSystem
 should not be exposed via 
Akka Cluster
 or 
Akka Remote
 over plain Aeron/UDP or TCP to an untrusted network (e.g. Internet). It should be protected by network security, such as a firewall. If that is not considered as enough protection 
TLS with mutual authentication
 should be enabled.


Best practice is that Akka remoting nodes should only be accessible from the adjacent network. Note that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.


By default, 
Java serialization
 is disabled in Akka. That is also security best-practice because of its multiple 
known attack surfaces
.




Configuring SSL/TLS for Akka Remoting


In addition to what is described here, see also 
mTLS with rotated certificates in Kubernetes
.


SSL can be used as the remote transport by using the 
tls-tcp
 transport:


akka.remote.artery {
  transport = tls-tcp
}



Next the actual SSL/TLS parameters have to be configured:


akka.remote.artery {
  transport = tls-tcp

  ssl.config-ssl-engine {
    key-store = "/example/path/to/mykeystore.jks"
    trust-store = "/example/path/to/mytruststore.jks"

    key-store-password = ${SSL_KEY_STORE_PASSWORD}
    key-password = ${SSL_KEY_PASSWORD}
    trust-store-password = ${SSL_TRUST_STORE_PASSWORD}

    protocol = "TLSv1.2"

    enabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]
  }
}



Always use 
substitution from environment variables
 for passwords. Don’t define real passwords in config files.


According to 
RFC 7525
 the recommended algorithms to use with TLS 1.2 (as of writing this document) are:




TLS_DHE_RSA_WITH_AES_128_GCM_SHA256


TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256


TLS_DHE_RSA_WITH_AES_256_GCM_SHA384


TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384




You should always check the latest information about security and algorithm recommendations though before you configure your system.


Creating and working with keystores and certificates is well documented in the 
Generating X.509 Certificates
 section of Lightbend’s SSL-Config library.


Since an Akka remoting is inherently 
peer-to-peer
 both the key-store as well as trust-store need to be configured on each remoting node participating in the cluster.


The official 
Java Secure Socket Extension documentation
 as well as the 
Oracle documentation on creating KeyStore and TrustStores
 are both great resources to research when setting up security on the JVM. Please consult those resources when troubleshooting and configuring SSL.


Mutual authentication between TLS peers is enabled by default. Mutual authentication means that the passive side (the TLS server side) of a connection will also request and verify a certificate from the connecting peer. Without this mode only the client side is requesting and verifying certificates. While Akka is a peer-to-peer technology, each connection between nodes starts out from one side (the “client”) towards the other (the “server”).


Note that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.


It’s recommended that you enable hostname verification with 
akka.remote.artery.ssl.config-ssl-engine.hostname-verification=on
. When enabled it will verify that the destination hostname matches the hostname in the peer’s certificate.


In deployments where hostnames are dynamic and not known up front it can make sense to leave the hostname verification off.


You have a few choices how to set up certificates and hostname verification:




Have a single set of keys and a single certificate for all nodes and 
disable
 hostname checking
    


The single set of keys and the single certificate is distributed to all nodes. The certificate can  be self-signed as it is distributed both as a certificate for authentication but also as the trusted certificate.


If the keys/certificate are lost, someone else can connect to your cluster.


Adding nodes to the cluster is simple as the key material can be deployed / distributed to the new node.






Have a single set of keys and a single certificate for all nodes that contains all of the host names and 
enable
  hostname checking.
    


This means that only the hosts mentioned in the certificate can connect to the cluster.


It cannot be checked, though, if the node you talk to is actually the node it is supposed to be (or if it is one  of the other nodes). This seems like a minor restriction as you’ll have to trust all cluster nodes the same in an  Akka cluster anyway.


The certificate can be self-signed in which case the same single certificate is distributed and trusted on all  nodes (but see the next bullet)


Adding a new node means that its host name needs to conform to the trusted host names in the certificate.  That either means to foresee new hosts, use a wildcard certificate, or use a full CA in the first place,  so you can later issue more certificates if more nodes are to be added (but then you already get into the  territory of the next solution).


If a certificate is stolen, it can only be used to connect to the cluster from a node reachable via a hostname  that is trusted in the certificate. It would require tampering with DNS to allow other nodes to get access to  the cluster (however, tampering DNS might be easier in an internal setting than on internet scale).






Have a CA and then keys/certificates, one for each node, and 
enable
 host name checking.
    


Basically like internet HTTPS but that you only trust the internal CA and then issue certificates for each new node.


Needs a PKI, the CA certificate is trusted on all nodes, the individual certificates are used for authentication.


Only the CA certificate and the key/certificate for a node is distributed.


If keys/certificates are stolen, only the same node can access the cluster (unless DNS is tampered with as well).  You can revoke single certificates.








See also a description of the settings in the 
Remote Configuration
 section.
Note


When using SHA1PRNG on Linux it’s recommended specify 
-Djava.security.egd=file:/dev/urandom
 as argument to the JVM to prevent blocking. It is NOT as secure because it reuses the seed.


mTLS with rotated certificates in Kubernetes


Akka remotingâs has support for using mTLS certificates that are frequently rotated. This support is designed to work with cert-manager and other Kubernetes based secret providers with a minimum of configuration. This feature is important for secure Akka deployments to prevent malicious hosts from joining the Akka Cluster or eavesdropping on your Akka Cluster communication, and therefore you need to use mTLS to secure the communication.
Service mesh


Encryption and authentication via a service mesh is not an option for Akka Cluster as described in 
Service mesh
. 


Provisioning of the certificates needed by Akka are using 
cert-manager
. The Kubernetes cluster should have a standard 
cert-manager installation
.


Understanding the certificates


First off, a few concepts: cert-manager has a concept of 
Certificates
 and 
Issuers
. A Certificate is a Custom Resource Definition (CRD) that you deploy that cert-manager will reconcile into a Kubernetes 
Secret
 containing a TLS certificate. The 
Certificate
 references an 
Issuer
, and the 
Issuer
 describes how Certificates that reference it should be issued.


In order to support frequently rotated certificates, Akka canât just use a self signed certificate, since self signed certificates need to be the same at both ends to authenticate each other properly, and during the time when the certificate is being rotated, two different Akka nodes may have different certificates. Instead, Akka needs certificates issued by a certificate authority (CA). The CA verifies whether a certificate should be trusted or not, so during rotation, both the old and the new certificate can work together, because both are signed by the same CA. So, when we issue our certificates, weâll use cert-managers CA 
Issuer
 type.


The CA Issuer itself needs a certificate to do its signing, and this certificate weâll also provision using cert-manager. That certificate weâre not going to rotate - its private key never gets shared with anything outside of cert-manager, and so rotating it is not as necessary. Because of this, it will use a self signed certificate, and provisioning that certificate can be done by using a cert-manager self signed 
Issuer
 type.


So, in total, weâre going to have two issuers, a self signed issuer that issues certificates for the CA issuer, and then that CA issuer will issue certificates that are frequently rotated for our Akka service to use. The self signed issuer, certificate, and CA issuer can be reused across different Akka deployments.


Kubernetes resources


First we deploy the self signed issuer:


apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: self-signed-issuer
spec:
  selfSigned: {}



Weâre creating this for the whole cluster, self signed issuers donât have any state or configuration, thereâs no reason to have more than one for your entire cluster.


Next we create a self signed certificate for our CA issuer to use that references this issuer:


apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: akka-tls-ca-certificate
  namespace: default
spec:
  issuerRef:
    name: self-signed-issuer
    kind: ClusterIssuer
  secretName: akka-tls-ca-certificate
  commonName: default.akka.cluster.local
  # 100 years
  duration: 876000h
  # 99 years
  renewBefore: 867240h
  isCA: true
  privateKey:
    rotationPolicy: Always



Weâve created this in the default namespace, which will be the same namespace that our Akka service is deployed to. If youâre using a different namespace, youâll need to update accordingly.


The 
commonName
 isnât very important, itâs not actually used anywhere, though may be useful for debugging purposes if youâre ever looking into why a particular certificate isnât trusted by a service. We use a naming convention for common names and DNS names that follows the pattern 
<service-name>.<namespace>.akka.cluster.local
. The CA uses the same convention without the service name. This convention doesnât need to be followed, but it makes it easy to reason about the purpose of any given certificate.


Now we create the CA issuer:


apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: akka-tls-ca-issuer
  namespace: default
spec:
  ca:
    secretName: akka-tls-ca-certificate



This uses the secret that we configured to be provisioned in the certificate above. Finally, we provision the certificate that our Akka service is going to use - weâre assuming that the name of the service in this case is 
my-service
:


apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: my-service-akka-tls-certificate
  namespace: default
spec:
  issuerRef:
    name: akka-tls-ca-issuer
  secretName: my-service-akka-tls-certificate
  dnsNames:
  - my-service.default.akka.cluster.local
  duration: 24h
  renewBefore: 16h
  privateKey:
    rotationPolicy: Always



The actual 
dnsName
 configured isnât important, since Akka cluster does not actually use these names for looking up the service, as long as itâs unique to the service within the issuer. Akkaâs mTLS support will verify that the DNS name supplied by an incoming connection matches the DNS name supplied in its own secret, and reject it otherwise. Again, weâre using the naming convention for the 
dnsName
 mentioned above.


This certificate is configured to last for 24 hours, and rotate every 16 hours.


If you have more Akka services that you wish to deploy in the same namespace, you can reuse the same CA Issuer, you only need to deploy an additional Certificate for each service.


Enable TLS with RotatingKeysSSLEngineProvider


Add the following Akka configuration:


akka.remote.artery {
  transport = tls-tcp
  ssl.ssl-engine-provider = "akka.remote.artery.tcp.ssl.RotatingKeysSSLEngineProvider"
}



This instructs Akka to use TLS, with the 
RotatingKeysSSLEngineProvider
, an SSL engine provider that is designed to pick up Kubernetes TLS secrets, and poll the file system for when they get rotated. It also applies authorization by matching the incoming DNS name with the DNS name of its own certificate.


Configuring the Kubernetes deployment


In the Kubernetes 
Deployment
 for the Akka application you need to mount the certificate at the path 
/var/run/secrets/akka-tls/rotating-keys-engine
. This is the default path that the 
RotatingKeysSSLEngineProvider
 uses to pick up its certificates. So, add the following volume to your pod:


      volumes:
      - name: akka-tls
        secret:
          secretName: my-service-akka-tls-certificate



And then you can mount that in your container:


        volumeMounts:
        - name: akka-tls
          mountPath: /var/run/secrets/akka-tls/rotating-keys-engine



The complete deployment YAML when adding those to the Deployment Spec from 
Deploying Akka Cluster to Kubernetes
:


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: appka
  name: appka
  namespace: appka-1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: appka
  template:
    metadata:
      labels:
        app: appka
    spec:
      containers:
      - name: appka
        image: akka-sample-cluster-kubernetes-scala:latest
        readinessProbe:
          httpGet:
            path: /ready
            port: management
        livenessProbe:
          httpGet:
            path: /alive
            port: management
        ports:
        - name: management
          containerPort: 8558
          protocol: TCP
        - name: http
          containerPort: 8080
          protocol: TCP
        resources:
          limits:
            memory: 1024Mi
          requests:
            cpu: 2
            memory: 1024Mi
        volumeMounts:
        - name: akka-tls
          mountPath: /var/run/secrets/akka-tls/rotating-keys-engine
      volumes:
      - name: akka-tls
        secret:
          secretName: my-service-akka-tls-certificate



Untrusted Mode


As soon as an actor system can connect to another remotely, it may in principle send any possible message to any actor contained within that remote system. One example may be sending a 
PoisonPill
PoisonPill
 to the system guardian, shutting that system down. This is not always desired, and it can be disabled with the following setting:


akka.remote.artery.untrusted-mode = on



This disallows sending of system messages (actor life-cycle commands, DeathWatch, etc.) and any message extending 
PossiblyHarmful
PossiblyHarmful
 to the system on which this flag is set. Should a client send them nonetheless they are dropped and logged (at DEBUG level in order to reduce the possibilities for a denial of service attack). 
PossiblyHarmful
 covers the predefined messages like 
PoisonPill
PoisonPill
 and 
Kill
Kill
, but it can also be added as a marker trait to user-defined messages.
Warning


Untrusted mode does not give full protection against attacks by itself. It makes it slightly harder to perform malicious or unintended actions but it should be noted that 
Java serialization
 should still not be enabled. Additional protection can be achieved when running in an untrusted network by network security (e.g. firewalls) and/or enabling 
TLS with mutual authentication
.


Messages sent with actor selection are by default discarded in untrusted mode, but permission to receive actor selection messages can be granted to specific actors defined in configuration:


akka.remote.artery.trusted-selection-paths = ["/user/receptionist", "/user/namingService"]



The actual message must still not be of type 
PossiblyHarmful
.


In summary, the following operations are ignored by a system configured in untrusted mode when incoming via the remoting layer:




remote deployment (which also means no remote supervision)


remote DeathWatch


system.stop()
system.stop()
, 
PoisonPill
PoisonPill
, 
Kill
Kill


sending any message which extends from the 
PossiblyHarmful
PossiblyHarmful
 marker interface, which includes 
Terminated
Terminated


messages sent with actor selection, unless destination defined in 
trusted-selection-paths
.


Note


Enabling the untrusted mode does not remove the capability of the client to freely choose the target of its message sends, which means that messages not prohibited by the above rules can be sent to any actor in the remote system. It is good practice for a client-facing system to only contain a well-defined set of entry point actors, which then forward requests (possibly after performing validation) to another actor system containing the actual worker actors. If messaging between these two server-side systems is done using local 
ActorRef
ActorRef
 (they can be exchanged safely between actor systems within the same JVM), you can restrict the messages on this interface by marking them 
PossiblyHarmful
PossiblyHarmful
 so that a client cannot forge them.














 
Remoting






Split Brain Resolver 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/index-cluster.html
Cluster • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster




Cluster Usage


Cluster Specification


Cluster Membership Service


Phi Accrual Failure Detector


Distributed Data


Cluster Singleton


Cluster Sharding


Cluster Sharding concepts


Sharded Daemon Process


Distributed Publish Subscribe in Cluster


Reliable delivery


Serialization


Serialization with Jackson


Multi JVM Testing


Multi Node Testing


Remoting


Remote Security


Split Brain Resolver


Coordination


Choosing Akka Cluster




Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Cluster






Cluster Usage




Module info


Cluster API Extension


Cluster Membership API


Node Roles


Failure Detector


How to test


Configuration


Higher level Cluster tools


Example project




Cluster Specification




Introduction


Terms




Cluster Membership Service




Introduction


Member States


Member Events


Membership Lifecycle


Leader


WeaklyUp Members


Full cluster shutdown


State Diagrams




Phi Accrual Failure Detector




Introduction


Failure Detector Heartbeats


Logging


Failure Detector Threshold




Distributed Data




Module info


Introduction


Using the Replicator


Replicated data types


Durable Storage


Limitations


Learn More about CRDTs


Configuration


Example project




Cluster Singleton




Module info


Introduction


Potential problems to be aware of


Example


Supervision


Application specific stop message


Lease


Accessing singleton of another data centre


Configuration




Cluster Sharding




Module info


Introduction


Basic example


Persistence example


Shard allocation


How it works


Passivation


Automatic Passivation


Sharding State


Remembering Entities


Startup after minimum number of members


Health check


Inspecting cluster sharding state


Lease


Removal of internal Cluster Sharding data


Configuration


Example project




Cluster Sharding concepts




Scenarios


Shard location


Shard rebalancing


ShardCoordinator state


Message ordering


Reliable delivery


Overhead




Sharded Daemon Process




Module info


Introduction


Basic example


Addressing the actors


Dynamic scaling of number of workers


Scalability


Configuration




Distributed Publish Subscribe in Cluster




Module info


The Topic Registry


The Topic Actor


Pub Sub Scalability


Delivery Guarantee




Reliable delivery




Module info


Introduction


Point-to-point


Work pulling


Sharding


Durable producer


Ask from the producer


Only flow control


Chunk large messages


Configuration




Serialization




Dependency


Introduction


Usage


Customization


Serialization of Akka’s messages


Java serialization


Rolling updates




Serialization with Jackson




Dependency


Introduction


Usage


Security


Annotations


Schema Evolution


Rolling updates


Jackson Modules


Using Akka Serialization for embedded types


Additional configuration


Additional features




Multi JVM Testing




Setup


Running tests


Creating application tests


Changing Defaults


Configuration of the JVM instances


ScalaTest


Multi Node Additions


Example project




Multi Node Testing




Module info


Multi Node Testing Concepts


The Test Conductor


The Multi Node Spec


The SbtMultiJvm Plugin


A Multi Node Testing Example


Things to Keep in Mind


Configuration




Remoting




Dependency


Configuration


Introduction


Selecting a transport


Migrating from classic remoting


Canonical address


Acquiring references to remote actors


Remote Security


Quarantine


Serialization


Routers with Remote Destinations


What is new in Artery


Performance tuning


Remote Configuration


Creating Actors Remotely




Remote Security




Configuring SSL/TLS for Akka Remoting


mTLS with rotated certificates in Kubernetes


Untrusted Mode




Split Brain Resolver




Module info


Enable the Split Brain Resolver


The Problem


Strategies


Indirectly connected nodes


Down all when unstable


Cluster Singleton and Cluster Sharding




Coordination




Module info


Lease


Using a lease


Usages in other Akka modules


Lease implementations


Implementing a lease




Choosing Akka Cluster




Microservices


Traditional distributed application


Distributed monolith




















 
Learning Akka Typed from Classic






Cluster Usage 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/additional/packaging.html
Packaging • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging




sbt: Native Packager


Maven: jarjar, onejar or assembly


Gradle: the Jar task from the Java plugin




Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run




Packaging




sbt: Native Packager


Maven: jarjar, onejar or assembly


Gradle: the Jar task from the Java plugin




Operating a Cluster


Deploying


Rolling Updates


Building Native Images




Project Information


Akka Classic




















Packaging


The simplest way to use Akka is as a regular library, adding the Akka jars you need to your classpath (in case of a web app, in 
WEB-INF/lib
).


In many cases, such as deploying to an analytics cluster, building your application into a single ‘fat jar’ is needed. When building fat jars, some additional configuration is needed to merge Akka config files, because each Akka jar contains a 
reference.conf
 resource with default values.


The method for ensuring 
reference.conf
 and other 
*.conf
 resources are merged depends on the tooling you use to create the fat jar:




sbt: as an application packaged with 
sbt-native-packager


Maven: as an application packaged with a bundler such as jarjar, onejar or assembly


Gradle: using the Jar task from the Java plugin




sbt: Native Packager


sbt-native-packager
 is a tool for creating distributions of any type of application, including Akka applications.


Define sbt version in 
project/build.properties
 file:


sbt.version=1.3.12



Add 
sbt-native-packager
 in 
project/plugins.sbt
 file:


addSbtPlugin("com.typesafe.sbt" % "sbt-native-packager" % "1.1.5")



Follow the instructions for the 
JavaAppPackaging
 in the 
sbt-native-packager plugin documentation
.


Maven: jarjar, onejar or assembly


You can use the 
Apache Maven Shade Plugin
 support for 
Resource Transformers
 to merge all the reference.confs on the build classpath into one.


The plugin configuration might look like this:


<plugin>
 <groupId>org.apache.maven.plugins</groupId>
 <artifactId>maven-shade-plugin</artifactId>
 <version>1.5</version>
 <executions>
  <execution>
   <id>shade-my-jar</id>
   <phase>package</phase>
   <goals>
    <goal>shade</goal>
   </goals>
   <configuration>
    <shadedArtifactAttached>true</shadedArtifactAttached>
    <shadedClassifierName>allinone</shadedClassifierName>
    <artifactSet>
     <includes>
      <include>*:*</include>
     </includes>
    </artifactSet>
    <transformers>
      <transformer
       implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
       <resource>reference.conf</resource>
      </transformer>
      <transformer
       implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
       <resource>version.conf</resource>
      </transformer>
      <transformer
       implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
       <manifestEntries>
        <Main-Class>myapp.Main</Main-Class>
       </manifestEntries>
      </transformer>
    </transformers>
   </configuration>
  </execution>
 </executions>
</plugin>



Gradle: the Jar task from the Java plugin


When using Gradle, you would typically use the 
Jar task from the Java plugin
 to create the fat jar.


To make sure the 
reference.conf
 resources are correctly merged, you might use the 
Shadow plugin
, which might look something like this:


import com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer

plugins {
    id 'java'
    id "com.github.johnrengelman.shadow" version "7.0.0"
}

shadowJar {
    append 'reference.conf'
    append 'version.conf'
    with jar
}



Or when you use the Kotlin DSL:


tasks.withType<ShadowJar> {
    val newTransformer = AppendingTransformer()
    newTransformer.resource = "reference.conf"
    transformers.add(newTransformer)
}















 
Package, Deploy and Run






Operating a Cluster 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/typed/guide/actors-intro.html
How the Actor Model Meets the Needs of Modern, Distributed Systems • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems




Usage of message passing avoids locking and blocking


Actors handle error situations gracefully




Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide




Introduction to Akka


Why modern systems need a new programming model


How the Actor Model Meets the Needs of Modern, Distributed Systems




Usage of message passing avoids locking and blocking


Actors handle error situations gracefully




Overview of Akka libraries and modules


Introduction to the Example


Part 1: Actor Architecture


Part 2: Creating the First Actor


Part 3: Working with Device Actors


Part 4: Working with Device Groups


Part 5: Querying Device Groups




General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















How the Actor Model Meets the Needs of Modern, Distributed Systems


As described in the previous topic, common programming practices do not properly address the needs of demanding modern systems. Thankfully, we don’t need to scrap everything we know. Instead, the actor model addresses these shortcomings in a principled way, allowing systems to behave in a way that better matches our mental model. The actor model abstraction allows you to think about your code in terms of communication, not unlike the exchanges that occur between people in a large organization.


Use of actors allows us to:




Enforce encapsulation without resorting to locks.


Use the model of cooperative entities reacting to signals, changing state, and sending signals to each other  to drive the whole application forward.


Stop worrying about an executing mechanism which is a mismatch to our world view.




Usage of message passing avoids locking and blocking


Instead of calling methods, actors send messages to each other. Sending a message does not transfer the thread of execution from the sender to the destination. An actor can send a message and continue without blocking. Therefore, it can accomplish more in the same amount of time.


With objects, when a method returns, it releases control of its executing thread. In this respect, actors behave much like objects, they react to messages and return execution when they finish processing the current message. In this way, actors actually achieve the execution we imagined for objects:




An important difference between passing messages and calling methods is that messages have no return value. By sending a message, an actor delegates work to another actor. As we saw in 
The illusion of a call stack
, if it expected a return value, the sending actor would either need to block or to execute the other actor’s work on the same thread. Instead, the receiving actor delivers the results in a reply message.


The second key change we need in our model is to reinstate encapsulation. Actors react to messages just like objects “react” to methods invoked on them. The difference is that instead of multiple threads “protruding” into our actor and wreaking havoc to internal state and invariants, actors execute independently from the senders of a message, and they react to incoming messages sequentially, one at a time. While each actor processes messages sent to it sequentially, different actors work concurrently with each other so that an actor system can process as many messages simultaneously as the hardware will support.


Since there is always at most one message being processed per actor, the invariants of an actor can be kept without synchronization. This happens automatically without using locks:




In summary, this is what happens when an actor receives a message:




The actor adds the message to the end of a queue.


If the actor was not scheduled for execution, it is marked as ready to execute.


A (hidden) scheduler entity takes the actor and starts executing it.


Actor picks the message from the front of the queue.


Actor modifies internal state, sends messages to other actors.


The actor is unscheduled.




To accomplish this behavior, actors have:




A mailbox (the queue where messages end up).


A behavior (the state of the actor, internal variables etc.).


Messages (pieces of data representing a signal, similar to method calls and their parameters).


An execution environment (the machinery that takes actors that have messages to react to and invokes  their message handling code).


An address (more on this later).




Messages go into actor mailboxes. The behavior of the actor describes how the actor responds to messages (like sending more messages and/or changing state). An execution environment orchestrates a pool of threads to drive all these actions completely transparently.


This is a very simple model and it solves the issues enumerated previously:




Encapsulation is preserved by decoupling execution from signaling (method calls transfer execution,  message passing does not).


There is no need for locks. Modifying the internal state of an actor is only possible via messages, which are  processed one at a time eliminating races when trying to keep invariants.


There are no locks used anywhere, and senders are not blocked. Millions of actors can be efficiently scheduled on a  dozen of threads reaching the full potential of modern CPUs. Task delegation is the natural mode of operation for actors.


State of actors is local and not shared, changes and data is propagated via messages, which maps to how modern  memory hierarchy actually works. In many cases, this means transferring over only the cache lines that contain the data in the message while keeping local state and data cached at the original core. The same model maps exactly to remote communication where the state is kept in the RAM of machines and changes/data is propagated over the network as packets.




Actors handle error situations gracefully


Since we no longer have a shared call stack between actors that send messages to each other, we need to handle error situations differently. There are two kinds of errors we need to consider:




The first case is when the delegated task on the target actor failed due to an error in the task (typically some  validation issue, like a non-existent user ID). In this case, the service encapsulated by the target actor is intact,  it is only the task itself that is erroneous.  The service actor should reply to the sender with a message, presenting the error case. There is nothing special here, errors are part of the domain and hence become ordinary messages.


The second case is when a service itself encounters an internal fault. Akka enforces that all actors are organized  into a tree-like hierarchy, i.e. an actor that creates another actor becomes the parent of that new actor. This is very similar to how operating systems organize processes into a tree. Just like with processes, when an actor fails,  its parent actor can decide how to react to the failure. Also, if the parent actor is stopped,  all of its children are recursively stopped, too. This service is called supervision and it is central to Akka.




A supervisor strategy is typically defined by the parent actor when it is starting a child actor. It can decide to restart the child actor on certain types of failures or stop it completely on others. Children never go silently dead (with the notable exception of entering an infinite loop) instead they are either failing and the supervisor strategy can react to the fault, or they are stopped (in which case interested parties are notified). There is always a responsible entity for managing an actor: its parent. Restarts are not visible from the outside: collaborating actors can keep sending messages while the target actor restarts.
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.


Now, let’s take a short tour of the functionality Akka provides.














 
Why modern systems need a new programming model






Overview of Akka libraries and modules 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/index-utilities-classic.html
Classic Utilities • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities




Dependency


Classic Event Bus


Classic Logging


Classic Scheduler


Classic Akka Extensions






















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




Classic Actors


Classic Clustering


Classic Networking


Classic Utilities




Dependency


Classic Event Bus


Classic Logging


Classic Scheduler


Classic Akka Extensions
























Classic Utilities


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Utilities, you must add the following dependency in your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor" % AkkaVersion,
  "com.typesafe.akka" %% "akka-testkit" % AkkaVersion % Test
)
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-actor_${scala.binary.version}</artifactId>
  </dependency>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-testkit_${scala.binary.version}</artifactId>
    <scope>test</scope>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-actor_${versions.ScalaBinary}"
  testImplementation "com.typesafe.akka:akka-testkit_${versions.ScalaBinary}"
}






Classic Event Bus




Dependency


Introduction


Classifiers


Event Stream




Classic Logging




Module info


Introduction


How to Log


Loggers


Logging to stdout during startup and shutdown


SLF4J




Classic Scheduler




Dependency


Introduction


Some examples


Schedule periodically


The Scheduler interface


The Cancellable interface




Classic Akka Extensions




Building an Extension


Loading from Configuration


Applicability


Library extensions




















 
DNS Extension






Classic Event Bus 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/stream/stream-rate.html
Buffers and working with rate • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate




Dependency


Introduction


Buffers for asynchronous operators


Buffers in Akka Streams


Rate transformation




Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts


Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams




Introduction


Streams Quickstart Guide


Design Principles behind Akka Streams


Basics and working with Flows


Working with Graphs


Modularity, Composition and Hierarchy


Buffers and working with rate




Dependency


Introduction


Buffers for asynchronous operators


Buffers in Akka Streams


Rate transformation




Context Propagation


Dynamic stream handling


Custom stream processing


Futures interop


Actors interop


Reactive Streams Interop


Error Handling in Streams


Working with streaming IO


StreamRefs - Reactive Streams over the network


Pipelining and Parallelism


Testing streams


Substreams


Streams Cookbook


Configuration


Operators




Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Buffers and working with rate


Dependency


The Akka dependencies are available from Akka’s library repository. To access them there, you need to configure the URL for this repository.
sbt
resolvers += "Akka library repository".at("https://repo.akka.io/maven")

Maven
<project>
  ...
  <repositories>
    <repository>
      <id>akka-repository</id>
      <name>Akka library repository</name>
      <url>https://repo.akka.io/maven</url>
    </repository>
  </repositories>
</project>

Gradle
repositories {
    mavenCentral()
    maven {
        url "https://repo.akka.io/maven"
    }
}



To use Akka Streams, add the module to your project:
sbt
val AkkaVersion = "2.10.1+5-4c570f91-SNAPSHOT"
libraryDependencies += "com.typesafe.akka" %% "akka-stream" % AkkaVersion
Maven
<properties>
  <scala.binary.version>2.13</scala.binary.version>
</properties>
<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-bom_${scala.binary.version}</artifactId>
      <version>2.10.1+5-4c570f91-SNAPSHOT</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>
<dependencies>
  <dependency>
    <groupId>com.typesafe.akka</groupId>
    <artifactId>akka-stream_${scala.binary.version}</artifactId>
  </dependency>
</dependencies>
Gradle
def versions = [
  ScalaBinary: "2.13"
]
dependencies {
  implementation platform("com.typesafe.akka:akka-bom_${versions.ScalaBinary}:2.10.1+5-4c570f91-SNAPSHOT")

  implementation "com.typesafe.akka:akka-stream_${versions.ScalaBinary}"
}


Introduction


When upstream and downstream rates differ, especially when the throughput has spikes, it can be useful to introduce buffers in a stream. In this chapter we cover how buffers are used in Akka Streams.




Buffers for asynchronous operators


In this section we will discuss internal buffers that are introduced as an optimization when using asynchronous operators.


To run an operator asynchronously it has to be marked explicitly as such using the 
`.async`
`.async()`
 method. Being run asynchronously means that an operator, after handing out an element to its downstream consumer is able to immediately process the next message. To demonstrate what we mean by this, let’s take a look at the following example:




Scala




copy
source
Source(1 to 3)
  .map { i =>
    println(s"A: $i"); i
  }
  .async
  .map { i =>
    println(s"B: $i"); i
  }
  .async
  .map { i =>
    println(s"C: $i"); i
  }
  .async
  .runWith(Sink.ignore)


Java




copy
source
Source.from(Arrays.asList(1, 2, 3))
    .map(
        i -> {
          System.out.println("A: " + i);
          return i;
        })
    .async()
    .map(
        i -> {
          System.out.println("B: " + i);
          return i;
        })
    .async()
    .map(
        i -> {
          System.out.println("C: " + i);
          return i;
        })
    .async()
    .runWith(Sink.ignore(), system);




Running the above example, one of the possible outputs looks like this:


A: 1
A: 2
B: 1
A: 3
B: 2
C: 1
B: 3
C: 2
C: 3



Note that the order is 
not
 
A:1, B:1, C:1, A:2, B:2, C:2,
 which would correspond to the normal fused synchronous execution model of flows where an element completely passes through the processing pipeline before the next element enters the flow. The next element is processed by an asynchronous operator as soon as it has emitted the previous one.


While pipelining in general increases throughput, in practice there is a cost of passing an element through the asynchronous (and therefore thread crossing) boundary which is significant. To amortize this cost Akka Streams uses a 
windowed
, 
batching
 backpressure strategy internally. It is windowed because as opposed to a 
Stop-And-Wait
 protocol multiple elements might be “in-flight” concurrently with requests for elements. It is also batching because a new element is not immediately requested once an element has been drained from the window-buffer but multiple elements are requested after multiple elements have been drained. This batching strategy reduces the communication cost of propagating the backpressure signal through the asynchronous boundary.


While this internal protocol is mostly invisible to the user (apart from its throughput increasing effects) there are situations when these details get exposed. In all of our previous examples we always assumed that the rate of the processing chain is strictly coordinated through the backpressure signal causing all operators to process no faster than the throughput of the connected chain. There are tools in Akka Streams however that enable the rates of different segments of a processing chain to be “detached” or to define the maximum throughput of the stream through external timing sources. These situations are exactly those where the internal batching buffering strategy suddenly becomes non-transparent.


Internal buffers and their effect


As we have explained, for performance reasons Akka Streams introduces a buffer for every asynchronous operator. The purpose of these buffers is solely optimization, in fact the size of 1 would be the most natural choice if there would be no need for throughput improvements. Therefore it is recommended to keep these buffer sizes small, and increase them only to a level suitable for the throughput requirements of the application. Default buffer sizes can be set through configuration:


akka.stream.materializer.max-input-buffer-size = 16



Alternatively they can be set per stream by adding an attribute to the complete 
RunnableGraph
 or on smaller segments of the stream it is possible by defining a separate 
`Flow`
`Flow`
 with these attributes:




Scala




copy
source
val section = Flow[Int].map(_ * 2).async.addAttributes(Attributes.inputBuffer(initial = 1, max = 1)) // the buffer size of this map is 1
val flow = section.via(Flow[Int].map(_ / 2)).async // the buffer size of this map is the default
val runnableGraph =
  Source(1 to 10).via(flow).to(Sink.foreach(elem => println(elem)))

val withOverriddenDefaults = runnableGraph.withAttributes(Attributes.inputBuffer(initial = 64, max = 64))


Java




copy
source
final Flow<Integer, Integer, NotUsed> flow1 =
    Flow.of(Integer.class)
        .map(elem -> elem * 2)
        .async()
        .addAttributes(Attributes.inputBuffer(1, 1)); // the buffer size of this map is 1
final Flow<Integer, Integer, NotUsed> flow2 =
    flow1
        .via(Flow.of(Integer.class).map(elem -> elem / 2))
        .async(); // the buffer size of this map is the value from the surrounding graph it is
// used in
final RunnableGraph<NotUsed> runnableGraph =
    Source.range(1, 10).via(flow1).to(Sink.foreach(elem -> System.out.println(elem)));

final RunnableGraph<NotUsed> withOverridenDefaults =
    runnableGraph.withAttributes(Attributes.inputBuffer(64, 64));




Here is an example of a code that demonstrate some of the issues caused by internal buffers:




Scala




copy
source
import scala.concurrent.duration._
case class Tick()

RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>
  import GraphDSL.Implicits._

  // this is the asynchronous stage in this graph
  val zipper = b.add(ZipWith[Tick, Int, Int]((tick, count) => count).async)

  Source.tick(initialDelay = 3.second, interval = 3.second, Tick()) ~> zipper.in0

  Source
    .tick(initialDelay = 1.second, interval = 1.second, "message!")
    .conflateWithSeed(seed = (_) => 1)((count, _) => count + 1) ~> zipper.in1

  zipper.out ~> Sink.foreach(println)
  ClosedShape
})


Java




copy
source
final Duration oneSecond = Duration.ofSeconds(1);
final Source<String, Cancellable> msgSource = Source.tick(oneSecond, oneSecond, "message!");
final Source<String, Cancellable> tickSource =
    Source.tick(oneSecond.multipliedBy(3), oneSecond.multipliedBy(3), "tick");
final Flow<String, Integer, NotUsed> conflate =
    Flow.of(String.class).conflateWithSeed(first -> 1, (count, elem) -> count + 1);

RunnableGraph.fromGraph(
        GraphDSL.create(
            b -> {
              // this is the asynchronous stage in this graph
              final FanInShape2<String, Integer, Integer> zipper =
                  b.add(ZipWith.create((String tick, Integer count) -> count).async());
              b.from(b.add(msgSource)).via(b.add(conflate)).toInlet(zipper.in1());
              b.from(b.add(tickSource)).toInlet(zipper.in0());
              b.from(zipper.out()).to(b.add(Sink.foreach(elem -> System.out.println(elem))));
              return ClosedShape.getInstance();
            }))
    .run(system);




Running the above example one would expect the number 
3
 to be printed in every 3 seconds (the 
conflateWithSeed
 step here is configured so that it counts the number of elements received before the downstream 
`ZipWith`
`ZipWith`
 consumes them). What is being printed is different though, we will see the number 
1
. The reason for this is the internal buffer which is by default 16 elements large, and prefetches elements before the 
`ZipWith`
`ZipWith`
 starts consuming them. It is possible to fix this issue by changing the buffer size of 
`ZipWith`
`ZipWith`
 to 1. We will still see a leading 1 though which is caused by an initial prefetch of the 
`ZipWith`
`ZipWith`
 element.
Note


In general, when time or rate driven operators exhibit strange behavior, one of the first solutions to try should be to decrease the input buffer of the affected elements to 1.


Buffers in Akka Streams


In this section we will discuss 
explicit
 user defined buffers that are part of the domain logic of the stream processing pipeline of an application.


The example below will ensure that 1000 jobs (but not more) are dequeued from an external (imaginary) system and stored locally in memory - relieving the external system:




Scala




copy
source
// Getting a stream of jobs from an imaginary external system as a Source
val jobs: Source[Job, NotUsed] = inboundJobsConnector()
jobs.buffer(1000, OverflowStrategy.backpressure)


Java




copy
source
// Getting a stream of jobs from an imaginary external system as a Source
final Source<Job, NotUsed> jobs = inboundJobsConnector;
jobs.buffer(1000, OverflowStrategy.backpressure());




The next example will also queue up 1000 jobs locally, but if there are more jobs waiting in the imaginary external systems, it makes space for the new element by dropping one element from the 
tail
 of the buffer. Dropping from the tail is a very common strategy but it must be noted that this will drop the 
youngest
 waiting job. If some “fairness” is desired in the sense that we want to be nice to jobs that has been waiting for long, then this option can be useful.




Scala




copy
source
jobs.buffer(1000, OverflowStrategy.dropTail)


Java




copy
source
jobs.buffer(1000, OverflowStrategy.dropTail());




Instead of dropping the youngest element from the tail of the buffer a new element can be dropped without enqueueing it to the buffer at all.




Scala




copy
source
jobs.buffer(1000, OverflowStrategy.dropNew)


Java




copy
source
jobs.buffer(1000, OverflowStrategy.dropNew());




Here is another example with a queue of 1000 jobs, but it makes space for the new element by dropping one element from the 
head
 of the buffer. This is the 
oldest
 waiting job. This is the preferred strategy if jobs are expected to be resent if not processed in a certain period. The oldest element will be retransmitted soon, (in fact a retransmitted duplicate might be already in the queue!) so it makes sense to drop it first.




Scala




copy
source
jobs.buffer(1000, OverflowStrategy.dropHead)


Java




copy
source
jobs.buffer(1000, OverflowStrategy.dropHead());




Compared to the dropping strategies above, dropBuffer drops all the 1000 jobs it has enqueued once the buffer gets full. This aggressive strategy is useful when dropping jobs is preferred to delaying jobs.




Scala




copy
source
jobs.buffer(1000, OverflowStrategy.dropBuffer)


Java




copy
source
jobs.buffer(1000, OverflowStrategy.dropBuffer());




If our imaginary external job provider is a client using our API, we might want to enforce that the client cannot have more than 1000 queued jobs otherwise we consider it flooding and terminate the connection. This is achievable by the error strategy which fails the stream once the buffer gets full.




Scala




copy
source
jobs.buffer(1000, OverflowStrategy.fail)


Java




copy
source
jobs.buffer(1000, OverflowStrategy.fail());




Rate transformation


Understanding conflate


When a fast producer can not be informed to slow down by backpressure or some other signal, 
conflate
 might be useful to combine elements from a producer until a demand signal comes from a consumer.


Below is an example snippet that summarizes fast stream of elements to a standard deviation, mean and count of elements that have arrived while the stats have been calculated.




Scala




copy
source
val statsFlow = Flow[Double].conflateWithSeed(immutable.Seq(_))(_ :+ _).map { s =>
  val Î¼ = s.sum / s.size
  val se = s.map(x => pow(x - Î¼, 2))
  val Ï = sqrt(se.sum / se.size)
  (Ï, Î¼, s.size)
}


Java




copy
source
final Flow<Double, Tuple3<Double, Double, Integer>, NotUsed> statsFlow =
    Flow.of(Double.class)
        .conflateWithSeed(
            elem -> Collections.singletonList(elem),
            (acc, elem) -> {
              return Stream.concat(acc.stream(), Collections.singletonList(elem).stream())
                  .collect(Collectors.toList());
            })
        .map(
            s -> {
              final Double mean = s.stream().mapToDouble(d -> d).sum() / s.size();
              final DoubleStream se = s.stream().mapToDouble(x -> Math.pow(x - mean, 2));
              final Double stdDev = Math.sqrt(se.sum() / s.size());
              return new Tuple3<>(stdDev, mean, s.size());
            });




This example demonstrates that such flow’s rate is decoupled. The element rate at the start of the flow can be much higher than the element rate at the end of the flow.


Another possible use of 
conflate
 is to not consider all elements for summary when the producer starts getting too fast. The example below demonstrates how 
conflate
 can be used to randomly drop elements when the consumer is not able to keep up with the producer.




Scala




copy
source
val p = 0.01
val sampleFlow = Flow[Double]
  .conflateWithSeed(immutable.Seq(_)) {
    case (acc, elem) if Random.nextDouble() < p => acc :+ elem
    case (acc, _)                               => acc
  }
  .mapConcat(identity)


Java




copy
source
final Double p = 0.01;
final Flow<Double, Double, NotUsed> sampleFlow =
    Flow.of(Double.class)
        .conflateWithSeed(
            elem -> Collections.singletonList(elem),
            (acc, elem) -> {
              if (r.nextDouble() < p) {
                return Stream.concat(acc.stream(), Collections.singletonList(elem).stream())
                    .collect(Collectors.toList());
              }
              return acc;
            })
        .mapConcat(d -> d);




See also 
conflate
 and 
conflateWithSeed`
 for more information and examples.


Understanding extrapolate and expand


Now we will discuss two operators, 
extrapolate
 and 
expand
, helping to deal with slow producers that are unable to keep up with the demand coming from consumers. They allow for additional values to be sent as elements to a consumer.


As a simple use case of 
extrapolate
, here is a flow that repeats the last emitted element to a consumer, whenever the consumer signals demand and the producer cannot supply new elements yet.




Scala




copy
source
val lastFlow = Flow[Double].extrapolate(Iterator.continually(_))


Java




copy
source
final Flow<Double, Double, NotUsed> lastFlow =
    Flow.of(Double.class).extrapolate(in -> Stream.iterate(in, i -> i).iterator());




For situations where there may be downstream demand before any element is emitted from upstream, you can use the 
initial
 parameter of 
extrapolate
 to “seed” the stream.




Scala




copy
source
val initial = 2.0
val seedFlow = Flow[Double].extrapolate(Iterator.continually(_), Some(initial))


Java




copy
source
Double initial = 2.0;
final Flow<Double, Double, NotUsed> lastFlow =
    Flow.of(Double.class).extrapolate(in -> Stream.iterate(in, i -> i).iterator(), initial);




extrapolate
 and 
expand
 also allow to produce meta-information based on demand signalled from the downstream. Leveraging this, here is a flow that tracks and reports a drift between a fast consumer and a slow producer. 




Scala




copy
source
val driftFlow = Flow[Double].map(_ -> 0).extrapolate[(Double, Int)] { case (i, _) => Iterator.from(1).map(i -> _) }


Java




copy
source
final Flow<Double, Pair<Double, Integer>, NotUsed> driftFlow =
    Flow.of(Double.class)
        .map(d -> new Pair<>(d, 0))
        .extrapolate(
            d -> Stream.iterate(1, i -> i + 1).map(i -> new Pair<>(d.first(), i)).iterator());




And here’s a more concise representation with 
expand
.




Scala




copy
source
val driftFlow = Flow[Double].expand(i => Iterator.from(0).map(i -> _))


Java




copy
source
final Flow<Double, Pair<Double, Integer>, NotUsed> driftFlow =
    Flow.of(Double.class)
        .expand(d -> Stream.iterate(0, i -> i + 1).map(i -> new Pair<>(d, i)).iterator());




The difference is due to the different handling of the 
Iterator
-generating argument.


While 
extrapolate
 uses an 
Iterator
 only when there is unmet downstream demand, 
expand
 
always
 creates an 
Iterator
 and emits elements downstream from it.


This makes 
expand
 able to transform or even filter out (by providing an empty 
Iterator
) the “original” elements.


Regardless, since we provide a non-empty 
Iterator
 in both examples, this means that the output of this flow is going to report a drift of zero if the producer is fast enough - or a larger drift otherwise.


See also 
extrapolate
 and 
expand
 for more information and examples.














 
Modularity, Composition and Hierarchy






Context Propagation 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/supervision.html
Supervision and Monitoring • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring




What Supervision Means


The Top-Level actors


What Restarting Means


What Lifecycle Monitoring Means


Actors and exceptions




Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring




What Supervision Means


The Top-Level actors


What Restarting Means


What Lifecycle Monitoring Means


Actors and exceptions




Actor References, Paths and Addresses


Location Transparency


Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Supervision and Monitoring


This chapter outlines the concept behind supervision, the primitives offered and their semantics. For details on how that translates into real code, please refer to 
supervision
.


Supervision has changed since classic, for details on classic supervision see 
Classic Supervision




What Supervision Means


There are two categories of exception that can happen in an actor:




Input validation errors, expected exceptions which can be handled with a regular try-catch or other language and standard library tools.


Unexpected 
failures
, for example a network resource being unavailable, a disk write failing or perhaps a bug in the application logic.




Supervision deals with failures and should be separated from the business logic while validating data and handling of expected exceptions is a vital part of the business logic. Therefore supervision is added to an actor as decoration rather than something that is intermingled with the message processing logic of the actor.


Depending on the nature of the work to be supervised and the nature of the failure, supervision provides the following three strategies:




Resume the actor, keeping its accumulated internal state


Restart the actor, clearing out its accumulated internal state, with a potential delay starting again


Stop the actor permanently




Since actors are part of a hierarchy it can often make sense to propagate the permanent failures upwards, if all children of an actor has stopped unexpectedly it may make sense for the actor itself to restart or stop to get back to a functional state. This can be achieved through a combination of supervision and watching the children to get notified when they terminate. An example of this can be found in 
Bubble failures up through the hierarchy
.


The Top-Level actors


An actor system will during its creation start at least two actors. 


/user
: the user guardian actor


This is the top level user provided actor, meant to bootstrap the application by spawning subsystems as children. When the user guardian stops the entire actor system is shut down.


/system
: the system guardian actor


This special guardian has been introduced in order to achieve an orderly shut-down sequence where logging remains active while all normal actors terminate, even though logging itself is implemented using actors. This is realized by having the system guardian watch the user guardian and initiate its own shut-down upon having seen the user guardian stop. 




What Restarting Means


When presented with an actor which failed while processing a certain message, causes for the failure fall into three categories:




Systematic (i.e. programming) error for the specific message received


(Transient) failure of some external resource used during processing the message


Corrupt internal state of the actor




Unless the failure is specifically recognizable, the third cause cannot be ruled out, which leads to the conclusion that the internal state needs to be cleared out. If the supervisor decides that its other children or itself is not affected by the corruptionâe.g. because of conscious application of the error kernel patternâit is therefore best to restart the actor. This is carried out by creating a new instance of the underlying 
Behavior
Behavior
 class and replacing the failed instance with the fresh one inside the child’s 
ActorRef
ActorRef
; the ability to do this is one of the reasons for encapsulating actors within special references. The new actor then resumes processing its mailbox, meaning that the restart is not visible outside of the actor itself with the notable exception that the message during which the failure occurred is not re-processed.


What Lifecycle Monitoring Means
Note


Lifecycle Monitoring in Akka is usually referred to as 
DeathWatch


In contrast to the special relationship between parent and child described above, each actor may monitor any other actor. Since actors emerge from creation fully alive and restarts are not visible outside of the affected supervisors, the only state change available for monitoring is the transition from alive to dead. Monitoring is thus used to tie one actor to another so that it may react to the other actorâs termination, in contrast to supervision which reacts to failure.


Lifecycle monitoring is implemented using a 
Terminated
Terminated
 message to be received by the monitoring actor, where the default behavior is to throw a special 
DeathPactException
DeathPactException
 if not otherwise handled. In order to start listening for 
Terminated
Terminated
 messages, invoke 
ActorContext.watch(targetActorRef)
ActorContext.watch(targetActorRef)
. To stop listening, invoke 
ActorContext.unwatch(targetActorRef)
ActorContext.unwatch(targetActorRef)
. One important property is that the message will be delivered irrespective of the order in which the monitoring request and targetâs termination occur, i.e. you still get the message even if at the time of registration the target is already dead.


Actors and exceptions


It can happen that while a message is being processed by an actor, that some kind of exception is thrown, e.g. a database exception.


What happens to the Message


If an exception is thrown while a message is being processed (i.e. taken out of its mailbox and handed over to the current behavior), then this message will be lost. It is important to understand that it is not put back on the mailbox. So if you want to retry processing of a message, you need to deal with it yourself by catching the exception and retry your flow. Make sure that you put a bound on the number of retries since you don’t want a system to livelock (so consuming a lot of cpu cycles without making progress).


What happens to the mailbox


If an exception is thrown while a message is being processed, nothing happens to the mailbox. If the actor is restarted, the same mailbox will be there. So all messages on that mailbox will be there as well.


What happens to the actor


If code within an actor throws an exception, that actor is suspended and the supervision process is started. Depending on the supervisorâs decision the actor is resumed (as if nothing happened), restarted (wiping out its internal state and starting from scratch) or terminated.
Note


ð For a deeper understanding of the Actor Model, consider the free online course 
Actor Fundamentals
 in Akkademy.














 
What is an Actor?






Actor References, Paths and Addresses 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

URL: https://doc.akka.io/libraries/akka/snapshot/general/remoting.html
Location Transparency • Akka Documentation







































































Ask AI about Akka Libraries







Developers 








Docs


SDK


Libraries






Resources


Ask AI


Blog


Release Notes


Support






Architecture


Principles


Free Training


Certification












Libraries 








Libraries


Actors


HTTP


Streams


Cluster


Event Sourcing






 


gRPC


Projections


Integrations


Observability






Guides


Libraries


Akka Edge


Versions


Current Versions










 


Contact Us


Sign In


Get Started
























Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle
















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency




Distributed by Default


Ways in which Transparency is Broken


Peer-to-Peer vs. Client-Server


Marking Points for Scaling Up with Routers




Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic


















Akka Documentation





Version 2.10.1+5-4c570f91-SNAPSHOT





Scala
Java


sbt
Maven
Gradle


















Security Announcements


Getting Started Guide


General Concepts




Terminology, Concepts


Actor Systems


What is an Actor?


Supervision and Monitoring


Actor References, Paths and Addresses


Location Transparency




Distributed by Default


Ways in which Transparency is Broken


Peer-to-Peer vs. Client-Server


Marking Points for Scaling Up with Routers




Akka and the Java Memory Model


Message Delivery Reliability


Configuration


Default configuration




Actors


Cluster


Persistence (Event Sourcing)


Persistence (Durable State)


Streams


Discovery


Utilities


Other Akka modules


Package, Deploy and Run


Project Information


Akka Classic




















Location Transparency


The previous section describes how actor paths are used to enable location transparency. This special feature deserves some extra explanation, because the related term âtransparent remotingâ was used quite differently in the context of programming languages, platforms and technologies.


Distributed by Default


Everything in Akka is designed to work in a distributed setting: all interactions of actors use purely message passing and everything is asynchronous. This effort has been undertaken to ensure that all functions are available equally when running within a single JVM or on a cluster of hundreds of machines. The key for enabling this is to go from remote to local by way of optimization instead of trying to go from local to remote by way of generalization. See 
this classic paper
 for a detailed discussion on why the second approach is bound to fail.


Ways in which Transparency is Broken


What is true of Akka need not be true of the application which uses it, since designing for distributed execution poses some restrictions on what is possible. The most obvious one is that all messages sent over the wire must be serializable.


Another consequence is that everything needs to be aware of all interactions being fully asynchronous, which in a computer network might mean that it may take several minutes for a message to reach its recipient (depending on configuration). It also means that the probability for a message to be lost is much higher than within one JVM, where it is close to zero (still: no hard guarantee!).




Peer-to-Peer vs. Client-Server


Akka Remoting is a communication module for connecting actor systems in a peer-to-peer fashion, and it is the foundation for Akka Clustering. The design of remoting is driven by two (related) design decisions:




Communication between involved systems is symmetric: if a system A can connect to a system B then system B must also be able to connect to system A independently.


The role of the communicating systems are symmetric in regards to connection patterns: there is no system that only accepts connections, and there is no system that only initiates connections.




The consequence of these decisions is that it is not possible to safely create pure client-server setups with predefined roles (violates assumption 2). For client-server setups it is better to use HTTP or Akka I/O.


Important
: Using setups involving Network Address Translation, Load Balancers or Docker containers violates assumption 1, unless additional steps are taken in the network configuration to allow symmetric communication between involved systems. In such situations Akka can be configured to bind to a different network address than the one used for establishing connections between Akka nodes. See 
Akka behind NAT or in a Docker container
.


Service mesh


In a Kubernetes environment, many people turn to a service mesh such as Istio, Linkerd or Consul to authenticate and encrypt their network communications, however this is not an option for Akka cluster communication. The goal of a service mesh is to ensure that services do not need to be aware of where and how the services they talk to are deployed, a service mesh hide this, a client thinks its only talking to one logical service, while the service mesh handles concerns such as load balancing, encryption, authentication and authorization, and so on. Akka clusters however need to understand how and where the individual nodes are deployed, in order to implement their stateful features such as sharding, replication and peer-to-peer messaging. When deploying a service that uses Akka Cluster to a service mesh, the Akka Cluster communication must bypass the service mesh.


Marking Points for Scaling Up with Routers


In addition to being able to run different parts of an actor system on different nodes of a cluster, it is also possible to scale up onto more cores by multiplying actor sub-trees which support parallelization (think for example a search engine processing different queries in parallel). The clones can then be routed to in different fashions, e.g. round-robin. See 
Routing
 for more details.














 
Actor References, Paths and Addresses






Akka and the Java Memory Model 











Found an error in this documentation? The source code for this page can be found 
here
.
Please feel free to edit and contribute a pull request.



















Akka is available under the 
Business Source License 1.1
.



© 2011-2025 
Lightbend, Inc.
 dba Akka All rights reserved. |

Terms
 |

Privacy Policy
 |

Cookie Settings

